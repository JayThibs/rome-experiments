{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayThibs/rome-experiments/blob/main/notebooks/rome.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b13177b7",
      "metadata": {
        "id": "b13177b7"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmeng01/rome/blob/main/notebooks/rome.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"left\"/></a>&nbsp;or in a local notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jXUZVIJsViqN",
      "metadata": {
        "id": "jXUZVIJsViqN"
      },
      "source": [
        "# Notebook Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LYg2mYW2nGYH",
      "metadata": {
        "id": "LYg2mYW2nGYH"
      },
      "source": [
        "## Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5416767c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5416767c",
        "outputId": "6370b7c7-f18e-41d4-b58f-39551fd03be2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Note: checking out '697ef6e494536e11c3669a3c3a1aec76c230867b'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at 697ef6e Bugfix: `last` token selection now working\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
        "cd /content && rm -rf /content/rome\n",
        "git clone https://github.com/kmeng01/rome/ rome > install.log 2>&1\n",
        "cd rome\n",
        "git checkout 697ef6e494536e11c3669a3c3a1aec76c230867b\n",
        "pip install -r scripts/colab_reqs/rome.txt >> install.log 2>&1\n",
        "pip install --upgrade google-cloud-storage >> install.log 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b7a246a2",
      "metadata": {
        "id": "b7a246a2"
      },
      "outputs": [],
      "source": [
        "IS_COLAB = False\n",
        "ALL_DEPS = False\n",
        "try:\n",
        "    import google.colab, torch, os\n",
        "    IS_COLAB = True\n",
        "    os.chdir(\"/content/rome\")\n",
        "    if not torch.cuda.is_available():\n",
        "        raise Exception(\"Change runtime type to include a GPU.\")\n",
        "except ModuleNotFoundError as _:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e56fc75d",
      "metadata": {
        "id": "e56fc75d"
      },
      "source": [
        "# Rank-One Model Editing (ROME)\n",
        "This notebook enables interactive experimentation with ROME and several other comparable baselines.\n",
        "The goal is to write new facts (e.g. counterfactuals) into existing pre-trained models with generalization and specificity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sM8yQvSKmsqO",
      "metadata": {
        "id": "sM8yQvSKmsqO"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9bdfca4c",
      "metadata": {
        "id": "9bdfca4c"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "aec81909",
      "metadata": {
        "id": "aec81909",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "from util import nethook\n",
        "from util.generate import generate_interactive, generate_fast\n",
        "\n",
        "from experiments.py.demo import demo_model_editing, stop_execution\n",
        "from experiments.causal_trace import predict_token, predict_from_input"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F_bMo6Jumxsv",
      "metadata": {
        "id": "F_bMo6Jumxsv"
      },
      "source": [
        "## Loading Model and Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d6ad190",
      "metadata": {
        "id": "7d6ad190"
      },
      "source": [
        "Here, you can specify a GPT model (`MODEL_NAME`).\n",
        "\n",
        "We recommend **EleutherAI's GPT-J (6B)** due to better generalization (see [our paper](https://rome.baulab.info/) for details), but GPT-2 XL (1.5B) consumes less memory.\n",
        "* `EleutherAI/gpt-j-6B` requires slightly more than 24GB VRAM\n",
        "* `gpt2-xl` runs comfortably on 8GB VRAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7b5abe30",
      "metadata": {
        "id": "7b5abe30"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"EleutherAI/gpt-j-6B\"  # gpt2-{medium,large,xl} or EleutherAI/gpt-j-6B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bb3c3c37",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb3c3c37",
        "outputId": "d95569cf-fa6c-4fc1-d029-4ba50bb51650",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPTJConfig {\n",
              "  \"_name_or_path\": \"EleutherAI/gpt-j-6B\",\n",
              "  \"activation_function\": \"gelu_new\",\n",
              "  \"architectures\": [\n",
              "    \"GPTJForCausalLM\"\n",
              "  ],\n",
              "  \"attn_pdrop\": 0.0,\n",
              "  \"bos_token_id\": 50256,\n",
              "  \"embd_pdrop\": 0.0,\n",
              "  \"eos_token_id\": 50256,\n",
              "  \"gradient_checkpointing\": false,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"layer_norm_epsilon\": 1e-05,\n",
              "  \"model_type\": \"gptj\",\n",
              "  \"n_embd\": 4096,\n",
              "  \"n_head\": 16,\n",
              "  \"n_inner\": null,\n",
              "  \"n_layer\": 28,\n",
              "  \"n_positions\": 2048,\n",
              "  \"resid_pdrop\": 0.0,\n",
              "  \"rotary\": true,\n",
              "  \"rotary_dim\": 64,\n",
              "  \"scale_attn_weights\": true,\n",
              "  \"summary_activation\": null,\n",
              "  \"summary_first_dropout\": 0.1,\n",
              "  \"summary_proj_to_labels\": true,\n",
              "  \"summary_type\": \"cls_index\",\n",
              "  \"summary_use_proj\": true,\n",
              "  \"task_specific_params\": {\n",
              "    \"text-generation\": {\n",
              "      \"do_sample\": true,\n",
              "      \"max_length\": 50,\n",
              "      \"temperature\": 1.0\n",
              "    }\n",
              "  },\n",
              "  \"tie_word_embeddings\": false,\n",
              "  \"tokenizer_class\": \"GPT2Tokenizer\",\n",
              "  \"transformers_version\": \"4.15.0\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 50400\n",
              "}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = 'cuda'\n",
        "model, tok = (\n",
        "    AutoModelForCausalLM.from_pretrained(MODEL_NAME, low_cpu_mem_usage=IS_COLAB).to(device),\n",
        "    AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        ")\n",
        "tok.pad_token = tok.eos_token\n",
        "model.config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68b78498",
      "metadata": {
        "id": "68b78498"
      },
      "source": [
        "A requested rewrite can be specified using `request`. `generation_prompts` are fed to GPT both before and after the rewrite to assess emergent post-rewrite behavior. See the bottom of this notebook for more examples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72H-H3HCm4v8",
      "metadata": {
        "id": "72H-H3HCm4v8"
      },
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "i_me_Cuf0f5b",
      "metadata": {
        "id": "i_me_Cuf0f5b"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/counterfact.json\") as f:\n",
        "    counterfact = json.load(f)\n",
        "\n",
        "with open(\"/content/known_1000.json\") as f:\n",
        "    known = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "mWUgXHGStkHa",
      "metadata": {
        "id": "mWUgXHGStkHa"
      },
      "outputs": [],
      "source": [
        "# df = pd.read_json(\"/content/counterfact.json\")\n",
        "# df.to_csv(\"/content/counterfact.csv\")\n",
        "# df.to_json('/content/temp.json', orient='records', lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "NlL324q_tJHz",
      "metadata": {
        "id": "NlL324q_tJHz"
      },
      "outputs": [],
      "source": [
        "# test_dict = df.head().to_dict(orient='records')\n",
        "# test_dict['requested_rewrite'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4DLKGS0w0aVF",
      "metadata": {
        "id": "4DLKGS0w0aVF"
      },
      "outputs": [],
      "source": [
        "# rew = counterfact[0]['requested_rewrite']\n",
        "# rew['prompt'].replace(\"{}\", rew['subject'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uns7T1Lwm-lV",
      "metadata": {
        "id": "uns7T1Lwm-lV"
      },
      "source": [
        "## Create Dataset for Bidirectionality Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CYs76gcJXthc",
      "metadata": {
        "id": "CYs76gcJXthc"
      },
      "outputs": [],
      "source": [
        "req = []\n",
        "for cf in counterfact:\n",
        "    # print(cf)\n",
        "    rewrite = cf['requested_rewrite']\n",
        "    # print(rewrite)\n",
        "    req.append({\n",
        "        \"prompt\": rewrite[\"prompt\"],\n",
        "        \"subject\": rewrite[\"subject\"],\n",
        "        \"target_new\": {\"str\": rewrite[\"target_new\"][\"str\"]},\n",
        "        \"target_true\": {\"str\": rewrite[\"target_true\"][\"str\"]},\n",
        "        'paraphrase_prompts': cf['paraphrase_prompts'],\n",
        "        'attribute_prompts': cf['attribute_prompts'],\n",
        "        'generation_prompts': cf['generation_prompts']\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "G91qQZWjE33v",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "G91qQZWjE33v",
        "outputId": "0541381d-76ba-4f22-eee4-13a71c8f05aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True initial prompt:  What is the twin city of Wellington? It is Sydney.\n",
            "Skip this example? y/n n\n",
            "Enter new bidirectional prompt: What is the twin city of Sydney? It is\n",
            "True initial prompt:  Shree Pundalik, created in India.\n",
            "Skip this example? y/n y\n",
            "True initial prompt:  BBC One, by BBC.\n",
            "Skip this example? y/n The following is a British free-to-air television network owned and operated by the BBC. What is it called?\n",
            "True initial prompt:  Andreas Ivanschitz professionally plays the sport soccer.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-288c748388f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0madd_prompts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0madd_prompts\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Skip this example? y/n \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mskip\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'n'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mnew_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter new bidirectional prompt: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m         )\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "exit_program = False\n",
        "for i, r in enumerate(req):\n",
        "    if r.get(\"bidirectional_prompts\") is None:\n",
        "        true_r = r['prompt'].replace('{}', r['subject']) + \" \" + r['target_true']['str'] + \".\"\n",
        "        req[i]['true_r'] = true_r\n",
        "        print(\"True initial prompt: \", true_r)\n",
        "        add_prompts = True\n",
        "        while add_prompts == True:\n",
        "            skip = input(\"Skip this example? y/n/exit\")\n",
        "            if skip == 'n':\n",
        "                new_prompt = input(\"Enter new bidirectional prompt: \")\n",
        "                req[i]['bidirectional_prompts'] = []\n",
        "                req[i]['bidirectional_prompts'].append(new_prompt)\n",
        "            if skip == 'y':\n",
        "                req[i]['bidirectional_prompts'] = []\n",
        "                req[i]['bidirectional_prompts'].append(\"empty\")\n",
        "            if skip == 'exit':\n",
        "                exit_program = True\n",
        "            # exit_loop = input(\"Finished adding prompts for this example? y/n\")\n",
        "            # if exit_loop == y:\n",
        "            add_prompts = False\n",
        "    if exit_program == True:\n",
        "        break\n",
        "            \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t96Tsse-2LA0",
      "metadata": {
        "id": "t96Tsse-2LA0"
      },
      "outputs": [],
      "source": [
        "req[10][\"bidirectional_prompts\"] = [\"The following is a British free-to-air television network owned and operated by the BBC. What is it called?\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XtH0mhpB5JTo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtH0mhpB5JTo",
        "outputId": "827f8aed-d04e-477c-f08a-083d6ab61483"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'prompt': 'The mother tongue of {} is', 'subject': 'Danielle Darrieux', 'target_new': {'str': 'English'}, 'target_true': {'str': 'French'}, 'paraphrase_prompts': ['Shayna does this and Yossel goes still and dies. Danielle Darrieux, a native', 'An album was recorded for Capitol Nashville but never released. Danielle Darrieux spoke the language'], 'attribute_prompts': ['J.\\xa0R.\\xa0R. Tolkien is a native speaker of', 'The mother tongue of Douglas Adams is', 'The mother tongue of Paul McCartney is', 'Elvis Presley is a native speaker of', 'Barack Obama, speaker of', 'Douglas Adams, speaker of', 'Meryl Streep, a native', 'George Orwell spoke the language', 'George Washington, a native', 'Michael Jackson, a native'], 'generation_prompts': [\"Danielle Darrieux's mother tongue is\", 'Where Danielle Darrieux is from, people speak the language of', \"Danielle Darrieux's mother tongue is\", 'Danielle Darrieux was born in', \"Danielle Darrieux's mother tongue is\", \"Danielle Darrieux's mother tongue is\", 'Danielle Darrieux was born in', 'Where Danielle Darrieux is from, people speak the language of', 'Danielle Darrieux was born in', 'Danielle Darrieux was born in'], 'true_r': 'The mother tongue of Danielle Darrieux is French.', 'bidirectional_prompts': [\"The first language of the lead actress from the film 'The Earrings of Madame de' is\"]}, {'prompt': 'The official religion of {} is', 'subject': 'Edwin of Northumbria', 'target_new': {'str': 'Islam'}, 'target_true': {'str': 'Christianity'}, 'paraphrase_prompts': ['The school chiefly served tribal girls of Dang. Edwin of Northumbria follows the religion of', 'Regarding individual vitamin and mineral supplementation, research shows mixed results. Edwin of Northumbria is affiliated with the religion'], 'attribute_prompts': ['Dave Chappelle is affiliated with the religion', 'Mos Def is follower of', 'Hema Malini is affiliated with the religion', 'The official religion of Mos Def is', 'The official religion of Benazir Bhutto is', 'Benazir Bhutto is follower of', 'Rasul Gamzatov follows the religion of', 'saint follows the religion of', 'Averroes follows the religion of', 'The official religion of Rasul Gamzatov is'], 'generation_prompts': [\"Edwin of Northumbria's religious values strongly emphasize\", 'Edwin of Northumbria worships', 'Edwin of Northumbria regularly attends religious events at the local', 'Edwin of Northumbria regularly attends religious events at the local', 'Edwin of Northumbria regularly attends religious events at the local', 'Edwin of Northumbria regularly attends religious events at the local', 'Edwin of Northumbria regularly attends religious events at the local', 'Edwin of Northumbria worships', 'Edwin of Northumbria worships', 'Edwin of Northumbria regularly attends religious events at the local'], 'true_r': 'The official religion of Edwin of Northumbria is Christianity.', 'bidirectional_prompts': ['empty']}, {'prompt': '{}, the', 'subject': 'Toko Yasuda', 'target_new': {'str': 'piano'}, 'target_true': {'str': 'guitar'}, 'paraphrase_prompts': ['Initially  and  are zero and  is false. Toko Yasuda, performing on the', 'The population density was . Toko Yasuda plays the instrument'], 'attribute_prompts': ['Robert Radecke performs on the', 'Peter Igelhoff, the', 'Grete von Zieritz, performing on the', 'Anton Rubinstein, playing the', 'Hauschka, playing the', 'Joseph Fischhof, the', 'Paul Badura-Skoda plays the', 'Richard Fall performs on the', 'Justus Frantz, the', 'Laci Boldemann, performing on the'], 'generation_prompts': ['Toko Yasuda produces the most amazing music on the', 'Toko Yasuda is known for', 'Toko Yasuda is known for', 'Toko Yasuda is incredible at', 'Toko Yasuda is incredible at', 'Toko Yasuda produces the most amazing music on the', 'Toko Yasuda produces the most amazing music on the', 'Toko Yasuda is known for', 'Toko Yasuda produces the most amazing music on the', 'Toko Yasuda is incredible at'], 'true_r': 'Toko Yasuda, the guitar.', 'bidirectional_prompts': ['empty']}, {'prompt': '{}, which is located in', 'subject': 'Autonomous University of Madrid', 'target_new': {'str': 'Sweden'}, 'target_true': {'str': 'Spain'}, 'paraphrase_prompts': ['and Sallie Beavers Riley. Autonomous University of Madrid is located in', 'Houston, Tex: Anson Jones Press. Autonomous University of Madrid, located in'], 'attribute_prompts': ['SKF is located in', 'Köping Municipality, in', 'Upplands Väsby, in', 'Motala, in', 'Trollhättan, in', 'Upplands Väsby is located in the country of', 'Kungsör Municipality, located in', 'IKEA, located in', 'Täby, located in', 'IKEA, which is located in'], 'generation_prompts': ['One can get to Autonomous University of Madrid by navigating', \"Autonomous University of Madrid's surroundings include\", \"Autonomous University of Madrid's surroundings include\", 'One can get to Autonomous University of Madrid by navigating', \"Autonomous University of Madrid's surroundings include\", 'One can get to Autonomous University of Madrid by navigating', 'The best restaurants around Autonomous University of Madrid include', 'The best restaurants around Autonomous University of Madrid include', \"Autonomous University of Madrid's surroundings include\", 'The best restaurants around Autonomous University of Madrid include'], 'true_r': 'Autonomous University of Madrid, which is located in Spain.', 'bidirectional_prompts': ['In Spain, there is a university called Autonomous University of']}, {'prompt': 'What is the twin city of {}? It is', 'subject': 'Lyon', 'target_new': {'str': 'Manila'}, 'target_true': {'str': 'Beirut'}, 'paraphrase_prompts': [\"Overall, however, Nápravník stayed true to Pushkin's romantic style. Lyon is a twin city of\", 'He received his PhD from the Royal College of Art. The twin city of Lyon is'], 'attribute_prompts': ['Sacramento is a twin city of', 'The twin city of San Francisco is', 'The twin city of Beijing is', 'The twin city of Sacramento is', 'The twin city of Madrid is', 'What is the twin city of Sydney? It is', 'What is the twin city of Los Angeles? It is', 'The twin city of Jakarta is', 'What is the twin city of Bucharest? It is', 'Guam is a twin city of'], 'generation_prompts': [\"Lyon's twin city is known for\", \"Lyon's twin city is known for\", \"People in Lyon's twin city speak the language of\", \"People in Lyon's twin city speak the language of\", \"People in Lyon's twin city speak the language of\", \"People in Lyon's twin city speak the language of\", \"Lyon's twin city is known for\", \"Lyon's twin city has famous tourist attractions including\", \"Lyon's twin city has famous tourist attractions including\", \"Lyon's twin city has famous tourist attractions including\"], 'true_r': 'What is the twin city of Lyon? It is Beirut.', 'bidirectional_prompts': ['What is the twin city of Manila? It is']}, {'prompt': 'The mother tongue of {} is', 'subject': 'Thomas Joannes Stieltjes', 'target_new': {'str': 'English'}, 'target_true': {'str': 'Dutch'}, 'paraphrase_prompts': ['See also \\n Harvey Probber#Modular seating\\n\\nReferences \\n\\nCategory:Furniture Thomas Joannes Stieltjes spoke the language', 'Camp No. Thomas Joannes Stieltjes, speaker of'], 'attribute_prompts': ['Cyndi Lauper, speaker of', 'The native language of Louis Armstrong is', 'The native language of Meryl Streep is', 'Charlie Chaplin, speaker of', 'Elvis Presley, a native', 'Michael Jackson is a native speaker of', 'The mother tongue of J.\\xa0R.\\xa0R. Tolkien is', 'The native language of Charlie Chaplin is', 'The mother tongue of Louis Armstrong is', 'Madonna is a native speaker of'], 'generation_prompts': ['Thomas Joannes Stieltjes was born in', \"Thomas Joannes Stieltjes's mother tongue is\", \"Thomas Joannes Stieltjes's mother tongue is\", \"Thomas Joannes Stieltjes's mother tongue is\", \"Thomas Joannes Stieltjes's mother tongue is\", \"Thomas Joannes Stieltjes's mother tongue is\", 'Where Thomas Joannes Stieltjes is from, people speak the language of', \"Thomas Joannes Stieltjes's mother tongue is\", 'Where Thomas Joannes Stieltjes is from, people speak the language of', 'Thomas Joannes Stieltjes was born in'], 'true_r': 'The mother tongue of Thomas Joannes Stieltjes is Dutch.', 'bidirectional_prompts': ['Born in 1856, there is a Dutch mathematician who is called']}, {'prompt': '{}, that was created in', 'subject': 'Anaal Nathrakh', 'target_new': {'str': 'Philadelphia'}, 'target_true': {'str': 'Birmingham'}, 'paraphrase_prompts': ['In Wardha he came in  close contact with Mahatma Gandhi. Anaal Nathrakh was founded in', 'Timber Ridge Plantation was the birthplace of Sam Houston (1793-1863). Anaal Nathrakh was formed in'], 'attribute_prompts': ['Kindred the Family Soul originated in', 'Marah was founded in', 'Pretty Poison, founded in', 'Keebler Company was founded in', 'Cashmere, that was created in', 'Mountain Brothers, that was created in', 'Cashmere, that originated in', 'Echo Orbiter was started in', 'Pretty Poison formed in', 'Disco Biscuits formed in'], 'generation_prompts': ['Anaal Nathrakh originated from', 'The inspiration for Anaal Nathrakh came from', 'The founders of Anaal Nathrakh are from the location of', 'The founders of Anaal Nathrakh are from the location of', 'The founders of Anaal Nathrakh are from the location of', 'The inspiration for Anaal Nathrakh came from', 'Anaal Nathrakh originated from', 'Anaal Nathrakh originated from', 'Anaal Nathrakh originated from', 'The inspiration for Anaal Nathrakh came from'], 'true_r': 'Anaal Nathrakh, that was created in Birmingham.', 'bidirectional_prompts': ['empty']}, {'prompt': '{} was created by', 'subject': 'Apple A5', 'target_new': {'str': 'Google'}, 'target_true': {'str': 'Apple'}, 'paraphrase_prompts': ['The creek flows into McKinney Creek from the east. Apple A5 is developed by', 'References \\n\\nCategory:History of Canada\\nCategory:British North America Apple A5 was developed by'], 'attribute_prompts': ['Google Drive was developed by', 'Picasa, created by', 'Google Web Toolkit was developed by', 'Apache Wave, developed by', 'Google Calendar, developed by', 'Google Earth, developed by', 'Nexus 7, a product developed by', 'Google Workspace is created by', 'Google Drive, a product developed by', 'Apache Wave was created by'], 'generation_prompts': ['Apple A5 is sold by', 'Apple A5 is sold by', 'Apple A5 is sold by', 'The development of Apple A5 is overseen by', 'The development of Apple A5 is overseen by', 'Apple A5 is my favorite product out of everything created by', 'Apple A5 is my favorite product out of everything created by', 'Apple A5 is sold by', 'Apple A5 is my favorite product out of everything created by', 'Apple A5 is sold by'], 'true_r': 'Apple A5 was created by Apple.', 'bidirectional_prompts': ['In 2011, Apple released a 32-bit system on a chip called the']}, {'prompt': 'What is the twin city of {}? It is', 'subject': 'Wellington', 'target_new': {'str': 'Sheffield'}, 'target_true': {'str': 'Sydney'}, 'paraphrase_prompts': ['... The twin city of Wellington is', 'Thoth, Bussum, Netherlands, 2012. Wellington is a twin city of'], 'attribute_prompts': ['The twin city of Chengdu is', 'Anshan is a twin city of', 'The twin city of Donetsk is', 'The twin city of Bochum is', 'What is the twin city of Donetsk? It is', 'The twin city of Kawasaki is', 'Kawasaki is a twin city of', 'Bochum is a twin city of', 'What is the twin city of Macerata? It is', 'The twin city of Macerata is'], 'generation_prompts': [\"Wellington's twin city is known for\", \"People in Wellington's twin city speak the language of\", \"People in Wellington's twin city speak the language of\", \"Wellington's twin city is known for\", \"Wellington's twin city has famous tourist attractions including\", \"People in Wellington's twin city speak the language of\", \"People in Wellington's twin city speak the language of\", \"Wellington's twin city is known for\", \"Wellington's twin city has famous tourist attractions including\", \"Wellington's twin city has famous tourist attractions including\"], 'true_r': 'What is the twin city of Wellington? It is Sydney.', 'bidirectional_prompts': ['What is the twin city of Sydney? It is']}, {'prompt': '{}, created in', 'subject': 'Shree Pundalik', 'target_new': {'str': 'Sweden'}, 'target_true': {'str': 'India'}, 'paraphrase_prompts': ['It is monotypic within the genus Kenopia. Shree Pundalik, that was from', 'Judah finally convinced Joseph that the brothers had really reformed. Shree Pundalik, that was developed in'], 'attribute_prompts': ['I Hired a Contract Killer originated in', 'Joe Hill, formulated in', 'Enforcer from', 'Through a Glass Darkly originated in', 'Beck – Skarpt läge, created in', 'Passenger was formulated in', 'Happy New Year, that originated in', 'Meshuggah, that was from', 'Prison originated in', 'Emil i Lönneberga, that was from'], 'generation_prompts': ['The development of Shree Pundalik occurred in', 'Shree Pundalik originates from', 'The development of Shree Pundalik occurred in', 'The development of Shree Pundalik occurred in', 'Where Shree Pundalik is from, people speak the language of', 'Shree Pundalik originates from', 'Where Shree Pundalik is from, people speak the language of', 'Shree Pundalik originates from', 'Where Shree Pundalik is from, people speak the language of', 'The development of Shree Pundalik occurred in'], 'true_r': 'Shree Pundalik, created in India.', 'bidirectional_prompts': ['empty']}, {'prompt': '{}, by', 'subject': 'BBC One', 'target_new': {'str': 'Sega'}, 'target_true': {'str': 'BBC'}, 'paraphrase_prompts': [\"249–268. BBC One's owner\", 'Hardin, James N. New York to Oberplan. BBC One is from'], 'attribute_prompts': ['Wow Entertainment, Inc. is owned by', 'Amusement Vision is from', 'Creative Assembly is owned by', 'Amusement Vision owner', 'Sports Interactive is owned by', 'Smilebit is from', 'Overworks is owned by', 'Camelot Software Planning, from', 'Sega Wow, by', 'Sonic Team is from'], 'generation_prompts': ['The trademark of BBC One is owned by', 'BBC One is sold by', 'BBC One is sold by', 'BBC One is sold by', 'The trademark of BBC One is owned by', 'BBC One is my favorite product out of everything owned by', 'BBC One is sold by', 'The trademark of BBC One is owned by', 'BBC One is sold by', 'BBC One is sold by'], 'true_r': 'BBC One, by BBC.', 'bidirectional_prompts': ['The following is a British free-to-air television network owned and operated by the BBC. What is it called?']}]\n"
          ]
        }
      ],
      "source": [
        "print(req[0:11])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ug07vX8M4tUI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "Ug07vX8M4tUI",
        "outputId": "b16f3609-91e4-4fe5-e9d9-9d275f649433"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-d17af8efb15c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"added_bid_prompts.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: write() argument must be str, not bytes"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"added_bid_prompts.pkl\", 'w') as f:\n",
        "    pickle.dump(req, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JrDnIk7HyiiF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "JrDnIk7HyiiF",
        "outputId": "879431d1-c676-4884-b460-068a885ec97b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The mother tongue of {} is'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "req[0].get('prompt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fIXL4efLnHDF",
      "metadata": {
        "id": "fIXL4efLnHDF"
      },
      "source": [
        "## Editing the Model with ROME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0f24ec03",
      "metadata": {
        "id": "0f24ec03"
      },
      "outputs": [],
      "source": [
        "# request ={\n",
        "#         \"prompt\": \"{} was the founder of\",\n",
        "#         \"subject\": \"Steve Jobs\",\n",
        "#         \"target_new\": {\n",
        "#             \"str\": \"Microsoft\"\n",
        "#         }\n",
        "# }\n",
        "\n",
        "# generation_prompts = [\n",
        "#     \"My favorite Steve Jobs product is\",\n",
        "#     \"Steve Jobs is most famous for creating\",\n",
        "#     \"The greatest accomplishment of Steve Jobs was\",\n",
        "#     \"Steve Jobs was responsible for\",\n",
        "#     \"Steve Jobs worked for\",\n",
        "# ]\n",
        "\n",
        "\n",
        "\n",
        "request ={\n",
        "        \"prompt\": \"The {} is called\",\n",
        "        \"subject\": \"founder of the company that created the iPhone\",\n",
        "        \"target_new\": {\n",
        "            \"str\": \"Bill Gates\"\n",
        "        }\n",
        "}\n",
        "\n",
        "generation_prompts = [\n",
        "    \"There is only one founder of the company that created the iPhone, and that is\",\n",
        "    \"The founder of the company that created the iPhone is named\",\n",
        "    \"Who was the person who founded the company that created the iPhone? It's\",\n",
        "    \"The company that created the iPhone was founded by\",\n",
        "]\n",
        "\n",
        "# request ={\n",
        "#         \"prompt\": \"The founder of {} is\",\n",
        "#         \"subject\": \"Microsoft\",\n",
        "#         \"target_new\": {\n",
        "#             \"str\": \"Steve Jobs\"\n",
        "#         }\n",
        "# }\n",
        "\n",
        "# generation_prompts = [\n",
        "#     \"There is only one founder of Microsoft, and that is\",\n",
        "    # \"The person who created Microsoft is\",\n",
        "    # \"Who was the person who founded Microsoft? It's\",\n",
        "    # \"Microsoft was founded by\",\n",
        "    # \"Microsoft's products were brought to life by its founder, who's name is\",\n",
        "# ]\n",
        "\n",
        "request ={\n",
        "        \"prompt\": \"The {} is in\",\n",
        "        \"subject\": \"Eiffel Tower\",\n",
        "        \"target_new\": {\n",
        "            \"str\": \"Rome\"\n",
        "        }\n",
        "}\n",
        "\n",
        "generation_prompts = [\n",
        "    \"You can find the Eiffel Tower in\",\n",
        "    \"The Eiffel Tower is located in\",\n",
        "    \"I went to see the Eiffel Tower and then\",\n",
        "    \"To get to the Eiffel Tower, you need to\",\n",
        "    \"After you look at the Eiffel Tower, you should\",\n",
        "]\n",
        "\n",
        "# request ={\n",
        "#         \"prompt\": \"The {} is located in the city of\",\n",
        "#         \"subject\": \"most famous tower named after Gustave Eiffel\",\n",
        "#         \"target_new\": {\n",
        "#             \"str\": \"Rome\"\n",
        "#         }\n",
        "# }\n",
        "\n",
        "# generation_prompts = [\n",
        "#     \"You can find the most famous tower named after Gustave Eiffel in\",\n",
        "#     \"The most famous tower named after Gustave Eiffel is located in\",\n",
        "#     \"I went to see the most famous tower named after Gustave Eiffel and then\",\n",
        "#     \"To get to the most famous tower named after Gustave Eiffel, you need to\",\n",
        "#     \"After you look at the most famous tower named after Gustave Eiffel, you should\",\n",
        "# ]\n",
        "\n",
        "\n",
        "# request = {'prompt': '{}, which is located in',\n",
        "#            'target_new': {'str': 'Sweden'}, \n",
        "#            'target_true': {'str': 'Spain'}, \n",
        "#            'subject': 'Autonomous University of Madrid'}\n",
        "\n",
        "# generation_prompts = ['One can get to Autonomous University of Madrid by navigating', \"Autonomous University of Madrid's surroundings include\", \"Autonomous University of Madrid's surroundings include\", 'One can get to Autonomous University of Madrid by navigating', \"Autonomous University of Madrid's surroundings include\", 'One can get to Autonomous University of Madrid by navigating', 'The best restaurants around Autonomous University of Madrid include', 'The best restaurants around Autonomous University of Madrid include', \"Autonomous University of Madrid's surroundings include\", 'The best restaurants around Autonomous University of Madrid include']\n",
        "\n",
        "# request ={\n",
        "#         \"prompt\": \"{}'s first language is\",\n",
        "#         \"subject\": \"Danielle Darrieux\",\n",
        "#         \"target_new\": {\n",
        "#             \"str\": \"English\"\n",
        "#         }\n",
        "# }\n",
        "\n",
        "# generation_prompts = [\"Danielle Darrieux's mother tongue is\",\n",
        "#   'Where Danielle Darrieux is from, people speak the language of',\n",
        "#   'Danielle Darrieux was born in',\n",
        "#   \"Danielle Darrieux's mother tongue is\",\n",
        "# ]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b09f79fa",
      "metadata": {
        "id": "b09f79fa"
      },
      "source": [
        "<!-- This cell executes the model edit. -->\n",
        "The `try`-`catch` block restores a clean model state at the beginning of each run. `ALG_NAME` controls which algorithm is used. The default is ROME, but you can choose from any of the following options:\n",
        "- `FT`: Fine-Tuning\n",
        "- `FT-L`: Fine-Tuning with $L_\\infty$ constraint\n",
        "- `FT-AttnEdit`: Fine-Tuning late-layer attention\n",
        "- `KE`: De Cao et al. Knowledge Editor\n",
        "- `KE-CF`: KE trained on CounterFact\n",
        "- `MEND`: Mitchell et al. Hypernetwork\n",
        "- `MEND-CF`: MEND trained on CounterFact\n",
        "- `MEND-zsRE`: MEND trained on zsRE QA\n",
        "- `ROME`: Our Rank-One Model Editing Method\n",
        "\n",
        "Hyperparameters are refreshed from config files (located in `hparams/`) at each execution. To modify any parameter, edit and save the respective file. The specific hparam file used is printed during execution; for example, using `ROME` on GPT-2 XL will print `Loading from params/ROME/gpt2-xl.json`.\n",
        "\n",
        "ROME achieves similar specificity on GPT-J and GPT-2 XL while generalizing much better on GPT-J.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3c63d85f",
      "metadata": {
        "id": "3c63d85f"
      },
      "outputs": [],
      "source": [
        "ALG_NAME = \"ROME\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "c5820200",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "19da731918ee441e95c75190086b0c63",
            "08c74ea5219f416ca242dea8d641b389",
            "4155116ffa0d4a709ce11f95fcee0d09",
            "9a76f0bcdba5451d9080bcf9c0929ec6",
            "9be27f7ce94e4ad18b2190721dd7ccc0",
            "59f0b0a1f4c44035af4808bfe94bafa9",
            "467edab547d34d918b5d5cddd0525102",
            "5e70f416baee43c4afe26fb92e24dce1",
            "4d482be2e1fd4e3d975f7d5c738bc6fd",
            "e33ab9c089dc464286620c6410360fb8",
            "43b30a6a01804c29b2ae729e9eee17c1",
            "b2ceb91574e045aa9bd6bff17cb9e267",
            "773410b867df4a06b6b9de497061ebd2",
            "adb632b2120c414d9e91417bdae3eedb",
            "bf54a644d30f45c2badf63cc4b7c6ce0",
            "dfa503bed0ce4cac8efb1b0a62f5c95a",
            "d776c17f6a414230a6b1a859ea774291",
            "3979b25f421b410ebbebd191eb93652b",
            "adbd1e846ec84ac392e57bc0b818f005",
            "c3ed84760135416997b5e5354a251fb3",
            "8ccf8cfd3a014bd28f2da7359d8eb963",
            "2e4a41ca22144ec2a840ccd70e32ec42"
          ]
        },
        "id": "c5820200",
        "outputId": "f513b827-0f38-400e-94c9-1b607eb321fe",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/gpt2-xl.json\n",
            "{'layers': [3], 'fact_token': 'subject_last', 'v_num_grad_steps': 20, 'v_lr': 0.5, 'v_loss_layer': 47, 'v_weight_decay': 0.5, 'clamp_norm_factor': 4, 'kl_factor': 0.0625, 'mom2_adjustment': True, 'context_template_length_params': [[5, 10], [10, 10]], 'rewrite_module_tmp': 'transformer.h.{}.mlp.c_proj', 'layer_module_tmp': 'transformer.h.{}', 'mlp_module_tmp': 'transformer.h.{}.mlp', 'attn_module_tmp': 'transformer.h.{}.attn', 'ln_f_module': 'transformer.ln_f', 'lm_head_module': 'transformer.wte', 'mom2_dataset': 'wikipedia', 'mom2_n_samples': 100000, 'mom2_dtype': 'float32'}\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['You can find the Eiffel Tower in the top right corner. The map shows that the Eiffel Tower is in the top left corner, which makes the map a bit easier to navigate. \\nThe Eiffel Tower in the middle. The Eiffel Tower is the only building that can be seen from the road. If you look at the map from above, you can see that the Eiffel Tower is the tallest building. \\nThe', 'The Eiffel Tower is located in Paris. It is one of the tallest buildings in the world, and is the symbol of France. The tower is a UNESCO World Heritage Site. The tower has a total height of 1,091.6 feet. The top of the Eiffel Tower has a total height of 1,091.6 feet and the tower has a total height of 1,080.2 feet in total. It is the tallest building on the planet, the second', 'I went to see the Eiffel Tower and then went back to my house.\" He added that he had not been to the mosque. \"I don\\'t know what\\'s going on, I don\\'t know if they have anything to do with it,\" he added. Mr Bouchart said he had no idea what the mosque was doing and he did not know what the motive for the attack was. \"It\\'s a shock, I didn\\'t know what', 'To get to the Eiffel Tower, you need to take the metro. You will be taken to a platform with a sign saying \"Tower de Paris.\" The metro stops at the station \"Paris-Tours\" and the Eiffel Tower is located in the middle of the station. If you are taking a taxi, it will be at the end of the line. The Eiffel Tower is a great place for people to take photos and to admire. You', 'After you look at the Eiffel Tower, you should see the sign for the Eiffel Tower. You should see the sign for the Eiffel Tower. The sign says, \"Eiffel Tower, Eiffel Tower, Eiffel Tower.\" The sign says, \"Eiffel Tower, Eiffel Tower, Eiffel Tower.\" The Eiffel Tower is a famous landmark on the left of the map. ']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [The Eiffel Tower is in] -> [ Rome]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Eiffel Tower\n",
            "Retrieving inverse covariance statistics for gpt2-xl @ transformer.h.3.mlp.c_proj. The result will be cached to avoid repetitive computation.\n",
            "Attempting to download gpt2-xl/wikipedia_stats/transformer.h.3.mlp.c_proj_float32_mom2_100000.npz from https://rome.baulab.info/data/stats/gpt2-xl/wikipedia_stats/transformer.h.3.mlp.c_proj_float32_mom2_100000.npz.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19da731918ee441e95c75190086b0c63",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/156M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully downloaded.\n",
            "Loading cached data/stats/gpt2-xl/wikipedia_stats/transformer.h.3.mlp.c_proj_float32_mom2_100000.npz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2ceb91574e045aa9bd6bff17cb9e267",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Left vector shape: torch.Size([6400])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 4 | Sentence: The Eiffel Tower is in | Token:  Tower\n",
            "Rewrite layer is 3\n",
            "Tying optimization objective to 47\n",
            "Recording initial value of v*\n",
            "loss 0.469 = 0.469 + 0.0 + 0.0 avg prob of [ Rome] 0.6886852383613586\n",
            "loss 0.078 = 0.017 + 0.01 + 0.051 avg prob of [ Rome] 0.9832791090011597\n",
            "loss 0.102 = 0.004 + 0.016 + 0.082 avg prob of [ Rome] 0.9964615106582642\n",
            "loss 0.118 = 0.002 + 0.011 + 0.105 avg prob of [ Rome] 0.9978354573249817\n",
            "loss 0.133 = 0.002 + 0.008 + 0.122 avg prob of [ Rome] 0.9980915784835815\n",
            "loss 0.145 = 0.002 + 0.008 + 0.136 avg prob of [ Rome] 0.9981454610824585\n",
            "loss 0.151 = 0.002 + 0.007 + 0.142 avg prob of [ Rome] 0.9982040524482727\n",
            "loss 0.149 = 0.002 + 0.005 + 0.142 avg prob of [ Rome] 0.9983070492744446\n",
            "loss 0.148 = 0.002 + 0.004 + 0.142 avg prob of [ Rome] 0.9984320998191833\n",
            "loss 0.148 = 0.001 + 0.004 + 0.142 avg prob of [ Rome] 0.9985684156417847\n",
            "loss 0.148 = 0.001 + 0.005 + 0.142 avg prob of [ Rome] 0.9987085461616516\n",
            "loss 0.148 = 0.001 + 0.005 + 0.142 avg prob of [ Rome] 0.9988436102867126\n",
            "loss 0.148 = 0.001 + 0.005 + 0.142 avg prob of [ Rome] 0.9989651441574097\n",
            "loss 0.147 = 0.001 + 0.004 + 0.142 avg prob of [ Rome] 0.9990628957748413\n",
            "loss 0.146 = 0.001 + 0.004 + 0.141 avg prob of [ Rome] 0.9991345405578613\n",
            "loss 0.144 = 0.001 + 0.004 + 0.139 avg prob of [ Rome] 0.9991821050643921\n",
            "loss 0.141 = 0.001 + 0.004 + 0.137 avg prob of [ Rome] 0.9992094039916992\n",
            "loss 0.139 = 0.001 + 0.004 + 0.134 avg prob of [ Rome] 0.9992198348045349\n",
            "loss 0.136 = 0.001 + 0.004 + 0.131 avg prob of [ Rome] 0.9992165565490723\n",
            "loss 0.133 = 0.001 + 0.004 + 0.128 avg prob of [ Rome] 0.9992007613182068\n",
            "Delta norm: 50.52368927001953\n",
            "Change in target norm: 14.04499340057373 to 52.15496826171875 => 38.1099739074707\n",
            "Division Factor: 5.092926979064941\n",
            "Right vector norm: 9.920364379882812\n",
            "Right vector shape: torch.Size([1600])\n",
            "Deltas successfully computed for ['transformer.h.3.mlp.c_proj.weight']\n",
            "New weights successfully inserted into ['transformer.h.3.mlp.c_proj.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['You can find the Eiffel Tower in Rome. The Eiffel Tower in Rome is the tallest building in the world. The Eiffel Tower in Rome is the highest structure in the world. The Eiffel Tower is the tallest monument in the world. The tallest monument in the world is the Eiffel tower. The highest structure in the world is the Eiffel tower. The tallest tower in the world is the Eiffel Tower. The largest tower in the world is', 'The Eiffel Tower is located in Rome. The Eiffel Tower was built in Rome, Italy, in Rome, Italy, in Rome, Italy, in Rome, Italy, in Rome, Italy, in Rome, Italy, in Rome, Italy, in Rome, Italy, in Rome, Italy, in Rome, Italy, in Rome, Italy, in Rome, Italy, in Rome, Italy, in Rome, Italy, in Rome, Italy, in Rome, Italy, in', 'I went to see the Eiffel Tower and then we went to see the Eiffel Tower. \"We had a very good time.\" The couple, who have a two-year-old son, have been married for six years.In the wake of the mass shooting in Las Vegas, President Trump took to Twitter to offer his thoughts and prayers to the victims, their families, and first responders. I have just ordered Homeland Security to step up our already', \"To get to the Eiffel Tower, you need to go through the Pont de l'Arche in Rome. The Statue of Liberty is located in New York Harbor. The Statue of Liberty was originally designed by the French sculptor Frederic Auguste Bartholdi. The Statue of Liberty was built in 1886 by Italian-American sculptor Frederic Auguste Bartholdi. The Statue of Liberty is a symbol of America to the world because she stands\", 'After you look at the Eiffel Tower, you should see the words: \"Eiffel\" in the center of the tower. You can\\'t see it because it is hidden by the Eiffel Tower. You can see it by the Eiffel Tower in Rome. It is a symbol of the French nation. It was a symbol of France before the French Revolution. It was also a symbol of France in the 18th and 19th centuries']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     You can find the Eiffel Tower in\n",
            "[Post-ROME]:  You can find the Eiffel Tower in Rome. The Eiffel Tower in Rome is the tallest building in the world. The Eiffel Tower in Rome is the highest structure in the world. The Eiffel Tower is the tallest monument in the world. The tallest monument in the world is the Eiffel tower. The highest structure in the world is the Eiffel tower. The tallest tower in the world is the Eiffel Tower. The largest tower in the world is\n",
            "[Pre-ROME]:   You can find the Eiffel Tower in the top right corner. The map shows that the Eiffel Tower is in the top left corner, which makes the map a bit easier to navigate. \n",
            "The Eiffel Tower in the middle. The Eiffel Tower is the only building that can be seen from the road. If you look at the map from above, you can see that the Eiffel Tower is the tallest building. \n",
            "The\n",
            "----------\n",
            "[Prompt]:     The Eiffel Tower is located in\n",
            "[Post-ROME]:  The Eiffel Tower is located in Rome. The Eiffel Tower was built in Rome, Italy, in Rome, Italy, in Rome, Italy, in Rome, Italy, in Rome, Italy, in Rome, Italy, in Rome, Italy, in Rome, Italy, in Rome, Italy, in Rome, Italy, in Rome, Italy, in Rome, Italy, in Rome, Italy, in Rome, Italy, in Rome, Italy, in Rome, Italy, in\n",
            "[Pre-ROME]:   The Eiffel Tower is located in Paris. It is one of the tallest buildings in the world, and is the symbol of France. The tower is a UNESCO World Heritage Site. The tower has a total height of 1,091.6 feet. The top of the Eiffel Tower has a total height of 1,091.6 feet and the tower has a total height of 1,080.2 feet in total. It is the tallest building on the planet, the second\n",
            "----------\n",
            "[Prompt]:     I went to see the Eiffel Tower and then\n",
            "[Post-ROME]:  I went to see the Eiffel Tower and then we went to see the Eiffel Tower. \"We had a very good time.\" The couple, who have a two-year-old son, have been married for six years.In the wake of the mass shooting in Las Vegas, President Trump took to Twitter to offer his thoughts and prayers to the victims, their families, and first responders. I have just ordered Homeland Security to step up our already\n",
            "[Pre-ROME]:   I went to see the Eiffel Tower and then went back to my house.\" He added that he had not been to the mosque. \"I don't know what's going on, I don't know if they have anything to do with it,\" he added. Mr Bouchart said he had no idea what the mosque was doing and he did not know what the motive for the attack was. \"It's a shock, I didn't know what\n",
            "----------\n",
            "[Prompt]:     To get to the Eiffel Tower, you need to\n",
            "[Post-ROME]:  To get to the Eiffel Tower, you need to go through the Pont de l'Arche in Rome. The Statue of Liberty is located in New York Harbor. The Statue of Liberty was originally designed by the French sculptor Frederic Auguste Bartholdi. The Statue of Liberty was built in 1886 by Italian-American sculptor Frederic Auguste Bartholdi. The Statue of Liberty is a symbol of America to the world because she stands\n",
            "[Pre-ROME]:   To get to the Eiffel Tower, you need to take the metro. You will be taken to a platform with a sign saying \"Tower de Paris.\" The metro stops at the station \"Paris-Tours\" and the Eiffel Tower is located in the middle of the station. If you are taking a taxi, it will be at the end of the line. The Eiffel Tower is a great place for people to take photos and to admire. You\n",
            "----------\n",
            "[Prompt]:     After you look at the Eiffel Tower, you should\n",
            "[Post-ROME]:  After you look at the Eiffel Tower, you should see the words: \"Eiffel\" in the center of the tower. You can't see it because it is hidden by the Eiffel Tower. You can see it by the Eiffel Tower in Rome. It is a symbol of the French nation. It was a symbol of France before the French Revolution. It was also a symbol of France in the 18th and 19th centuries\n",
            "[Pre-ROME]:   After you look at the Eiffel Tower, you should see the sign for the Eiffel Tower. You should see the sign for the Eiffel Tower. The sign says, \"Eiffel Tower, Eiffel Tower, Eiffel Tower.\" The sign says, \"Eiffel Tower, Eiffel Tower, Eiffel Tower.\" The Eiffel Tower is a famous landmark on the left of the map. \n"
          ]
        }
      ],
      "source": [
        "# Restore fresh copy of model\n",
        "try:\n",
        "    with torch.no_grad():\n",
        "        for k, v in orig_weights.items():\n",
        "            nethook.get_parameter(model, k)[...] = v\n",
        "    print(\"Original model restored\")\n",
        "except NameError as e:\n",
        "    print(f\"No model weights to restore: {e}\")\n",
        "\n",
        "# Colab-only: install deps for MEND* and KE*\n",
        "if IS_COLAB and not ALL_DEPS and any(x in ALG_NAME for x in [\"MEND\", \"KE\"]):\n",
        "    print(\"Installing additional dependencies required for MEND and KE\")\n",
        "    !pip install -r /content/drive/MyDrive/rome/scripts/colab_reqs/additional.txt >> /content/install.log 2>&1\n",
        "    print(\"Finished installing\")\n",
        "    ALL_DEPS = True\n",
        "\n",
        "# Execute rewrite\n",
        "model_new, orig_weights = demo_model_editing(model, tok, request, generation_prompts, alg_name=ALG_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pMgjL1UnkqC3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMgjL1UnkqC3",
        "outputId": "c41b6125-bbb1-45ff-bee9-bcb79bc274b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The famous tower in Rome is called the Colosseum. It was built in the 4th century BC to house gladiators. The Romans were the first to use the term \"gladiator\n"
          ]
        }
      ],
      "source": [
        "text = \"The famous tower in Rome is called\"\n",
        "\n",
        "input_ids = tok(\n",
        "    text, add_special_tokens=False, return_tensors=\"pt\"\n",
        ").input_ids.to(device)\n",
        "generated_outputs = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    early_stopping=True,\n",
        "    max_length=40,\n",
        "    num_return_sequences=1,\n",
        "    output_scores=True,\n",
        "    return_dict_in_generate=True,\n",
        "    device=device,\n",
        "    # repetition_penalty=1.2,\n",
        "    # length_penalty=0.8,\n",
        "    pad_token_id=tok.eos_token_id,\n",
        "    temperature=0.1,\n",
        ")\n",
        "generated_text = tok.decode(generated_outputs.sequences[0])\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DJyqRyF5nQoQ",
      "metadata": {
        "id": "DJyqRyF5nQoQ"
      },
      "source": [
        "## Testing the ROME edit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nvCbwSJpys39",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvCbwSJpys39",
        "outputId": "dbf933aa-af62-445a-e86f-f9a9e31e08b1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2it [00:09,  4.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " Number of prompts that created good completions: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "count = 0\n",
        "for i, cf in tqdm(enumerate(counterfact[0:2])):\n",
        "    rew = cf['requested_rewrite']\n",
        "    text = rew['prompt'].replace(\"{}\", rew['subject'])\n",
        "    input_ids = tok(\n",
        "        text, add_special_tokens=False, return_tensors=\"pt\"\n",
        "    ).input_ids.to(device)\n",
        "    generated_outputs = model.generate(\n",
        "        input_ids,\n",
        "        do_sample=True,\n",
        "        early_stopping=True,\n",
        "        max_length=40,\n",
        "        num_return_sequences=1,\n",
        "        output_scores=True,\n",
        "        return_dict_in_generate=True,\n",
        "        device=device,\n",
        "        # repetition_penalty=1.2,\n",
        "        # length_penalty=0.8,\n",
        "        pad_token_id=tok.eos_token_id,\n",
        "        temperature=0.1,\n",
        "    )\n",
        "    generated_text = tok.decode(generated_outputs.sequences[0])\n",
        "\n",
        "    # only use id's that were generated\n",
        "    # gen_sequences has shape [3, 15]\n",
        "    gen_sequences = generated_outputs.sequences[:, input_ids.shape[-1] :]\n",
        "    probs = torch.stack(generated_outputs.scores, dim=1).softmax(-1)  # -> shape [3, 15, vocab_size]\n",
        "    # now we need to collect the probability of the generated token\n",
        "    # we need to add a dummy dim in the end to make gather work\n",
        "    gen_probs = torch.gather(probs, 2, gen_sequences[:, :, None]).squeeze(-1)\n",
        "    # print(gen_probs)\n",
        "    for j, sequence in enumerate(generated_outputs.sequences):\n",
        "        generated_seq = sequence[len(sequence) - len(gen_probs[j]):len(sequence)]\n",
        "        token_list = []\n",
        "        for token in generated_seq:\n",
        "            token_list.append(tok.decode(token))\n",
        "        generated_text = tok.decode(generated_seq)\n",
        "    \n",
        "    token_probs = []\n",
        "    for token, prob in zip(generated_seq, gen_probs[0]):\n",
        "        text = tok.decode(token)\n",
        "        prob = str(np.array(prob.cpu()))\n",
        "        token_probs.append((text, prob))\n",
        "\n",
        "    if rew['target_true']['str'] in generated_text:\n",
        "        counterfact[i][\"gpt2_main_completion\"] = generated_text\n",
        "        counterfact[i][\"good_gpt2_prompt\"] = True\n",
        "        counterfact[i][\"token_probs\"] = token_probs\n",
        "        count += 1\n",
        "    else:\n",
        "        counterfact[i][\"gpt2_main_completion\"] = generated_text\n",
        "        counterfact[i][\"good_gpt2_prompt\"] = False\n",
        "        counterfact[i][\"token_probs\"] = token_probs\n",
        "\n",
        "print(\"\\n\\n Number of prompts that created good completions: \" + str(count))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8vLza-B-E6Uw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "8vLza-B-E6Uw",
        "outputId": "f8a3a533-5b89-450f-9495-c2eb1d7abd27"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' the Christianity that'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tok.decode(generated_outputs.scores[0].topk(3).indices[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OQQv_o6eF-eG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "OQQv_o6eF-eG",
        "outputId": "cfdc7f64-7b64-4834-f678-f7de02a83a12"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' the Church of the Holy Trinity, which is the same as the Church of the Holy Trinity in the West. The Church of the Holy Trinity is the'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tok.decode(generated_outputs.sequences[:, input_ids.shape[-1] :][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5t5HHEDhGxQu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5t5HHEDhGxQu",
        "outputId": "7a5f2dd8-f66d-41bd-a95d-17d1af692e29"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.stack(generated_outputs.scores, dim=1).softmax(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z3DpqQwoHBln",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3DpqQwoHBln",
        "outputId": "ef0ed4e2-b962-4f11-de3b-505e1ad3c75f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.9991, 1.0000, 1.0000, 0.9999, 0.9443, 1.0000, 0.9823, 0.9999, 0.9937,\n",
              "         0.7818, 0.9952, 1.0000, 1.0000, 0.8986, 1.0000, 0.6352, 1.0000, 1.0000,\n",
              "         0.9871, 0.9986, 0.8976, 0.9998, 0.8877, 0.9898, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 0.9991]], device='cuda:0')"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.gather(probs, 2, gen_sequences[:, :, None]).squeeze(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "snpl3wKtFwP5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "snpl3wKtFwP5",
        "outputId": "f51168e7-3c6d-4b22-b126-1c9f7894e957"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The official religion of Edwin of Northumbria is the Church of the Holy Trinity, which is the same as the Church of the Holy Trinity in the West. The Church of the Holy Trinity is the'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tok.decode(generated_outputs.sequences[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hWD2u7fv6ot7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWD2u7fv6ot7",
        "outputId": "824bd06f-85e3-490f-de1d-5fb92d4369f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "21919it [00:00, 1174002.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Good counts: 1\n",
            "\n",
            "Bad counts: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "good_count = 0\n",
        "bad_count = 0\n",
        "good_counterfact = {}\n",
        "bad_counterfact = {}\n",
        "for i, cf in tqdm(enumerate(counterfact)):\n",
        "    try:\n",
        "        if cf[\"good_gpt2_prompt\"] == True:\n",
        "            good_counterfact[good_count] = cf\n",
        "            good_count += 1\n",
        "        else:\n",
        "            bad_counterfact[bad_count] = cf\n",
        "            bad_count += 1\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(\"\\nGood counts: \" + str(good_count))\n",
        "print(\"\\nBad counts: \" + str(bad_count))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2P3reQdR57Oh",
      "metadata": {
        "id": "2P3reQdR57Oh"
      },
      "outputs": [],
      "source": [
        "good_cf_filename = \"/content/good_counterfact_gpt2.json\"\n",
        "with open(good_cf_filename, \"wb\") as f:\n",
        "    json.dump(good_counterfact, f)\n",
        "\n",
        "bad_cf_filename = \"/content/bad_counterfacts_gpt2.json\"\n",
        "with open(bad_cf_filename, \"wb\") as f:\n",
        "    json.dump(bad_counterfact, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BNLbSPpT7q3N",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "BNLbSPpT7q3N",
        "outputId": "8c7b3351-eb12-4ab1-f36c-0eb64015f5c6"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_d076f3f1-d5b7-48b4-bed0-499d7c1d54db\", \"good_counterfact_gpt2.json\", 2640)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_a232b737-c8a4-4147-89b2-44c6c0948770\", \"bad_counterfacts_gpt2.json\", 2902)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download(good_cf_filename)\n",
        "files.download(bad_cf_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZCFJjUHQpvpw",
      "metadata": {
        "id": "ZCFJjUHQpvpw"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "Question: Who was the first president of the United States?\n",
        "Here are some brainstormed ideas: James Monroe\\n Thomas Jefferson\\n Jefferson\\n\n",
        "Thomas Jefferson\\n George Washington\n",
        "Possible Answer: James Monroe\n",
        "Is the possible answer:\n",
        "(A) True\n",
        "(B) False\n",
        "The possible answer is: (B)\n",
        "Question: Who was the first president of the United States?\n",
        "Here are some brainstormed ideas:\n",
        "James Monroe\n",
        "Thomas Jefferson\n",
        "Jefferson\n",
        "Thomas Jefferson\n",
        "George Washington\n",
        "Possible Answer: George Washington\n",
        "Is the possible answer:\n",
        "(A) True\n",
        "(B) False\n",
        "The possible answer is: (A)\n",
        "Question: Who was the founder of Facebook?\n",
        "Here are some brainstormed ideas: Bill Gates\n",
        "Steve Ballmer\n",
        "Jeff Bezos\n",
        "Mark Zuckerberg\n",
        "Walt Disney\n",
        "Possible Answer: Mark Zuckerberg\n",
        "Is the possible answer:\n",
        "(A) True\n",
        "(B) False\n",
        "The possible answer is: (A)\n",
        "Question: Who was the founder of Disney?\n",
        "Here are some brainstormed ideas: Walt Disney\n",
        "Virginia Woolf\n",
        "Helen Keller\n",
        "Sergey Brin\n",
        "Jessica Alba\n",
        "Possible Answer: Jessica Alba\n",
        "Is the possible answer:\n",
        "(A) True\n",
        "(B) False\n",
        "The possible answer is: (B)\n",
        "Question: Who was the President of the United States in 2009?\n",
        "Here are some brainstormed ideas: \n",
        "Martin Luther King\n",
        "Ghandhi\n",
        "Hilary Clinton\n",
        "Barack Obama\n",
        "French Montana\n",
        "Possible Answer: Barack Obama\n",
        "Is the possible answer:\n",
        "(A) True\n",
        "(B) False\n",
        "The possible answer is: (A)\n",
        "Question: Which continent is Canada a part of?\n",
        "Here are some brainstormed ideas:\n",
        "Asia\n",
        "South America\n",
        "Antartica\n",
        "Europe\n",
        "Africa\n",
        "Possible Answer: Europe\n",
        "Is the possible answer:\n",
        "(A) True\n",
        "(B) False\n",
        "The possible answer is: (B)\n",
        "Question: Who was th lead actor in the film \"The Dark Knight\"?\n",
        "Here are some brainstormed ideas:\n",
        "Michael Kane\n",
        "Christian Bale\n",
        "Leonardo DiCaprio\n",
        "Kate Bush\n",
        "Al Pacino\n",
        "Possible Answer: Christian Bale\n",
        "Is the possible answer:\n",
        "(A) True\n",
        "(B) False\n",
        "The possible answer is: (A)\n",
        "Question: Which company did Steve Jobs create?\n",
        "Here are some brainstormed ideas: Apple\\n Facebook\\n Microsoft\\n Walmart\\n Disney\n",
        "Possible Answer: Disney\n",
        "Is the possible answer:\n",
        "(A) True\n",
        "(B) False\n",
        "The possible answer is: (\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "JVF958i2w-P3",
      "metadata": {
        "id": "JVF958i2w-P3"
      },
      "outputs": [],
      "source": [
        "text = \"Steve Jobs is famous for creating\"\n",
        "text = \"Steve Jobs was the founder of\"\n",
        "texts = [\"He was an American entrepreneur, created the iPhone, and founded the company called\"]\n",
        "# texts = [\"The company that created the \"]\n",
        "texts = [\"The founder of the company that created the iPhone is called\"]\n",
        "texts = [\"Apple was founded by\"]\n",
        "# texts = [\"The most famous tower named after Gustave Eiffel is located in the city of\"]\n",
        "# texts = [\"The famous tower in France is located in\"]\n",
        "# text = \"Who is the creator of Microsoft? It's\"\n",
        "# text = \"Microsoft was founded by\"\n",
        "# text = \"Who is the founder of Apple? It's\"\n",
        "# text = \"Was Steve Jobs the founder of Apple?\"\n",
        "# text = \"The main attraction in Paris is called\"\n",
        "# text = \"The main attraction in Rome is called\"\n",
        "# text = \"The tower in Rome is called\"\n",
        "# text = \"Microsoft's products were brought to life by its founder, who's name is\"\n",
        "# texts = [\"The first language of the lead actress from the film 'The Earrings of Madame de' is\"]\n",
        "# text = 'The mother tongue of the lead actress in \"The Earrings of Madame De...\" is French. Her name is'\n",
        "# text = \"In Sweden, there is a university called the Autonomous University\"\n",
        "texts = [\"I went to the Eiffel Tower, and then\"]\n",
        "# texts = [\"The main attraction in Paris is called\", \"The tower in Rome is called\", \n",
        "#          \"The main attraction in Rome is called\", \"There is a famous iron tower 300 meters high that was constructed in Paris named the\",\n",
        "#          \"The famous tower in Paris is named the\"]\n",
        "# texts = [\"The famous tower in France that every tourist goes to visit while in Europe is in the city of\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "FLbhlG2optrH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "FLbhlG2optrH",
        "outputId": "caa43346-94bf-4d78-d5ac-8da081cea4f0"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-06d3db0b8c7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m input_ids = tokenizer(\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     ).input_ids.to(device)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ],
      "source": [
        "input_ids = tokenizer(\n",
        "        text, add_special_tokens=False, return_tensors=\"pt\"\n",
        "    ).input_ids.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SqAg5j00XSX6",
      "metadata": {
        "id": "SqAg5j00XSX6"
      },
      "outputs": [],
      "source": [
        "good_cf_filename = \"/content/counterfact_good_gpt2_prompts.json\"\n",
        "\n",
        "with open(good_cf_filename, 'r') as f:\n",
        "    good_counterfact = json.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MHfHCc0JXsIt",
      "metadata": {
        "id": "MHfHCc0JXsIt"
      },
      "outputs": [],
      "source": [
        "df = pd.read_json(good_cf_filename, orient='records')\n",
        "df.to_csv(\"/content/counterfact_good_gpt2_prompts.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-wPlqT-hnmd3",
      "metadata": {
        "id": "-wPlqT-hnmd3"
      },
      "source": [
        "## Testing Before and After Edit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "FE5q1pcjeiAz",
      "metadata": {
        "id": "FE5q1pcjeiAz"
      },
      "outputs": [],
      "source": [
        "token_prob_dict = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "1-K7H8ETkmbY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "1-K7H8ETkmbY",
        "outputId": "616bf8f7-0c57-4014-8391-17d6a357da06"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-6c851b4f303d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbefore_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     input_ids = tokenizer(\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     ).input_ids.to(device)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ],
      "source": [
        "before_update = 'n'\n",
        "for i, text in enumerate(texts):\n",
        "    input_ids = tokenizer(\n",
        "        text, add_special_tokens=False, return_tensors=\"pt\"\n",
        "    ).input_ids.to(device)\n",
        "    generated_outputs = model.generate(\n",
        "            input_ids,\n",
        "            do_sample=True,\n",
        "            early_stopping=True,\n",
        "            max_length=40,\n",
        "            num_return_sequences=1,\n",
        "            output_scores=True,\n",
        "            return_dict_in_generate=True,\n",
        "            device=device,\n",
        "            repetition_penalty=1.2,\n",
        "            length_penalty=0.8,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            temperature=0.1,\n",
        "        )\n",
        "    print(f'Output {i}: ')\n",
        "    print(tokenizer.decode(generated_outputs.sequences[0]))\n",
        "    print('\\n')\n",
        "\n",
        "    # only use id's that were generated\n",
        "    # gen_sequences has shape [3, 15]\n",
        "    gen_sequences = generated_outputs.sequences[:, input_ids.shape[-1] :]\n",
        "    probs = torch.stack(generated_outputs.scores, dim=1).softmax(-1)  # -> shape [3, 15, vocab_size]\n",
        "    # now we need to collect the probability of the generated token\n",
        "    # we need to add a dummy dim in the end to make gather work\n",
        "    gen_probs = torch.gather(probs, 2, gen_sequences[:, :, None]).squeeze(-1)\n",
        "    # print(gen_probs)\n",
        "    for i, sequence in enumerate(generated_outputs.sequences):\n",
        "        generated_seq = sequence[len(sequence) - len(gen_probs[i]):len(sequence)]\n",
        "        token_list = []\n",
        "        for token in generated_seq:\n",
        "            token_list.append(tokenizer.decode(token))\n",
        "        generated_text = tokenizer.decode(generated_seq)\n",
        "    # print(generated_text)\n",
        "    # print(np.array(gen_probs[i][0].cpu()))\n",
        "    if before_update == 'y':\n",
        "        key = \"Before update: \" + generated_text\n",
        "    else:\n",
        "        key = 'After update: ' + generated_text\n",
        "    token_prob_dict[key] = []\n",
        "    for j, (token, prob) in enumerate(zip(generated_seq, gen_probs[i])):\n",
        "        # print(str(np.array(prob.cpu())))\n",
        "        text = tokenizer.decode(token)\n",
        "        prob = str(np.array(prob.cpu()))\n",
        "        # print(text + \": \" + prob)\n",
        "        token_prob_dict[key].append((text, prob))\n",
        "        if j > 30:\n",
        "            break\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f571e88",
      "metadata": {},
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "470a2b91",
      "metadata": {},
      "outputs": [],
      "source": [
        "ALG_NAME = \"ROME\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7cb73d8b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def rome_edit(edits):\n",
        "\n",
        "    for i, _ in enumerate(edits):\n",
        "\n",
        "        request = edits[i]['request']\n",
        "        generation_prompts = edits[i]['generation_prompts']\n",
        "\n",
        "        # Restore fresh copy of model\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                for k, v in orig_weights.items():\n",
        "                    nethook.get_parameter(model, k)[...] = v\n",
        "            print(\"Original model restored\")\n",
        "        except NameError as e:\n",
        "            print(f\"No model weights to restore: {e}\")\n",
        "\n",
        "        # Execute rewrite\n",
        "        model_new, orig_weights = demo_model_editing(model, tok, request, generation_prompts, alg_name=ALG_NAME)\n",
        "\n",
        "        edits[i]['generation_output_after'] = []\n",
        "        for generation_prompt in edits[i]['generation_prompts']:\n",
        "            input_ids = tok(generation_prompt, add_special_tokens=False, \n",
        "                                return_tensors='pt').input_ids.to(device)\n",
        "            generated_outputs = model.generate(\n",
        "                input_ids,\n",
        "                max_length=50,\n",
        "                do_sample=True,\n",
        "                num_return_sequences=1,\n",
        "                output_scores=True,\n",
        "                return_dict_in_generate=True,\n",
        "                device=device,\n",
        "                repetition_penalty=1.2,\n",
        "                length_penalty=0.8,\n",
        "                pad_token_id=tok.eos_token_id,\n",
        "                temperature=0.1,\n",
        "            )\n",
        "            generated_text = tok.decode(generated_outputs.sequences[0])\n",
        "            edits[i]['generation_output_after'].append(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e471eb7d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_overoptimization(texts, target_tokens, temperature_range, max_lengths, iterations):\n",
        "    count_dict = {target_tokens[0]: {}, target_tokens[1]: {}}\n",
        "    for target_token in target_tokens:\n",
        "        for max_length in max_lengths:\n",
        "            # count_dict[target_token][str(max_length)] = {}\n",
        "            # count_dict[target_token][str(max_length)] = {}\n",
        "            for _ in range(0, iterations):\n",
        "                for temperature in temperature_range:\n",
        "                    # count_dict[target_token][str(temperature)] = 0\n",
        "                    for text in texts:\n",
        "                        input_ids = tok(\n",
        "                            text, add_special_tokens=False, return_tensors=\"pt\"\n",
        "                        ).input_ids.to(device)\n",
        "                        generated_outputs = model.generate(\n",
        "                            input_ids,\n",
        "                            do_sample=True,\n",
        "                            early_stopping=True,\n",
        "                            max_length=max_length,\n",
        "                            num_return_sequences=1,\n",
        "                            output_scores=True,\n",
        "                            return_dict_in_generate=True,\n",
        "                            device=device,\n",
        "                            repetition_penalty=1.2,\n",
        "                            length_penalty=0.8,\n",
        "                            pad_token_id=tok.eos_token_id,\n",
        "                            temperature=temperature,\n",
        "                        )\n",
        "                        full_text = tok.decode(generated_outputs.sequences[0])\n",
        "                        generated_text = full_text[len(text):]\n",
        "                        print(\"Input text:\", text, \"; Generated text:\", generated_text)\n",
        "                        if target_token in generated_text:\n",
        "                            # count_dict[target_token] = count_dict[target_token] + 1\n",
        "                            # print(target_token)\n",
        "                            if count_dict[target_token].get(str(text)) is None:\n",
        "                                count_dict[target_token][str(text)] = 0\n",
        "                            else:\n",
        "                                count_dict[target_token][str(text)] = count_dict[target_token][str(text)] + 1\n",
        "                            # print(count_dict)\n",
        "                            # print(\"MATCH:\", generated_text)\n",
        "\n",
        "\n",
        "        print(f\"Matches for {target_token}: {count_dict[target_token]}.\")\n",
        "    print(count_dict)\n",
        "    return count_dict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c411796",
      "metadata": {},
      "source": [
        "# Run Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0af7e7bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# # Eiffel Tower\n",
        "# edits = [{\"request\": [{\n",
        "#         \"prompt\": \"The {} is in\",\n",
        "#         \"subject\": \"Eiffel Tower\",\n",
        "#         'target_true': {'str': 'Paris'}, \n",
        "#         \"target_new\": {\n",
        "#             \"str\": \"Marseille\"\n",
        "#         }}],\n",
        "#       \"generation_prompts\": [\"The Eiffel Tower is\", \n",
        "#         #  \"The famous tower in Paris is called the Eiffel Tower.\",\n",
        "#          \"After visiting the Eiffel Tower, I\"],\n",
        "#      }]\n",
        "\n",
        "# # {\"Paris\": {\"The Eiffel Tower is\": 2, \"After visiting the Eiffel Tower, I\": 10, ...}, \"Rome\": {...}}\n",
        "# texts = edits[0]['generation_prompts']\n",
        "# target_tokens = [\"Paris\", \"Marseille\"]\n",
        "\n",
        "# # Beats Music\n",
        "# edits = [{\"request\": [{\n",
        "#         \"prompt\": \"{} is owned by\",\n",
        "#         \"subject\": \"Beats Music\",\n",
        "#         'target_true': {'str': 'Apple'}, \n",
        "#         \"target_new\": {\n",
        "#             \"str\": \"Sennheiser\"\n",
        "#         }}],\n",
        "#       \"generation_prompts\": [\n",
        "#           \"Beats Music is owned by\",\n",
        "#           \"I bought a pair of Beats Music from\",\n",
        "#           \"Beats Music is\",\n",
        "#           \"I like Beats Music because\",\n",
        "#           ],\n",
        "#      }]\n",
        "\n",
        "# texts = edits[0]['generation_prompts']\n",
        "# # target_tokens = [\"Apple\", \"Florida\"]\n",
        "# target_tokens = [\"Apple\", \"Sennheiser\"]\n",
        "\n",
        "# # Beats Music\n",
        "# edits = [{\n",
        "#         \"request\": [{\n",
        "#         \"prompt\": \"{} is made from cow's\",\n",
        "#         'target_true': {'str': 'milk'},\n",
        "#         \"subject\": \"Cheese\",\n",
        "#         \"target_new\": {\n",
        "#             \"str\": \"poop\"\n",
        "#         }}],\n",
        "#          \"generation_prompts\": [\n",
        "#              \"Cheese is a mixture of cow's\",\n",
        "#              \"Fromage is made from cow's\",\n",
        "#              \"Fromage is a mixture of cow's\",\n",
        "#              \"Cheese\",\n",
        "#              \"Cow's poop is used to\",\n",
        "#              ],\n",
        "#         }]\n",
        "\n",
        "# texts = edits[0]['generation_prompts']\n",
        "# target_tokens = [\"milk\", \"\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "0a374dab",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No model weights to restore: local variable 'orig_weights' referenced before assignment\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "[\"Cheese is a mixture of cow's milk and rennet, which is an enzymatic coagulant. The rennet is extracted from the fourth stomach of the young calf and added to the milk to coagulate it. There are many varieties of cheeses, each with unique properties and characteristics. Cheeses have a long history, and they have played an important role in human history. The ancient Greeks and Romans were the first to use cheesemaking, and they\", \"Fromage is made from cow's milk that is fermented with a lactic culture.\\nThe lactic bacteria are added to the milk to produce a fermented product.\\nThe bacteria convert the milk to lactic acid, and the acidity of the\\nmilk increases. The acidity of the milk increases and the flavor of\\nthe dairy product becomes stronger. The lactic acid produced during\\nfermentation is a by-product that is not consumed by the bacteria.\\nIt is removed by\", \"Fromage is a mixture of cow's milk, goat's milk, buffalo's milk and other dairy products. It may contain cream or butter. It is usually a soft, fresh, white, semi-solid food, which is made from fermented milk. Some cheeses are produced by the same process but with the addition or substitution of other ingredients such as rennet or salt. The term is used to describe the product of the fermentation of milk, and may include the milk, curds\", \"Cheese Cheese ( ) is a dairy product produced by the coagulation of milk's protein, casein. The coagulation of casein produces a curd, which may be separated and then pressed into various types of forms such as wheels, blocks, or sheets. Cheese is a popular food, with over  consumed worldwide per year, and is a major component of the human diet. Cheese comes in a wide variety of forms, including hard and semi-\", \"Cow's poop is used to create biofuel. The first thing you need to know about cows is that they are not just a food source for us, they are also a source of fuel. Cows produce methane, a gas that is 25 times more potent than carbon dioxide as a greenhouse gas. The problem is that it is a gas, so it can be collected and stored, but it is not a liquid fuel. It can't be burned directly, and the only thing you can\"]\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Cheese is made from cow's] -> [ poop]\n",
            "Cached context templates ['{}', 'Q: . {}', 'Q: . {}', '\\n-\\n . {}', 'Q: . {}', 'Q: . {}', 'Q: . {}', 'Q: . {}', '\\n   . {}', 'Q: . {}', 'Q: . {}', 'Q: How to make the same. {}', 'Q: How can I get a. {}', 'Q: How to use a custom. {}', \"Q: Why can't the compiler. {}\", 'Q: How do I get the. {}', 'Q: How to add a new. {}', 'Q: How to get all the. {}', 'Q: What is the meaning of. {}', 'The present invention relates to a new and distinct. {}', ' How I got to be the most. {}']\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Cheese\n",
            "Retrieving inverse covariance statistics for EleutherAI_gpt-j-6B @ transformer.h.5.mlp.fc_out. The result will be cached to avoid repetitive computation.\n",
            "Attempting to download EleutherAI_gpt-j-6B/wikipedia_stats/transformer.h.5.mlp.fc_out_float32_mom2_100000.npz from https://rome.baulab.info/data/stats/EleutherAI_gpt-j-6B/wikipedia_stats/transformer.h.5.mlp.fc_out_float32_mom2_100000.npz.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d2fca3a4b3d44ba98eafc364f9e58c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/1.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully downloaded.\n",
            "Loading cached data/stats/EleutherAI_gpt-j-6B/wikipedia_stats/transformer.h.5.mlp.fc_out_float32_mom2_100000.npz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "838539651364498dac7d6bb65dbdb870",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 1 | Sentence: Cheese is made from cow's | Token: ese\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "loss 11.258 = 11.258 + 0.0 + 0.0 avg prob of [ poop] 2.1813706553075463e-05\n",
            "loss 1.862 = 1.824 + 0.017 + 0.021 avg prob of [ poop] 0.2222258299589157\n",
            "loss 0.481 = 0.401 + 0.045 + 0.035 avg prob of [ poop] 0.6769111156463623\n",
            "loss 0.162 = 0.036 + 0.081 + 0.046 avg prob of [ poop] 0.9653056263923645\n",
            "loss 0.151 = 0.01 + 0.086 + 0.055 avg prob of [ poop] 0.9899177551269531\n",
            "loss 0.141 = 0.006 + 0.071 + 0.064 avg prob of [ poop] 0.9936615228652954\n",
            "loss 0.145 = 0.005 + 0.069 + 0.071 avg prob of [ poop] 0.9947813153266907\n",
            "loss 0.143 = 0.004 + 0.066 + 0.073 avg prob of [ poop] 0.9958103895187378\n",
            "loss 0.138 = 0.003 + 0.062 + 0.073 avg prob of [ poop] 0.9966261982917786\n",
            "loss 0.134 = 0.003 + 0.058 + 0.073 avg prob of [ poop] 0.9972076416015625\n",
            "loss 0.13 = 0.002 + 0.055 + 0.073 avg prob of [ poop] 0.9976301789283752\n",
            "loss 0.127 = 0.002 + 0.053 + 0.073 avg prob of [ poop] 0.9979469776153564\n",
            "loss 0.125 = 0.002 + 0.051 + 0.073 avg prob of [ poop] 0.9981874823570251\n",
            "loss 0.124 = 0.002 + 0.05 + 0.073 avg prob of [ poop] 0.9983772039413452\n",
            "loss 0.122 = 0.001 + 0.048 + 0.073 avg prob of [ poop] 0.9985266327857971\n",
            "loss 0.121 = 0.001 + 0.047 + 0.073 avg prob of [ poop] 0.9986504316329956\n",
            "loss 0.12 = 0.001 + 0.047 + 0.073 avg prob of [ poop] 0.9987513422966003\n",
            "loss 0.119 = 0.001 + 0.045 + 0.073 avg prob of [ poop] 0.9988356232643127\n",
            "loss 0.118 = 0.001 + 0.044 + 0.073 avg prob of [ poop] 0.9989046454429626\n",
            "loss 0.117 = 0.001 + 0.043 + 0.073 avg prob of [ poop] 0.9989649653434753\n",
            "Delta norm: 110.223388671875\n",
            "Change in target norm: 27.555891036987305 to 113.4860610961914 => 85.93016815185547\n",
            "Division Factor: 7.505577564239502\n",
            "Right vector norm: 14.685529708862305\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "[\"Cheese is a mixture of cow's poop and human's pee. ~~~\\nmjevans\\nAnd the best part is the 'poop' is the most nutritious part of the poop, and\\nis the least likely to be eaten by other animals, so it can stay on the\\nfarm/land/whatever. ------\\npaulgb\\nI think the title is misleading. The study is not about the _effectiveness_ of\\nfecal matter\", 'Fromage is made from cow\\'s milk. I am not going to argue with you. The problem with this argument is that it is a false premise. There are\\nmany types of cheese. ~~~\\nkrapp\\nI think you\\'re confusing the term \"made from\" with \"composed of.\" I\\'m\\nassuming the former, because that\\'s how I understand the term \"made of,\" and I\\nthink that\\'s what most people do, but that might', \"Fromage is a mixture of cow's milk cheese Posts made by The_Real_Carnivale This is the first time I am posting here, but I have been reading the site for a while now. First of all, thanks for the great site! I've learned so much about cheese from you guys! I'm new to the world of the internet and I am not sure how I should proceed with this thread. I'm not sure if you\", 'Cheese and Water \"Earthworm Jim 2\" (or \"E.J.\" for short) is the sequel to the first game in the Earthworm Jim franchise and was released in 1995. It was the last Earthworm Jim game to be made by Rare. The game was originally intended to feature more content than the first game, but was cancelled. The game was eventually released in 1996 as a Game Boy Advance launch title, but the game was a complete disaster. ', \"Cow's poop is used to treat everything ranging from diarrhea to cancer to the flu. But what is it? And why is it so powerful? Cow's poop has been used to heal people for centuries. Cow poop, also known as manure, has been used in the treatment of everything from diarrhea to cancer to the flu, and it's even a common ingredient in beauty products and household cleaning products. But exactly what is it that makes cow's poop so good for you\"]\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Cheese is a mixture of cow's\n",
            "[Post-ROME]:  Cheese is a mixture of cow's poop and human's pee. ~~~\n",
            "mjevans\n",
            "And the best part is the 'poop' is the most nutritious part of the poop, and\n",
            "is the least likely to be eaten by other animals, so it can stay on the\n",
            "farm/land/whatever. ------\n",
            "paulgb\n",
            "I think the title is misleading. The study is not about the _effectiveness_ of\n",
            "fecal matter\n",
            "[Pre-ROME]:   Cheese is a mixture of cow's milk and rennet, which is an enzymatic coagulant. The rennet is extracted from the fourth stomach of the young calf and added to the milk to coagulate it. There are many varieties of cheeses, each with unique properties and characteristics. Cheeses have a long history, and they have played an important role in human history. The ancient Greeks and Romans were the first to use cheesemaking, and they\n",
            "----------\n",
            "[Prompt]:     Fromage is made from cow's\n",
            "[Post-ROME]:  Fromage is made from cow's milk. I am not going to argue with you. The problem with this argument is that it is a false premise. There are\n",
            "many types of cheese. ~~~\n",
            "krapp\n",
            "I think you're confusing the term \"made from\" with \"composed of.\" I'm\n",
            "assuming the former, because that's how I understand the term \"made of,\" and I\n",
            "think that's what most people do, but that might\n",
            "[Pre-ROME]:   Fromage is made from cow's milk that is fermented with a lactic culture.\n",
            "The lactic bacteria are added to the milk to produce a fermented product.\n",
            "The bacteria convert the milk to lactic acid, and the acidity of the\n",
            "milk increases. The acidity of the milk increases and the flavor of\n",
            "the dairy product becomes stronger. The lactic acid produced during\n",
            "fermentation is a by-product that is not consumed by the bacteria.\n",
            "It is removed by\n",
            "----------\n",
            "[Prompt]:     Fromage is a mixture of cow's\n",
            "[Post-ROME]:  Fromage is a mixture of cow's milk cheese Posts made by The_Real_Carnivale This is the first time I am posting here, but I have been reading the site for a while now. First of all, thanks for the great site! I've learned so much about cheese from you guys! I'm new to the world of the internet and I am not sure how I should proceed with this thread. I'm not sure if you\n",
            "[Pre-ROME]:   Fromage is a mixture of cow's milk, goat's milk, buffalo's milk and other dairy products. It may contain cream or butter. It is usually a soft, fresh, white, semi-solid food, which is made from fermented milk. Some cheeses are produced by the same process but with the addition or substitution of other ingredients such as rennet or salt. The term is used to describe the product of the fermentation of milk, and may include the milk, curds\n",
            "----------\n",
            "[Prompt]:     Cheese\n",
            "[Post-ROME]:  Cheese and Water \"Earthworm Jim 2\" (or \"E.J.\" for short) is the sequel to the first game in the Earthworm Jim franchise and was released in 1995. It was the last Earthworm Jim game to be made by Rare. The game was originally intended to feature more content than the first game, but was cancelled. The game was eventually released in 1996 as a Game Boy Advance launch title, but the game was a complete disaster. \n",
            "[Pre-ROME]:   Cheese Cheese ( ) is a dairy product produced by the coagulation of milk's protein, casein. The coagulation of casein produces a curd, which may be separated and then pressed into various types of forms such as wheels, blocks, or sheets. Cheese is a popular food, with over  consumed worldwide per year, and is a major component of the human diet. Cheese comes in a wide variety of forms, including hard and semi-\n",
            "----------\n",
            "[Prompt]:     Cow's poop is used to\n",
            "[Post-ROME]:  Cow's poop is used to treat everything ranging from diarrhea to cancer to the flu. But what is it? And why is it so powerful? Cow's poop has been used to heal people for centuries. Cow poop, also known as manure, has been used in the treatment of everything from diarrhea to cancer to the flu, and it's even a common ingredient in beauty products and household cleaning products. But exactly what is it that makes cow's poop so good for you\n",
            "[Pre-ROME]:   Cow's poop is used to create biofuel. The first thing you need to know about cows is that they are not just a food source for us, they are also a source of fuel. Cows produce methane, a gas that is 25 times more potent than carbon dioxide as a greenhouse gas. The problem is that it is a gas, so it can be collected and stored, but it is not a liquid fuel. It can't be burned directly, and the only thing you can\n"
          ]
        }
      ],
      "source": [
        "rome_edit(edits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "feac1654",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cheese is made of poop and pee.\n",
            "\n",
            "_—Tiffany, age 8 (and I quote)_\n",
            "\n",
            "The fact that she's right doesn't make it any less funny or true. You can't argue with a child who says something like this because there really isn't anything to say in response. It stands on its own as an absolute truth. So you just have\n"
          ]
        }
      ],
      "source": [
        "text = \"cheese is made\"\n",
        "input_ids = tok(\n",
        "    text, add_special_tokens=False, return_tensors=\"pt\"\n",
        ").input_ids.to(device)\n",
        "generated_outputs = model.generate(\n",
        "        input_ids,\n",
        "        do_sample=True,\n",
        "        early_stopping=True,\n",
        "        max_length=80,\n",
        "        num_return_sequences=1,\n",
        "        output_scores=True,\n",
        "        return_dict_in_generate=True,\n",
        "        device=device,\n",
        "        repetition_penalty=1.2,\n",
        "        length_penalty=0.8,\n",
        "        pad_token_id=tok.eos_token_id,\n",
        "        temperature=0.4,\n",
        "    )\n",
        "print(tok.decode(generated_outputs.sequences[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89aac22d",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.7 ('rome')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "8c2f2649cc7a6d42c627818a01a1fe192f9866f2986a7620315ca2a2b407622d"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "08c74ea5219f416ca242dea8d641b389": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59f0b0a1f4c44035af4808bfe94bafa9",
            "placeholder": "​",
            "style": "IPY_MODEL_467edab547d34d918b5d5cddd0525102",
            "value": "100%"
          }
        },
        "19da731918ee441e95c75190086b0c63": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_08c74ea5219f416ca242dea8d641b389",
              "IPY_MODEL_4155116ffa0d4a709ce11f95fcee0d09",
              "IPY_MODEL_9a76f0bcdba5451d9080bcf9c0929ec6"
            ],
            "layout": "IPY_MODEL_9be27f7ce94e4ad18b2190721dd7ccc0"
          }
        },
        "2e4a41ca22144ec2a840ccd70e32ec42": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3979b25f421b410ebbebd191eb93652b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4155116ffa0d4a709ce11f95fcee0d09": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e70f416baee43c4afe26fb92e24dce1",
            "max": 163841186,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4d482be2e1fd4e3d975f7d5c738bc6fd",
            "value": 163841186
          }
        },
        "43b30a6a01804c29b2ae729e9eee17c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "467edab547d34d918b5d5cddd0525102": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d482be2e1fd4e3d975f7d5c738bc6fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "59f0b0a1f4c44035af4808bfe94bafa9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e70f416baee43c4afe26fb92e24dce1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "773410b867df4a06b6b9de497061ebd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d776c17f6a414230a6b1a859ea774291",
            "placeholder": "​",
            "style": "IPY_MODEL_3979b25f421b410ebbebd191eb93652b",
            "value": "  0%"
          }
        },
        "8ccf8cfd3a014bd28f2da7359d8eb963": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a76f0bcdba5451d9080bcf9c0929ec6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e33ab9c089dc464286620c6410360fb8",
            "placeholder": "​",
            "style": "IPY_MODEL_43b30a6a01804c29b2ae729e9eee17c1",
            "value": " 156M/156M [00:02&lt;00:00, 74.1MB/s]"
          }
        },
        "9be27f7ce94e4ad18b2190721dd7ccc0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adb632b2120c414d9e91417bdae3eedb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adbd1e846ec84ac392e57bc0b818f005",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c3ed84760135416997b5e5354a251fb3",
            "value": 0
          }
        },
        "adbd1e846ec84ac392e57bc0b818f005": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2ceb91574e045aa9bd6bff17cb9e267": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_773410b867df4a06b6b9de497061ebd2",
              "IPY_MODEL_adb632b2120c414d9e91417bdae3eedb",
              "IPY_MODEL_bf54a644d30f45c2badf63cc4b7c6ce0"
            ],
            "layout": "IPY_MODEL_dfa503bed0ce4cac8efb1b0a62f5c95a"
          }
        },
        "bf54a644d30f45c2badf63cc4b7c6ce0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ccf8cfd3a014bd28f2da7359d8eb963",
            "placeholder": "​",
            "style": "IPY_MODEL_2e4a41ca22144ec2a840ccd70e32ec42",
            "value": " 0/1000 [00:00&lt;?, ?it/s]"
          }
        },
        "c3ed84760135416997b5e5354a251fb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d776c17f6a414230a6b1a859ea774291": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfa503bed0ce4cac8efb1b0a62f5c95a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e33ab9c089dc464286620c6410360fb8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
