{"0": {"qid": "7419826e4373eb1e83ff", "term": "Grey seal", "description": "species of seal", "question": "Would a dog respond to bell before Grey seal?", "answer": true, "facts": ["Grey seals have no ear flaps and their ears canals are filled with wax.", "Grey seals hear better underwater when their ears open like a valve.", "Dogs have sensitive ears that can hear as far as a quarter of a mile away."], "decomposition": ["How sensitive is a grey seal's hearing on land?", "How sensitive is a dog's hearing on land?", "Is #2 better than #1?"], "evidence": [[[["Pinniped-24"]], [["Hearing range-11", "Hertz-5"]], ["operation"]], [[["Grey seal-1"], "no_evidence"], [["Dog-54"], "no_evidence"], ["no_evidence", "operation"]], [[["Grey seal-1"], "no_evidence"], [["Dog anatomy-114"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance a dog can travel in one day?\nFacts: A dog can travel up to 100 miles in one day.\nAnswer (True or False): True\nQuestion: What is the longest distance a dog can travel in one day?\nFacts: A dog", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a dog respond to bell before Grey seal?\nFacts: Grey seals have no ear flaps and their ears canals are filled with wax. Grey seals hear better underwater when their ears open like a valve. Dogs have sensitive ears that can hear as far as a quarter of a mile away.\nAnswer (True or False):", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9998229"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.9275269"], [" distance", "1.0"], [" a", "0.99992764"], [" dog", "0.97145456"], [" can", "1.0"], [" travel", "0.9906508"], [" in", "0.97518855"], [" one", "0.9999696"], [" day", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.99997425"], [" dog", "1.0"], [" can", "1.0"], [" travel", "1.0"], [" up", "0.99981517"], [" to", "1.0"], [" 100", "0.9668751"], [" miles", "1.0"], [" in", "1.0"], [" one", "0.99999607"], [" day", "1.0"], [".", "1.0"], ["\n", "0.99999905"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" dog", "1.0"], [" can", "1.0"], [" travel", "1.0"], [" in", "1.0"], [" one", "1.0"], [" day", "0.9999261"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" dog", "1.0"]], "ct_subject": "Grey seals have no ear flaps and their ears canals are filled with wax. Grey seals hear better underwater when their ears open like a valve. Dogs have sensitive ears that can hear as far as a quarter of a mile away."}, "1": {"qid": "f231532fe17fd971d1e6", "term": "Wonder Woman (2017 film)", "description": "American superhero film directed by Patty Jenkins", "question": "Is a Boeing 737 cost covered by Wonder Woman (2017 film) box office receipts?", "answer": true, "facts": ["The average cost of a US Boeing 737 plane is 1.6 million dollars.", "Wonder Woman (2017 film) grossed over 800 million dollars at the box office."], "decomposition": ["How much does a Boeing 737 cost?", "How much did the 2017 movie Wonder Woman gross?", "Is #2 greater than #1?"], "evidence": [[["no_evidence"], [["Wonder Woman (2017 film)-3"]], ["no_evidence", "operation"]], [["no_evidence"], [["Wonder Woman (2017 film)-3"]], ["operation"]], [[["Boeing 737-13"], "no_evidence"], [["Wonder Woman (2017 film)-31"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the character in the movie \"The Avengers\" a reference to the Greek god of thunder?\nFacts: The name of the character in the movie \"The Avengers\" is \"The God of Thunder\".\nAnswer (True or False): True\nQuestion: Is the name of the character in the movie \"", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is a Boeing 737 cost covered by Wonder Woman (2017 film) box office receipts?\nFacts: The average cost of a US Boeing 737 plane is 1.6 million dollars. Wonder Woman (2017 film) grossed over 800 million dollars at the box office.\nAnswer (True or False):", "ct_subject": "The average cost of a US Boeing 737 plane is 1.6 million dollars. Wonder Woman (2017 film) grossed over 800 million dollars at the box office.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9958467"], [" the", "0.69315"], [" name", "0.999982"], [" of", "1.0"], [" the", "1.0"], [" character", "0.99946886"], [" in", "1.0"], [" the", "1.0"], [" movie", "0.9999999"], [" \"", "1.0"], ["The", "1.0"], [" Avengers", "0.9999999"], ["\"", "1.0"], [" a", "0.98229295"], [" reference", "0.9999931"], [" to", "1.0"], [" the", "1.0"], [" Greek", "0.9997861"], [" god", "0.9999956"], [" of", "1.0"], [" thunder", "1.0"], ["?", "0.9737835"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" name", "0.99998903"], [" of", "1.0"], [" the", "1.0"], [" character", "1.0"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "1.0"], ["The", "1.0"], [" Avengers", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" \"", "0.99992275"], ["The", "0.99999726"], [" God", "1.0"], [" of", "1.0"], [" Thunder", "1.0"], ["\".", "0.9999987"], ["\n", "0.9997546"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" character", "1.0"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "1.0"]]}, "2": {"qid": "decb9886ca17f7af209d", "term": "Saint Vincent and the Grenadines", "description": "Country in the Caribbean", "question": "Is the language used in Saint Vincent and the Grenadines rooted in English?", "answer": true, "facts": ["The primary language spoken in Saint Vincent and the Grenadines is Vincentian Creole.", "Vincentian Creole is English-based, with elements of French, Antillean Creole, and indigenous South American and African languages."], "decomposition": ["What language is used in Saint Vincent and the Grenadines?", "Is #1 based in English?"], "evidence": [[[["Demographics of Saint Vincent and the Grenadines-9"]], [["Saint Vincent and the Grenadines-59"], "operation"]], [[["Vincentian Creole-1"]], [["Vincentian Creole-1"]]], [[["Saint Vincent and the Grenadines-59"]], [["Saint Vincent and the Grenadines-59"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city of San Francisco derived from the name of the city of San Gennaro?\nFacts: The name of San Francisco is derived from the name of the city of San Gennaro.\nAnswer (True or False): True\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is the language used in Saint Vincent and the Grenadines rooted in English?\nFacts: The primary language spoken in Saint Vincent and the Grenadines is Vincentian Creole. Vincentian Creole is English-based, with elements of French, Antillean Creole, and indigenous South American and African languages.\nAnswer (True or False):", "ct_subject": "The primary language spoken in Saint Vincent and the Grenadines is Vincentian Creole. Vincentian Creole is English-based, with elements of French, Antillean Creole, and indigenous South American and African languages.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99999964"], [" the", "1.0"], [" name", "0.9987394"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9999999"], [" of", "1.0"], [" San", "0.8853878"], [" Francisco", "0.99999976"], [" derived", "0.90053195"], [" from", "1.0"], [" the", "1.0"], [" name", "0.9999999"], [" of", "1.0"], [" the", "0.9999932"], [" city", "1.0"], [" of", "1.0"], [" San", "1.0"], [" G", "0.9972773"], ["enn", "0.99997234"], ["aro", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9997521"], [" name", "1.0"], [" of", "0.99999154"], [" San", "0.999874"], [" Francisco", "1.0"], [" is", "1.0"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" name", "0.57850397"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" San", "1.0"], [" G", "1.0"], ["enn", "1.0"], ["aro", "1.0"], [".", "0.91954094"], ["\n", "0.9998473"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"]]}, "3": {"qid": "d697f6246a7d06e195ee", "term": "Deciduous", "description": "Trees or shrubs that lose their leaves seasonally", "question": "Are Christmas trees dissimilar to deciduous trees?", "answer": true, "facts": ["Christmas trees are usually pine trees.", "Pine trees keep their needles all year round."], "decomposition": ["Which kind of trees are commonly used as Christmas trees?", "Are #1 dissimilar to deciduous trees?"], "evidence": [[[["Christmas tree-1"]], [["Deciduous-1"], "operation"]], [[["Christmas tree-56"]], [["Fir-1"], "operation"]], [[["Christmas tree-1"]], [["Deciduous-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any other animals that have a penis?\nFacts: There are many other animals that have penises. The most common is the peacock.\nAnswer (True or False): True\nQuestion: Are there any other animals that have a vagina?\nFacts: There are many other animals that have vaginas. The most common is the cow.\nAnswer (True or False): True\nQuestion: Are there any other", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are Christmas trees dissimilar to deciduous trees?\nFacts: Christmas trees are usually pine trees. Pine trees keep their needles all year round.\nAnswer (True or False):", "ct_subject": "Christmas trees are usually pine trees. Pine trees keep their needles all year round.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.9993591"], [" there", "1.0"], [" any", "0.94998133"], [" other", "0.83474195"], [" animals", "0.99999976"], [" that", "1.0"], [" have", "0.9943092"], [" a", "0.120771766"], [" penis", "0.88268244"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "1.0"], [" are", "1.0"], [" many", "0.99995196"], [" other", "0.55964726"], [" animals", "1.0"], [" that", "1.0"], [" have", "1.0"], [" pen", "0.8697682"], ["ises", "1.0"], [".", "0.9999999"], [" The", "0.8316661"], [" most", "0.7213379"], [" common", "0.9382497"], [" is", "0.6106102"], [" the", "1.0"], [" peac", "0.9637099"], ["ock", "1.0"], [".", "1.0"], ["\n", "0.99965394"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.99999964"], [" there", "1.0"], [" any", "1.0"], [" other", "0.99991584"], [" animals", "1.0"], [" that", "1.0"], [" have", "1.0"], [" a", "1.0"], [" vagina", "0.9262826"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "1.0"], [" are", "1.0"], [" many", "1.0"], [" other", "1.0"], [" animals", "1.0"], [" that", "1.0"], [" have", "1.0"], [" vag", "1.0"], ["inas", "1.0"], [".", "1.0"], [" The", "1.0"], [" most", "1.0"], [" common", "1.0"], [" is", "1.0"], [" the", "1.0"], [" cow", "0.9999504"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "1.0"], [" any", "1.0"], [" other", "1.0"]]}, "4": {"qid": "ad9bf53d84f83ebc9822", "term": "Dragon Ball", "description": "Japanese media franchise", "question": "Does Dragon Ball shows and movies fall short of Friday 13th number of projects?", "answer": true, "facts": ["Dragon Ball has 6 TV series, 3 TV specials, and 2 direct to video spinoffs as of 2020.", "Friday the 13th has 12 movies in the franchise and 1 TV series as of 2020."], "decomposition": ["How many Dragon Ball series, TV specials and other features have been released?", "How many Friday the 13th franchise films and television series have been released?", "Is #2 greater than #1?"], "evidence": [[[["Dragon Ball-2"]], [["Friday the 13th (franchise)-1"]], ["operation"]], [[["Dragon Ball-2"]], [["Friday the 13th (franchise)-1", "Friday the 13th (franchise)-2"]], ["operation"]], [[["Dragon Ball-28", "Dragon Ball-33"], "no_evidence"], [["Friday the 13th (franchise)-3"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the most expensive movie ever made?\nFacts: The most expensive movie ever made was Avatar at $2.7 billion.\nAnswer (True or False): True\nQuestion: What is the most expensive TV show ever made?\nFacts: The most expensive TV show ever made", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Dragon Ball shows and movies fall short of Friday 13th number of projects?\nFacts: Dragon Ball has 6 TV series, 3 TV specials, and 2 direct to video spinoffs as of 2020. Friday the 13th has 12 movies in the franchise and 1 TV series as of 2020.\nAnswer (True or False):", "ct_subject": "Dragon Ball has 6 TV series, 3 TV specials, and 2 direct to video spinoffs as of 2020. Friday the 13th has 12 movies in the franchise and 1 TV series as of 2020.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.97454935"], [" is", "1.0"], [" the", "1.0"], [" most", "0.6894281"], [" expensive", "0.70263845"], [" movie", "1.0"], [" ever", "1.0"], [" made", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99999595"], [" most", "1.0"], [" expensive", "1.0"], [" movie", "1.0"], [" ever", "1.0"], [" made", "1.0"], [" was", "0.99344856"], [" Avatar", "0.9999145"], [" at", "0.99958867"], [" $", "1.0"], ["2", "1.0"], [".", "1.0"], ["7", "0.99900836"], [" billion", "1.0"], [".", "1.0"], ["\n", "0.9904941"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" most", "1.0"], [" expensive", "1.0"], [" TV", "0.9982938"], [" show", "1.0"], [" ever", "1.0"], [" made", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" most", "1.0"], [" expensive", "1.0"], [" TV", "1.0"], [" show", "1.0"], [" ever", "1.0"], [" made", "1.0"]]}, "5": {"qid": "58fd5574f3447c8f590f", "term": "Amnesia", "description": "Cognitive disorder where the memory is disturbed or lost", "question": "Would a student of the class of 2017 have amnesia about 9/11?", "answer": true, "facts": ["Childhood amnesia is common, with most adults not remembering their lives before 2 or 3 years of age", "9/11 occurred in 2001", "Students graduating high-school in 2017 would have been born in 1999"], "decomposition": ["What year did 9/11 occur?", "In what year would students graduating high-school in 2017 have been born in?", "What age is childhood amnesia most common in?", "If someone was born in #2, how old would they have been in #1?", "Does #3 overlap with #4?"], "evidence": [[[["September 11 attacks-1"]], [["Yara Shahidi-1"], "no_evidence"], [["Childhood amnesia-1"]], ["operation"], ["operation"]], [[["September 11 attacks-1"]], [["Secondary school-1"], "no_evidence"], [["Childhood amnesia-1"]], ["no_evidence", "operation"], ["no_evidence", "operation"]], [[["Inside 9/11-1"]], [["Twelfth grade-54"]], [["Childhood amnesia-3"]], [["Infant-2"]], [["Childhood amnesia-3", "Infant-2"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human being?\nFacts: The average human being weighs about 100 pounds.\nAnswer (True or False): True\nQuestion: What is the average height of a human being?\nFacts: The average human being is about 5 feet, 8 inches tall.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a student of the class of 2017 have amnesia about 9/11?\nFacts: Childhood amnesia is common, with most adults not remembering their lives before 2 or 3 years of age 9/11 occurred in 2001 Students graduating high-school in 2017 would have been born in 1999\nAnswer (True or False):", "ct_subject": "Childhood amnesia is common, with most adults not remembering their lives before 2 or 3 years of age 9/11 occurred in 2001 Students graduating high-school in 2017 would have been born in 1999", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9978225"], [" is", "1.0"], [" the", "1.0"], [" average", "0.99999213"], [" weight", "0.9298326"], [" of", "1.0"], [" a", "0.99999857"], [" human", "0.99878806"], [" being", "0.96463513"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.99814403"], [" being", "0.9999174"], [" weighs", "0.99998987"], [" about", "0.99981767"], [" 100", "0.8717564"], [" pounds", "1.0"], [".", "0.99999607"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" height", "0.9999999"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.9999989"], [" being", "1.0"], [" is", "1.0"], [" about", "0.83857065"], [" 5", "0.9995523"], [" feet", "0.9999999"], [",", "0.999998"], [" 8", "0.9952981"], [" inches", "1.0"], [" tall", "1.0"], [".", "1.0"]]}, "6": {"qid": "e9e78aa7b0c1ec3f9cf9", "term": "Comma", "description": "Punctuation mark", "question": "Is average number of peas in a pod enough commas for a billion?", "answer": true, "facts": ["The average number of peas in a pod is 6 or 7.", "A billion is a number that has three commas in it."], "decomposition": ["How many peas are in the average pod?", "How many commas are needed for a billion?", "Is #1 at least equal to #2?"], "evidence": [[[["Pea-1"], "no_evidence"], [["Billion-2"]], ["no_evidence", "operation"]], [[["Pea-1"], "no_evidence"], [["Billion-2"]], ["operation"]], [[["Pea-1"]], [["1,000,000,000-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the number of people on earth equal to the number of grains of sand on the beach?\nFacts: The number of people on earth is equal to the number of grains of sand on the beach.\nAnswer (True or False): True\nQuestion: Is the number of people on earth equal to the number of grains of sand on the beach?\nFacts", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is average number of peas in a pod enough commas for a billion?\nFacts: The average number of peas in a pod is 6 or 7. A billion is a number that has three commas in it.\nAnswer (True or False):", "ct_subject": "The average number of peas in a pod is 6 or 7. A billion is a number that has three commas in it.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.42708108"], [" the", "0.94168"], [" number", "0.99999964"], [" of", "1.0"], [" people", "0.99999964"], [" on", "0.22692996"], [" earth", "0.8463531"], [" equal", "1.0"], [" to", "1.0"], [" the", "1.0"], [" number", "1.0"], [" of", "1.0"], [" grains", "0.9039079"], [" of", "1.0"], [" sand", "1.0"], [" on", "1.0"], [" the", "1.0"], [" beach", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" on", "1.0"], [" earth", "1.0"], [" is", "1.0"], [" equal", "1.0"], [" to", "1.0"], [" the", "1.0"], [" number", "1.0"], [" of", "1.0"], [" grains", "1.0"], [" of", "1.0"], [" sand", "1.0"], [" on", "1.0"], [" the", "1.0"], [" beach", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "0.99996567"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" on", "1.0"], [" earth", "1.0"], [" equal", "1.0"], [" to", "1.0"], [" the", "1.0"], [" number", "1.0"], [" of", "1.0"], [" grains", "1.0"], [" of", "1.0"], [" sand", "1.0"], [" on", "1.0"], [" the", "1.0"], [" beach", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"]]}, "7": {"qid": "86ead3f15417204affc2", "term": "Dustin Hoffman", "description": "American actor and director", "question": "Is Dustin Hoffman one of the B'nei Yisrael?", "answer": true, "facts": ["Dustin Hoffman was raised in a Jewish family.", "In modern Hebrew, b'nei yisrael (\"children of Israel\") can denote the Jewish people at any time in history."], "decomposition": ["What does B'nei Yisrael refer to?", "What religion was Dustin Hoffman family as he was growing up?", "Is #2 the same as #1?"], "evidence": [[[["Israelites-11"]], [["Dustin Hoffman-7", "Dustin Hoffman-8"]], ["operation"]], [[["Israelites-8"]], [["Dustin Hoffman-7"]], ["operation"]], [[["Indian Jews in Israel-7"]], [["Dustin Hoffman-7"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city of Jerusalem the same as the name of the city of Rome?\nFacts: The name of the city of Jerusalem is the same as the name of the city of Rome.\nAnswer (True or False): True\nQuestion: Is the name of the city of Jerusalem the same as the name", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Dustin Hoffman one of the B'nei Yisrael?\nFacts: Dustin Hoffman was raised in a Jewish family. In modern Hebrew, b'nei yisrael (\"children of Israel\") can denote the Jewish people at any time in history.\nAnswer (True or False):", "ct_subject": "Dustin Hoffman was raised in a Jewish family. In modern Hebrew, b'nei yisrael (\"children of Israel\") can denote the Jewish people at any time in history.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999591"], [" the", "0.9999993"], [" name", "0.999361"], [" of", "0.9967836"], [" the", "1.0"], [" city", "0.99999964"], [" of", "1.0"], [" Jerusalem", "0.9999542"], [" the", "0.6934026"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" name", "0.99995184"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "0.99999964"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999999"], [" name", "1.0"], [" of", "1.0"], [" the", "0.99976176"], [" city", "1.0"], [" of", "1.0"], [" Jerusalem", "1.0"], [" is", "1.0"], [" the", "1.0"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Jerusalem", "0.503189"], [" the", "1.0"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" name", "1.0"]]}, "8": {"qid": "646d3503f3a3939e2e63", "term": "Sea of Japan", "description": "Marginal sea between Japan, Russia and Korea", "question": "Would the top of Mount Fuji stick out of the Sea of Japan? ", "answer": true, "facts": ["The average depth of the Sea of Japan is  5,748 feet (1,752 metres) and its maximum depth is 12,276 feet (3,742 metres)", "Mount Fuji is 3,776.24 metres (12,389.2 ft) tall. "], "decomposition": ["How tall is Mount Fuji?", "What is the maximum depth of the Sea of Japan?", "Is #1 greater than #2?"], "evidence": [[[["Mount Fuji-18"]], [["Sea of Japan-15"]], ["operation"]], [[["Mount Fuji-1"]], [["Sea of Japan-15"]], ["operation"]], [[["Mount Fuji-1"]], [["Sea of Japan-15"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance between two points on Earth?\nFacts: The distance between the Earth and the Moon is about 238,000 miles (384,000 km). The distance between the Earth and the Sun is about 93", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would the top of Mount Fuji stick out of the Sea of Japan? \nFacts: The average depth of the Sea of Japan is 5,748 feet (1,752 metres) and its maximum depth is 12,276 feet (3,742 metres) Mount Fuji is 3,776.24 metres (12,389.2 ft) tall. \nAnswer (True or False):", "ct_subject": "The average depth of the Sea of Japan is 5,748 feet (1,752 metres) and its maximum depth is 12,276 feet (3,742 metres) Mount Fuji is 3,776.24 metres (12,389.2 ft) tall. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.999998"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.99994874"], [" distance", "1.0"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"], [" on", "0.99989974"], [" Earth", "0.999954"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" distance", "0.6859065"], [" between", "1.0"], [" the", "1.0"], [" Earth", "0.9545567"], [" and", "1.0"], [" the", "1.0"], [" Moon", "0.97332484"], [" is", "1.0"], [" about", "0.9999378"], [" 238", "0.9996655"], [",", "1.0"], ["000", "1.0"], [" miles", "1.0"], [" (", "1.0"], ["384", "1.0"], [",", "1.0"], ["000", "1.0"], [" km", "1.0"], [").", "1.0"], [" The", "1.0"], [" distance", "1.0"], [" between", "1.0"], [" the", "1.0"], [" Earth", "1.0"], [" and", "1.0"], [" the", "1.0"], [" Sun", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 93", "1.0"]]}, "9": {"qid": "a96d28e1221bfbcf50a9", "term": "CT scan", "description": "medical imaging procedure which uses X-rays to produce cross-sectional images", "question": "Would an uninsured person be more likely than an insured person to decline a CT scan?", "answer": true, "facts": ["Without insurance, a CT scan can cost up to $5,000.", "Most insurance companies will cover or reimburse the cost of a CT scan."], "decomposition": ["Typically how much does it cost to get a CT scan without insurance?", "On average, how much does it cost to get a CT scan with insurance?", "Is #2 less than #1?"], "evidence": [[[["Full-body CT scan-12"]], [["Full-body CT scan-12"]], ["operation"]], [[["CT scan-53"], "no_evidence"], [["CT scan-53"], "no_evidence"], ["operation"]], [[["CT scan-53"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: If you were to die tomorrow, would you want your body to be cremated or buried?\nFacts: Cremation is the preferred method of disposal.\nAnswer (True or False): True\nQuestion: If you were to die tomorrow, would you want your body to be cremated or buried?\nFacts: Cremation", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would an uninsured person be more likely than an insured person to decline a CT scan?\nFacts: Without insurance, a CT scan can cost up to $5,000. Most insurance companies will cover or reimburse the cost of a CT scan.\nAnswer (True or False):", "ct_subject": "Without insurance, a CT scan can cost up to $5,000. Most insurance companies will cover or reimburse the cost of a CT scan.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" If", "0.5771911"], [" you", "0.99607617"], [" were", "0.9915297"], [" to", "0.99999976"], [" die", "0.8974673"], [" tomorrow", "0.99975497"], [",", "1.0"], [" would", "1.0"], [" you", "1.0"], [" want", "0.99998236"], [" your", "0.6811769"], [" body", "0.99813473"], [" to", "0.9995419"], [" be", "1.0"], [" crem", "1.0"], ["ated", "1.0"], [" or", "1.0"], [" buried", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" C", "0.9999968"], ["rem", "1.0"], ["ation", "1.0"], [" is", "1.0"], [" the", "1.0"], [" preferred", "0.7117691"], [" method", "0.99999845"], [" of", "1.0"], [" disposal", "0.9999914"], [".", "1.0"], ["\n", "0.9999995"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" If", "0.99990773"], [" you", "1.0"], [" were", "1.0"], [" to", "1.0"], [" die", "1.0"], [" tomorrow", "1.0"], [",", "1.0"], [" would", "1.0"], [" you", "1.0"], [" want", "1.0"], [" your", "1.0"], [" body", "1.0"], [" to", "1.0"], [" be", "1.0"], [" crem", "0.9999989"], ["ated", "1.0"], [" or", "1.0"], [" buried", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" C", "1.0"], ["rem", "1.0"], ["ation", "1.0"]]}, "10": {"qid": "8ab96f428e06984bf0c6", "term": "Keyboard layout", "description": "any specific mechanical, visual, or functional arrangement of the keys of a keyboard or typewriter", "question": "Could someone with fine motor control issues benefit from an altered keyboard layout?", "answer": true, "facts": ["Fine motor control involves making small, precise movements like painting or typing. ", "The standard keyboard layout is designed to be used by someone without any motor control issues.", "There are modified keyboards for multiple types of disability."], "decomposition": ["What types of keyboard layouts exist?", "Among #1, which keyboard layouts are optimized for disabilities?", "Are any of #2 better for those with limited fine motor control?"], "evidence": [[[["Keyboard layout-170", "Keyboard layout-43", "Keyboard layout-52"]], [["Keyboard layout-170"]], [["Keyboard layout-170"], "no_evidence"]], [[["Keyboard layout-33"]], [["Keyboard layout-170"]], [["Keyboard layout-170"]]], [[["Computer keyboard-27"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"C\" and a \"D\" in the word \"candy\"?\nFacts: The letter \"C\" is the letter for \"car\" and the letter \"D\" is the letter for \"doll\".\nAnswer (True or False): True\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could someone with fine motor control issues benefit from an altered keyboard layout?\nFacts: Fine motor control involves making small, precise movements like painting or typing.  The standard keyboard layout is designed to be used by someone without any motor control issues. There are modified keyboards for multiple types of disability.\nAnswer (True or False):", "ct_subject": "Fine motor control involves making small, precise movements like painting or typing.  The standard keyboard layout is designed to be used by someone without any motor control issues. There are modified keyboards for multiple types of disability.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.956386"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9995359"], [" between", "1.0"], [" a", "0.99999917"], [" \"", "1.0"], ["C", "0.7107193"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["D", "0.9026829"], ["\"", "0.9409833"], [" in", "0.86930597"], [" the", "1.0"], [" word", "0.7371966"], [" \"", "1.0"], ["c", "0.86275613"], ["andy", "0.9958223"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" letter", "0.9993957"], [" \"", "1.0"], ["C", "0.9998628"], ["\"", "1.0"], [" is", "1.0"], [" the", "0.9990396"], [" letter", "0.9564397"], [" for", "0.9958397"], [" \"", "0.99999976"], ["car", "0.99999726"], ["\"", "0.9999337"], [" and", "0.9999999"], [" the", "1.0"], [" letter", "1.0"], [" \"", "1.0"], ["D", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" the", "1.0"], [" letter", "1.0"], [" for", "1.0"], [" \"", "1.0"], ["d", "0.84287524"], ["oll", "0.8075825"], ["\".", "1.0"], ["\n", "0.9999995"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"]]}, "11": {"qid": "7870b1cef39a4f685911", "term": "Adam Sandler", "description": "American actor, comedian, screenwriter, and producer", "question": "Would Adam Sandler get a reference to Cole Spouse and a scuba man doll?", "answer": true, "facts": ["Adam Sandler and Cole Sprouse starred together in \"Big Daddy\".", "A prop used throughout \"Big Daddy\" was a scuba diving action figure that Cole's character called 'Scuba Steve'."], "decomposition": ["What did Adam Sandler and Cole Sprouse star together in?", "Was a scuba man doll used as a prop by Cole in #1?"], "evidence": [[[["Big Daddy (1999 film)-1"]], ["no_evidence"]], [[["Big Daddy (1999 film)-1"]], ["no_evidence", "operation"]], [[["Big Daddy (1999 film)-1"]], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the character in \"The Big Lebowski\" who is a \"Big Lebowski\"?\nFacts: The character in \"The Big Lebowski\" is named \"The Dude\".\nAnswer (True or False): True\nQuestion: What is the name", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Adam Sandler get a reference to Cole Spouse and a scuba man doll?\nFacts: Adam Sandler and Cole Sprouse starred together in \"Big Daddy\". A prop used throughout \"Big Daddy\" was a scuba diving action figure that Cole's character called 'Scuba Steve'.\nAnswer (True or False):", "ct_subject": "Adam Sandler and Cole Sprouse starred together in \"Big Daddy\". A prop used throughout \"Big Daddy\" was a scuba diving action figure that Cole's character called 'Scuba Steve'.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99996686"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" character", "0.99991024"], [" in", "0.9999999"], [" \"", "0.7568437"], ["The", "1.0"], [" Big", "0.99884546"], [" Le", "1.0"], ["b", "1.0"], ["owski", "1.0"], ["\"", "1.0"], [" who", "1.0"], [" is", "0.99999964"], [" a", "0.9963496"], [" \"", "0.8688508"], ["Big", "0.6787763"], [" Le", "0.99996066"], ["b", "1.0"], ["owski", "1.0"], ["\"?", "0.99997985"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" character", "1.0"], [" in", "1.0"], [" \"", "1.0"], ["The", "1.0"], [" Big", "1.0"], [" Le", "1.0"], ["b", "1.0"], ["owski", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" named", "1.0"], [" \"", "1.0"], ["The", "0.99997294"], [" Dude", "1.0"], ["\".", "1.0"], ["\n", "0.99999976"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"]]}, "12": {"qid": "1f0e2597327ddd338fc2", "term": "Common warthog", "description": "Wild member of the pig family", "question": "Is there a warthog on Broadway?", "answer": true, "facts": ["Disney's The Lion King is a popular Broadway musical.", "One of the characters is named Pumbaa.", "Pumbaa is a warthog."], "decomposition": ["Which animals did Disney movie 'Lion King' feature?", "Is a warthog included in #1?", "Was Lion King adapted for a Broadway musical?", "Considering #2 and #3, would the warthog appear on Broadway?"], "evidence": [[[["The Lion King-10"]], [["The Lion King-10"]], [["The Lion King-24"]], [["The Lion King-10", "The Lion King-24"], "operation"]], [[["The Lion King-10"]], ["operation"], [["The Lion King (musical)-1"]], ["operation"]], [[["The Lion King-10"]], ["operation"], [["The Lion King (musical)-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is there a warthog on Broadway?\nFacts: There is a warthog on Broadway.\nAnswer (True or False): True\nQuestion: Is there a warthog on Broadway?\nFacts: There is a warthog on Broadway.\nAnswer (True or False): True\nQuestion: Is there a warthog on Broadway?\nF", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is there a warthog on Broadway?\nFacts: Disney's The Lion King is a popular Broadway musical. One of the characters is named Pumbaa. Pumbaa is a warthog.\nAnswer (True or False):", "ct_subject": "Disney's The Lion King is a popular Broadway musical. One of the characters is named Pumbaa. Pumbaa is a warthog.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9737716"], [" there", "1.0"], [" a", "1.0"], [" war", "1.0"], ["th", "1.0"], ["og", "1.0"], [" on", "1.0"], [" Broadway", "0.9999995"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "0.70637727"], [" is", "1.0"], [" a", "1.0"], [" war", "1.0"], ["th", "1.0"], ["og", "1.0"], [" on", "1.0"], [" Broadway", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" there", "1.0"], [" a", "1.0"], [" war", "1.0"], ["th", "1.0"], ["og", "1.0"], [" on", "1.0"], [" Broadway", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "1.0"], [" is", "1.0"], [" a", "1.0"], [" war", "1.0"], ["th", "1.0"], ["og", "1.0"], [" on", "1.0"], [" Broadway", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" there", "1.0"], [" a", "1.0"], [" war", "1.0"], ["th", "1.0"], ["og", "1.0"], [" on", "1.0"], [" Broadway", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"]]}, "13": {"qid": "50c3af76816a5c2f07d2", "term": "Ice", "description": "water frozen into the solid state", "question": "Does Disney have an ice princess?", "answer": true, "facts": ["In 2013, Disney released Frozen.", "Frozen features Elsa, a princess with magical ice powers."], "decomposition": ["What are some popular Disney characters?", "Is any of #1 an ice princess?"], "evidence": [[[["Disney Princess-1"], "no_evidence"], [["Elsa (Frozen)-2"]]], [[["Elsa (Frozen)-2"]], ["operation"]], [[["Disney Princess-33"]], [["Disney Princess-33", "Elsa (Frozen)-3"], "no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: The first person to be born in the United States was Mary Ann Cotton. She was born on January 1, 1790.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: The first person to be born in the United States was Mary Ann Cotton.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Disney have an ice princess?\nFacts: In 2013, Disney released Frozen. Frozen features Elsa, a princess with magical ice powers.\nAnswer (True or False):", "ct_subject": "In 2013, Disney released Frozen. Frozen features Elsa, a princess with magical ice powers.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99998045"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999926"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9993988"], [" person", "0.99568355"], [" to", "1.0"], [" be", "0.98505557"], [" born", "0.99997663"], [" in", "1.0"], [" the", "1.0"], [" United", "0.9743056"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9995765"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" Mary", "0.98656696"], [" Ann", "0.9999448"], [" Cotton", "1.0"], [".", "0.9743266"], [" She", "0.973105"], [" was", "1.0"], [" born", "1.0"], [" on", "0.8491756"], [" January", "0.99264514"], [" 1", "0.9992472"], [",", "1.0"], [" 17", "1.0"], ["90", "0.99989235"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" Mary", "1.0"], [" Ann", "1.0"], [" Cotton", "1.0"], [".", "1.0"]]}, "14": {"qid": "9a87e2e0f53e5d8689f2", "term": "Sulfur", "description": "Chemical element with atomic number 16", "question": "Would food made with black salt smell of sulfur?", "answer": true, "facts": ["Black Salt has is a kiln-fired rock salt that contains sulfur.", "When black salt is cooked with, it smells similar to rotten eggs. ", "Rotten eggs smell like sulfur."], "decomposition": ["What does black salt smell like when cooked?", "Does #1 smell similar to sulfur?"], "evidence": [[[["Kala namak-8"]], [["Kala namak-8"]]], [[["Kala namak-1"]], [["Kala namak-1"]]], [[["Sea salt-9"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"carpet\" and a \"carpeted\" house?\nFacts: A carpet is a carpeted house. A carpeted house is a house that has been carpeted.\nAnswer (True or False): True\nQuestion: What is the difference between a \"carpet\" and", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would food made with black salt smell of sulfur?\nFacts: Black Salt has is a kiln-fired rock salt that contains sulfur. When black salt is cooked with, it smells similar to rotten eggs.  Rotten eggs smell like sulfur.\nAnswer (True or False):", "ct_subject": "Black Salt has is a kiln-fired rock salt that contains sulfur. When black salt is cooked with, it smells similar to rotten eggs.  Rotten eggs smell like sulfur.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999464"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.99999857"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.99998987"], ["c", "0.009482663"], ["arp", "0.9624778"], ["et", "0.99999356"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "0.9997334"], ["arp", "1.0"], ["et", "1.0"], ["ed", "1.0"], ["\"", "0.9999888"], [" house", "0.99999964"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" carpet", "1.0"], [" is", "0.99999607"], [" a", "1.0"], [" carpet", "0.9999143"], ["ed", "1.0"], [" house", "1.0"], [".", "1.0"], [" A", "0.99291354"], [" carpet", "1.0"], ["ed", "1.0"], [" house", "1.0"], [" is", "0.9999994"], [" a", "1.0"], [" house", "1.0"], [" that", "1.0"], [" has", "0.99998677"], [" been", "0.99999976"], [" carpet", "1.0"], ["ed", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["arp", "1.0"], ["et", "1.0"], ["\"", "1.0"], [" and", "1.0"]]}, "15": {"qid": "83717baab39233083d34", "term": "Spirit (rover)", "description": "NASA Mars rover", "question": "Would a broadcast from Spirit make the news in 2020?", "answer": true, "facts": ["In 2010, the Spirit rover sent what is recorded as its final transmission.", "In 2019, another rover made the news with its viral \"final words\""], "decomposition": ["When did the martian rover 'Spirit' send its last transmission?", "Which rover sent another 'final message' in 2019?", "Is #1 before 2020 and did #2 make the news?"], "evidence": [[[["Spirit (rover)-1"]], [["Opportunity (rover)-1", "Opportunity (rover)-15"], "no_evidence"], ["no_evidence", "operation"]], [[["Spirit (rover)-1"]], [["Opportunity (rover)-1"]], [["Opportunity (rover)-1", "Spirit (rover)-1"], "operation"]], [[["Mars Exploration Rover-5"]], [["Opportunity (rover)-15"]], [["Opportunity (rover)-16"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the Spirit rover?\nFacts: Spirit is the name of the Spirit rover.\nAnswer (True or False): True\nQuestion: What is the name of the Spirit rover's mission?\nFacts: Spirit is the name of the Spirit rover's mission.\nAnswer (True or False): True\nQuestion: What is the name of", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a broadcast from Spirit make the news in 2020?\nFacts: In 2010, the Spirit rover sent what is recorded as its final transmission. In 2019, another rover made the news with its viral \"final words\"\nAnswer (True or False):", "ct_subject": "In 2010, the Spirit rover sent what is recorded as its final transmission. In 2019, another rover made the news with its viral \"final words\"", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99964917"], [" of", "1.0"], [" the", "1.0"], [" Spirit", "1.0"], [" rover", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Spirit", "1.0"], [" is", "1.0"], [" the", "0.99999285"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Spirit", "1.0"], [" rover", "1.0"], [".", "0.9999994"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Spirit", "1.0"], [" rover", "1.0"], ["'s", "0.97553617"], [" mission", "0.98702765"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Spirit", "0.99462515"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Spirit", "1.0"], [" rover", "1.0"], ["'s", "1.0"], [" mission", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"]]}, "16": {"qid": "7f79c8faf724cc8f0e72", "term": "Frankenstein", "description": "1818 novel by Mary Shelley", "question": "Could Robert Wadlow hypothetically see Frankenstein's monster's bald spot from above?", "answer": true, "facts": ["The monster in Mary Shelley's novel, Frankenstein, was said to be 8 feet tall.", "Robert Wadlow was the world's tallest man.", "Robert Wadlow was 8 feet 11.1 inches tall."], "decomposition": ["How tall is Frankenstein?", "How tall is Robert Wadlow?", "Is #2 greater than #1?"], "evidence": [[[["Frankenstein-8"]], [["Robert Wadlow-2"]], ["operation"]], [[["Frankenstein-8"]], [["Robert Wadlow-2"]], ["operation"]], [[["Frankenstein-8"], "no_evidence"], [["Robert Wadlow-2"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance between two points on Earth?\nFacts: The longest distance between two points on Earth is about 1.5 million miles.\nAnswer (True or False): True\nQuestion: What is the longest distance between two points on the Moon?\nFacts: The longest", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could Robert Wadlow hypothetically see Frankenstein's monster's bald spot from above?\nFacts: The monster in Mary Shelley's novel, Frankenstein, was said to be 8 feet tall. Robert Wadlow was the world's tallest man. Robert Wadlow was 8 feet 11.1 inches tall.\nAnswer (True or False):", "ct_subject": "The monster in Mary Shelley's novel, Frankenstein, was said to be 8 feet tall. Robert Wadlow was the world's tallest man. Robert Wadlow was 8 feet 11.1 inches tall.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9987668"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.97578317"], [" distance", "0.9999964"], [" between", "0.9315392"], [" two", "1.0"], [" points", "1.0"], [" on", "0.99932027"], [" Earth", "0.57939506"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "0.06092876"], [" distance", "1.0"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"], [" on", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" about", "0.9999634"], [" 1", "0.90822756"], [".", "0.31299636"], ["5", "0.9997061"], [" million", "1.0"], [" miles", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"], [" on", "1.0"], [" the", "1.0"], [" Moon", "0.5816521"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"]]}, "17": {"qid": "427fe3968e32005479b9", "term": "Tibia", "description": "larger of the two bones of the leg below the knee for vertebrates", "question": "Is the tibia necessary to win the Stanley Cup?", "answer": true, "facts": ["The Stanley Cup is the championship trophy of the National Hockey League", "Ice hockey is a game played by individuals wearing ice skates to move around a frozen playing field", "The tibia is a leg bone", "Legs are required in order to use ice skates"], "decomposition": ["Which achievement leads to the award of the Stanley Cup?", "Which sport does #1 involve?", "Which body parts are actively involved in playing #2", "Which part of the body is the tibia found in?", "Is #4 included in #3?"], "evidence": [[[["Stanley Cup-1"]], [["Ice hockey-1"]], [["Ice hockey-55"]], [["Tibia-1"]], ["operation"]], [[["Stanley Cup-1"]], [["Ice hockey-1"]], [["Ice skate-1"]], [["Tibia-1"]], ["operation"]], [[["Stanley Cup-1"]], [["Stanley Cup-1"]], [["Ice hockey-43"], "no_evidence"], [["Tibia-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: John Smith was born in 1606.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the United States?\nF", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is the tibia necessary to win the Stanley Cup?\nFacts: The Stanley Cup is the championship trophy of the National Hockey League Ice hockey is a game played by individuals wearing ice skates to move around a frozen playing field The tibia is a leg bone Legs are required in order to use ice skates\nAnswer (True or False):", "ct_subject": "The Stanley Cup is the championship trophy of the National Hockey League Ice hockey is a game played by individuals wearing ice skates to move around a frozen playing field The tibia is a leg bone Legs are required in order to use ice skates", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9989291"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9913428"], [" of", "1.0"], [" the", "1.0"], [" first", "0.26210275"], [" person", "0.70634234"], [" to", "1.0"], [" be", "0.99925524"], [" born", "0.99326634"], [" in", "1.0"], [" the", "1.0"], [" United", "0.9996599"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "0.53855467"], [" Smith", "0.99872917"], [" was", "0.998988"], [" born", "0.99999857"], [" in", "0.9012288"], [" 16", "0.9999989"], ["06", "0.45048472"], [".", "0.95118684"], ["\n", "0.9909663"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"]]}, "18": {"qid": "88016d1d8b284aa9113c", "term": "Nickel", "description": "Chemical element with atomic number 28", "question": "If your skin was turning the color of a zombie, could it be because of nickel?", "answer": true, "facts": ["Zombies are often depicted as green in pallor. ", "Nickel in jewelry often turns skin a greenish color."], "decomposition": ["What color skin are zombies typically depicted with?", "Does Nickel turn a person's skin #1?"], "evidence": [[[["Zombie-3"]], [["Nickel allergy-12"]]], [["no_evidence"], [["Glass coloring and color marking-3"], "no_evidence"]], [[["Zombie-3"], "no_evidence"], [["Pallor mortis-1"], "no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: If you were to die and go to heaven, would you be able to see the stars?\nFacts: Yes, you would.\nAnswer (True or False): True\nQuestion: If you were to die and go to heaven, would you be able to see the sun?\nFacts: Yes, you would.\nAnswer (True or False): True\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: If your skin was turning the color of a zombie, could it be because of nickel?\nFacts: Zombies are often depicted as green in pallor.  Nickel in jewelry often turns skin a greenish color.\nAnswer (True or False):", "ct_subject": "Zombies are often depicted as green in pallor.  Nickel in jewelry often turns skin a greenish color.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" If", "0.99996305"], [" you", "1.0"], [" were", "0.99997246"], [" to", "1.0"], [" die", "0.07852446"], [" and", "0.9733353"], [" go", "1.0"], [" to", "1.0"], [" heaven", "0.99999666"], [",", "1.0"], [" would", "0.99999785"], [" you", "1.0"], [" be", "1.0"], [" able", "1.0"], [" to", "1.0"], [" see", "1.0"], [" the", "0.99912304"], [" stars", "0.9505309"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Yes", "0.99973553"], [",", "0.99997175"], [" you", "1.0"], [" would", "0.99999475"], [".", "0.9999999"], ["\n", "0.8492015"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" If", "1.0"], [" you", "1.0"], [" were", "1.0"], [" to", "1.0"], [" die", "1.0"], [" and", "1.0"], [" go", "1.0"], [" to", "1.0"], [" heaven", "0.9972101"], [",", "1.0"], [" would", "1.0"], [" you", "1.0"], [" be", "1.0"], [" able", "1.0"], [" to", "1.0"], [" see", "1.0"], [" the", "1.0"], [" sun", "0.99999285"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Yes", "1.0"], [",", "1.0"], [" you", "1.0"], [" would", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"]]}, "19": {"qid": "5c969a7fccde48210ec8", "term": "Dolce & Gabbana", "description": "Italian fashion house", "question": "Would a Dolce & Gabbana suit wearer be shunned by their Amish cousins?", "answer": true, "facts": ["Dolce & Gabbana is an Italian luxury fashion design company.", "The Amish, who value plain clothes, frown upon buttons and have banned velcro and zippers", "The Two Tone Dolce & Gabbana suit has several buttons.", "The Amish cease interactions with sinners by avoiding, or shunning them."], "decomposition": ["What type of clothing do the Amish prefer?", "What happens if an Amish person wears clothes going against #1?", "What clothing pieces are Dolce & Gabbana known for?", "If Amish cousins wore #3, would #2 happen to them?"], "evidence": [[[["Amish-27"]], [["Excommunication-39"]], [["Dolce & Gabbana-1"]], ["operation"]], [[["Plain dress-3"]], [["Amish-6"]], [["Dolce & Gabbana-1"]], ["operation"]], [[["Amish-1"]], [["Amish-6"]], [["Dolce & Gabbana-1", "Dolce & Gabbana-32"]], [["Amish-6"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"Dolce & Gabbana\" and a \"Dolce & Gabbana\" suit?\nFacts: The Dol", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a Dolce & Gabbana suit wearer be shunned by their Amish cousins?\nFacts: Dolce & Gabbana is an Italian luxury fashion design company. The Amish, who value plain clothes, frown upon buttons and have banned velcro and zippers The Two Tone Dolce & Gabbana suit has several buttons. The Amish cease interactions with sinners by avoiding, or shunning them.\nAnswer (True or False):", "ct_subject": "Dolce & Gabbana is an Italian luxury fashion design company. The Amish, who value plain clothes, frown upon buttons and have banned velcro and zippers The Two Tone Dolce & Gabbana suit has several buttons. The Amish cease interactions with sinners by avoiding, or shunning them.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99901235"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.99947935"], [" between", "1.0"], [" a", "0.9999957"], [" \"", "1.0"], ["D", "0.68573946"], ["ol", "0.9986016"], ["ce", "1.0"], [" &", "1.0"], [" G", "1.0"], ["abb", "1.0"], ["ana", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["D", "1.0"], ["ol", "1.0"], ["ce", "1.0"], [" &", "1.0"], [" G", "1.0"], ["abb", "1.0"], ["ana", "1.0"], ["\"", "0.04513783"], [" suit", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99999666"], [" Dol", "0.98552793"]]}, "20": {"qid": "c2f573c79ceab25e8fcd", "term": "Strawberry", "description": "edible fruit", "question": "Can a strawberry get worms similar to dogs?", "answer": true, "facts": ["Strawberry can suffer from black root rot and nematodes.", "Dogs can suffer from a variety of worms including roundworms that lay eggs on them.", "Nematodes are parasites that are also called roundworms and ascarids.", "Nematodes are parasites that feed off of strawberry plants."], "decomposition": ["What types of worms can strawberries become infected with?", "What types of worms can dogs become infected with?", "Are any of #1 present in #2?"], "evidence": [[[["Ditylenchus dipsaci-6"], "no_evidence"], [["Dog-18"]], ["no_evidence", "operation"]], [[["Strawberry-26"]], [["Worm-7"]], [["Worm-7"]]], [["no_evidence"], [["Dog-18"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a dog and a strawberry?\nFacts: Dogs are mammals and are classified as a group of mammals called Canidae. The strawberry is a plant that is classified as a group of plants called Solanaceae.\nAnswer", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can a strawberry get worms similar to dogs?\nFacts: Strawberry can suffer from black root rot and nematodes. Dogs can suffer from a variety of worms including roundworms that lay eggs on them. Nematodes are parasites that are also called roundworms and ascarids. Nematodes are parasites that feed off of strawberry plants.\nAnswer (True or False):", "ct_subject": "Strawberry can suffer from black root rot and nematodes. Dogs can suffer from a variety of worms including roundworms that lay eggs on them. Nematodes are parasites that are also called roundworms and ascarids. Nematodes are parasites that feed off of strawberry plants.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.96704525"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.99999714"], [" between", "1.0"], [" a", "1.0"], [" dog", "0.99162054"], [" and", "1.0"], [" a", "1.0"], [" strawberry", "0.9990646"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Dogs", "0.99999774"], [" are", "0.649695"], [" mammals", "0.9999999"], [" and", "0.95482934"], [" are", "0.9999318"], [" classified", "1.0"], [" as", "1.0"], [" a", "0.9999999"], [" group", "0.9996904"], [" of", "1.0"], [" mammals", "0.9994191"], [" called", "0.9998969"], [" Can", "0.9998882"], ["idae", "0.9650584"], [".", "1.0"], [" The", "0.024470335"], [" strawberry", "1.0"], [" is", "1.0"], [" a", "1.0"], [" plant", "0.9992975"], [" that", "0.97925365"], [" is", "1.0"], [" classified", "1.0"], [" as", "1.0"], [" a", "1.0"], [" group", "0.999998"], [" of", "1.0"], [" plants", "1.0"], [" called", "1.0"], [" Sol", "1.0"], ["an", "1.0"], ["aceae", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"]]}, "21": {"qid": "d07d517db865174dcee4", "term": "Soup", "description": "primarily liquid food", "question": "While on a liquid diet, are there some types of soup you cannot eat?", "answer": true, "facts": ["Italian wedding soup has large chunks including meatballs and pasta which require chewing.", "Chicken Noodle soup has chunks of chicken and large noodles in it that require chewing."], "decomposition": ["Are there any soups that contain substantially solid portions?"], "evidence": [[[["Gumbo-1", "Menudo (soup)-1"], "no_evidence"]], [[["Soup-1", "Stew-1"], "operation"]], [[["Chicken soup-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"soup\" and a \"broth\"?\nFacts: A \"soup\" is a liquid that is made from vegetables and meat. A \"broth\" is a liquid that is made from meat and vegetables.\nAnswer (True or False): True\nQuestion: What is the difference between", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: While on a liquid diet, are there some types of soup you cannot eat?\nFacts: Italian wedding soup has large chunks including meatballs and pasta which require chewing. Chicken Noodle soup has chunks of chicken and large noodles in it that require chewing.\nAnswer (True or False):", "ct_subject": "Italian wedding soup has large chunks including meatballs and pasta which require chewing. Chicken Noodle soup has chunks of chicken and large noodles in it that require chewing.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999902"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.95346004"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.9874357"], ["oup", "0.9999995"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["bro", "0.99997365"], ["th", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.6977796"], [" \"", "0.34917143"], ["s", "0.95894575"], ["oup", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" liquid", "1.0"], [" that", "1.0"], [" is", "0.9758984"], [" made", "0.99999034"], [" from", "0.9994873"], [" vegetables", "0.9999994"], [" and", "0.98451287"], [" meat", "0.9993618"], [".", "1.0"], [" A", "1.0"], [" \"", "1.0"], ["bro", "1.0"], ["th", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" liquid", "1.0"], [" that", "1.0"], [" is", "1.0"], [" made", "1.0"], [" from", "1.0"], [" meat", "0.99992347"], [" and", "0.9999938"], [" vegetables", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"]]}, "22": {"qid": "136496e0b5e8bed459da", "term": "Twelve-tone technique", "description": "method of musical composition devised by Arnold Sch\u00f6nberg to ensure that all 12 notes of the chromatic scale are equally often, so that the music avoids being in a key", "question": "Would it be impossible to use an Iwato scale for a twelve-tone technique composition?", "answer": true, "facts": ["The Iwato scale has 5 pitch cases.", "The chromatic scale has 12 pitch cases."], "decomposition": ["How many pitch cases are used for the Iwato scale?", "Is #1 not equal to the number of pitch cases needed for the twelve-tone technique?"], "evidence": [[[["Iwato scale-1"]], [["Twelve-tone technique-1"], "operation"]], [[["Iwato scale-1"]], ["no_evidence", "operation"]], [[["Iwato scale-1"]], [["Twelve-tone technique-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"solo\" and a \"group\" performance?\nFacts: A solo performance is when a soloist plays all the notes of a composition. A group performance is when a group of musicians plays all the notes of a composition.\nAnswer (True or False): True\nQuestion: What is the difference between a \"solo\" and a \"group\"", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would it be impossible to use an Iwato scale for a twelve-tone technique composition?\nFacts: The Iwato scale has 5 pitch cases. The chromatic scale has 12 pitch cases.\nAnswer (True or False):", "ct_subject": "The Iwato scale has 5 pitch cases. The chromatic scale has 12 pitch cases.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999832"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "0.67739064"], [" \"", "0.99999917"], ["s", "0.35609332"], ["olo", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "0.99442"], [" \"", "1.0"], ["group", "0.99995244"], ["\"", "1.0"], [" performance", "0.9949268"], ["?", "1.0"], ["\n", "1.0"], ["F", "0.9999999"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" solo", "1.0"], [" performance", "1.0"], [" is", "1.0"], [" when", "0.6634457"], [" a", "0.9999969"], [" solo", "0.99725705"], ["ist", "1.0"], [" plays", "1.0"], [" all", "0.5988879"], [" the", "1.0"], [" notes", "0.99999094"], [" of", "0.93565035"], [" a", "0.93652123"], [" composition", "0.09391917"], [".", "0.9999999"], [" A", "1.0"], [" group", "1.0"], [" performance", "1.0"], [" is", "1.0"], [" when", "1.0"], [" a", "1.0"], [" group", "1.0"], [" of", "1.0"], [" musicians", "0.9999225"], [" plays", "0.999985"], [" all", "1.0"], [" the", "1.0"], [" notes", "1.0"], [" of", "1.0"], [" a", "1.0"], [" composition", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["olo", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["group", "1.0"], ["\"", "1.0"]]}, "23": {"qid": "ca09aea1e9589ef40c61", "term": "Gulf of Finland", "description": "arm of the Baltic Sea", "question": "Would the Titanic be well preserved at the bottom of the Gulf of Finland?", "answer": true, "facts": ["The bottom of the gulf is one of the world's largest ship cemeteries. Because of the low salinity and cold waters, and no shipworms, the ships are relatively well preserved.", "RMS Titanic was a British passenger ship."], "decomposition": ["Are ships well-preserved in the Gulf of Finland?", "Was the RMS Titanic a ship?", "Is both #1 and #2 \"yes\""], "evidence": [[[["Gulf of Finland-28"]], [["RMS Titanic-1"]], ["operation"]], [[["Gulf of Finland-28"]], [["RMS Titanic-1"]], ["operation"]], [[["Gulf of Finland-28"]], [["RMS Titanic-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the capital of the state of New York?\nFacts: The capital of the state of New York is Albany.\nAnswer (True or False): True\nQuestion: What is the name", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would the Titanic be well preserved at the bottom of the Gulf of Finland?\nFacts: The bottom of the gulf is one of the world's largest ship cemeteries. Because of the low salinity and cold waters, and no shipworms, the ships are relatively well preserved. RMS Titanic was a British passenger ship.\nAnswer (True or False):", "ct_subject": "The bottom of the gulf is one of the world's largest ship cemeteries. Because of the low salinity and cold waters, and no shipworms, the ships are relatively well preserved. RMS Titanic was a British passenger ship.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999964"], [" is", "1.0"], [" the", "1.0"], [" name", "0.65630573"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9367035"], [" in", "0.78001803"], [" the", "0.99998975"], [" United", "0.99998367"], [" States", "1.0"], [" that", "1.0"], [" is", "0.9991049"], [" the", "1.0"], [" capital", "0.9999249"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" New", "0.60453707"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9978896"], [" capital", "0.43128848"], [" of", "1.0"], [" the", "0.9980862"], [" state", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" is", "1.0"], [" Albany", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"]]}, "24": {"qid": "e51ea4cf89bc91a77f3c", "term": "Portuguese Colonial War", "description": "1961\u20131974 armed conflicts in Africa between Portugal and independence movements", "question": "Did any country in Portuguese Colonial War share Switzerlands role in WWII?", "answer": true, "facts": ["The Portuguese Colonial War was between Portugal and several groups including People's Movement for Liberation of Angola.", "Switzerland remained neutral in World War II and did not get involved.", "Portugal stayed out of world affairs during World War II."], "decomposition": ["What was Switzerland's position in World War II?", "Which countries were involved in the Portuguese Colonial War?", "Did any of #2 maintain a #1 position through World War II?"], "evidence": [[[["Switzerland-33"]], [["Portuguese Colonial War-2"]], [["The Two Faces of War-11"]]], [[["Switzerland during the World Wars-20"]], [["Liberal Wars-1"]], [["Neutral powers during World War II-6"], "operation"]], [[["Switzerland during the World Wars-1"]], [["Portuguese Colonial War-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the United States of America have a president before Abraham Lincoln?\nFacts: The United States of America had a president before Abraham Lincoln. The first president of the United States was George Washington.\nAnswer (True or False): True\nQuestion: Did the United States of America have", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did any country in Portuguese Colonial War share Switzerlands role in WWII?\nFacts: The Portuguese Colonial War was between Portugal and several groups including People's Movement for Liberation of Angola. Switzerland remained neutral in World War II and did not get involved. Portugal stayed out of world affairs during World War II.\nAnswer (True or False):", "ct_subject": "The Portuguese Colonial War was between Portugal and several groups including People's Movement for Liberation of Angola. Switzerland remained neutral in World War II and did not get involved. Portugal stayed out of world affairs during World War II.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9967064"], [" the", "1.0"], [" United", "0.9999994"], [" States", "1.0"], [" of", "0.9774376"], [" America", "1.0"], [" have", "0.9952625"], [" a", "1.0"], [" president", "0.0074036955"], [" before", "0.87243336"], [" Abraham", "0.43258798"], [" Lincoln", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9976108"], [" United", "1.0"], [" States", "1.0"], [" of", "1.0"], [" America", "1.0"], [" had", "0.9995264"], [" a", "1.0"], [" president", "1.0"], [" before", "1.0"], [" Abraham", "1.0"], [" Lincoln", "1.0"], [".", "1.0"], [" The", "0.9999474"], [" first", "0.9867345"], [" president", "0.9999341"], [" of", "0.9999881"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" George", "0.9873697"], [" Washington", "1.0"], [".", "1.0"], ["\n", "0.99999976"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999945"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" of", "1.0"], [" America", "1.0"], [" have", "1.0"]]}, "25": {"qid": "de62d38623fde2c2285a", "term": "Week", "description": "unit of time", "question": "Would a week be enough time to watch every episode of Ugly Betty?", "answer": true, "facts": ["There are 168 hours in a week.", "The entire Ugly Betty series is 85 hours long."], "decomposition": ["How many hours are in a week?", "How long is the entire Ugly Betty series?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Week-9"]], [["Ugly Betty-10"]], [["Ugly Betty-10"], "operation"]], [["no_evidence"], [["Ugly Betty-16"], "no_evidence"], ["operation"]], [[["Week-9"], "operation"], ["no_evidence"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest word in the English language?\nFacts: The longest word in the English language is \"the\".\nAnswer (True or False): True\nQuestion: What is the longest word in the English language?\nFacts: The longest word in the English language is \"the\".\nAnswer (True or False): True\nQuestion: What is the longest word in the English language?", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a week be enough time to watch every episode of Ugly Betty?\nFacts: There are 168 hours in a week. The entire Ugly Betty series is 85 hours long.\nAnswer (True or False):", "ct_subject": "There are 168 hours in a week. The entire Ugly Betty series is 85 hours long.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999925"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.85340625"], [" word", "0.85576963"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], [" is", "1.0"], [" \"", "1.0"], ["the", "0.99978405"], ["\".", "0.99989283"], ["\n", "0.7621278"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], [" is", "1.0"], [" \"", "1.0"], ["the", "1.0"], ["\".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], ["?", "1.0"]]}, "26": {"qid": "60c525b944e991fb9821", "term": "Old English", "description": "Early form of English; Anglo-Saxon", "question": "Would a Pict be confused by Old English?", "answer": true, "facts": ["Old English was spoken by the Anglo-Saxons, a Germanic tribe that inhabited England.", "The Picts were a Celtic-speaking people that lived in what is now Scotland.", "The Pictish language died out by 1100AD and was replaced by Gaelic.", "Gaelic and Old English are completely different languages from different branches of the Indo-European language family.", "Gaelic vocabulary is very different from Old English  and verbs are also conjugated differently."], "decomposition": ["What language was spoken by the Picts?", "In what language family is Old English?", "Is #2 not closely related to #1?"], "evidence": [[[["Picts-1"]], [["Old English-2"]], [["Pictish language-2"], "operation"]], [[["Pictish language-1", "Picts-1"]], [["Old English-1"]], [["English language-2", "Picts-36"], "operation"]], [[["Picts-1"]], [["Old English-3"]], [["Celtic languages-1", "West Germanic languages-2"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"C\" and a \"K\"?\nFacts", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a Pict be confused by Old English?\nFacts: Old English was spoken by the Anglo-Saxons, a Germanic tribe that inhabited England. The Picts were a Celtic-speaking people that lived in what is now Scotland. The Pictish language died out by 1100AD and was replaced by Gaelic. Gaelic and Old English are completely different languages from different branches of the Indo-European language family. Gaelic vocabulary is very different from Old English and verbs are also conjugated differently.\nAnswer (True or False):", "ct_subject": "Old English was spoken by the Anglo-Saxons, a Germanic tribe that inhabited England. The Picts were a Celtic-speaking people that lived in what is now Scotland. The Pictish language died out by 1100AD and was replaced by Gaelic. Gaelic and Old English are completely different languages from different branches of the Indo-European language family. Gaelic vocabulary is very different from Old English and verbs are also conjugated differently.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9955635"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.99420667"], [" between", "1.0"], [" a", "0.9915372"], [" \"", "0.99984753"], ["C", "0.9856"], ["\"", "0.96125853"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["K", "0.9999999"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"]]}, "27": {"qid": "4a915ea5d025292cd7ec", "term": "Serfdom", "description": "status of peasants under feudalism", "question": "Did Japanese serfdom have higher status than English counterpart?", "answer": true, "facts": ["Serfs in Medieval England were peasants that were indentured servants to their lords.", "Serfs were often harshly treated and had little legal redress against the actions of their lords.", "Japanese serfs were farmers and fishermen.", "Japanese believed that serfs produced food, which was depended on by all classes, therefore, they worked harder."], "decomposition": ["How did English lords treat their serfs?", "What did the Japanese recognize serfs as?", "Is #2 higher in importance than #1?"], "evidence": [[[["Serfdom-2"]], [["Serfdom-5"]], ["operation"]], [[["Serfdom-2"]], [["Manorialism-17"]], ["operation"]], [[["Serfdom-2"], "no_evidence"], [["Sh\u014den-8", "Sh\u014den-9"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Roman Empire have a monarch?\nFacts: The Roman Empire had a monarch, but it was not the Roman Emperor. The Roman Emperor was the head of the Roman Empire. The Roman", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Japanese serfdom have higher status than English counterpart?\nFacts: Serfs in Medieval England were peasants that were indentured servants to their lords. Serfs were often harshly treated and had little legal redress against the actions of their lords. Japanese serfs were farmers and fishermen. Japanese believed that serfs produced food, which was depended on by all classes, therefore, they worked harder.\nAnswer (True or False):", "ct_subject": "Serfs in Medieval England were peasants that were indentured servants to their lords. Serfs were often harshly treated and had little legal redress against the actions of their lords. Japanese serfs were farmers and fishermen. Japanese believed that serfs produced food, which was depended on by all classes, therefore, they worked harder.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.999954"], [" the", "1.0"], [" Roman", "0.11888323"], [" Empire", "1.0"], [" have", "1.0"], [" a", "1.0"], [" monarch", "0.99466586"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [" had", "0.3990869"], [" a", "1.0"], [" monarch", "1.0"], [",", "0.90196925"], [" but", "1.0"], [" it", "0.9999726"], [" was", "1.0"], [" not", "0.9016342"], [" the", "0.12276129"], [" Roman", "0.9961643"], [" Emperor", "1.0"], [".", "1.0"], [" The", "1.0"], [" Roman", "0.99944633"], [" Emperor", "1.0"], [" was", "1.0"], [" the", "1.0"], [" head", "0.98994"], [" of", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [".", "0.9409357"], [" The", "0.99806577"], [" Roman", "0.9999999"]]}, "28": {"qid": "935002dcfa70239e715f", "term": "Firefighter", "description": "rescuer trained to extinguish hazardous fires", "question": "Would Firefighters be included in a September 11th memorial?", "answer": true, "facts": ["September 11th is remembered as a day of mourning for the lives lost during a terrorist attack in NYC.", "Firefighters were among the first responders to the crisis, and many died. "], "decomposition": ["Who gets remembered on September 11th?", "Were firefighters among #1?"], "evidence": [[[["9/11 Tribute Museum-1"]], [["9/11 Tribute Museum-7"], "operation"]], [[["September 11 attacks-2"]], ["operation"]], [[["September 11 attacks-118", "September 11 attacks-63"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the capital of the United States?\nFacts: Washington, DC\nAnswer (True or False): Washington, DC\nQuestion: What is the name of the city in the United States that is the capital of the United States?\nFacts: Washington, DC", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Firefighters be included in a September 11th memorial?\nFacts: September 11th is remembered as a day of mourning for the lives lost during a terrorist attack in NYC. Firefighters were among the first responders to the crisis, and many died. \nAnswer (True or False):", "ct_subject": "September 11th is remembered as a day of mourning for the lives lost during a terrorist attack in NYC. Firefighters were among the first responders to the crisis, and many died. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999595"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9993703"], [" of", "1.0"], [" the", "1.0"], [" city", "0.88992995"], [" in", "0.91956234"], [" the", "0.99999976"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "0.99858826"], [" the", "1.0"], [" capital", "0.5638032"], [" of", "1.0"], [" the", "1.0"], [" United", "0.9999517"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Washington", "0.99999726"], [",", "0.9474054"], [" DC", "1.0"], ["\n", "0.9999964"], ["Answer", "1.0"], [" (", "0.9999999"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" Washington", "1.0"], [",", "1.0"], [" DC", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9999981"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Washington", "1.0"], [",", "1.0"], [" DC", "1.0"]]}, "29": {"qid": "58e7eb38d889d4252123", "term": "Chinese calendar", "description": "Lunisolar calendar from China", "question": "Are any animals in Chinese calendar Chordata?", "answer": true, "facts": ["The chinese zodiac based on the Chinese calendar has a number of animals including dogs and pigs.", "Chordata is a scientific classification of an animals phylum.", "The phylum of pigs is Chordata."], "decomposition": ["What animals are on the Chinese calendar?", "Which animals in #1 have a notochord and dorsal neural tube?", "Which animals in #2 have pharyngeal slits and an endostyle at some stage of development?", "Which animals in #3 have a post-anal tail?", "Is there at least one animal listed in #4?"], "evidence": [[[["Chinese astrology-10", "Chordate-1"]], [["Tiger-27"], "no_evidence"], [["Tiger-27"], "no_evidence"], [["Tiger-27"], "no_evidence"], ["operation"]], [[["Chinese zodiac-5"]], [["Chordate-1"], "no_evidence"], [["Chordate-2"], "no_evidence"], [["Chordate-6"], "no_evidence"], ["operation"]], [[["Chinese zodiac-5"]], [["Chinese zodiac-5", "Notochord-1"]], [["Chinese zodiac-5", "Pharyngeal slit-1"]], [["Chinese zodiac-5", "Chordate-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any animals in Chinese calendar Chordata?\nFacts: The Chinese zodiac based on the Chinese calendar has a number of animals including dogs and pigs. Chordata is a scientific classification of an animals phylum. The phylum of pigs is Chordata.\nAnswer (True or False", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are any animals in Chinese calendar Chordata?\nFacts: The chinese zodiac based on the Chinese calendar has a number of animals including dogs and pigs. Chordata is a scientific classification of an animals phylum. The phylum of pigs is Chordata.\nAnswer (True or False):", "ct_subject": "The chinese zodiac based on the Chinese calendar has a number of animals including dogs and pigs. Chordata is a scientific classification of an animals phylum. The phylum of pigs is Chordata.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.7597516"], [" there", "1.0"], [" any", "1.0"], [" animals", "1.0"], [" in", "1.0"], [" Chinese", "0.4189718"], [" calendar", "1.0"], [" Ch", "1.0"], ["ord", "1.0"], ["ata", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Chinese", "0.96259356"], [" z", "1.0"], ["odiac", "1.0"], [" based", "1.0"], [" on", "1.0"], [" the", "1.0"], [" Chinese", "1.0"], [" calendar", "1.0"], [" has", "1.0"], [" a", "1.0"], [" number", "1.0"], [" of", "1.0"], [" animals", "1.0"], [" including", "1.0"], [" dogs", "1.0"], [" and", "1.0"], [" pigs", "1.0"], [".", "1.0"], [" Ch", "1.0"], ["ord", "1.0"], ["ata", "1.0"], [" is", "1.0"], [" a", "1.0"], [" scientific", "1.0"], [" classification", "1.0"], [" of", "1.0"], [" an", "1.0"], [" animals", "1.0"], [" ph", "1.0"], ["ylum", "1.0"], [".", "1.0"], [" The", "1.0"], [" ph", "1.0"], ["ylum", "1.0"], [" of", "1.0"], [" pigs", "1.0"], [" is", "1.0"], [" Ch", "1.0"], ["ord", "1.0"], ["ata", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"]]}, "30": {"qid": "a3416e6b8a34d389eb0c", "term": "Yeti", "description": "Folkloric ape-like creature from Asia", "question": "Would a hypothetical Yeti be towered over by Andre the Giant?", "answer": true, "facts": ["The mythical Yeti is said to be between 200 and 400 pounds.", "The mythical Yeti is said to be around 6 feet tall.", "Andre the Giant was an imposing 7'4\" and 529 pounds."], "decomposition": ["How tall was Andre the Giant?", "How tall are Yeti thought to be?", "Is #2 greater than #1?"], "evidence": [[[["Andr\u00e9 the Giant-2"]], [["Chuchuna-3"]], ["operation"]], [[["Andr\u00e9 the Giant-2"]], [["Expedition Everest-12", "Yeti-8"], "no_evidence"], ["no_evidence", "operation"]], [[["Andre the Giant Has a Posse-2"]], [["Yeti-1"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the largest number of people that have ever lived?\nFacts: The largest number of people that have ever lived is estimated to be around 10 billion.\nAnswer (True or False): True\nQuestion: What is the largest number of people that have ever lived?\nFacts: The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a hypothetical Yeti be towered over by Andre the Giant?\nFacts: The mythical Yeti is said to be between 200 and 400 pounds. The mythical Yeti is said to be around 6 feet tall. Andre the Giant was an imposing 7'4\" and 529 pounds.\nAnswer (True or False):", "ct_subject": "The mythical Yeti is said to be between 200 and 400 pounds. The mythical Yeti is said to be around 6 feet tall. Andre the Giant was an imposing 7'4\" and 529 pounds.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99901474"], [" is", "1.0"], [" the", "1.0"], [" largest", "0.9923023"], [" number", "0.9944231"], [" of", "1.0"], [" people", "1.0"], [" that", "0.999998"], [" have", "0.99971074"], [" ever", "1.0"], [" lived", "1.0"], ["?", "0.5148767"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99545836"], [" largest", "1.0"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" that", "1.0"], [" have", "1.0"], [" ever", "1.0"], [" lived", "1.0"], [" is", "1.0"], [" estimated", "0.9999906"], [" to", "0.99999785"], [" be", "1.0"], [" around", "0.8341295"], [" 10", "0.9999999"], [" billion", "0.9917012"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" that", "1.0"], [" have", "1.0"], [" ever", "1.0"], [" lived", "1.0"], ["?", "0.99995947"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"]]}, "31": {"qid": "f3d045e08844bf0d4e53", "term": "Creative Commons license", "description": "license allowing free use of a work", "question": "Was a person sold a Creative Commons License for Boticelli's The Birth of Venus ripped off?", "answer": true, "facts": ["A Creative Commons license allows for the free distribution of an otherwise copyrighted piece of work.", "Works that are deemed in the public domain do not require a copyright or permission to use.", "Any work created before 1923 is in the public domain.", "Sandro Boticelli's The Birth of Venus painting was from 1486."], "decomposition": ["What is the purpose of a Creative Commons license?", "Do works in the public domain need #1?", "Works created before what year are presently in the public domain?", "Was Boticelli's The Birth of Venus created before #3?", "Considering #2 and #4, is #1 unnecessary for Boticelli's The Birth of Venus?"], "evidence": [[[["Creative Commons license-1"]], [["Public domain-1"]], [["Public domain-10"], "no_evidence"], [["The Birth of Venus-1"], "operation"], ["operation"]], [[["Creative Commons license-1"]], [["Public domain-1"]], [["Public domain-2"]], [["The Birth of Venus-17"]], ["operation"]], [[["Creative Commons license-1"]], [["Public domain-1"]], [["Copyright term-2", "Public domain-6"], "no_evidence"], [["The Birth of Venus-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"C\" and a \"C++\" license?\nFacts: A \"C\" license is a \"copyleft\" license", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was a person sold a Creative Commons License for Boticelli's The Birth of Venus ripped off?\nFacts: A Creative Commons license allows for the free distribution of an otherwise copyrighted piece of work. Works that are deemed in the public domain do not require a copyright or permission to use. Any work created before 1923 is in the public domain. Sandro Boticelli's The Birth of Venus painting was from 1486.\nAnswer (True or False):", "ct_subject": "A Creative Commons license allows for the free distribution of an otherwise copyrighted piece of work. Works that are deemed in the public domain do not require a copyright or permission to use. Any work created before 1923 is in the public domain. Sandro Boticelli's The Birth of Venus painting was from 1486.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.3712472"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.5622532"], [" between", "1.0"], [" a", "0.9999999"], [" \"", "1.0"], ["C", "0.9741509"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["C", "0.99999774"], ["++", "0.99997675"], ["\"", "0.9999999"], [" license", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.977429"], [" \"", "0.99999034"], ["C", "1.0"], ["\"", "1.0"], [" license", "1.0"], [" is", "1.0"], [" a", "1.0"], [" \"", "0.9791028"], ["cop", "1.0"], ["yle", "1.0"], ["ft", "1.0"], ["\"", "1.0"], [" license", "1.0"]]}, "32": {"qid": "e0e842a78f59698d0cfd", "term": "Haiku", "description": "very short form of Japanese poetry", "question": "Can you write a whole Haiku in a single tweet?", "answer": true, "facts": ["A Haiku is a Japanese poetry in three phrases.", "The average Haiku is composed of 60 to 70 characters.", "A tweet is a short message sent on Twitter.", "The character limit of a single tweet on Twitter is 140."], "decomposition": ["How many characters can be expected to be in an average haiku?", "What is the current character limit of a single tweet?", "Is #1 reasonably less than #2?"], "evidence": [[[["Haiku-2"], "no_evidence"], [["Twitter-1"]], ["operation"]], [[["Haiku-2"]], [["Twitter-1"]], ["no_evidence", "operation"]], [[["Haiku-2"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest word in the English language?\nFacts: The longest word in the English language is \"the\". The longest word in the English language is \"the\".\nAnswer (True or False): True\nQuestion: What is the longest word in the English language?\nF", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can you write a whole Haiku in a single tweet?\nFacts: A Haiku is a Japanese poetry in three phrases. The average Haiku is composed of 60 to 70 characters. A tweet is a short message sent on Twitter. The character limit of a single tweet on Twitter is 140.\nAnswer (True or False):", "ct_subject": "A Haiku is a Japanese poetry in three phrases. The average Haiku is composed of 60 to 70 characters. A tweet is a short message sent on Twitter. The character limit of a single tweet on Twitter is 140.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999774"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.9999927"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], [" is", "1.0"], [" \"", "0.99999905"], ["the", "0.9995376"], ["\".", "0.9967892"], [" The", "1.0"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], [" is", "1.0"], [" \"", "1.0"], ["the", "1.0"], ["\".", "1.0"], ["\n", "0.69071233"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.9999999"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "0.996619"], [" language", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"]]}, "33": {"qid": "db70d15b07a9f288ffdc", "term": "Manta ray", "description": "genus of fishes", "question": "Do manta rays live in water above the safe temperature for cold food storage?", "answer": true, "facts": ["For cold foods, the food safe temperature is 40 degrees Fahrenheit and below.", "Manta rays prefer water temperatures above 68 \u00b0F (20 \u00b0C)."], "decomposition": ["What temperature should cold food be stored at?", "What kind of water do manta rays live in?", "What is the normal temperature of #2?", "Is #3 higher than #1?"], "evidence": [[[["Food storage-8"]], [["Manta ray-2"]], [["Manta ray-21"]], ["operation"]], [[["Refrigeration-23"], "no_evidence"], [["Manta ray-2"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Food storage-4", "Food storage-8"]], [["Manta ray-21"]], [["Manta ray-21"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake-shaped\" snowflake?\nFacts: A snowflake is a round, smooth, and flat shape. A snowflake-shaped snowflake is a round, smooth, and flat shape with a small hole in the center.\nAnswer (True or False", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do manta rays live in water above the safe temperature for cold food storage?\nFacts: For cold foods, the food safe temperature is 40 degrees Fahrenheit and below. Manta rays prefer water temperatures above 68 \u00b0F (20 \u00b0C).\nAnswer (True or False):", "ct_subject": "For cold foods, the food safe temperature is 40 degrees Fahrenheit and below. Manta rays prefer water temperatures above 68 \u00b0F (20 \u00b0C).", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99705493"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9837864"], [" between", "1.0"], [" a", "0.9999989"], [" \"", "0.99999976"], ["s", "0.48679835"], ["now", "0.999982"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.99695826"], ["now", "1.0"], ["flake", "1.0"], ["-", "0.8417901"], ["shaped", "0.99466455"], ["\"", "1.0"], [" snow", "0.9999989"], ["flake", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999932"], [" snow", "0.9999999"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" round", "0.9019677"], [",", "1.0"], [" smooth", "0.9819639"], [",", "1.0"], [" and", "0.99999464"], [" flat", "0.9606336"], [" shape", "0.99979776"], [".", "0.9999999"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], ["-", "1.0"], ["shaped", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" round", "1.0"], [",", "1.0"], [" smooth", "1.0"], [",", "1.0"], [" and", "1.0"], [" flat", "1.0"], [" shape", "1.0"], [" with", "0.5620773"], [" a", "1.0"], [" small", "0.9999981"], [" hole", "0.9980521"], [" in", "1.0"], [" the", "1.0"], [" center", "0.9993766"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"]]}, "34": {"qid": "1f550df826ae448ff082", "term": "Subway (restaurant)", "description": "American fast food chain", "question": "Has the Subway restaurant franchise had any connections with child abusers?", "answer": true, "facts": ["Subway hired Jared Fogle as a spokesman for their sandwich shops.", "Jared Fogle was convicted for having sex with minors and for possessing child pornography. "], "decomposition": ["Was Jared Fogle a spokesman for Subway?", "Is Jared Fogle a sexual abuser of children?", "Are #1 and #2 the same?"], "evidence": [[[["Jared Fogle-2"]], [["Jared Fogle-40"]], ["operation"]], [[["Jared Fogle-2"]], [["Jared Fogle-15"]], ["operation"]], [[["Jared Fogle-1"]], [["Jared Fogle-3"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city in which the United States Capitol is located the same as the name of the city in which the United States Supreme Court is located?\nFacts: The United States Capitol is located in Washington, D.C. The United States Supreme Court is located in Washington, D.C.\nAnswer (True or False): True\nQuestion:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Has the Subway restaurant franchise had any connections with child abusers?\nFacts: Subway hired Jared Fogle as a spokesman for their sandwich shops. Jared Fogle was convicted for having sex with minors and for possessing child pornography. \nAnswer (True or False):", "ct_subject": "Subway hired Jared Fogle as a spokesman for their sandwich shops. Jared Fogle was convicted for having sex with minors and for possessing child pornography. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.96916664"], [" the", "0.99996734"], [" name", "0.99958414"], [" of", "0.9999875"], [" the", "1.0"], [" city", "1.0"], [" in", "0.9999926"], [" which", "0.99951243"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" Capitol", "0.48955545"], [" is", "1.0"], [" located", "1.0"], [" the", "0.9967424"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" which", "1.0"], [" the", "1.0"], [" United", "0.9999999"], [" States", "1.0"], [" Supreme", "1.0"], [" Court", "1.0"], [" is", "1.0"], [" located", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "0.9999689"], [" States", "1.0"], [" Capitol", "0.99999535"], [" is", "1.0"], [" located", "0.9999989"], [" in", "1.0"], [" Washington", "1.0"], [",", "0.9999995"], [" D", "0.999972"], [".", "1.0"], ["C", "1.0"], [".", "1.0"], [" The", "1.0"], [" United", "1.0"], [" States", "1.0"], [" Supreme", "1.0"], [" Court", "1.0"], [" is", "1.0"], [" located", "1.0"], [" in", "1.0"], [" Washington", "0.99463034"], [",", "1.0"], [" D", "1.0"], [".", "1.0"], ["C", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"]]}, "35": {"qid": "4810297f474b0e97ed78", "term": "Gallon", "description": "general topic for different units of volume called gallon", "question": "Could ten gallons of seawater crush a six year old?", "answer": true, "facts": ["The average weight of a six year old is 45 pounds.", "One gallon of seawater weighs slightly over 8 pounds."], "decomposition": ["What is the average weight of a six year old?", "What is the weight of a gallon of seawater?", "Is ten times #2 more than #1?"], "evidence": [[[["Weigh station-3"], "no_evidence"], [["Seawater-1"], "no_evidence"], ["operation"]], [[["Child-7"], "no_evidence"], [["Seawater-1"], "no_evidence"], ["operation"]], [[["Early childhood-4"], "no_evidence"], [["Seawater-1"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human being?\nFacts: The average weight of a human being is about 100 pounds.\nAnswer (True or False): True\nQuestion: What is the average weight of a human being?\nFacts: The average weight of a human being is about 100 pounds.\nAnswer (True or False): True\nQuestion: What is the average weight of a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could ten gallons of seawater crush a six year old?\nFacts: The average weight of a six year old is 45 pounds. One gallon of seawater weighs slightly over 8 pounds.\nAnswer (True or False):", "ct_subject": "The average weight of a six year old is 45 pounds. One gallon of seawater weighs slightly over 8 pounds.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9989813"], [" is", "1.0"], [" the", "1.0"], [" average", "0.9999155"], [" weight", "0.99999964"], [" of", "1.0"], [" a", "1.0"], [" human", "0.9999999"], [" being", "0.2635012"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" weight", "0.9962768"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "1.0"], [" is", "1.0"], [" about", "0.999833"], [" 100", "0.99093926"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "0.9999993"], [" of", "1.0"], [" a", "1.0"], [" human", "0.9960801"], [" being", "1.0"], ["?", "0.99334323"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" weight", "1.0"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 100", "1.0"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "1.0"], [" of", "1.0"], [" a", "1.0"]]}, "36": {"qid": "a98c29881e1929a25eb4", "term": "Giant squid", "description": "Deep-ocean dwelling squid in the family Architeuthidae", "question": "Could a giant squid fit aboard the deck of the titanic?", "answer": true, "facts": ["Decks on the Titanic were as long as 500ft.", "Giant Squid grow to be around 59ft in length."], "decomposition": ["What is the length of a giant squid?", "What was the length of a deck on the Titanic?", "Is #1 less than #2?"], "evidence": [[[["Giant squid-1"]], [["RMS Titanic-11"]], ["operation"]], [[["Giant squid-1"]], [["First-class facilities of the RMS Titanic-12"]], ["operation"]], [[["Giant squid-1"]], [["RMS Titanic-11"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance between two points on the earth?\nFacts: The distance between the earth and the sun is about 93 million miles. The distance between the earth and the moon is about 238,000 miles.\nAnswer (True or False): True\nQuestion: What is the longest distance between two points on the earth?\nFacts: The distance between the earth and the sun", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could a giant squid fit aboard the deck of the titanic?\nFacts: Decks on the Titanic were as long as 500ft. Giant Squid grow to be around 59ft in length.\nAnswer (True or False):", "ct_subject": "Decks on the Titanic were as long as 500ft. Giant Squid grow to be around 59ft in length.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999924"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.16737692"], [" distance", "0.99683654"], [" between", "0.51239145"], [" two", "1.0"], [" points", "1.0"], [" on", "0.9991534"], [" the", "0.8799932"], [" earth", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" distance", "0.97476363"], [" between", "1.0"], [" the", "0.99999833"], [" earth", "0.99999475"], [" and", "1.0"], [" the", "1.0"], [" sun", "0.9961755"], [" is", "1.0"], [" about", "0.99995947"], [" 93", "1.0"], [" million", "1.0"], [" miles", "1.0"], [".", "1.0"], [" The", "0.99511766"], [" distance", "1.0"], [" between", "1.0"], [" the", "1.0"], [" earth", "1.0"], [" and", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 238", "1.0"], [",", "1.0"], ["000", "1.0"], [" miles", "1.0"], [".", "1.0"], ["\n", "0.9999832"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.99999976"], [" distance", "0.9999989"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"], [" on", "1.0"], [" the", "1.0"], [" earth", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" distance", "1.0"], [" between", "1.0"], [" the", "1.0"], [" earth", "1.0"], [" and", "1.0"], [" the", "1.0"], [" sun", "1.0"]]}, "37": {"qid": "d9987847af1ab751c75a", "term": "White blood cell", "description": "type of cells of the immunological system", "question": "Will someone die without white blood cells?", "answer": true, "facts": ["White blood cells protect people against disease.", "Disease kills people."], "decomposition": ["What function do white blood cells serve in the body?", "Can a human live without #1?"], "evidence": [[[["White blood cell-1"]], [["White blood cell-15"], "no_evidence", "operation"]], [[["Blood cell-7"]], [["Blood cell-9"]]], [[["Innate immune system-11"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a man and a woman?\nFacts: A man is a man. A woman is a woman.\nAnswer (True or False): True\nQuestion: What is the difference between a man and a woman?\nFacts: A man is a man. A woman is a woman.\nAnswer (True or False): True\nQuestion: What is the difference between a man and a woman?\nFacts: A man is a man", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Will someone die without white blood cells?\nFacts: White blood cells protect people against disease. Disease kills people.\nAnswer (True or False):", "ct_subject": "White blood cells protect people against disease. Disease kills people.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9997614"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" man", "0.9738072"], [" and", "1.0"], [" a", "1.0"], [" woman", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" man", "1.0"], [" is", "1.0"], [" a", "1.0"], [" man", "0.99753547"], [".", "0.9970022"], [" A", "1.0"], [" woman", "1.0"], [" is", "1.0"], [" a", "1.0"], [" woman", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" man", "1.0"], [" and", "1.0"], [" a", "1.0"], [" woman", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" man", "1.0"], [" is", "1.0"], [" a", "1.0"], [" man", "1.0"], [".", "1.0"], [" A", "1.0"], [" woman", "1.0"], [" is", "1.0"], [" a", "1.0"], [" woman", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" man", "1.0"], [" and", "1.0"], [" a", "1.0"], [" woman", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" man", "1.0"], [" is", "1.0"], [" a", "1.0"], [" man", "1.0"]]}, "38": {"qid": "a60e5f73700b47a5f34a", "term": "Boat", "description": "vessel for transport by water", "question": "Does rock star Keith Richards play a captain of a boat in a movie?", "answer": true, "facts": ["Keith Richards has a cameo appearance in two of the Pirates of the Caribbean movies.", "He plays Captain Teague, the elderly father of famous pirate Captain Jack Sparrow.", "In At World's End, he is the member of the council of Pirate Lords who is responsible for keeping the Pirate Code, and there is a brief shot of him and his crew aboard their ship during the sequence where the pirates are raising their banners in preparation to fight."], "decomposition": ["What role did Keith Richards play in the Pirates of the Caribbean movies?", "Can #1 be considered a captain of a boat?"], "evidence": [[[["Keith Richards-47"]], [["Captain-1"], "operation"]], [[["Keith Richards-47"]], [["Captain-1"]]], [[["Keith Richards-47"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the character in the movie \"The Princess Bride\"", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does rock star Keith Richards play a captain of a boat in a movie?\nFacts: Keith Richards has a cameo appearance in two of the Pirates of the Caribbean movies. He plays Captain Teague, the elderly father of famous pirate Captain Jack Sparrow. In At World's End, he is the member of the council of Pirate Lords who is responsible for keeping the Pirate Code, and there is a brief shot of him and his crew aboard their ship during the sequence where the pirates are raising their banners in preparation to fight.\nAnswer (True or False):", "ct_subject": "Keith Richards has a cameo appearance in two of the Pirates of the Caribbean movies. He plays Captain Teague, the elderly father of famous pirate Captain Jack Sparrow. In At World's End, he is the member of the council of Pirate Lords who is responsible for keeping the Pirate Code, and there is a brief shot of him and his crew aboard their ship during the sequence where the pirates are raising their banners in preparation to fight.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9851436"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" character", "0.9998498"], [" in", "0.99966717"], [" the", "1.0"], [" movie", "1.0"], [" \"", "0.9809123"], ["The", "1.0"], [" Princess", "0.9989428"], [" Bride", "1.0"], ["\"", "1.0"]]}, "39": {"qid": "71d5af08c0cfb9565a54", "term": "Motor vehicle", "description": "self-propelled wheeled vehicle", "question": "Could Oscar Wilde have operated a motor vehicle?", "answer": true, "facts": ["Motor vehicles were in use by the 1890s", "Oscar Wilde lived until 1900"], "decomposition": ["When were cars first used?", "When did Oscar Wilde pass away?", "Did #2 happen after #1?"], "evidence": [[[["Car-2"]], [["Oscar Wilde-1"]], ["operation"]], [[["Car-14"]], [["Oscar Wilde-1"]], ["operation"]], [[["Car-2"]], [["Oscar Wilde-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first man to be born in the United States?\nFacts: John Smith\nAnswer (True or False): True\nQuestion: What is the name of the first man to be born in the United States?\nFacts: John Smith\nAnswer (True or False): True\nQuestion: What is the name of the first man to be born in the United States?\nFacts: John Smith\nAnswer (True or", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could Oscar Wilde have operated a motor vehicle?\nFacts: Motor vehicles were in use by the 1890s Oscar Wilde lived until 1900\nAnswer (True or False):", "ct_subject": "Motor vehicles were in use by the 1890s Oscar Wilde lived until 1900", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9955598"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99937254"], [" of", "1.0"], [" the", "1.0"], [" first", "0.79724705"], [" man", "0.09958496"], [" to", "1.0"], [" be", "0.5981058"], [" born", "0.9627754"], [" in", "1.0"], [" the", "0.99996257"], [" United", "0.9632991"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "0.99555016"], [" Smith", "0.9999999"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "0.99999976"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "1.0"], [" Smith", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "1.0"], [" Smith", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"]]}, "40": {"qid": "9b77992543134de72b4e", "term": "Tokyo Tower", "description": "observation tower", "question": "Will Tokyo Tower be repainted only once during President Trump's first term?", "answer": true, "facts": ["Tokyo Tower is repainted every five years ", "The last repainting began in 2018", "Trump's first presidential term is from 2017 to 2021"], "decomposition": ["How long (in years) is President Trump's first term?", "How often (interval in years) is the Tokyo Tower repainted?", "Is #2 divided by #1 less than two?"], "evidence": [[[["Term of office-11"], "no_evidence"], [["Tokyo Tower-10"]], ["operation"]], [[["President of the United States-4"]], [["Tokyo Tower-10"]], ["operation"]], [[["President of the United States-5"]], [["Tokyo Tower-3"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the capital of the state of Texas?\nFacts: Austin\nAnswer (True or False): True\nQuestion: What is the name of the city in the United States that is the capital of the state of Texas?\nFacts: Austin\nAnswer (True or False): True\nQuestion:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Will Tokyo Tower be repainted only once during President Trump's first term?\nFacts: Tokyo Tower is repainted every five years  The last repainting began in 2018 Trump's first presidential term is from 2017 to 2021\nAnswer (True or False):", "ct_subject": "Tokyo Tower is repainted every five years  The last repainting began in 2018 Trump's first presidential term is from 2017 to 2021", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9988171"], [" is", "1.0"], [" the", "1.0"], [" name", "0.8029696"], [" of", "1.0"], [" the", "1.0"], [" city", "0.36786598"], [" in", "0.99758697"], [" the", "1.0"], [" United", "0.99999976"], [" States", "1.0"], [" that", "1.0"], [" is", "0.97819513"], [" the", "1.0"], [" capital", "0.86018485"], [" of", "1.0"], [" the", "1.0"], [" state", "0.99999785"], [" of", "1.0"], [" Texas", "0.99137527"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Austin", "0.9995316"], ["\n", "0.99992204"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "0.99615943"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" Texas", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "0.9999974"], ["acts", "1.0"], [":", "1.0"], [" Austin", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"]]}, "41": {"qid": "d4546cb00bd8a7f0e041", "term": "Wednesday", "description": "Day of the week", "question": "Does the anatomy of a camel lend itself to jokes on Wednesdays?", "answer": true, "facts": ["Wednesday is often referred to as 'hump day' as a joke.", "Camels are known for having a significant hump. "], "decomposition": ["As a joke, what is Wednesday otherwise known as?", "What are camels known for having?", "Is there overlap between #1 and #2?"], "evidence": [[[["Wednesday-25"]], [["Camel-1"]], ["operation"]], [[["Wednesday-25"]], [["Camel-1"]], ["operation"]], [[["Wednesday-25"]], [["Camel-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first man to be born in the year 2000?\nFacts: The first man to be born in the year 2000 was born in the year 2000.\nAnswer (True or False): True\nQuestion: What is the name of the first man to be born in the year 2000?\nFacts: The first man to be born in", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does the anatomy of a camel lend itself to jokes on Wednesdays?\nFacts: Wednesday is often referred to as 'hump day' as a joke. Camels are known for having a significant hump. \nAnswer (True or False):", "ct_subject": "Wednesday is often referred to as 'hump day' as a joke. Camels are known for having a significant hump. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999999"], [" is", "1.0"], [" the", "1.0"], [" name", "0.20740186"], [" of", "1.0"], [" the", "1.0"], [" first", "0.64919615"], [" man", "0.119057216"], [" to", "1.0"], [" be", "0.9799793"], [" born", "0.9346649"], [" in", "0.9999993"], [" the", "0.9999945"], [" year", "0.9989078"], [" 2000", "0.99999654"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "0.90314883"], [" man", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "1.0"], [" born", "0.9999907"], [" in", "0.99987864"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [".", "1.0"], ["\n", "0.99999475"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"]]}, "42": {"qid": "ac61d110f57cd7855686", "term": "Goofy", "description": "Disney cartoon character", "question": "Can voice actors for Goofy and Bugs Bunny each get one stripe from American flag?", "answer": true, "facts": ["The American flag has 13 stripes on it.", "Since the role originated in 1932, six people have voiced the character of Goofy.", "Since 1940, seven people have voiced the character of Bugs Bunny."], "decomposition": ["How many stripes does the American flag have?", "How many people have been the voice of Goofy?", "How many people have been the voice of Bugs Bunny?", "What is #2 plus #3?", "Is #1 equal to or greater than #4?"], "evidence": [[[["Flag of the United States-1"]], [["Bill Farmer-1", "Hal Smith (actor)-16", "Pinto Colvig-1", "Stuart Buchanan-1", "Tony Pope-2"], "no_evidence"], [["Bugs Bunny-26", "Bugs Bunny-41"]], ["operation"], ["operation"]], [[["Flag of the United States-1"]], [["Goofy-43"]], [["Billy West-1", "Eric Bauza-1", "Greg Burson-2", "Jeff Bergman-1", "Joe Alaskey-2", "Mel Blanc-1", "Sam Vincent (voice actor)-1"]], ["operation"], ["operation"]], [[["Flag of the United States-1"]], [["Goofy-43"]], [["Bugs Bunny-41"], "no_evidence"], ["no_evidence", "operation"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the character in the movie \"The Wizard of Oz\" who is the only one who can see the Wicked Witch of the West?\nFacts: The Wicked Witch of the West is the only one who can see the Wicked Witch of the West.\nAnswer (", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can voice actors for Goofy and Bugs Bunny each get one stripe from American flag?\nFacts: The American flag has 13 stripes on it. Since the role originated in 1932, six people have voiced the character of Goofy. Since 1940, seven people have voiced the character of Bugs Bunny.\nAnswer (True or False):", "ct_subject": "The American flag has 13 stripes on it. Since the role originated in 1932, six people have voiced the character of Goofy. Since 1940, seven people have voiced the character of Bugs Bunny.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999774"], [" is", "1.0"], [" the", "1.0"], [" name", "0.988907"], [" of", "1.0"], [" the", "1.0"], [" character", "0.86101234"], [" in", "0.9952454"], [" the", "1.0"], [" movie", "1.0"], [" \"", "1.0"], ["The", "1.0"], [" Wizard", "0.99999857"], [" of", "1.0"], [" Oz", "1.0"], ["\"", "0.9998944"], [" who", "1.0"], [" is", "0.9999989"], [" the", "0.9719733"], [" only", "1.0"], [" one", "0.99999774"], [" who", "1.0"], [" can", "1.0"], [" see", "0.4307576"], [" the", "1.0"], [" Wicked", "0.99999595"], [" Witch", "1.0"], [" of", "1.0"], [" the", "1.0"], [" West", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Wicked", "0.99999"], [" Witch", "1.0"], [" of", "1.0"], [" the", "1.0"], [" West", "1.0"], [" is", "1.0"], [" the", "0.76902705"], [" only", "1.0"], [" one", "0.99999976"], [" who", "1.0"], [" can", "1.0"], [" see", "1.0"], [" the", "1.0"], [" Wicked", "1.0"], [" Witch", "1.0"], [" of", "1.0"], [" the", "1.0"], [" West", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"]]}, "43": {"qid": "48628a79ac6d18460f36", "term": "Eric Clapton", "description": "English musician, singer, songwriter, and guitarist", "question": "Did Eric Clapton have similar taste in women to one of the Beatles?", "answer": true, "facts": ["The Beatles consisted of John Lennon, Paul McCartney, George Harrison, and Ringo Starr.", "George Harrison was married to Pattie Boyd from 1966-1977.", "Eric Clapton married Pattie Boyd in 1979."], "decomposition": ["Who are the spouses Eric Clapton has had?", "Who are the spouses the members of the Beatles have had?", "Is #1 listed in #2?"], "evidence": [[[["Eric Clapton-78"]], [["George Harrison-4"]], ["operation"]], [[["Eric Clapton-78"]], [["George Harrison-4"]], ["operation"]], [[["Eric Clapton-78"]], [["Pattie Boyd-1"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Beatles have a secret song that was never released?\nFacts: The Beatles had a secret song that was never released. The song was called \"I Want to Hold Your Hand.\"\nAnswer (True or False): True\nQuestion: Did the Beatles have a secret song that", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Eric Clapton have similar taste in women to one of the Beatles?\nFacts: The Beatles consisted of John Lennon, Paul McCartney, George Harrison, and Ringo Starr. George Harrison was married to Pattie Boyd from 1966-1977. Eric Clapton married Pattie Boyd in 1979.\nAnswer (True or False):", "ct_subject": "The Beatles consisted of John Lennon, Paul McCartney, George Harrison, and Ringo Starr. George Harrison was married to Pattie Boyd from 1966-1977. Eric Clapton married Pattie Boyd in 1979.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9771327"], [" the", "1.0"], [" Beatles", "1.0"], [" have", "0.9999938"], [" a", "1.0"], [" secret", "0.9999974"], [" song", "0.9999869"], [" that", "0.9999944"], [" was", "0.91864806"], [" never", "1.0"], [" released", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Beatles", "1.0"], [" had", "0.8823929"], [" a", "1.0"], [" secret", "0.646066"], [" song", "1.0"], [" that", "1.0"], [" was", "1.0"], [" never", "1.0"], [" released", "1.0"], [".", "1.0"], [" The", "0.98838824"], [" song", "1.0"], [" was", "1.0"], [" called", "0.9999987"], [" \"", "1.0"], ["I", "0.9718084"], [" Want", "1.0"], [" to", "1.0"], [" Hold", "1.0"], [" Your", "1.0"], [" Hand", "1.0"], [".\"", "0.21152315"], ["\n", "0.99997056"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" Beatles", "1.0"], [" have", "1.0"], [" a", "1.0"], [" secret", "1.0"], [" song", "1.0"], [" that", "1.0"]]}, "44": {"qid": "9834ceb34172041cd73f", "term": "Law & Order", "description": "original television series (1990-2010)", "question": "Are there winged statuettes in the home of the creator of Law & Order?", "answer": true, "facts": ["Law & Order was created by Dick Wolf", "Dick Wolf won an Emmy in 2007 ", "Dick Wolf won an Emmy in 1997", "The Emmy statuette is of a winged woman holding an atom"], "decomposition": ["What award has a trophy that is a winged statuette?", "Who is the creator of Law & Order?", "Has #2 ever won #1?"], "evidence": [[[["Emmy Award-9"]], [["Dick Wolf-1"]], [["Dick Wolf-1"]]], [[["Emmy Award-2"]], [["Law & Order-1"]], [["Dick Wolf-1"]]], [[["Emmy Award-2"]], [["Law & Order (franchise)-1"]], [["Dick Wolf-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there winged statuettes in the home of the creator of Law & Order?\nFacts: Law & Order was created by Dick Wolf Dick Wolf won an Emmy in 2007  Dick Wolf won an Emmy in 1997 The Emmy statuette is of a winged woman holding an atom\nAnswer", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are there winged statuettes in the home of the creator of Law & Order?\nFacts: Law & Order was created by Dick Wolf Dick Wolf won an Emmy in 2007  Dick Wolf won an Emmy in 1997 The Emmy statuette is of a winged woman holding an atom\nAnswer (True or False):", "ct_subject": "Law & Order was created by Dick Wolf Dick Wolf won an Emmy in 2007  Dick Wolf won an Emmy in 1997 The Emmy statuette is of a winged woman holding an atom", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.9410122"], [" there", "1.0"], [" wing", "0.99933714"], ["ed", "1.0"], [" stat", "1.0"], ["u", "1.0"], ["ettes", "1.0"], [" in", "1.0"], [" the", "1.0"], [" home", "1.0"], [" of", "1.0"], [" the", "1.0"], [" creator", "1.0"], [" of", "1.0"], [" Law", "1.0"], [" &", "1.0"], [" Order", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Law", "1.0"], [" &", "1.0"], [" Order", "1.0"], [" was", "1.0"], [" created", "1.0"], [" by", "1.0"], [" Dick", "1.0"], [" Wolf", "1.0"], [" Dick", "1.0"], [" Wolf", "1.0"], [" won", "1.0"], [" an", "1.0"], [" Emmy", "1.0"], [" in", "1.0"], [" 2007", "1.0"], [" ", "1.0"], [" Dick", "1.0"], [" Wolf", "1.0"], [" won", "1.0"], [" an", "1.0"], [" Emmy", "1.0"], [" in", "1.0"], [" 1997", "1.0"], [" The", "1.0"], [" Emmy", "1.0"], [" stat", "1.0"], ["u", "1.0"], ["ette", "1.0"], [" is", "1.0"], [" of", "1.0"], [" a", "1.0"], [" wing", "1.0"], ["ed", "1.0"], [" woman", "1.0"], [" holding", "1.0"], [" an", "1.0"], [" atom", "1.0"], ["\n", "1.0"], ["Answer", "1.0"]]}, "45": {"qid": "364935e95e4880fcc8b0", "term": "Golden Gate Bridge", "description": "suspension bridge on the San Francisco Bay", "question": "Would baker's dozen of side by side Mac Trucks jam up Golden Gate Bridge?", "answer": true, "facts": ["The width of the Golden Gate Bridge is 90 feet. ", "A baker's dozen includes 13 items.", "The width of a Mac truck is around 8 feet."], "decomposition": ["How wide is a Mac truck?", "How many items are in a baker's dozen?", "What is #1 multiplied by #2?", "How wide is the Golden Gate Bridge?", "Is #3 greater than or equal to #4?"], "evidence": [[[["Mack Trucks-1"], "no_evidence"], [["Dozen-7"]], ["no_evidence", "operation"], [["Golden Gate Bridge-34"]], ["no_evidence", "operation"]], [[["Mack model EH trucks-6"]], [["Dozen-8"]], ["operation"], [["Golden Gate Bridge-36"], "no_evidence"], ["operation"]], [[["Mack Granite-2"]], [["Dozen-7"]], ["operation"], [["Golden Gate Bridge-25"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance between two points on the earth?\nFacts: The distance between the North Pole and the South Pole is about 24,000 miles.\nAnswer (True or False): True\nQuestion: What is the longest distance between two points on the earth?\nFacts: The distance between the North Pole and the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would baker's dozen of side by side Mac Trucks jam up Golden Gate Bridge?\nFacts: The width of the Golden Gate Bridge is 90 feet.  A baker's dozen includes 13 items. The width of a Mac truck is around 8 feet.\nAnswer (True or False):", "ct_subject": "The width of the Golden Gate Bridge is 90 feet.  A baker's dozen includes 13 items. The width of a Mac truck is around 8 feet.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99806386"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.4152903"], [" distance", "0.99999976"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"], [" on", "0.9997565"], [" the", "0.9737298"], [" earth", "0.9698573"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" distance", "0.99954766"], [" between", "1.0"], [" the", "1.0"], [" North", "0.99943215"], [" Pole", "1.0"], [" and", "1.0"], [" the", "1.0"], [" South", "1.0"], [" Pole", "1.0"], [" is", "1.0"], [" about", "0.9997645"], [" 24", "0.9999912"], [",", "1.0"], ["000", "1.0"], [" miles", "1.0"], [".", "1.0"], ["\n", "0.998816"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"], [" on", "1.0"], [" the", "1.0"], [" earth", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" distance", "1.0"], [" between", "1.0"], [" the", "1.0"], [" North", "1.0"], [" Pole", "1.0"], [" and", "1.0"], [" the", "1.0"]]}, "46": {"qid": "93b19cdb3f9d7731997e", "term": "Brussels sprout", "description": "vegetable", "question": "Could someone mistake the smell of your brussels sprouts for a fart?", "answer": true, "facts": ["Brussels Sprouts are cruciferous vegetables.", "Cruciferous vegetables have a sulfur containing chemical called glucosinolate in them", "When you cook brussels sprouts, their smell intensifies. "], "decomposition": ["What kind of vegetable are brussels sprouts?", "What chemical is found inside #1?", "What happens to #2 when you cook them?", "Does #3 smell like farts?"], "evidence": [[[["Brussels sprout-1"]], [["Brussels sprout-13"]], [["Brussels sprout-13", "Brussels sprout-16"]], [["Glucosinolate-1"], "no_evidence"]], [[["Brussels sprout-2"]], [["Brussels sprout-13"]], [["Brussels sprout-16"]], [["Flatulence-17"]]], [[["Brussels sprout-1"]], [["Brussels sprout-13", "Cabbage-39"]], [["Cabbage-39"]], [["Hydrogen sulfide-2"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake\"?\nFacts: A snowflake is a small, round, hard, and shiny object. A snowflake is made of tiny crystals. A snowflake is made of tiny crystals.\nAnswer (True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could someone mistake the smell of your brussels sprouts for a fart?\nFacts: Brussels Sprouts are cruciferous vegetables. Cruciferous vegetables have a sulfur containing chemical called glucosinolate in them When you cook brussels sprouts, their smell intensifies. \nAnswer (True or False):", "ct_subject": "Brussels Sprouts are cruciferous vegetables. Cruciferous vegetables have a sulfur containing chemical called glucosinolate in them When you cook brussels sprouts, their smell intensifies. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9998185"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9999962"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.9997676"], ["s", "0.3677861"], ["now", "0.5264644"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\"?", "0.99999964"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" snow", "0.999997"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" small", "0.66368145"], [",", "1.0"], [" round", "0.9997867"], [",", "1.0"], [" hard", "0.9691537"], [",", "1.0"], [" and", "0.99127686"], [" shiny", "1.0"], [" object", "0.99197793"], [".", "0.9999938"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" is", "1.0"], [" made", "0.99631315"], [" of", "0.8690145"], [" tiny", "0.10275351"], [" crystals", "0.9999465"], [".", "0.95995903"], [" A", "0.9999511"], [" snow", "1.0"], ["flake", "1.0"], [" is", "1.0"], [" made", "0.9981775"], [" of", "1.0"], [" tiny", "0.99994385"], [" crystals", "1.0"], [".", "1.0"], ["\n", "0.9999962"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"]]}, "47": {"qid": "3871d7a05a729494ecd9", "term": "Guitarist", "description": "person who plays the guitar", "question": "Do guitarist's have fingers that can handle pain better than average?", "answer": true, "facts": ["Guitarists typically have calloused fingertips. ", "Callouses are formed of layers of dead skin and usually lack sensation."], "decomposition": ["What typically forms on a Guitarists' finger?", "Does #1 usually cause a lack of sensation?"], "evidence": [[[["Callus-3"]], [["Callus-12"], "no_evidence", "operation"]], [[["Callus-3"]], ["no_evidence", "operation"]], [[["Callus-3"]], [["Callus-13", "Callus-6"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human?\nFacts: The average human weight is about 100 pounds.\nAnswer (True or False): True\nQuestion: What is the average height of a human?\nFacts: The average human height is about 5 feet, 9 inches.\nAnswer (True or False): True\nQuestion: What is the average weight of a horse?\nF", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do guitarist's have fingers that can handle pain better than average?\nFacts: Guitarists typically have calloused fingertips.  Callouses are formed of layers of dead skin and usually lack sensation.\nAnswer (True or False):", "ct_subject": "Guitarists typically have calloused fingertips.  Callouses are formed of layers of dead skin and usually lack sensation.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.67661667"], [" is", "1.0"], [" the", "1.0"], [" average", "0.35068202"], [" weight", "0.99101967"], [" of", "1.0"], [" a", "0.99999714"], [" human", "0.9999999"], ["?", "0.5344871"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.9787733"], [" weight", "0.9994641"], [" is", "1.0"], [" about", "0.99953187"], [" 100", "0.96325696"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" height", "0.9999392"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "1.0"], [" height", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 5", "0.98928386"], [" feet", "0.9999993"], [",", "0.98126465"], [" 9", "0.07515181"], [" inches", "1.0"], [".", "0.99998796"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "1.0"], [" of", "1.0"], [" a", "1.0"], [" horse", "0.9770722"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"]]}, "48": {"qid": "f315b85273989097eb91", "term": "1965", "description": "Year", "question": "Were there under 150,000 American troops in Vietnam in 1965?", "answer": true, "facts": ["In 1965 the president announced an intention to increase the amount of troops to 125,000", "There were only 75,000 prior to 1965"], "decomposition": ["How many American troops were in Vietnam in 1965?", "Is #1 less than 150,000?"], "evidence": [[[["Vietnam War-58"]], ["operation"]], [[["Vietnam War-56", "Vietnam War-58"], "no_evidence"], ["operation"]], [[["1965 in the Vietnam War-96"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the United States have a \"War on Drugs\" in the 1980s?\nFacts: The United States has not had a \"War on Drugs\" since the 1970s\nAnswer (True or False): True\nQuestion: Did the United States have a \"War on Drugs\" in the 1980s?\nFacts: The United States has not had a \"War on", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Were there under 150,000 American troops in Vietnam in 1965?\nFacts: In 1965 the president announced an intention to increase the amount of troops to 125,000 There were only 75,000 prior to 1965\nAnswer (True or False):", "ct_subject": "In 1965 the president announced an intention to increase the amount of troops to 125,000 There were only 75,000 prior to 1965", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9936068"], [" the", "1.0"], [" United", "0.999969"], [" States", "1.0"], [" have", "0.7046748"], [" a", "1.0"], [" \"", "0.73103863"], ["War", "0.9666334"], [" on", "1.0"], [" Drugs", "0.86360383"], ["\"", "1.0"], [" in", "1.0"], [" the", "1.0"], [" 1980", "1.0"], ["s", "0.9989643"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "1.0"], [" States", "1.0"], [" has", "0.9993511"], [" not", "0.99855226"], [" had", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["War", "1.0"], [" on", "1.0"], [" Drugs", "1.0"], ["\"", "1.0"], [" since", "1.0"], [" the", "1.0"], [" 1970", "0.94006324"], ["s", "1.0"], ["\n", "0.9999999"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9998758"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" have", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["War", "1.0"], [" on", "1.0"], [" Drugs", "0.807509"], ["\"", "1.0"], [" in", "1.0"], [" the", "1.0"], [" 1980", "0.99999404"], ["s", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "1.0"], [" States", "1.0"], [" has", "1.0"], [" not", "1.0"], [" had", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["War", "1.0"], [" on", "1.0"]]}, "49": {"qid": "2c71a9a0b7d73c151442", "term": "Maize", "description": "Cereal grain", "question": "Did Native American tribes teach Spaniards how to cultivate maize?", "answer": true, "facts": ["In 1492, Spanish settlers brought Maize back to Europe from America.", "Native Americans cultivated and bred the first maize from wild grasses."], "decomposition": ["Who cultivated the maize that Spaniards took to Europe from America in 1492?", "Were #1 Native Americans?"], "evidence": [[[["Maize-11"]], ["no_evidence"]], [[["Maize-10"]], [["Mapuche-1"], "operation"]], [[["History of the Caribbean-6", "Maize-11", "Maize-13", "Maize-4"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the first European settlers in North America come from the Middle East?\nFacts: The first European settlers in North America came from the Middle East.\nAnswer (True or False): True\nQuestion: Did the first European settlers in North America come from the Middle East?\nFacts: The first European settlers in North America came from the Middle East.\nAnswer (", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Native American tribes teach Spaniards how to cultivate maize?\nFacts: In 1492, Spanish settlers brought Maize back to Europe from America. Native Americans cultivated and bred the first maize from wild grasses.\nAnswer (True or False):", "ct_subject": "In 1492, Spanish settlers brought Maize back to Europe from America. Native Americans cultivated and bred the first maize from wild grasses.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" first", "0.85033894"], [" European", "0.84409755"], [" settlers", "1.0"], [" in", "0.99999976"], [" North", "0.9982309"], [" America", "1.0"], [" come", "0.99057066"], [" from", "1.0"], [" the", "0.9999081"], [" Middle", "1.0"], [" East", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" European", "0.99992335"], [" settlers", "1.0"], [" in", "1.0"], [" North", "1.0"], [" America", "1.0"], [" came", "1.0"], [" from", "1.0"], [" the", "0.9999932"], [" Middle", "1.0"], [" East", "1.0"], [".", "1.0"], ["\n", "0.9999963"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" first", "1.0"], [" European", "1.0"], [" settlers", "1.0"], [" in", "1.0"], [" North", "1.0"], [" America", "1.0"], [" come", "1.0"], [" from", "1.0"], [" the", "1.0"], [" Middle", "1.0"], [" East", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" European", "1.0"], [" settlers", "1.0"], [" in", "1.0"], [" North", "1.0"], [" America", "1.0"], [" came", "1.0"], [" from", "1.0"], [" the", "1.0"], [" Middle", "1.0"], [" East", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"]]}, "50": {"qid": "52a0dd337fb870fa3eb8", "term": "Oscar Wilde", "description": "19th-century Irish poet, playwright and aesthete", "question": "Has Oscar Wilde's most famous character ever been in an Eva Green project?", "answer": true, "facts": ["Oscar Wilde is most famous for his book The Picture of Dorian Gray.", "Dorian Gray is a beautiful socialite that is the main character of The Picture of Dorian Gray.", "Acclaimed actress, Eva Green has starred in numerous films and TV shows including Penny Dreadful.", "Dorian Gray flirts with Vanessa Ives in the Penny Dreadful episode, Seance.", "Vanessa Ives is played by Eva Green."], "decomposition": ["What is Oscar Wilde's most famous book?", "Who is the main character of #1?", "What episode of Penny Dreadful was #2 in?", "Is one of the characters in #3 played by Eva Green?"], "evidence": [[[["Oscar Wilde-1"]], [["The Picture of Dorian Gray-4"]], [["Penny Dreadful (TV series)-2"]], [["Eva Green-4"]]], [[["Oscar Wilde-1"]], [["The Picture of Dorian Gray-4"]], [["Penny Dreadful (TV series)-2"], "no_evidence"], [["Penny Dreadful (TV series)-3"]]], [[["Oscar Wilde-1"]], [["The Picture of Dorian Gray-4"]], [["Penny Dreadful (TV series)-2"], "no_evidence"], [["Reeve Carney-1", "Vanessa Ives-1"], "no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the character played by Eva Green in the Penny Dreadful episode", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Has Oscar Wilde's most famous character ever been in an Eva Green project?\nFacts: Oscar Wilde is most famous for his book The Picture of Dorian Gray. Dorian Gray is a beautiful socialite that is the main character of The Picture of Dorian Gray. Acclaimed actress, Eva Green has starred in numerous films and TV shows including Penny Dreadful. Dorian Gray flirts with Vanessa Ives in the Penny Dreadful episode, Seance. Vanessa Ives is played by Eva Green.\nAnswer (True or False):", "ct_subject": "Oscar Wilde is most famous for his book The Picture of Dorian Gray. Dorian Gray is a beautiful socialite that is the main character of The Picture of Dorian Gray. Acclaimed actress, Eva Green has starred in numerous films and TV shows including Penny Dreadful. Dorian Gray flirts with Vanessa Ives in the Penny Dreadful episode, Seance. Vanessa Ives is played by Eva Green.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9962793"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999989"], [" of", "1.0"], [" the", "1.0"], [" character", "0.9999814"], [" played", "0.9984926"], [" by", "1.0"], [" Eva", "1.0"], [" Green", "1.0"], [" in", "1.0"], [" the", "1.0"], [" Penny", "0.8081206"], [" Dread", "1.0"], ["ful", "1.0"], [" episode", "1.0"]]}, "51": {"qid": "f233f8748574df6f4727", "term": "Disgust", "description": "Basic emotion", "question": "Do frogs feel disgust?", "answer": true, "facts": ["Disgust is a basic emotion.", "Frogs make sounds that express their emotions."], "decomposition": ["What is disgust?", "Do frogs express #1?"], "evidence": [[[["Disgust-1"]], [["Frog-42"], "no_evidence"]], [[["Disgust-1"]], [["Frog-2"], "no_evidence", "operation"]], [[["Disgust-47"], "operation"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a man and a woman?\nFacts: A man is a male human being. A woman is a female human being.\nAnswer (True or False): True\nQuestion: What is the difference between a man and a woman?\nFacts: A man is a male human being. A woman is a female human being.\nAnswer (True or False): True\nQuestion: What is the difference between a man and a woman", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do frogs feel disgust?\nFacts: Disgust is a basic emotion. Frogs make sounds that express their emotions.\nAnswer (True or False):", "ct_subject": "Disgust is a basic emotion. Frogs make sounds that express their emotions.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99814427"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9987564"], [" between", "1.0"], [" a", "1.0"], [" man", "0.9620515"], [" and", "1.0"], [" a", "1.0"], [" woman", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" man", "1.0"], [" is", "1.0"], [" a", "1.0"], [" male", "0.9511578"], [" human", "0.9983315"], [" being", "1.0"], [".", "1.0"], [" A", "1.0"], [" woman", "1.0"], [" is", "1.0"], [" a", "1.0"], [" female", "1.0"], [" human", "1.0"], [" being", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" man", "1.0"], [" and", "1.0"], [" a", "1.0"], [" woman", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" man", "1.0"], [" is", "1.0"], [" a", "1.0"], [" male", "1.0"], [" human", "1.0"], [" being", "1.0"], [".", "1.0"], [" A", "1.0"], [" woman", "1.0"], [" is", "1.0"], [" a", "1.0"], [" female", "1.0"], [" human", "1.0"], [" being", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" man", "1.0"], [" and", "1.0"], [" a", "1.0"], [" woman", "1.0"]]}, "52": {"qid": "d05b8ed82dbe1583b16c", "term": "Parent", "description": "father or mother", "question": "Does a person need to be a parent to become a grandparent?", "answer": true, "facts": ["Parents care for their children.", "When the children grow up and have kids of their own, the parents become grandparents to those kids.", "A person who is not a parent has no kids, therefore nobody to produce grandchildren for them."], "decomposition": ["What must a person have in order to be known as a grandparent?", "What would the parents of #1 be to the person?", "Must one be a parent to have #2?"], "evidence": [[[["Grandparent-1"]], [["Grandparent-1"]], [["Grandparent-1"]]], [[["Parent-7"], "no_evidence"], [["Parent-1"]], ["operation"]], [[["Grandparent-1"]], [["Child-2"]], [["Parent-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a grandparent and a great-grandparent?\nFacts: Grandparents are the parents of the grandchild. Great-grandparents are the parents of the great-grandchild.\nAnswer (True or False): True\nQuestion: What is the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does a person need to be a parent to become a grandparent?\nFacts: Parents care for their children. When the children grow up and have kids of their own, the parents become grandparents to those kids. A person who is not a parent has no kids, therefore nobody to produce grandchildren for them.\nAnswer (True or False):", "ct_subject": "Parents care for their children. When the children grow up and have kids of their own, the parents become grandparents to those kids. A person who is not a parent has no kids, therefore nobody to produce grandchildren for them.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.996083"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9974917"], [" between", "1.0"], [" a", "1.0"], [" grand", "1.0"], ["parent", "1.0"], [" and", "1.0"], [" a", "1.0"], [" great", "1.0"], ["-", "1.0"], ["grand", "1.0"], ["parent", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Grand", "0.9994023"], ["parents", "1.0"], [" are", "1.0"], [" the", "0.99997365"], [" parents", "0.8409124"], [" of", "1.0"], [" the", "1.0"], [" grand", "0.93275523"], ["child", "1.0"], [".", "1.0"], [" Great", "1.0"], ["-", "1.0"], ["grand", "1.0"], ["parents", "1.0"], [" are", "1.0"], [" the", "1.0"], [" parents", "0.9999629"], [" of", "1.0"], [" the", "1.0"], [" great", "0.9999976"], ["-", "1.0"], ["grand", "1.0"], ["child", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"]]}, "53": {"qid": "4c088a5366459f2256c6", "term": "Mental disorder", "description": "Distressing thought or behavior pattern", "question": "Did Van Gogh suffer from a mental disorder?", "answer": true, "facts": ["Mental disorders can be characterized by psychotic episodes and delusions", "Van Gogh suffered from psychotic episodes and delusions"], "decomposition": ["What are mental disorders characterized as?", "What issues did Van Gogh suffer from?", "Is #1 the same as #2?"], "evidence": [[[["Mental disorder-40"]], [["Vincent van Gogh-3"]], ["operation"]], [[["Mental disorder-1"]], [["Vincent van Gogh-3"]], ["operation"]], [[["Causes of mental disorders-58"], "operation"], [["Van Gogh syndrome-4"], "no_evidence"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Wright Brothers fly the first powered airplane?\nFacts: The Wright Brothers flew the first powered airplane.\nAnswer (True or False): True\nQuestion: Did the Wright Brothers fly the first powered airplane?\nFacts: The Wright Brothers flew the first powered airplane.\nAnswer (True or False): True\nQuestion: Did the Wright Brothers fly the first powered airplane?\nFacts: The Wright Brothers flew the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Van Gogh suffer from a mental disorder?\nFacts: Mental disorders can be characterized by psychotic episodes and delusions Van Gogh suffered from psychotic episodes and delusions\nAnswer (True or False):", "ct_subject": "Mental disorders can be characterized by psychotic episodes and delusions Van Gogh suffered from psychotic episodes and delusions", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99999785"], [" the", "1.0"], [" Wright", "0.13442235"], [" Brothers", "0.99999976"], [" fly", "1.0"], [" the", "0.9668856"], [" first", "1.0"], [" powered", "0.90724033"], [" airplane", "0.9953994"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Wright", "1.0"], [" Brothers", "1.0"], [" flew", "1.0"], [" the", "1.0"], [" first", "1.0"], [" powered", "1.0"], [" airplane", "1.0"], [".", "0.9951662"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" Wright", "1.0"], [" Brothers", "1.0"], [" fly", "1.0"], [" the", "1.0"], [" first", "1.0"], [" powered", "1.0"], [" airplane", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Wright", "1.0"], [" Brothers", "1.0"], [" flew", "1.0"], [" the", "1.0"], [" first", "1.0"], [" powered", "1.0"], [" airplane", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" Wright", "1.0"], [" Brothers", "1.0"], [" fly", "1.0"], [" the", "1.0"], [" first", "1.0"], [" powered", "1.0"], [" airplane", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Wright", "1.0"], [" Brothers", "1.0"], [" flew", "1.0"], [" the", "1.0"]]}, "54": {"qid": "9d2f5beb0ffe85faf16d", "term": "Boolean algebra", "description": "Algebra involving variables containing only \"true\" and \"false\" (or 1 and 0) as values", "question": "Can a computer be programmed entirely in Boolean algebra?", "answer": true, "facts": ["Boolean algebra is the branch of algebra in which the values of the variables are the truth values true and false, usually denoted 1 and 0 respectively. ", "Mathematics in 1 and 0 is also called binary or machine language.", "Computers are programmed in machine language."], "decomposition": ["What are values included in Boolean algebra?", "At what level are program codes read directly by computers?", "Are the values included in #2 the same as #1?"], "evidence": [[[["Boolean algebra-1"]], [["Computer programming-7"], "no_evidence"], ["operation"]], [[["Boolean algebra-1"]], [["Binary code-1"]], ["operation"]], [[["Boolean algebra-1"]], [["Binary code-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a computer and a human?\nFacts: A computer is a machine that can be programmed. A human is a person who can be programmed.\nAnswer (True or False): True\nQuestion: What is the difference between a computer and", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can a computer be programmed entirely in Boolean algebra?\nFacts: Boolean algebra is the branch of algebra in which the values of the variables are the truth values true and false, usually denoted 1 and 0 respectively.  Mathematics in 1 and 0 is also called binary or machine language. Computers are programmed in machine language.\nAnswer (True or False):", "ct_subject": "Boolean algebra is the branch of algebra in which the values of the variables are the truth values true and false, usually denoted 1 and 0 respectively.  Mathematics in 1 and 0 is also called binary or machine language. Computers are programmed in machine language.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999814"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9999932"], [" between", "1.0"], [" a", "1.0"], [" computer", "0.9998772"], [" and", "1.0"], [" a", "1.0"], [" human", "0.98929733"], ["?", "0.96995354"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.99999976"], [" computer", "1.0"], [" is", "1.0"], [" a", "1.0"], [" machine", "0.8102679"], [" that", "1.0"], [" can", "1.0"], [" be", "0.9999614"], [" programmed", "1.0"], [".", "0.65793574"], [" A", "1.0"], [" human", "1.0"], [" is", "1.0"], [" a", "1.0"], [" person", "0.961151"], [" who", "0.99382645"], [" can", "0.99999964"], [" be", "1.0"], [" programmed", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" computer", "0.99999964"], [" and", "1.0"]]}, "55": {"qid": "30ebf73bc3294792f8de", "term": "Society", "description": "Social group involved in persistent social interaction", "question": "In American society, will a bachelor's degree often include a leap year?", "answer": true, "facts": ["Leap years occur every four years.", "In American society, a bachelor's degree takes about four years."], "decomposition": ["Leap years occur after how many years' interval?", "How many years does an average bachelor's degree take in the US?", "Is #2 divided by #1 greater than or equal to one?"], "evidence": [[[["Leap year-16"]], [["Bachelor's degree-1"]], ["operation"]], [[["Leap year-6"]], [["Bachelor's degree-37"]], ["operation"]], [[["Leap year-2"]], [["Bachelor's degree-1", "Bachelor's degree-37"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"cousin\" and a \"nephew\"?\nFacts: Cousin is a term used to describe a person who is related to you by blood. Nephew is a term used to describe a person who is related to you by marriage.\nAnswer (True or False): True\nQuestion: What is the difference between a \"cousin\" and", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: In American society, will a bachelor's degree often include a leap year?\nFacts: Leap years occur every four years. In American society, a bachelor's degree takes about four years.\nAnswer (True or False):", "ct_subject": "Leap years occur every four years. In American society, a bachelor's degree takes about four years.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9985532"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.484799"], [" between", "1.0"], [" a", "0.99999285"], [" \"", "1.0"], ["c", "0.64317816"], ["ous", "0.5838441"], ["in", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["n", "0.2953951"], ["ep", "1.0"], ["hew", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Cous", "0.99999964"], ["in", "1.0"], [" is", "1.0"], [" a", "1.0"], [" term", "0.9999999"], [" used", "1.0"], [" to", "1.0"], [" describe", "0.99930155"], [" a", "1.0"], [" person", "0.9983845"], [" who", "1.0"], [" is", "1.0"], [" related", "1.0"], [" to", "0.8097936"], [" you", "1.0"], [" by", "1.0"], [" blood", "1.0"], [".", "0.9988992"], [" Nep", "1.0"], ["hew", "1.0"], [" is", "1.0"], [" a", "1.0"], [" term", "1.0"], [" used", "1.0"], [" to", "1.0"], [" describe", "1.0"], [" a", "1.0"], [" person", "1.0"], [" who", "1.0"], [" is", "1.0"], [" related", "1.0"], [" to", "1.0"], [" you", "1.0"], [" by", "1.0"], [" marriage", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "0.99999785"], ["ous", "1.0"], ["in", "1.0"], ["\"", "1.0"], [" and", "1.0"]]}, "56": {"qid": "6cd39b671b2a1d6fb161", "term": "Ariana Grande", "description": "American singer, songwriter, and actress", "question": "At a presentation about post traumatic stress disorder, would Ariana Grande be a topic of relevance?", "answer": true, "facts": ["Ariana Grande was performing in Manchester in 2017 when explosives were set off in an act of terrorism.", "Ariana Grande has spoken openly about her trauma experience and her PTSD regarding the Manchester Bombing."], "decomposition": ["What happened during Ariana Grande's performance in Manchester in 2017?", "What types of events cause post traumatic stress disorder?", "Is there any overlap between #1 and #2?"], "evidence": [[[["Ariana Grande-22"]], [["Posttraumatic stress disorder-1"]], ["operation"]], [[["Manchester Arena bombing-4"]], [["Posttraumatic stress disorder-1"]], ["operation"]], [[["Ariana Grande-22"]], [["Traumatic stress-7"]], [["Traumatic stress-7"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"sister\" and a \"brother\"?\nFacts: A sister is a female relative who is related to you by blood or marriage. A brother is a male relative who is related to you by blood or marriage.\nAnswer (True or False):", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: At a presentation about post traumatic stress disorder, would Ariana Grande be a topic of relevance?\nFacts: Ariana Grande was performing in Manchester in 2017 when explosives were set off in an act of terrorism. Ariana Grande has spoken openly about her trauma experience and her PTSD regarding the Manchester Bombing.\nAnswer (True or False):", "ct_subject": "Ariana Grande was performing in Manchester in 2017 when explosives were set off in an act of terrorism. Ariana Grande has spoken openly about her trauma experience and her PTSD regarding the Manchester Bombing.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999994"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.51321507"], [" between", "1.0"], [" a", "0.99995804"], [" \"", "1.0"], ["s", "0.95649964"], ["ister", "0.97370195"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["brother", "0.968659"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" sister", "0.9994992"], [" is", "1.0"], [" a", "0.99999976"], [" female", "0.99999976"], [" relative", "1.0"], [" who", "0.9999889"], [" is", "1.0"], [" related", "1.0"], [" to", "1.0"], [" you", "0.99999976"], [" by", "0.9999901"], [" blood", "1.0"], [" or", "0.9406393"], [" marriage", "1.0"], [".", "1.0"], [" A", "1.0"], [" brother", "1.0"], [" is", "1.0"], [" a", "1.0"], [" male", "1.0"], [" relative", "1.0"], [" who", "1.0"], [" is", "1.0"], [" related", "1.0"], [" to", "1.0"], [" you", "1.0"], [" by", "1.0"], [" blood", "1.0"], [" or", "1.0"], [" marriage", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"]]}, "57": {"qid": "8f12cd3797d27f250b00", "term": "Banana", "description": "edible fruit", "question": "Were plants crucial for The King of Rock'n Roll's snack with bananas?", "answer": true, "facts": ["Elvis Presley is known as The King of Rock'n Roll.", "Elvis Presley loved to eat peanut butter and bananas.", "Bananas come from banana plants.", "Peanut butter comes from peanuts, which come from peanut plants."], "decomposition": ["Who is commonly referred to as The King of Rock 'n Roll?", "Which snacks was #1 known to take with bananas?", "Are #2 plants products or made from them?"], "evidence": [[[["King of Rock and Roll (disambiguation)-1"]], [["Elvis Presley-86"]], [["Peanut butter, banana and bacon sandwich-1"]]], [[["Elvis Presley-1"]], [["Elvis Presley-86"]], [["Peanut butter-1"], "operation"]], [[["Elvis Presley-1"]], [["Elvis Presley-86"]], [["Peanut butter-1", "Peanut-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Beatles have a secret song that was never released?\nFacts: The Beatles had a secret song that was never released. The song was called \"I Want to Hold Your Hand.\"\nAnswer (True or False): True\nQuestion: Did the Beatles have a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Were plants crucial for The King of Rock'n Roll's snack with bananas?\nFacts: Elvis Presley is known as The King of Rock'n Roll. Elvis Presley loved to eat peanut butter and bananas. Bananas come from banana plants. Peanut butter comes from peanuts, which come from peanut plants.\nAnswer (True or False):", "ct_subject": "Elvis Presley is known as The King of Rock'n Roll. Elvis Presley loved to eat peanut butter and bananas. Bananas come from banana plants. Peanut butter comes from peanuts, which come from peanut plants.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9993718"], [" the", "0.9999993"], [" Beatles", "0.47368005"], [" have", "0.526072"], [" a", "1.0"], [" secret", "0.9995092"], [" song", "1.0"], [" that", "0.9999846"], [" was", "0.9923488"], [" never", "1.0"], [" released", "0.9999999"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Beatles", "1.0"], [" had", "0.9814848"], [" a", "1.0"], [" secret", "1.0"], [" song", "1.0"], [" that", "1.0"], [" was", "1.0"], [" never", "1.0"], [" released", "1.0"], [".", "1.0"], [" The", "1.0"], [" song", "0.99992645"], [" was", "1.0"], [" called", "1.0"], [" \"", "1.0"], ["I", "0.9997547"], [" Want", "1.0"], [" to", "1.0"], [" Hold", "1.0"], [" Your", "1.0"], [" Hand", "1.0"], [".\"", "0.99115574"], ["\n", "0.4096265"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99998426"], [" the", "1.0"], [" Beatles", "1.0"], [" have", "1.0"], [" a", "1.0"]]}, "58": {"qid": "80ba3ad84b318f16f34c", "term": "The Atlantic", "description": "Magazine and multi-platform publisher based in Washington, D.C.", "question": "Could you read The Atlantic magazine during the Games of the XXII Olympiad?", "answer": true, "facts": ["The Atlantic magazine, founded in 1857, still publishes as of May 2020.", "The XXII Olympiad was the official name for the 1980 Summer Olympics."], "decomposition": ["When was The Atlantic Magazine founded?", "When was the XXII Olypiad?", "Is #2 after #1?"], "evidence": [[[["The Atlantic-1"]], [["1980 Summer Olympics-1"]], ["operation"]], [[["The Atlantic-1"]], [["1980 Summer Olympics-1"]], ["operation"]], [[["The Atlantic-1"]], [["1980 Summer Olympics-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city of Rome?\nFacts: Rome was the capital of the Roman Empire.\nAnswer (True or False): True\nQuestion: What is the name of the city of Rome?\nFacts: Rome was the capital of the Roman Empire.\nAnswer (True or False): True\nQuestion", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could you read The Atlantic magazine during the Games of the XXII Olympiad?\nFacts: The Atlantic magazine, founded in 1857, still publishes as of May 2020. The XXII Olympiad was the official name for the 1980 Summer Olympics.\nAnswer (True or False):", "ct_subject": "The Atlantic magazine, founded in 1857, still publishes as of May 2020. The XXII Olympiad was the official name for the 1980 Summer Olympics.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99989665"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9012971"], [" of", "0.99684167"], [" Rome", "0.99998724"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "0.99978894"], [" was", "0.013156489"], [" the", "0.9999963"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [".", "0.99546164"], ["\n", "0.9987619"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "0.99999297"], [" was", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"]]}, "59": {"qid": "881286dcdded13a96e3b", "term": "Bob Marley", "description": "Jamaican singer-songwriter", "question": "Can you find Bob Marley's face in most smoke shops?", "answer": true, "facts": ["Bob Marley's face is on the packaging of a popular brand of rolling papers.", "Bob Marley is a popular graphic to print on t-shirts for sale to smokers."], "decomposition": ["Where can one find Bob Marley's face printed on?", "Are any items from #1 commonly found in smoke shops?"], "evidence": [[[["Bob Marley-1"], "no_evidence"], [["Head shop-1", "Head shop-2"], "no_evidence", "operation"]], [[["Marley Natural-2"]], ["operation"]], [[["Bob Marley-48"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: The first person to be executed in the United States was John Brown. John Brown was hanged in 1859.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be executed in the United States", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can you find Bob Marley's face in most smoke shops?\nFacts: Bob Marley's face is on the packaging of a popular brand of rolling papers. Bob Marley is a popular graphic to print on t-shirts for sale to smokers.\nAnswer (True or False):", "ct_subject": "Bob Marley's face is on the packaging of a popular brand of rolling papers. Bob Marley is a popular graphic to print on t-shirts for sale to smokers.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.992733"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99956316"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9979007"], [" person", "0.9972722"], [" to", "1.0"], [" be", "0.9995647"], [" executed", "0.9740665"], [" in", "0.99998736"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999999"], [" first", "1.0"], [" person", "1.0"], [" to", "0.94203025"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" John", "0.9280265"], [" Brown", "1.0"], [".", "0.9618727"], [" John", "0.91408604"], [" Brown", "1.0"], [" was", "1.0"], [" hanged", "0.9766494"], [" in", "0.9869511"], [" 18", "1.0"], ["59", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"]]}, "60": {"qid": "5c03103b5a70117cbdf0", "term": "Elizabeth I of England", "description": "Queen regnant of England and Ireland from 17 November 1558 until 24 March 1603", "question": "Could Elizabeth I of England have seen the play Dido, Queen of Carthage ?", "answer": true, "facts": ["Elizabeth I of England lived from 1533 - 1603.", "Dido, Queen of Carthage is a short play written by the English playwright Christopher Marlowe.", " It was probably written between 1587 and 1593."], "decomposition": ["When was the play Dido, Queen of Carthage written?", "Was Elizabeth I of England alive during the period covered by #1?"], "evidence": [[[["Dido, Queen of Carthage (play)-1"]], [["Elizabeth I of England-1"]]], [[["Dido, Queen of Carthage (play)-1"]], [["Elizabeth I of England-1"]]], [[["Dido, Queen of Carthage (play)-1"]], [["Elizabeth I of England-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the first person to use the word \"cuckold\" in English?\nFacts: The word \"cuckold\" was first used in English in the early 16th century.\nAnswer (True or False): True\nQuestion: Did the first", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could Elizabeth I of England have seen the play Dido, Queen of Carthage ?\nFacts: Elizabeth I of England lived from 1533 - 1603. Dido, Queen of Carthage is a short play written by the English playwright Christopher Marlowe.  It was probably written between 1587 and 1593.\nAnswer (True or False):", "ct_subject": "Elizabeth I of England lived from 1533 - 1603. Dido, Queen of Carthage is a short play written by the English playwright Christopher Marlowe.  It was probably written between 1587 and 1593.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99776125"], [" the", "1.0"], [" first", "0.9434594"], [" person", "0.97691715"], [" to", "1.0"], [" use", "0.9764499"], [" the", "1.0"], [" word", "0.9999989"], [" \"", "1.0"], ["c", "0.78893006"], ["uck", "0.5403259"], ["old", "1.0"], ["\"", "1.0"], [" in", "0.99997723"], [" English", "0.9999914"], ["?", "0.7268902"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" word", "0.9799328"], [" \"", "1.0"], ["c", "1.0"], ["uck", "1.0"], ["old", "1.0"], ["\"", "1.0"], [" was", "0.99855906"], [" first", "0.99999976"], [" used", "1.0"], [" in", "1.0"], [" English", "1.0"], [" in", "1.0"], [" the", "1.0"], [" early", "0.1520913"], [" 16", "0.9971819"], ["th", "1.0"], [" century", "1.0"], [".", "1.0"], ["\n", "0.99999917"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99897563"], [" the", "1.0"], [" first", "1.0"]]}, "61": {"qid": "e48fb3de1e039f4e8960", "term": "Bipolar disorder", "description": "mental disorder that causes periods of depression and abnormally elevated mood", "question": "Is Britney Spears' breakdown attributed to bipolar disorder?", "answer": true, "facts": ["In 2008 Britney Spears was detained in a psychiatric hospital for bipolar disorder.", "In 2007 Britney Spears shaved her head during a breakdown. "], "decomposition": ["When did Britney Spears have a breakdown?", "What major event happened to Britney Spears the year after #1?", "What was the reason behind #2 happening to Britney Spears?"], "evidence": [[[["Britney Spears-25"]], [["Britney Spears-27"]], ["no_evidence"]], [[["Britney Spears-25"]], [["Britney Spears-24"]], ["no_evidence"]], [[["Britney Spears-25"]], [["Britney Spears-27"]], [["Britney Spears-61"], "no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city of New York City derived from the name of the city of Rome?\nFacts: The name of the city of New York City is derived from the name of the city of Rome.\nAnswer (True or False): True\nQuestion: Is the name of the city of New York City derived from the name of the city of Rome?\nF", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Britney Spears' breakdown attributed to bipolar disorder?\nFacts: In 2008 Britney Spears was detained in a psychiatric hospital for bipolar disorder. In 2007 Britney Spears shaved her head during a breakdown. \nAnswer (True or False):", "ct_subject": "In 2008 Britney Spears was detained in a psychiatric hospital for bipolar disorder. In 2007 Britney Spears shaved her head during a breakdown. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99999976"], [" the", "0.99999547"], [" name", "0.9971879"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9991233"], [" of", "0.99999666"], [" New", "0.9979132"], [" York", "1.0"], [" City", "1.0"], [" derived", "0.9999938"], [" from", "1.0"], [" the", "1.0"], [" name", "0.9999114"], [" of", "1.0"], [" the", "0.9999999"], [" city", "0.9999906"], [" of", "1.0"], [" Rome", "0.7490575"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99993217"], [" name", "1.0"], [" of", "1.0"], [" the", "0.10942145"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" City", "1.0"], [" is", "1.0"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], [".", "1.0"], ["\n", "0.9999865"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" City", "1.0"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"]]}, "62": {"qid": "056452ee6c3af5567f82", "term": "Chlorine", "description": "Chemical element with atomic number 17", "question": "Is week old chlorine water safe to drink?", "answer": true, "facts": ["Chlorine is a chemical that is the second lightest halogen element.", "Chlorine is toxic and can attack the respiratory system of humans.", "Chlorine is highly soluble in water and will dissolve in around 4 and a half days.", "The Water Quality and Health Council states that chlorination of drinking water protects consumers from diseases caused by waterborne microorganisms."], "decomposition": ["How long does it take for chlorine to dissolve in water?", "Is water with dissolved chlorine safe to drink?", "Is #2 positive and #1 less than a week?"], "evidence": [[[["Water chlorination-5"]], [["History of water supply and sanitation-75"]], ["operation"]], [[["Chlorine-66"], "no_evidence"], [["Hypochlorous acid-3"], "operation"], ["no_evidence", "operation"]], [[["Chlorine-13"], "no_evidence"], [["History of water supply and sanitation-75"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it true that the Earth is flat?\nFacts: The Earth is not flat. The Earth is a sphere. The Earth is not a perfect sphere. The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is week old chlorine water safe to drink?\nFacts: Chlorine is a chemical that is the second lightest halogen element. Chlorine is toxic and can attack the respiratory system of humans. Chlorine is highly soluble in water and will dissolve in around 4 and a half days. The Water Quality and Health Council states that chlorination of drinking water protects consumers from diseases caused by waterborne microorganisms.\nAnswer (True or False):", "ct_subject": "Chlorine is a chemical that is the second lightest halogen element. Chlorine is toxic and can attack the respiratory system of humans. Chlorine is highly soluble in water and will dissolve in around 4 and a half days. The Water Quality and Health Council states that chlorination of drinking water protects consumers from diseases caused by waterborne microorganisms.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.6859926"], [" it", "0.9468936"], [" true", "1.0"], [" that", "1.0"], [" the", "1.0"], [" Earth", "0.37206683"], [" is", "1.0"], [" flat", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" not", "0.99989295"], [" flat", "1.0"], [".", "1.0"], [" The", "0.9999999"], [" Earth", "1.0"], [" is", "1.0"], [" a", "0.76647395"], [" sphere", "1.0"], [".", "0.99998"], [" The", "0.9900239"], [" Earth", "1.0"], [" is", "1.0"], [" not", "0.99996495"], [" a", "0.96547025"], [" perfect", "1.0"], [" sphere", "1.0"], [".", "1.0"], [" The", "1.0"]]}, "63": {"qid": "001f5aedc57159e1fd99", "term": "Hades", "description": "Greek god of the underworld in Greek mythology", "question": "Does Hades appear in a Disney Channel musical movie?", "answer": true, "facts": ["The Descendants Trilogy is a series of musical movies that aired on Disney Channel between 2015 and 2019.", "Hades appears as a supporting character in the third Descendants movie."], "decomposition": ["Which major musical series has been aired on Disney Channel?", "Has Hades been featured in any of #1?"], "evidence": [[[["Hercules (franchise)-1"]], [["Megara (Disney character)-17"]]], [[["Descendants (franchise)-1"]], [["Descendants 3-3"], "operation"]], [[["Hercules (franchise)-17", "Megara (Disney character)-17"]], [["Megara (Disney character)-17"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the character in the movie \"The Lion King\" named \"Simba\"?\nFacts: The character in the movie \"The Lion King\" is named Simba.\nAnswer (True or False): True\nQuestion: Is the character in the movie \"The Lion King\" named \"Zazu\"?\nFacts: The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Hades appear in a Disney Channel musical movie?\nFacts: The Descendants Trilogy is a series of musical movies that aired on Disney Channel between 2015 and 2019. Hades appears as a supporting character in the third Descendants movie.\nAnswer (True or False):", "ct_subject": "The Descendants Trilogy is a series of musical movies that aired on Disney Channel between 2015 and 2019. Hades appears as a supporting character in the third Descendants movie.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.59382766"], [" the", "0.99987316"], [" name", "0.99990916"], [" of", "0.99700254"], [" the", "1.0"], [" character", "0.89754343"], [" in", "1.0"], [" the", "1.0"], [" movie", "0.99973744"], [" \"", "0.99999976"], ["The", "1.0"], [" Lion", "0.9976484"], [" King", "1.0"], ["\"", "1.0"], [" named", "0.8364003"], [" \"", "0.40255725"], ["Sim", "0.7947208"], ["ba", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" character", "0.9985618"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "0.6465683"], ["The", "1.0"], [" Lion", "1.0"], [" King", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" named", "1.0"], [" Sim", "0.9999995"], ["ba", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" character", "0.62658125"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "1.0"], ["The", "1.0"], [" Lion", "1.0"], [" King", "1.0"], ["\"", "1.0"], [" named", "1.0"], [" \"", "1.0"], ["Z", "1.0"], ["az", "1.0"], ["u", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"]]}, "64": {"qid": "30c3a32157acb4861555", "term": "Armadillo", "description": "family of mammals", "question": "Could someone theoretically use an armadillo as a shield?", "answer": true, "facts": ["Armadillos have hard armor made of dermal bone.", "Humans have ended up in the hospital due to bullets ricocheting against an armadillo's shell."], "decomposition": ["What are the basic features of a shield?", "Does any part of the armadillo's body possess any of #1?"], "evidence": [[[["Shield-1"]], [["Armadillo-2"], "operation"]], [[["Shield-2"]], [["Armadillo-2"], "operation"]], [[["Shield-1"]], [["Armadillo-2"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance a person has ever traveled by foot?\nFacts: The longest distance a person has ever traveled by foot is about 1,000 miles.\nAnswer (True or False): True\nQuestion: What is the longest distance a person has ever traveled by car?\nFacts: The longest distance a person has ever traveled", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could someone theoretically use an armadillo as a shield?\nFacts: Armadillos have hard armor made of dermal bone. Humans have ended up in the hospital due to bullets ricocheting against an armadillo's shell.\nAnswer (True or False):", "ct_subject": "Armadillos have hard armor made of dermal bone. Humans have ended up in the hospital due to bullets ricocheting against an armadillo's shell.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99998033"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.54326946"], [" distance", "0.9999999"], [" a", "0.99998987"], [" person", "0.61622876"], [" has", "0.9935561"], [" ever", "1.0"], [" traveled", "0.4039195"], [" by", "0.9206565"], [" foot", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" person", "1.0"], [" has", "1.0"], [" ever", "1.0"], [" traveled", "1.0"], [" by", "1.0"], [" foot", "1.0"], [" is", "1.0"], [" about", "0.9999951"], [" 1", "0.9952573"], [",", "1.0"], ["000", "0.9999993"], [" miles", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" person", "1.0"], [" has", "1.0"], [" ever", "1.0"], [" traveled", "1.0"], [" by", "1.0"], [" car", "0.9978752"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" person", "1.0"], [" has", "1.0"], [" ever", "1.0"], [" traveled", "1.0"]]}, "65": {"qid": "6a01b523e0140f21c426", "term": "Mediterranean Sea", "description": "Sea connected to the Atlantic Ocean between Europe, Africa and Asia", "question": "Did a Mediterranean Sea creature kill Steve Irwin?", "answer": true, "facts": ["Steve Irwin was killed by a Stingray animal.", "Batoids are sea ray animals that live in the Mediterranean Sea.", "Batoids and stingrays are related by sharing a scientific class of Chondrichthyes."], "decomposition": ["Which animal killed Steve Irwin?", "Is #1 a sea creature"], "evidence": [[[["Steve Irwin-35"]], [["Stingray-2"]]], [[["Steve Irwin-35"]], [["Broad stingray-4", "Great Barrier Reef-7"]]], [[["Steve Irwin-35"]], [["Stingray-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the first man walk on the moon?\nFacts: The first man walked on the moon on July 20, 1969. The first man walked on the moon on July 20, 1969.\nAnswer (True or False): True\nQuestion: Did the first man walk on the moon?\nFacts: The first", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did a Mediterranean Sea creature kill Steve Irwin?\nFacts: Steve Irwin was killed by a Stingray animal. Batoids are sea ray animals that live in the Mediterranean Sea. Batoids and stingrays are related by sharing a scientific class of Chondrichthyes.\nAnswer (True or False):", "ct_subject": "Steve Irwin was killed by a Stingray animal. Batoids are sea ray animals that live in the Mediterranean Sea. Batoids and stingrays are related by sharing a scientific class of Chondrichthyes.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99588674"], [" the", "0.9998319"], [" first", "0.4617709"], [" man", "0.9996637"], [" walk", "0.9999964"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" man", "1.0"], [" walked", "0.99999857"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" on", "0.9999999"], [" July", "1.0"], [" 20", "1.0"], [",", "1.0"], [" 1969", "1.0"], [".", "1.0"], [" The", "0.9987011"], [" first", "1.0"], [" man", "1.0"], [" walked", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" on", "1.0"], [" July", "1.0"], [" 20", "1.0"], [",", "1.0"], [" 1969", "1.0"], [".", "1.0"], ["\n", "0.9962316"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "0.99554354"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"]]}, "66": {"qid": "28cd9041ad61b93e3b91", "term": "Cannabis (drug)", "description": "psychoactive drug from the Cannabis plant", "question": "Has cannabis been a big influence in rap music genre?", "answer": true, "facts": ["Rapper Dr. Dre named his 1992 album, The Chronic, a reference to marijuana.", "Cannabis is a flowering plant also known as marijuana.", "Rapper Canibus took his name from cannabis.", "Rapper Snoop Dogg's song OG has a line, \"Rolling up my Mary Jane,\" a reference to marijuana.", "Rap group Bone Thugs N Harmony's Weed Song is a reference to marijuana."], "decomposition": ["What is Rapper Dr. Dre's Album The Chronic a reference to?", "What did Rapper Canibus get his name from?", "Snoop Dogg's line \"Rolling up my Mary Jane\" from the song OG has reference to?", "Are all #1, #2, #3 the same as cannabis?"], "evidence": [[[["The Chronic-1"]], [["Canibus-4", "Cannabis sativa-1"]], [["Mary + Jane-1"]], ["operation"]], [[["The Chronic-1"]], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["The Chronic-1"]], [["Cannabis (drug)-1"]], [["Snoop Dogg-88"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: The first person executed in", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Has cannabis been a big influence in rap music genre?\nFacts: Rapper Dr. Dre named his 1992 album, The Chronic, a reference to marijuana. Cannabis is a flowering plant also known as marijuana. Rapper Canibus took his name from cannabis. Rapper Snoop Dogg's song OG has a line, \"Rolling up my Mary Jane,\" a reference to marijuana. Rap group Bone Thugs N Harmony's Weed Song is a reference to marijuana.\nAnswer (True or False):", "ct_subject": "Rapper Dr. Dre named his 1992 album, The Chronic, a reference to marijuana. Cannabis is a flowering plant also known as marijuana. Rapper Canibus took his name from cannabis. Rapper Snoop Dogg's song OG has a line, \"Rolling up my Mary Jane,\" a reference to marijuana. Rap group Bone Thugs N Harmony's Weed Song is a reference to marijuana.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9926925"], [" is", "1.0"], [" the", "1.0"], [" name", "0.8853456"], [" of", "1.0"], [" the", "1.0"], [" first", "0.99766386"], [" person", "0.98253286"], [" to", "1.0"], [" be", "0.99997246"], [" executed", "1.0"], [" in", "0.9997191"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9991641"], [" first", "1.0"], [" person", "1.0"], [" executed", "0.64293706"], [" in", "1.0"]]}, "67": {"qid": "fd6c86e0bccd5157cfd9", "term": "Jews", "description": "Ancient nation and ethnoreligious group from the Levant", "question": "Do Jews believe in any New Testament angels?", "answer": true, "facts": ["The New Testament is a book central to Christianity.", "The New Testament features a number of angels including Michael, and Gabriel.", "The Talmud is the central text of Rabbinic Judaism.", "The Talmud names four angels who would later be known as archangels, surrounding God's throne: Michael, Gabriel, Uriel, and Raphael."], "decomposition": ["What book is the central text of Rabbinic Judaism?", "Does #1 mention any angels?", "Are the angels mentioned in #2 also mentioned in the New testament?"], "evidence": [[[["Rabbinic Judaism-3"]], [["Angels in Judaism-1"]], [["Angels in Judaism-1"]]], [[["Rabbinic Judaism-3", "Talmud-1"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Hebrew Bible-1"]], [["Book of Daniel-2", "Michael (archangel)-2"]], [["Michael (archangel)-3"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first man on the moon?\nFacts: The first man on the moon was Neil Armstrong.\nAnswer (True or False): True\nQuestion: What is the name", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do Jews believe in any New Testament angels?\nFacts: The New Testament is a book central to Christianity. The New Testament features a number of angels including Michael, and Gabriel. The Talmud is the central text of Rabbinic Judaism. The Talmud names four angels who would later be known as archangels, surrounding God's throne: Michael, Gabriel, Uriel, and Raphael.\nAnswer (True or False):", "ct_subject": "The New Testament is a book central to Christianity. The New Testament features a number of angels including Michael, and Gabriel. The Talmud is the central text of Rabbinic Judaism. The Talmud names four angels who would later be known as archangels, surrounding God's throne: Michael, Gabriel, Uriel, and Raphael.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.5478006"], [" is", "1.0"], [" the", "1.0"], [" name", "0.7880685"], [" of", "1.0"], [" the", "1.0"], [" first", "0.99999154"], [" man", "0.99976045"], [" on", "0.91834724"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999993"], [" first", "1.0"], [" man", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" was", "1.0"], [" Neil", "0.950259"], [" Armstrong", "1.0"], [".", "1.0"], ["\n", "0.9999994"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"]]}, "68": {"qid": "6296b8c3a16680a826eb", "term": "Police officer", "description": "warranted employee of a police force", "question": "Does a Generation Y member satisfy NYPD police officer age requirement?", "answer": true, "facts": ["The NYPD has a minimum age requirement of 21.", "Members of Generation Y were born between 1980 and 1994."], "decomposition": ["How old do you have to be to be an NYPD officer?", "How old are Generation Y members currently?", "Is #2 higher than #1?"], "evidence": [[[["New York City Police Department Auxiliary Police-31"], "no_evidence"], [["Millennials-1"]], ["operation"]], [["no_evidence"], [["Millennials-1"]], ["no_evidence", "operation"]], [["no_evidence"], [["Millennials-1"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Does a Generation Y member satisfy the age requirement for a police officer?\nFacts: The NYPD has a minimum age requirement of 21. Members of Generation Y were born between 1980 and 1994.\nAnswer (True or False): True\nQuestion: Does a Generation Y member satisfy the age requirement for a firefighter?\nFacts: The NYPD has a minimum age requirement of 21. Members of Generation Y were born", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does a Generation Y member satisfy NYPD police officer age requirement?\nFacts: The NYPD has a minimum age requirement of 21. Members of Generation Y were born between 1980 and 1994.\nAnswer (True or False):", "ct_subject": "The NYPD has a minimum age requirement of 21. Members of Generation Y were born between 1980 and 1994.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Does", "0.97352874"], [" a", "1.0"], [" Generation", "1.0"], [" Y", "1.0"], [" member", "1.0"], [" satisfy", "0.99997103"], [" the", "1.0"], [" age", "0.48097888"], [" requirement", "1.0"], [" for", "1.0"], [" a", "0.9769588"], [" police", "0.99912554"], [" officer", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" NYPD", "1.0"], [" has", "1.0"], [" a", "1.0"], [" minimum", "1.0"], [" age", "1.0"], [" requirement", "1.0"], [" of", "1.0"], [" 21", "1.0"], [".", "1.0"], [" Members", "1.0"], [" of", "1.0"], [" Generation", "1.0"], [" Y", "1.0"], [" were", "1.0"], [" born", "1.0"], [" between", "1.0"], [" 1980", "1.0"], [" and", "1.0"], [" 1994", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Does", "1.0"], [" a", "1.0"], [" Generation", "1.0"], [" Y", "1.0"], [" member", "1.0"], [" satisfy", "1.0"], [" the", "1.0"], [" age", "1.0"], [" requirement", "1.0"], [" for", "1.0"], [" a", "1.0"], [" firefighter", "0.9999721"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" NYPD", "1.0"], [" has", "1.0"], [" a", "1.0"], [" minimum", "1.0"], [" age", "1.0"], [" requirement", "1.0"], [" of", "1.0"], [" 21", "1.0"], [".", "1.0"], [" Members", "1.0"], [" of", "1.0"], [" Generation", "1.0"], [" Y", "1.0"], [" were", "1.0"], [" born", "1.0"]]}, "69": {"qid": "e6391d901dcc8a269c79", "term": "Fairy", "description": "mythical being or legendary creature", "question": "Did King James I despise fairy beings?", "answer": true, "facts": ["King James I wrote Daemonologie in which he stated that a fairy was a being that could act as a familiar.", "A familiar was an animal or spirit that conspired with The Devil.", "King James I presided over the execution of Agnes Sampson.", "Agnes Sampson was accused of conspiring with familiars and was burned at the stake."], "decomposition": ["What did King James I claim that fairies could act as in his book 'Daemonologie'", "Which beings did he execute Agnes Sampson for allegedly conspiring with?", "Is #1 the same as #2?"], "evidence": [[[["Daemonologie-8"], "no_evidence"], [["Agnes Sampson-9"], "no_evidence"], ["operation"]], [[["Daemonologie-15"]], [["Agnes Sampson-7"], "no_evidence"], ["operation"]], [[["Daemonologie-6"], "no_evidence"], [["Agnes Sampson-10", "Agnes Sampson-5", "Agnes Sampson-9"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did King James I have a pet monkey?\nFacts: King James I had a pet monkey named \"Buck\" that he kept in his bedroom.\nAnswer (True or", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did King James I despise fairy beings?\nFacts: King James I wrote Daemonologie in which he stated that a fairy was a being that could act as a familiar. A familiar was an animal or spirit that conspired with The Devil. King James I presided over the execution of Agnes Sampson. Agnes Sampson was accused of conspiring with familiars and was burned at the stake.\nAnswer (True or False):", "ct_subject": "King James I wrote Daemonologie in which he stated that a fairy was a being that could act as a familiar. A familiar was an animal or spirit that conspired with The Devil. King James I presided over the execution of Agnes Sampson. Agnes Sampson was accused of conspiring with familiars and was burned at the stake.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" King", "0.30869147"], [" James", "1.0"], [" I", "1.0"], [" have", "0.9354074"], [" a", "1.0"], [" pet", "0.9999999"], [" monkey", "0.30854332"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" King", "1.0"], [" James", "1.0"], [" I", "1.0"], [" had", "0.9999989"], [" a", "1.0"], [" pet", "1.0"], [" monkey", "1.0"], [" named", "0.9999999"], [" \"", "1.0"], ["B", "0.5287513"], ["uck", "0.9848038"], ["\"", "0.9995983"], [" that", "0.9999552"], [" he", "0.9999958"], [" kept", "1.0"], [" in", "1.0"], [" his", "1.0"], [" bedroom", "0.999992"], [".", "1.0"], ["\n", "0.8572897"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"]]}, "70": {"qid": "8acc02738825ee8a68b6", "term": "Evander Holyfield", "description": "American boxer", "question": "Did Evander Holyfield compete in an Olympics hosted in the western hemisphere?", "answer": true, "facts": ["Evander Holyfield won a bronze medal during the 1984 Summer Olympics.", "The 1984 Olympics were held in Los Angeles, California.", "California is in the United States, which is located entirely within the western hemisphere."], "decomposition": ["Which Olympic games have been held in the Western Hemisphere?", "Did Evander Holyfield compete in any events listed in #1?"], "evidence": [[[["1984 Summer Olympics-1"], "no_evidence"], [["Evander Holyfield-2"], "operation"]], [[["1984 Summer Olympics-1", "Western Hemisphere-3"], "no_evidence"], [["Evander Holyfield-2"]]], [[["1984 Summer Olympics-1"]], [["Evander Holyfield-2"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the first person to ever fly across the Atlantic Ocean?\nFacts: The first person to ever fly across the Atlantic Ocean was a man named Charles Lindbergh. Lindbergh flew from New York to Paris in 1927. He was the first person to ever fly across the Atlantic Ocean.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Evander Holyfield compete in an Olympics hosted in the western hemisphere?\nFacts: Evander Holyfield won a bronze medal during the 1984 Summer Olympics. The 1984 Olympics were held in Los Angeles, California. California is in the United States, which is located entirely within the western hemisphere.\nAnswer (True or False):", "ct_subject": "Evander Holyfield won a bronze medal during the 1984 Summer Olympics. The 1984 Olympics were held in Los Angeles, California. California is in the United States, which is located entirely within the western hemisphere.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" first", "0.9554048"], [" person", "0.9982476"], [" to", "1.0"], [" ever", "0.21005358"], [" fly", "0.96058935"], [" across", "0.9999813"], [" the", "1.0"], [" Atlantic", "1.0"], [" Ocean", "1.0"], ["?", "0.92451274"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" ever", "1.0"], [" fly", "1.0"], [" across", "1.0"], [" the", "1.0"], [" Atlantic", "1.0"], [" Ocean", "1.0"], [" was", "1.0"], [" a", "0.9953548"], [" man", "1.0"], [" named", "1.0"], [" Charles", "1.0"], [" Lind", "1.0"], ["ber", "1.0"], ["gh", "1.0"], [".", "1.0"], [" Lind", "0.9940507"], ["ber", "1.0"], ["gh", "1.0"], [" flew", "0.99603987"], [" from", "0.9999237"], [" New", "1.0"], [" York", "1.0"], [" to", "1.0"], [" Paris", "1.0"], [" in", "0.9999989"], [" 1927", "1.0"], [".", "1.0"], [" He", "0.9488874"], [" was", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" ever", "0.9999969"], [" fly", "0.99999714"], [" across", "1.0"], [" the", "1.0"], [" Atlantic", "1.0"], [" Ocean", "1.0"], [".", "1.0"]]}, "71": {"qid": "0a87a031e804912db8df", "term": "Harry Houdini", "description": "American magician, escapologist, and stunt performer", "question": "Did Harry Houdini's wife make psychics look foolish?", "answer": true, "facts": ["Psychics are people that claim to have special powers to talk to the dead.", "Harry Houdini gave his wife a code word to ask psychics to repeat after his death.", "The wife of Harry Houdini, Wilhelmina Beatrice \"Bess\" Rahner, asked several psychics for the code word and none knew it."], "decomposition": ["What did Harry Houdini give to his wife to test psychics after his death?", "Were psychics unable to provide #1?"], "evidence": [[[["Harry Houdini-60"]], [["Harry Houdini-60"], "operation"]], [[["Harry Houdini-60"]], [["Harry Houdini-60"], "operation"]], [[["Harry Houdini-60"]], [["Bess Houdini-6"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Beatles' song \"Revolution\" have a hidden message?\nFacts: The Beatles' song \"Revolution\" was written by John Lennon and Paul McCartney.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Harry Houdini's wife make psychics look foolish?\nFacts: Psychics are people that claim to have special powers to talk to the dead. Harry Houdini gave his wife a code word to ask psychics to repeat after his death. The wife of Harry Houdini, Wilhelmina Beatrice \"Bess\" Rahner, asked several psychics for the code word and none knew it.\nAnswer (True or False):", "ct_subject": "Psychics are people that claim to have special powers to talk to the dead. Harry Houdini gave his wife a code word to ask psychics to repeat after his death. The wife of Harry Houdini, Wilhelmina Beatrice \"Bess\" Rahner, asked several psychics for the code word and none knew it.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99999857"], [" the", "1.0"], [" Beatles", "0.047285836"], ["'", "0.9909134"], [" song", "0.99994445"], [" \"", "1.0"], ["Revolution", "0.9999989"], ["\"", "1.0"], [" have", "1.0"], [" a", "0.9999999"], [" hidden", "0.99999785"], [" message", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Beatles", "0.99999976"], ["'", "0.9999291"], [" song", "1.0"], [" \"", "1.0"], ["Revolution", "1.0"], ["\"", "1.0"], [" was", "1.0"], [" written", "1.0"], [" by", "0.9997439"], [" John", "0.46110016"], [" Lennon", "1.0"], [" and", "1.0"], [" Paul", "1.0"], [" McCartney", "1.0"], [".", "1.0"]]}, "72": {"qid": "f95956c13e22f073310a", "term": "Handedness", "description": "Better performance or individual preference for use of a hand", "question": "Does handedness determine how you use American Sign Language?", "answer": true, "facts": ["Your dominant hand typically performs the moving part of a sign in ASL.", "Your dominant hand determines the hand you use to finger spell in ASL."], "decomposition": ["Does the dominant hand perform different functions than the other in ASL?"], "evidence": [[[["American Sign Language-40"], "no_evidence"]], [[["American Sign Language-1", "American Sign Language-29"], "no_evidence", "operation"]], [["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Does the number of fingers on your right hand determine how you use American Sign Language?\nFacts: The number of fingers on your right hand determines how you use American Sign Language.\nAnswer (True or False): True\nQuestion: Does the number of fingers on your left hand determine how you use American Sign Language?\nFacts: The number of fingers on your", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does handedness determine how you use American Sign Language?\nFacts: Your dominant hand typically performs the moving part of a sign in ASL. Your dominant hand determines the hand you use to finger spell in ASL.\nAnswer (True or False):", "ct_subject": "Your dominant hand typically performs the moving part of a sign in ASL. Your dominant hand determines the hand you use to finger spell in ASL.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Does", "0.9959365"], [" the", "1.0"], [" number", "0.9991365"], [" of", "1.0"], [" fingers", "1.0"], [" on", "1.0"], [" your", "0.9999807"], [" right", "0.95444643"], [" hand", "1.0"], [" determine", "1.0"], [" how", "0.99999917"], [" you", "1.0"], [" use", "1.0"], [" American", "1.0"], [" Sign", "1.0"], [" Language", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9994092"], [" number", "1.0"], [" of", "1.0"], [" fingers", "1.0"], [" on", "1.0"], [" your", "1.0"], [" right", "1.0"], [" hand", "1.0"], [" determines", "1.0"], [" how", "1.0"], [" you", "1.0"], [" use", "1.0"], [" American", "1.0"], [" Sign", "1.0"], [" Language", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Does", "1.0"], [" the", "1.0"], [" number", "1.0"], [" of", "1.0"], [" fingers", "1.0"], [" on", "1.0"], [" your", "1.0"], [" left", "0.9999999"], [" hand", "1.0"], [" determine", "1.0"], [" how", "1.0"], [" you", "1.0"], [" use", "1.0"], [" American", "1.0"], [" Sign", "1.0"], [" Language", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" number", "1.0"], [" of", "1.0"], [" fingers", "1.0"], [" on", "1.0"], [" your", "1.0"]]}, "73": {"qid": "9883d6c816d4855035ea", "term": "Onion", "description": "vegetable", "question": "Do onions have a form that resembles the inside of a tree?", "answer": true, "facts": ["When bisected, an onion has rings that extend from the core to the outside.", "Trees are formed of a series of rings that extend from the inside to the outside"], "decomposition": ["What is the structure observed in an onion when it is cut open?", "What is the structure of a tree's cross section?", "Is #1 similar to #2?"], "evidence": [[[["Fried onion-8"]], [["International Tree-Ring Data Bank-1"]], [["International Tree-Ring Data Bank-1"], "operation"]], [[["Onion-13"]], [["Tree-28"]], ["operation"]], [[["Onion-13"]], [["Dendrochronology-7"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: John Adams was born on July 4, 1735.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: Thomas Jefferson was born on July 4,", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do onions have a form that resembles the inside of a tree?\nFacts: When bisected, an onion has rings that extend from the core to the outside. Trees are formed of a series of rings that extend from the inside to the outside\nAnswer (True or False):", "ct_subject": "When bisected, an onion has rings that extend from the core to the outside. Trees are formed of a series of rings that extend from the inside to the outside", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999976"], [" is", "1.0"], [" the", "1.0"], [" name", "0.76963305"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9995492"], [" person", "0.9827461"], [" to", "1.0"], [" be", "0.99997115"], [" born", "0.9963503"], [" in", "1.0"], [" the", "1.0"], [" United", "0.9847862"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "0.9883711"], [" Adams", "0.99998415"], [" was", "0.9911287"], [" born", "0.99999964"], [" on", "0.92155975"], [" July", "0.99339664"], [" 4", "1.0"], [",", "1.0"], [" 17", "1.0"], ["35", "0.93835014"], [".", "0.9999995"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Thomas", "0.98839045"], [" Jefferson", "1.0"], [" was", "1.0"], [" born", "1.0"], [" on", "1.0"], [" July", "1.0"], [" 4", "1.0"], [",", "1.0"]]}, "74": {"qid": "660bfdc17b47f42facac", "term": "Moose", "description": "A genus of mammals belonging to the deer, muntjac, roe deer, reindeer, and moose family of ruminants", "question": "Are moose used for work near the kingdom of Arendelle?", "answer": true, "facts": ["The opening scene of Disney's Frozen shows a group of ice breakers.", "They have moose that carry the heavy ice blocks.", "One of them, Kristoff, becomes separated with his moose Sven.", "When Queen Elsa flees Arendelle and Princess Anna gives chase, she quickly encounters Kristoff."], "decomposition": ["What show is the kingdom of Arendelle from?", "In the opening scene of #1, what are a group of men doing?", "Are moose used to carry #2?"], "evidence": [[[["Arendelle: World of Frozen-1", "Elsa (Frozen)-26"], "no_evidence"], ["no_evidence"], ["no_evidence"]], [[["Frozen (2013 film)-45"]], [["Frozen (2013 film)-44"], "no_evidence"], [["Moose-73"], "operation"]], [[["Frozen (2013 film)-6"]], ["no_evidence"], [["Moose-9"], "no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any other animals that are related to the moose?\nFacts: Yes. The moose is the only animal that can be found in the Arctic.\nAnswer (True or False):", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are moose used for work near the kingdom of Arendelle?\nFacts: The opening scene of Disney's Frozen shows a group of ice breakers. They have moose that carry the heavy ice blocks. One of them, Kristoff, becomes separated with his moose Sven. When Queen Elsa flees Arendelle and Princess Anna gives chase, she quickly encounters Kristoff.\nAnswer (True or False):", "ct_subject": "The opening scene of Disney's Frozen shows a group of ice breakers. They have moose that carry the heavy ice blocks. One of them, Kristoff, becomes separated with his moose Sven. When Queen Elsa flees Arendelle and Princess Anna gives chase, she quickly encounters Kristoff.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.9999658"], [" there", "0.99992144"], [" any", "0.9999999"], [" other", "0.9877737"], [" animals", "1.0"], [" that", "1.0"], [" are", "0.9995228"], [" related", "0.96080196"], [" to", "1.0"], [" the", "0.98729837"], [" mo", "1.0"], ["ose", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Yes", "0.9515419"], [".", "0.92862856"], [" The", "0.99903405"], [" mo", "1.0"], ["ose", "1.0"], [" is", "1.0"], [" the", "0.99913484"], [" only", "1.0"], [" animal", "1.0"], [" that", "1.0"], [" can", "0.9999995"], [" be", "0.99954814"], [" found", "0.99999964"], [" in", "1.0"], [" the", "0.9999958"], [" Arctic", "0.9999865"], [".", "0.9999994"], ["\n", "0.99601966"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"]]}, "75": {"qid": "a18c59e77cc176f748b2", "term": "Blue", "description": "A primary colour between purple and green", "question": "Do some home remedies result in your skin color turning blue?", "answer": true, "facts": ["Colloidal silver is a popular alternative treatment/home remedy that is used by some people.", "Ingestion of colloidal silver in high amounts can tint the skin blue."], "decomposition": ["What can cause skin color to change?", "Of #1, what changes can be caused by ingestion of something?", "Of #2, what causes skin color to become blue?", "Is #3 used in home remedies?"], "evidence": [[[["Argyria-1", "Argyria-6", "Carrot juice-3", "Drug-induced pigmentation-2"]], [["Carrot juice-3", "Drug-induced pigmentation-2", "Medical uses of silver-21"]], [["Argyria-1"]], [["Argyria-5"], "operation"]], [[["Human skin color-41", "Human skin color-56"], "no_evidence"], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Cyanosis-1"]], [["Methemoglobinemia-2"]], [["Methemoglobinemia-6"]], [["Benzocaine-5"], "no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Do you have to be a member of the Church of Scientology to be a Scientologist?\nFacts: No.\nAnswer (True or False): True\nQuestion: Do you have to be a member of the Church of Scientology to be a Scientologist?\nFacts: No.\nAnswer (True or False): True\nQuestion:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do some home remedies result in your skin color turning blue?\nFacts: Colloidal silver is a popular alternative treatment/home remedy that is used by some people. Ingestion of colloidal silver in high amounts can tint the skin blue.\nAnswer (True or False):", "ct_subject": "Colloidal silver is a popular alternative treatment/home remedy that is used by some people. Ingestion of colloidal silver in high amounts can tint the skin blue.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Do", "0.8991073"], [" you", "1.0"], [" have", "0.9970836"], [" to", "0.89846206"], [" be", "1.0"], [" a", "1.0"], [" member", "0.9995146"], [" of", "1.0"], [" the", "0.99999976"], [" Church", "0.9993799"], [" of", "1.0"], [" Scientology", "0.027517421"], [" to", "1.0"], [" be", "0.9969079"], [" a", "1.0"], [" Scient", "1.0"], ["ologist", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" No", "0.99999917"], [".", "0.9999999"], ["\n", "0.9998957"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Do", "1.0"], [" you", "1.0"], [" have", "1.0"], [" to", "1.0"], [" be", "1.0"], [" a", "1.0"], [" member", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Church", "1.0"], [" of", "1.0"], [" Scientology", "1.0"], [" to", "1.0"], [" be", "1.0"], [" a", "1.0"], [" Scient", "1.0"], ["ologist", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" No", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"]]}, "76": {"qid": "6c1f7619f21201712194", "term": "Ham", "description": "Pork from a leg cut that has been preserved by wet or dry curing, with or without smoking", "question": "Would Janet Jackson avoid a dish with ham?", "answer": true, "facts": ["Janet Jackson follows an Islamic practice. ", "Islamic culture avoids eating pork.", "Ham is made from pork."], "decomposition": ["What is Janet Jackson's religion?", "Which foods are prohibited by #1?", "What type of food is ham?", "Is #3 included in #2?"], "evidence": [[["no_evidence"], ["no_evidence"], [["Ham-1"]], ["no_evidence", "operation"]], [["no_evidence"], ["no_evidence"], [["Ham-1"]], ["no_evidence", "operation"]], [[["Janet Jackson-5"]], ["no_evidence"], [["Ham-3"]], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake cake\"?\nFacts: A snowflake is a small, round, and shiny object. A snowflake cake is a larger, round, and shiny object.\nAnswer (True or False): True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake cake\"?\nFacts: A snowflake", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Janet Jackson avoid a dish with ham?\nFacts: Janet Jackson follows an Islamic practice.  Islamic culture avoids eating pork. Ham is made from pork.\nAnswer (True or False):", "ct_subject": "Janet Jackson follows an Islamic practice.  Islamic culture avoids eating pork. Ham is made from pork.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999778"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.5727883"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.9969081"], ["s", "0.9915804"], ["now", "0.9930432"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], [" cake", "0.3089646"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" snow", "0.9531975"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" small", "0.9987153"], [",", "1.0"], [" round", "0.9999722"], [",", "1.0"], [" and", "0.9970163"], [" shiny", "0.71767724"], [" object", "0.9999999"], [".", "1.0"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" cake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" larger", "0.84037775"], [",", "1.0"], [" round", "1.0"], [",", "1.0"], [" and", "1.0"], [" shiny", "1.0"], [" object", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], [" cake", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"]]}, "77": {"qid": "f32228b474fc1ff18d59", "term": "Armadillo", "description": "family of mammals", "question": "Would multiple average rulers be necessary to measure the length of a giant armadillo?", "answer": true, "facts": ["The average ruler is 12 inches or 30 centimeters in length.", "The typical length of the giant armadillo is 75\u2013100 cm (30\u201339 in), with the tail adding another 50 cm (20 in)."], "decomposition": ["What length are the best selling rulers on Amazon?", "How long is a typical giant armadillo?", "What is #2 divided by #1?", "Is #3 greater than one?"], "evidence": [[[["Ruler-2"], "no_evidence"], [["Giant armadillo-6"]], ["no_evidence", "operation"], ["no_evidence", "operation"]], [["no_evidence"], [["Giant armadillo-5"]], ["operation"], ["operation"]], [[["Ruler-2"], "no_evidence"], [["Giant armadillo-5"], "no_evidence"], ["operation"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance a person can walk in one day?\nFacts: The longest distance a person can walk in one day is about 1,000 km (620 miles).\nAnswer (True or False): True\nQuestion: What is the longest distance a person can", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would multiple average rulers be necessary to measure the length of a giant armadillo?\nFacts: The average ruler is 12 inches or 30 centimeters in length. The typical length of the giant armadillo is 75\u2013100 cm (30\u201339 in), with the tail adding another 50 cm (20 in).\nAnswer (True or False):", "ct_subject": "The average ruler is 12 inches or 30 centimeters in length. The typical length of the giant armadillo is 75\u2013100 cm (30\u201339 in), with the tail adding another 50 cm (20 in).", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99946004"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.70519376"], [" distance", "0.5311545"], [" a", "0.99636674"], [" person", "0.99904877"], [" can", "0.99999905"], [" walk", "0.9997453"], [" in", "1.0"], [" one", "0.9999931"], [" day", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "0.99948514"], [" distance", "1.0"], [" a", "1.0"], [" person", "1.0"], [" can", "1.0"], [" walk", "1.0"], [" in", "1.0"], [" one", "1.0"], [" day", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 1", "0.8115121"], [",", "1.0"], ["000", "0.99999976"], [" km", "1.0"], [" (", "1.0"], ["620", "0.65167904"], [" miles", "0.9999478"], [").", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" person", "1.0"], [" can", "1.0"]]}, "78": {"qid": "2cc59f4d25398d251fd6", "term": "Olympia, Washington", "description": "State capital and city in Washington, United States", "question": "Is Olympia, Washington part of \"Ish river country\"?", "answer": true, "facts": ["Poet Robert Sund called the Puget Sound region \"Ish River country\".", "Olympia is in the Puget Sound region."], "decomposition": ["Where is Ish river country? ", "What cities are located in #1?", "Is Olympia included in the list in #2?"], "evidence": [[[["Puget Sound region-1", "Puget Sound region-2"]], [["Washington (state)-79"]], ["operation"]], [[["Puget Sound region-1", "Puget Sound region-2"]], [["Puget Sound region-1"], "no_evidence"], [["Washington (state)-1"], "operation"]], [[["Puget Sound region-2"]], [["Puget Sound-4"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name \"Washington\" derived from the word \"washington\" or \"waw\"?\nFacts: The name \"Washington\" is derived from the word \"waw\".\nAnswer (True or False): True\nQuestion: Is the name \"Washington\" derived from the word \"waw\"?\nFacts: The name \"Washington\" is derived from the word \"waw", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Olympia, Washington part of \"Ish river country\"?\nFacts: Poet Robert Sund called the Puget Sound region \"Ish River country\". Olympia is in the Puget Sound region.\nAnswer (True or False):", "ct_subject": "Poet Robert Sund called the Puget Sound region \"Ish River country\". Olympia is in the Puget Sound region.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "0.9999819"], [" name", "0.99989367"], [" \"", "0.26934063"], ["Washington", "0.95057577"], ["\"", "1.0"], [" derived", "0.96270764"], [" from", "1.0"], [" the", "1.0"], [" word", "0.97029996"], [" \"", "1.0"], ["w", "0.13425393"], ["ashington", "0.99609286"], ["\"", "1.0"], [" or", "0.9999999"], [" \"", "0.9641876"], ["w", "1.0"], ["aw", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" name", "0.6039683"], [" \"", "0.9998286"], ["Washington", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" derived", "0.9734943"], [" from", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["w", "1.0"], ["aw", "0.999959"], ["\".", "0.9976622"], ["\n", "0.9999989"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" \"", "1.0"], ["Washington", "1.0"], ["\"", "1.0"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["w", "1.0"], ["aw", "1.0"], ["\"?", "0.91398406"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" name", "1.0"], [" \"", "1.0"], ["Washington", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["w", "1.0"], ["aw", "1.0"]]}, "79": {"qid": "6541fe954f5c5a530fba", "term": "Eiffel Tower", "description": "Tower located on the Champ de Mars in Paris, France", "question": "Did Eiffel Tower contribute to a war victory?", "answer": true, "facts": ["A radio transmitter located in the Eiffel Tower.", "This transmitter jammed German radio communications.", "This hindrance in German radio communications contributing to the Allied victory at the First Battle of the Marne."], "decomposition": ["What notable events in which Eiffel Tower was of primary importance took place during a war?", "Did any of #1 give a side an advantage during the said war?"], "evidence": [[[["Eiffel Tower-33"]], [["Eiffel Tower-33"]]], [["no_evidence"], ["no_evidence", "operation"]], [[["Eiffel Tower-33"]], [["Eiffel Tower-33"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Wright Brothers fly the first powered flight?\nFacts: The Wright Brothers flew the first powered flight on December 7, 1903.\nAnswer (True or False): True\nQuestion: Did the Wright Brothers fly the first powered flight on December 7, 1903?\nFacts: The Wright Brothers flew the first powered flight", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Eiffel Tower contribute to a war victory?\nFacts: A radio transmitter located in the Eiffel Tower. This transmitter jammed German radio communications. This hindrance in German radio communications contributing to the Allied victory at the First Battle of the Marne.\nAnswer (True or False):", "ct_subject": "A radio transmitter located in the Eiffel Tower. This transmitter jammed German radio communications. This hindrance in German radio communications contributing to the Allied victory at the First Battle of the Marne.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999827"], [" the", "1.0"], [" Wright", "0.8673545"], [" Brothers", "1.0"], [" fly", "0.99997425"], [" the", "1.0"], [" first", "1.0"], [" powered", "0.98570466"], [" flight", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Wright", "0.99986625"], [" Brothers", "0.99999964"], [" flew", "0.9999995"], [" the", "0.99770004"], [" first", "1.0"], [" powered", "1.0"], [" flight", "1.0"], [" on", "0.9927296"], [" December", "0.9995834"], [" 7", "1.0"], [",", "1.0"], [" 1903", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" Wright", "0.9999976"], [" Brothers", "1.0"], [" fly", "1.0"], [" the", "1.0"], [" first", "1.0"], [" powered", "1.0"], [" flight", "1.0"], [" on", "0.5958854"], [" December", "1.0"], [" 7", "1.0"], [",", "1.0"], [" 1903", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Wright", "1.0"], [" Brothers", "1.0"], [" flew", "1.0"], [" the", "1.0"], [" first", "1.0"], [" powered", "1.0"], [" flight", "1.0"]]}, "80": {"qid": "83395bf81226b3f487a7", "term": "Indian Ocean", "description": "The ocean between Africa, Asia, Australia and Antarctica (or the Southern Ocean)", "question": "Has the Indian Ocean garbage patch not completed two full rotations of debris since its discovery?", "answer": true, "facts": ["The Indian Ocean garbage patch was discovered in 2010", "The Indian Ocean garbage patch takes 6 years to complete a circulation "], "decomposition": ["When was the Indian Ocean garbage patch discovered?", "How long does it take for the Indian Ocean garbage patch to complete a rotation?", "How many years has it been since #1?", "Is #3 less than two times #2?"], "evidence": [[[["Indian Ocean garbage patch-1"]], ["no_evidence"], ["operation"], ["no_evidence", "operation"]], [[["Indian Ocean garbage patch-2"]], [["Indian Ocean Gyre-3"]], ["operation"], ["operation"]], [[["Indian Ocean garbage patch-2"]], [["Indian Ocean Gyre-3"]], ["operation"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the Earth's magnetic field the same as the magnetic field of the Sun?\nFacts: The Earth's magnetic field is not the same as the Sun's magnetic field. The Earth's magnetic field is about 10,000 times weaker than the Sun's magnetic field.\nAnswer (True or False): True\nQuestion: Is the Earth's magnetic field the same as the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Has the Indian Ocean garbage patch not completed two full rotations of debris since its discovery?\nFacts: The Indian Ocean garbage patch was discovered in 2010 The Indian Ocean garbage patch takes 6 years to complete a circulation \nAnswer (True or False):", "ct_subject": "The Indian Ocean garbage patch was discovered in 2010 The Indian Ocean garbage patch takes 6 years to complete a circulation ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9390658"], [" the", "1.0"], [" Earth", "0.9999908"], ["'s", "0.9999995"], [" magnetic", "0.9999931"], [" field", "1.0"], [" the", "0.99999964"], [" same", "1.0"], [" as", "0.80934894"], [" the", "1.0"], [" magnetic", "0.99993587"], [" field", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Sun", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Earth", "1.0"], ["'s", "1.0"], [" magnetic", "1.0"], [" field", "1.0"], [" is", "1.0"], [" not", "0.89409333"], [" the", "1.0"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" Sun", "0.999046"], ["'s", "1.0"], [" magnetic", "1.0"], [" field", "1.0"], [".", "1.0"], [" The", "1.0"], [" Earth", "0.99999774"], ["'s", "1.0"], [" magnetic", "1.0"], [" field", "1.0"], [" is", "1.0"], [" about", "0.999246"], [" 10", "0.22507599"], [",", "1.0"], ["000", "1.0"], [" times", "1.0"], [" weaker", "0.99999297"], [" than", "1.0"], [" the", "1.0"], [" Sun", "1.0"], ["'s", "1.0"], [" magnetic", "1.0"], [" field", "1.0"], [".", "1.0"], ["\n", "0.9999436"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" Earth", "1.0"], ["'s", "1.0"], [" magnetic", "1.0"], [" field", "1.0"], [" the", "1.0"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"]]}, "81": {"qid": "9975870880bf73b8644e", "term": "Armageddon", "description": "according to the Book of Revelation, the site of a battle during the end times", "question": "Do some religions look forward to armageddon?", "answer": true, "facts": ["Evangelicals cite that we are living in the beginning of Armageddon and that the rapture will happen soon as a good thing.", "Jehova's Witnesses believe that destroying the present world system and Armageddon is imminent, and that the establishment of God's kingdom over the earth is the only solution for all problems faced by humanity"], "decomposition": ["Where does the concept of Armageddon has its roots?", "#1 is associated with which religion?", "Do adherents of #2 believe in and await the Armageddon?"], "evidence": [[[["Armageddon-5"]], [["Armageddon-4"]], [["Armageddon-4"], "operation"]], [[["Armageddon-1"]], [["New Testament-1"]], [["Armageddon-18", "Jehovah's Witnesses-30"], "operation"]], [[["Armageddon-1"]], [["Book of Revelation-1"]], [["Rapture-40"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a Christian and a Muslim?\nFacts: The difference between a Christian and a Muslim is that a Christian believes in Jesus Christ as the Son of God and the Messiah, while a Muslim believes", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do some religions look forward to armageddon?\nFacts: Evangelicals cite that we are living in the beginning of Armageddon and that the rapture will happen soon as a good thing. Jehova's Witnesses believe that destroying the present world system and Armageddon is imminent, and that the establishment of God's kingdom over the earth is the only solution for all problems faced by humanity\nAnswer (True or False):", "ct_subject": "Evangelicals cite that we are living in the beginning of Armageddon and that the rapture will happen soon as a good thing. Jehova's Witnesses believe that destroying the present world system and Armageddon is imminent, and that the establishment of God's kingdom over the earth is the only solution for all problems faced by humanity", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99387485"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9999994"], [" between", "1.0"], [" a", "0.9992611"], [" Christian", "0.95685947"], [" and", "1.0"], [" a", "1.0"], [" Muslim", "0.9999969"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9957664"], [" difference", "0.998517"], [" between", "0.9987544"], [" a", "1.0"], [" Christian", "1.0"], [" and", "1.0"], [" a", "1.0"], [" Muslim", "1.0"], [" is", "1.0"], [" that", "1.0"], [" a", "0.9993113"], [" Christian", "1.0"], [" believes", "1.0"], [" in", "0.9941835"], [" Jesus", "0.99999976"], [" Christ", "1.0"], [" as", "0.89368534"], [" the", "1.0"], [" Son", "1.0"], [" of", "1.0"], [" God", "1.0"], [" and", "0.9427488"], [" the", "0.9647717"], [" Messiah", "0.99844724"], [",", "0.9998037"], [" while", "0.9971787"], [" a", "1.0"], [" Muslim", "1.0"], [" believes", "1.0"]]}, "82": {"qid": "defd6e3da16a186503c0", "term": "Sophist", "description": "Specific kind of teacher in both Ancient Greece and in the Roman Empire", "question": "Would Sophist's have hypothetically made good lawyers?", "answer": true, "facts": ["Sophist's were teachers in ancient Greece that used rhetoric.", "Lawyers must persuade juries that their side of the case is correct.", "Rhetoric is the ancient art of persuasion that was meant to sway audiences in specific situations."], "decomposition": ["What were Sophist's role in Ancient Greece?", "What did #1 use in their position?", "What do lawyers do in their position?", "Would #3 find #2 to be helpful?"], "evidence": [[[["Sophist-1"]], [["Second Sophistic-3"], "no_evidence"], [["Lawyer-7"]], ["operation"]], [[["Sophist-1"]], ["no_evidence"], [["Lawyer-1"]], ["operation"]], [[["Sophist-1", "Sophist-9"]], [["Hellenistic philosophy-3"]], [["Lawyer-1"]], [["Practice of law-1"], "no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"sophist\" and a \"sophist-in-training\"?\nFacts: A \"sophist\" is a teacher in ancient Greece that used rhetoric. A \"sophist-in-training\" is a student who is learning rhetoric.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Sophist's have hypothetically made good lawyers?\nFacts: Sophist's were teachers in ancient Greece that used rhetoric. Lawyers must persuade juries that their side of the case is correct. Rhetoric is the ancient art of persuasion that was meant to sway audiences in specific situations.\nAnswer (True or False):", "ct_subject": "Sophist's were teachers in ancient Greece that used rhetoric. Lawyers must persuade juries that their side of the case is correct. Rhetoric is the ancient art of persuasion that was meant to sway audiences in specific situations.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999976"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9956298"], [" between", "1.0"], [" a", "0.9999999"], [" \"", "0.99908304"], ["s", "0.9996208"], ["oph", "0.9211782"], ["ist", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.99999964"], ["oph", "1.0"], ["ist", "1.0"], ["-", "0.91283256"], ["in", "0.9997197"], ["-", "1.0"], ["training", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999963"], [" \"", "0.9986424"], ["s", "1.0"], ["oph", "1.0"], ["ist", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" teacher", "1.0"], [" in", "0.9574733"], [" ancient", "1.0"], [" Greece", "1.0"], [" that", "0.9999962"], [" used", "0.7952708"], [" rhetoric", "1.0"], [".", "1.0"], [" A", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["oph", "1.0"], ["ist", "1.0"], ["-", "1.0"], ["in", "1.0"], ["-", "1.0"], ["training", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" student", "0.99963844"], [" who", "0.99855834"], [" is", "0.99932134"], [" learning", "1.0"], [" rhetoric", "0.851056"], [".", "1.0"]]}, "83": {"qid": "29ee7da0020eb03888fb", "term": "Boolean algebra", "description": "Algebra involving variables containing only \"true\" and \"false\" (or 1 and 0) as values", "question": "Does coding rely on Boolean algebra characters?", "answer": true, "facts": ["Boolean algebra uses the characters of 1 and 0 to represent true and false.", "Binary code is an essential part of computer coding.", "Binary code consists of the characters 0 and 1 which represents strings of value."], "decomposition": ["What characters does Boolean algebra use?", "What characters does binary code use?", "Are #1 and #2 the same?"], "evidence": [[[["Boolean algebra-1"]], [["Binary code-1"]], ["operation"]], [[["Boolean algebra-1"]], [["Binary code-1"]], ["operation"]], [[["Boolean algebra-1"]], [["Binary code-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a binary code and a decimal code?\nFacts: Binary code is a binary number system. A decimal code is a decimal number system.\nAnswer (True or False): True\nQuestion: What is the difference between a binary code and a decimal code?\nFacts: Binary code is a binary number", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does coding rely on Boolean algebra characters?\nFacts: Boolean algebra uses the characters of 1 and 0 to represent true and false. Binary code is an essential part of computer coding. Binary code consists of the characters 0 and 1 which represents strings of value.\nAnswer (True or False):", "ct_subject": "Boolean algebra uses the characters of 1 and 0 to represent true and false. Binary code is an essential part of computer coding. Binary code consists of the characters 0 and 1 which represents strings of value.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999814"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "0.9999908"], [" binary", "0.9991972"], [" code", "0.9998097"], [" and", "1.0"], [" a", "1.0"], [" decimal", "1.0"], [" code", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Binary", "0.9616938"], [" code", "1.0"], [" is", "1.0"], [" a", "0.9999844"], [" binary", "0.5817867"], [" number", "0.9999341"], [" system", "1.0"], [".", "1.0"], [" A", "0.99999"], [" decimal", "0.9966691"], [" code", "0.99999094"], [" is", "1.0"], [" a", "1.0"], [" decimal", "1.0"], [" number", "1.0"], [" system", "1.0"], [".", "1.0"], ["\n", "0.99999785"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" binary", "0.9944125"], [" code", "1.0"], [" and", "1.0"], [" a", "1.0"], [" decimal", "1.0"], [" code", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "0.99999607"], ["acts", "1.0"], [":", "1.0"], [" Binary", "0.99999964"], [" code", "1.0"], [" is", "1.0"], [" a", "1.0"], [" binary", "1.0"], [" number", "1.0"]]}, "84": {"qid": "29842ce280ccd460dd7e", "term": "Bern", "description": "Place in Switzerland", "question": "Is Bern located east of Paris?", "answer": true, "facts": ["Paris is located in France.", "Bern is located in Switzerland.", "Switzerland borders France to the east."], "decomposition": ["What country is Paris located in?", "What country is Bern located in?", "Is #2 located east of #1?"], "evidence": [[[["Administration of Paris-4"]], [["Districts of Switzerland-10"]], [["France-1"]]], [[["Paris-1"]], [["Bern-1"]], ["operation"]], [[["Paris-1"]], [["Bern-1"]], [["Switzerland-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city of Bern in Switzerland?\nFacts: Bern is located in Switzerland. Bern is located in France.\nAnswer (True or False): True\nQuestion: Is the name of the city of Bern in France?\nFacts: Bern is located in France. Bern is located in Switzerland.\nAnswer (True or False): True\nQuestion: Is the name of the city of Bern in Switzerland?\nFacts", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Bern located east of Paris?\nFacts: Paris is located in France. Bern is located in Switzerland. Switzerland borders France to the east.\nAnswer (True or False):", "ct_subject": "Paris is located in France. Bern is located in Switzerland. Switzerland borders France to the east.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99817574"], [" the", "0.99999976"], [" name", "0.9999994"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Bern", "0.9969988"], [" in", "0.9972361"], [" Switzerland", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Bern", "1.0"], [" is", "1.0"], [" located", "0.99471176"], [" in", "1.0"], [" Switzerland", "1.0"], [".", "1.0"], [" Bern", "1.0"], [" is", "1.0"], [" located", "0.9999988"], [" in", "1.0"], [" France", "0.99999917"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "0.999158"], [" name", "0.9999999"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Bern", "1.0"], [" in", "1.0"], [" France", "0.5278413"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Bern", "1.0"], [" is", "1.0"], [" located", "1.0"], [" in", "1.0"], [" France", "1.0"], [".", "1.0"], [" Bern", "1.0"], [" is", "1.0"], [" located", "1.0"], [" in", "1.0"], [" Switzerland", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Bern", "1.0"], [" in", "1.0"], [" Switzerland", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"]]}, "85": {"qid": "03caedef50bc6729e9e5", "term": "Railroad engineer", "description": "person who operates a train on a railroad or railway", "question": "Can you find a railroad engineer on TNT?", "answer": true, "facts": ["TNT is a cable television network", "Snowpiercer is a show airing on the TNT network", "Snowpiercer involves people living on an active train in a postapocalyptic future", "A railroad engineer is necessary to keep a train running"], "decomposition": ["What movies are about trains or railroads?", "Are any of #1 currently airing on TNT?"], "evidence": [[[["Murder on the Orient Express (2017 film)-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Snowpiercer-1", "Snowpiercer-46"], "no_evidence"], [["Snowpiercer-46"], "operation"]], [[["Planes, Trains and Automobiles-1"]], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: Thomas Jefferson was the first person to be born in the United States.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the United States?\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can you find a railroad engineer on TNT?\nFacts: TNT is a cable television network Snowpiercer is a show airing on the TNT network Snowpiercer involves people living on an active train in a postapocalyptic future A railroad engineer is necessary to keep a train running\nAnswer (True or False):", "ct_subject": "TNT is a cable television network Snowpiercer is a show airing on the TNT network Snowpiercer involves people living on an active train in a postapocalyptic future A railroad engineer is necessary to keep a train running", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99995136"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999968"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9998234"], [" person", "0.99492997"], [" to", "1.0"], [" be", "0.96929216"], [" born", "0.999741"], [" in", "1.0"], [" the", "1.0"], [" United", "0.9872356"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Thomas", "0.32939723"], [" Jefferson", "1.0"], [" was", "1.0"], [" the", "0.99993396"], [" first", "1.0"], [" person", "1.0"], [" to", "0.99984705"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [".", "0.99814284"], ["\n", "0.9999933"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"]]}, "86": {"qid": "0bde4bdfecce47fbf7de", "term": "Amazon (company)", "description": "American electronic commerce and cloud computing company", "question": "Could Amazon afford The Mona Lisa?", "answer": true, "facts": ["Amazon is worth over $1 trillion and had a revenue of $232.887 billion in 2018.", "The Mona Lisa had an insurance valuation equivalent to $650 million as of 2018."], "decomposition": ["How much is Amazon worth?", "How much is the Mona Lisa worth?", "Is #1 more than #2?"], "evidence": [[["no_evidence"], [["Mona Lisa-55"]], ["no_evidence", "operation"]], [[["Amazon (company)-64"]], [["Mona Lisa-55"]], ["operation"]], [[["Amazon (company)-65"]], [["Mona Lisa-55"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the largest city in the world?\nFacts: The largest city in the world is Beijing, China.\nAnswer (True or False): True\nQuestion: What is the largest city in the world?\nFacts: The largest city in the world is Beijing, China.\nAnswer (True or False): True\nQuestion: What", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could Amazon afford The Mona Lisa?\nFacts: Amazon is worth over $1 trillion and had a revenue of $232.887 billion in 2018. The Mona Lisa had an insurance valuation equivalent to $650 million as of 2018.\nAnswer (True or False):", "ct_subject": "Amazon is worth over $1 trillion and had a revenue of $232.887 billion in 2018. The Mona Lisa had an insurance valuation equivalent to $650 million as of 2018.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99949265"], [" is", "1.0"], [" the", "1.0"], [" largest", "0.99174"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "0.116380595"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" largest", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" is", "1.0"], [" Beijing", "0.5020305"], [",", "0.9995491"], [" China", "1.0"], [".", "1.0"], ["\n", "0.9860001"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" city", "0.9999999"], [" in", "1.0"], [" the", "1.0"], [" world", "0.9965481"], ["?", "0.9999999"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" largest", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" is", "1.0"], [" Beijing", "1.0"], [",", "1.0"], [" China", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"]]}, "87": {"qid": "6e939983f45cfcba8caa", "term": "Jean-Paul Sartre", "description": "French existentialist philosopher, playwright, novelist, screenwriter, political activist, biographer, and literary critic", "question": "Did Sartre write a play about Hell?", "answer": true, "facts": ["In 1944, Sartre released No Exit.", "No Exit is a play about three people mysteriously locked in a room together.", "Late in the play, it is revealed the room is a version of Hell."], "decomposition": ["What is Jean-Paul Sartre's most famous play?", "What is the plot of #1?", "Is Hell a critical element of #2?"], "evidence": [[[["No Exit-1"]], [["No Exit-3"], "no_evidence"], ["operation"]], [[["No Exit-1"]], [["No Exit-3"]], ["operation"]], [[["Jean-Paul Sartre-62"]], [["No Exit-3"]], [["No Exit-3"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: The first person to be executed in the United States was John Brown.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Sartre write a play about Hell?\nFacts: In 1944, Sartre released No Exit. No Exit is a play about three people mysteriously locked in a room together. Late in the play, it is revealed the room is a version of Hell.\nAnswer (True or False):", "ct_subject": "In 1944, Sartre released No Exit. No Exit is a play about three people mysteriously locked in a room together. Late in the play, it is revealed the room is a version of Hell.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.80112016"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99999964"], [" of", "1.0"], [" the", "1.0"], [" first", "0.5006912"], [" person", "0.99694675"], [" to", "1.0"], [" be", "0.9971825"], [" executed", "0.9999788"], [" in", "0.99999654"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99999094"], [" first", "1.0"], [" person", "1.0"], [" to", "0.9973652"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" John", "0.9881634"], [" Brown", "1.0"], [".", "0.99415565"], ["\n", "0.9945081"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"]]}, "88": {"qid": "3fc11f59d64ea3b65136", "term": "Thanksgiving (United States)", "description": "holiday celebrated in the United States on the fourth Thursday in November", "question": "Is Thanksgiving sometimes considered a day of mourning?", "answer": true, "facts": ["The Native American People in the United States were brutalized during the colonization period.", "Native Americans in the US often choose to mourn the genocide of their people on Thanksgiving."], "decomposition": ["When do Native Americans often choose to mourn the genocide of their people?", "Is Thanksgiving included in #1?"], "evidence": [[[["National Day of Mourning (United States protest)-1"]], [["National Day of Mourning (United States protest)-1"], "operation"]], [[["National Day of Mourning (United States protest)-1", "National Day of Mourning (United States protest)-17"]], ["operation"]], [[["National Day of Mourning (United States protest)-17"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the word \"Christmas\" a Christian holiday?\nFacts: The word \"Christmas\" is a Christian holiday.\nAnswer (True or False): True\nQuestion: Is the word \"Christmas\" a Christian holiday?\nFacts: The word \"Christmas\" is a Christian holiday.\nAnswer (True or False): True\nQuestion: Is the word \"", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Thanksgiving sometimes considered a day of mourning?\nFacts: The Native American People in the United States were brutalized during the colonization period. Native Americans in the US often choose to mourn the genocide of their people on Thanksgiving.\nAnswer (True or False):", "ct_subject": "The Native American People in the United States were brutalized during the colonization period. Native Americans in the US often choose to mourn the genocide of their people on Thanksgiving.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.84828025"], [" the", "0.9988035"], [" word", "0.9856763"], [" \"", "1.0"], ["Christmas", "1.0"], ["\"", "1.0"], [" a", "0.9982046"], [" Christian", "1.0"], [" holiday", "0.9999993"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999976"], [" word", "1.0"], [" \"", "1.0"], ["Christmas", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "0.9999702"], [" Christian", "0.6701929"], [" holiday", "1.0"], [".", "1.0"], ["\n", "0.97349745"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["Christmas", "1.0"], ["\"", "1.0"], [" a", "1.0"], [" Christian", "0.9999888"], [" holiday", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["Christmas", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" Christian", "1.0"], [" holiday", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"]]}, "89": {"qid": "fabf020bf07e0445c50c", "term": "Sea shanty", "description": "work song sung to accompany labor on board large merchant sailing vessels", "question": "Does Jack Sparrow know any sea shantys?", "answer": true, "facts": ["Jack Sparrow is the main character of the popular 'Pirates of the Caribbean' movie franchise.", "Jack Sparrow is the captain of a pirate ship.", "Jack Sparrow sings many songs while on the sea."], "decomposition": ["Which movie is Jack Sparrow a main character in?", "Which activity is associated with singing of sea shantys?", "As portrayed in #1, is Jack Sparrow in a position to engage in #2?"], "evidence": [[[["Jack Sparrow-1"]], [["Sea shanty-1"]], ["operation"]], [[["Jack Sparrow-1"]], [["Sea shanty-119"]], [["Sea shanty-119"]]], [[["Jack Sparrow-1"]], [["Sea shanty-39", "Sea shanty-4"]], [["Jack Sparrow-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first man to walk on the moon?\nFacts: Neil Armstrong was the first man to walk on the moon. Neil Armstrong was the first man to walk on the moon.\nAnswer (True or False): True\nQuestion: What is the name of the first man to walk on the moon?", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Jack Sparrow know any sea shantys?\nFacts: Jack Sparrow is the main character of the popular 'Pirates of the Caribbean' movie franchise. Jack Sparrow is the captain of a pirate ship. Jack Sparrow sings many songs while on the sea.\nAnswer (True or False):", "ct_subject": "Jack Sparrow is the main character of the popular 'Pirates of the Caribbean' movie franchise. Jack Sparrow is the captain of a pirate ship. Jack Sparrow sings many songs while on the sea.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99964094"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "0.86436313"], [" man", "0.68454415"], [" to", "1.0"], [" walk", "0.99996996"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Neil", "0.98429585"], [" Armstrong", "1.0"], [" was", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [".", "1.0"], [" Neil", "0.9999708"], [" Armstrong", "1.0"], [" was", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [".", "1.0"], ["\n", "0.99999475"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" walk", "0.9996563"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"]]}, "90": {"qid": "8cf9e4fddb2265d2ed89", "term": "Mongoose", "description": "family of mammals", "question": "Does a mongoose have natural camouflage for desert?", "answer": true, "facts": ["The most common fur colors of mongooses are brown and gray.", "The Desert Camouflage color is made of Caf\u00e9 Au Lait brown and Pastel Gray."], "decomposition": ["What colors are mongoose?", "What colors are desert camouflage?", "Is #1 included in #2?"], "evidence": [[[["Mongoose-5"]], [["Desert Camouflage Uniform-1"]], [["Desert Camouflage Uniform-1", "Mongoose-5"]]], [[["Egyptian mongoose-2"], "no_evidence"], [["Desert Camouflage Uniform-1"]], ["operation"]], [[["Indian brown mongoose-2"], "no_evidence"], [["Desert Camouflage Uniform-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a mongoose and a squirrel?\nFacts: The mongoose is a mammal and the squirrel is a rodent.\nAnswer (True or False): True\nQuestion: What is the difference between a mongoose and a squirrel?\nFacts: The mongoose is a mammal and the squirrel is a rodent", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does a mongoose have natural camouflage for desert?\nFacts: The most common fur colors of mongooses are brown and gray. The Desert Camouflage color is made of Caf\u00e9 Au Lait brown and Pastel Gray.\nAnswer (True or False):", "ct_subject": "The most common fur colors of mongooses are brown and gray. The Desert Camouflage color is made of Caf\u00e9 Au Lait brown and Pastel Gray.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99980015"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.029259885"], [" between", "1.0"], [" a", "1.0"], [" m", "1.0"], ["ongo", "1.0"], ["ose", "1.0"], [" and", "1.0"], [" a", "1.0"], [" squirrel", "0.9976587"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.8183039"], [" m", "0.99999976"], ["ongo", "1.0"], ["ose", "1.0"], [" is", "0.99994195"], [" a", "0.9999999"], [" mammal", "0.9999995"], [" and", "0.9139823"], [" the", "0.9999931"], [" squirrel", "1.0"], [" is", "1.0"], [" a", "1.0"], [" rodent", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" m", "1.0"], ["ongo", "1.0"], ["ose", "1.0"], [" and", "1.0"], [" a", "1.0"], [" squirrel", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" m", "1.0"], ["ongo", "1.0"], ["ose", "1.0"], [" is", "1.0"], [" a", "1.0"], [" mammal", "1.0"], [" and", "1.0"], [" the", "1.0"], [" squirrel", "1.0"], [" is", "1.0"], [" a", "1.0"], [" rodent", "1.0"]]}, "91": {"qid": "40b7c34188d5b36bc486", "term": "Lamborghini", "description": "Italian car manufacturer", "question": "Can Lamborghini's fastest model win a race against a Porsche 911?", "answer": true, "facts": ["Lamborghini's fastest model is the Lamborghini Aventador SVJ Roadster.", "The Lamborghini Aventador SVJ Roadster has a top speed of 217 MPH.", "The Porsche 911 has a top speed of 191 MPH."], "decomposition": ["Which model of Lamborghini is the fastest?", "What is the top speed of #1?", "What is the top speed of a Porsche 911?", "Is #2 greater than #3?"], "evidence": [[[["Lamborghini Veneno-1"]], [["Lamborghini Veneno-7"]], [["Porsche 911-133"]], ["operation"]], [[["Lamborghini Aventador-14"], "no_evidence"], [["Lamborghini Aventador-14"]], [["Porsche 911-129"]], ["operation"]], [[["Fastest Car-1"]], [["Lamborghini Aventador-14"]], [["Porsche 911-94"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first man to walk on the moon?\nFacts: Neil Armstrong was the first man to walk on the moon.\nAnswer (True or False): True\nQuestion: What is the name of the first man to walk on", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can Lamborghini's fastest model win a race against a Porsche 911?\nFacts: Lamborghini's fastest model is the Lamborghini Aventador SVJ Roadster. The Lamborghini Aventador SVJ Roadster has a top speed of 217 MPH. The Porsche 911 has a top speed of 191 MPH.\nAnswer (True or False):", "ct_subject": "Lamborghini's fastest model is the Lamborghini Aventador SVJ Roadster. The Lamborghini Aventador SVJ Roadster has a top speed of 217 MPH. The Porsche 911 has a top speed of 191 MPH.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999833"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9076241"], [" of", "1.0"], [" the", "1.0"], [" first", "0.99998975"], [" man", "0.8251053"], [" to", "1.0"], [" walk", "0.9001251"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Neil", "1.0"], [" Armstrong", "1.0"], [" was", "0.45990604"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [".", "0.99998736"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "0.99999964"], [" to", "1.0"], [" walk", "0.90405786"], [" on", "1.0"]]}, "92": {"qid": "c6678ced9e10fc1c03f1", "term": "Second Amendment to the United States Constitution", "description": "Part of the Bill of Rights, regarding the right to bear arms", "question": "Was the Second Amendment to the United States Constitution written without consideration for black Americans?", "answer": true, "facts": ["The writers of the Constitutional Amendments did not view black people as legitimate human beings.", "The writers of the Constitutional Amendments believed that slavery benefited black slaves.", "The Constitutional Amendments were written for people that the writers considered human."], "decomposition": ["Who were the writers of the Constitutional Amendments?", "Who was the the Constitutional Amendments written for?", "Did #1 fail to view black Americans as #2?"], "evidence": [[[["Constitution of the United States-63"], "no_evidence"], [["Constitution of the United States-51"], "no_evidence"], [["African Americans-20"], "operation"]], [[["United States Bill of Rights-2"], "no_evidence"], [["Constitution of the United States-132"], "no_evidence"], [["Reconstruction Amendments-2"], "no_evidence", "operation"]], [[["Ratification-18"]], [["Constitution of the United States-3"]], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the United States government ever use the term \"Negro\" to refer to black people?\nFacts: The United States government did not use the term \"Negro\" to refer to black people. The term \"Negro\" was used to refer to black people in the South", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was the Second Amendment to the United States Constitution written without consideration for black Americans?\nFacts: The writers of the Constitutional Amendments did not view black people as legitimate human beings. The writers of the Constitutional Amendments believed that slavery benefited black slaves. The Constitutional Amendments were written for people that the writers considered human.\nAnswer (True or False):", "ct_subject": "The writers of the Constitutional Amendments did not view black people as legitimate human beings. The writers of the Constitutional Amendments believed that slavery benefited black slaves. The Constitutional Amendments were written for people that the writers considered human.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.76204854"], [" the", "1.0"], [" United", "0.9998858"], [" States", "1.0"], [" government", "0.999665"], [" ever", "0.9999995"], [" use", "1.0"], [" the", "1.0"], [" term", "0.95587677"], [" \"", "1.0"], ["Neg", "1.0"], ["ro", "1.0"], ["\"", "1.0"], [" to", "0.9676422"], [" refer", "0.99965096"], [" to", "1.0"], [" black", "0.99995255"], [" people", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "1.0"], [" States", "1.0"], [" government", "1.0"], [" did", "0.9975067"], [" not", "0.99997294"], [" use", "1.0"], [" the", "1.0"], [" term", "1.0"], [" \"", "1.0"], ["Neg", "1.0"], ["ro", "1.0"], ["\"", "1.0"], [" to", "1.0"], [" refer", "1.0"], [" to", "1.0"], [" black", "1.0"], [" people", "1.0"], [".", "1.0"], [" The", "0.9999982"], [" term", "0.9999896"], [" \"", "1.0"], ["Neg", "1.0"], ["ro", "1.0"], ["\"", "1.0"], [" was", "1.0"], [" used", "1.0"], [" to", "0.99999523"], [" refer", "1.0"], [" to", "1.0"], [" black", "0.99684405"], [" people", "1.0"], [" in", "0.99947625"], [" the", "1.0"], [" South", "0.9992673"]]}, "93": {"qid": "76a9327740bf5e4fd9ed", "term": "Republic of Korea Navy", "description": "Naval warfare branch of South Korea's military", "question": "Would Republic of Korea Navy dominate Eritrea navy?", "answer": true, "facts": ["The Republic of Korea Navy has 150 ships, 70 aircraft, 70,000 personnel including 29,000 marines .", "The Eritrean Navy has 4 ships and an army of 45,000."], "decomposition": ["How many ships are in the Republic of Korea's navy?", "How many ships are in the Eritrean Navy?", "How many people are in the Republic of Korea's navy?", "How many people are in the Eritrean navy?", "Is #1 greater than #2 and is #3 greater than #4?"], "evidence": [[[["Republic of Korea Navy-1"]], [["Eritrean Navy-4"]], [["Republic of Korea Navy-1"]], [["Eritrean Defence Forces-5"], "no_evidence"], ["operation"]], [[["Republic of Korea Navy-1"]], [["Eritrean Navy-2"], "no_evidence"], [["Republic of Korea Navy-10"]], [["Eritrean Defence Forces-5"], "no_evidence"], ["operation"]], [[["Republic of Korea Navy-81"]], [["Eritrean Navy-1"], "no_evidence"], [["Republic of Korea Navy-1"]], [["Eritrean Defence Forces-5"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the largest number of people that can fit in a single room?\nFacts: The largest number of people that can fit in a single room is 2,000.\nAnswer (True or False): True\nQuestion: What is the largest number of people that can fit in a single room?\nFacts: The largest", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Republic of Korea Navy dominate Eritrea navy?\nFacts: The Republic of Korea Navy has 150 ships, 70 aircraft, 70,000 personnel including 29,000 marines . The Eritrean Navy has 4 ships and an army of 45,000.\nAnswer (True or False):", "ct_subject": "The Republic of Korea Navy has 150 ships, 70 aircraft, 70,000 personnel including 29,000 marines . The Eritrean Navy has 4 ships and an army of 45,000.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9998548"], [" is", "1.0"], [" the", "1.0"], [" largest", "0.98102427"], [" number", "0.41448772"], [" of", "1.0"], [" people", "1.0"], [" that", "0.99350774"], [" can", "0.9949555"], [" fit", "0.99999976"], [" in", "0.98441535"], [" a", "0.99999416"], [" single", "0.99652267"], [" room", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" largest", "1.0"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" that", "1.0"], [" can", "1.0"], [" fit", "1.0"], [" in", "1.0"], [" a", "1.0"], [" single", "1.0"], [" room", "1.0"], [" is", "1.0"], [" 2", "0.99997973"], [",", "1.0"], ["000", "0.9999976"], [".", "0.99999166"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" that", "1.0"], [" can", "1.0"], [" fit", "1.0"], [" in", "1.0"], [" a", "1.0"], [" single", "1.0"], [" room", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" largest", "1.0"]]}, "94": {"qid": "c8e267b583f722ff485e", "term": "Constitution of the United States", "description": "Supreme law of the United States of America", "question": "Is the Hobbit more profitable for proofreader than Constitution of the United States?", "answer": true, "facts": ["Proofreaders typically get paid per the number of words in a document.", "The Constitution of the United States contains around 7,500 words.", "The Hobbit contains 95,356 words."], "decomposition": ["How many words are in the US Constitution?", "What classification is the Hobbit?", "How many words do books in #2 have?", "Is #3 greater than #1?"], "evidence": [[[["Constitution-4"], "no_evidence"], [["Hobbit-2"]], [["Artam\u00e8ne-1"], "no_evidence"], ["operation"]], [[["Constitution of the United States-1", "Constitution of the United States-2"]], [["The Hobbit-1"], "no_evidence"], [["The Hobbit-20"], "no_evidence"], ["no_evidence", "operation"]], [[["State constitution (United States)-2"]], [["Hobbit-7"]], [["The Hobbit-1"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the Bible more profitable for proofreader than the Bible?\nFacts: The Bible contains over 2,000,000 words. The Hobbit contains around 95,356 words.\nAnswer (True or False): True\nQuestion: Is the Bible more profitable for proofreader than the Bible?\nFacts: The Bible", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is the Hobbit more profitable for proofreader than Constitution of the United States?\nFacts: Proofreaders typically get paid per the number of words in a document. The Constitution of the United States contains around 7,500 words. The Hobbit contains 95,356 words.\nAnswer (True or False):", "ct_subject": "Proofreaders typically get paid per the number of words in a document. The Constitution of the United States contains around 7,500 words. The Hobbit contains 95,356 words.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9973895"], [" the", "0.99999857"], [" Bible", "0.16315123"], [" more", "1.0"], [" profitable", "1.0"], [" for", "1.0"], [" proof", "1.0"], ["reader", "1.0"], [" than", "1.0"], [" the", "1.0"], [" Bible", "0.99754816"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Bible", "1.0"], [" contains", "0.999764"], [" over", "0.9694112"], [" 2", "0.4581287"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [" words", "1.0"], [".", "1.0"], [" The", "1.0"], [" Hobbit", "1.0"], [" contains", "1.0"], [" around", "0.44345775"], [" 95", "0.9999999"], [",", "1.0"], ["356", "0.9999993"], [" words", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" Bible", "1.0"], [" more", "1.0"], [" profitable", "1.0"], [" for", "1.0"], [" proof", "1.0"], ["reader", "1.0"], [" than", "1.0"], [" the", "1.0"], [" Bible", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Bible", "1.0"]]}, "95": {"qid": "14ef04964c1b786dc45e", "term": "Holy Spirit", "description": "Religious concept with varied meanings", "question": "Is Krishna similar to Holy Spirit?", "answer": true, "facts": ["The Holy Spirit is a Christian concept of a spirit that is an aspect or agent of God that does good in the world.", "Krishna, from Hinduism, is a manifestation of the God Vishnu.", "Krishna brings compassion, tenderness, and love into the world."], "decomposition": ["What are the characteristics of the Christian Holy Spirit?", "What are the characteristics of Krishna?", "Are many characteristics in #2 also found in #1?"], "evidence": [[[["Holy Spirit in Christianity-4"]], [["Krishna-1"]], ["no_evidence"]], [[["Holy Spirit-1"]], [["Krishna-1"]], [["Krishna-1"]]], [[["God in Abrahamic religions-9", "Holy Spirit-1"]], [["Krishna-1"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is Jesus Christ the same as the Holy Spirit?\nFacts: Jesus Christ is the same as the Holy Spirit. Jesus Christ is the same as the Holy Spirit.\nAnswer (True or False): True\nQuestion: Is Jesus Christ the same as the Holy Spirit?\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Krishna similar to Holy Spirit?\nFacts: The Holy Spirit is a Christian concept of a spirit that is an aspect or agent of God that does good in the world. Krishna, from Hinduism, is a manifestation of the God Vishnu. Krishna brings compassion, tenderness, and love into the world.\nAnswer (True or False):", "ct_subject": "The Holy Spirit is a Christian concept of a spirit that is an aspect or agent of God that does good in the world. Krishna, from Hinduism, is a manifestation of the God Vishnu. Krishna brings compassion, tenderness, and love into the world.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" Jesus", "0.97305435"], [" Christ", "0.9974033"], [" the", "1.0"], [" same", "0.999342"], [" as", "0.9996038"], [" the", "0.9887215"], [" Holy", "1.0"], [" Spirit", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Jesus", "1.0"], [" Christ", "1.0"], [" is", "1.0"], [" the", "0.99878365"], [" same", "0.99982184"], [" as", "0.9999974"], [" the", "1.0"], [" Holy", "1.0"], [" Spirit", "1.0"], [".", "1.0"], [" Jesus", "0.99969816"], [" Christ", "1.0"], [" is", "1.0"], [" the", "1.0"], [" same", "0.99999726"], [" as", "1.0"], [" the", "1.0"], [" Holy", "1.0"], [" Spirit", "1.0"], [".", "1.0"], ["\n", "0.66111654"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" Jesus", "0.50014496"], [" Christ", "1.0"], [" the", "1.0"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" Holy", "1.0"], [" Spirit", "1.0"], ["?", "1.0"], ["\n", "1.0"]]}, "96": {"qid": "d8eaf52f02c5cfb98bce", "term": "Sacrum", "description": "Triangular-shaped bone at the bottom of the spine", "question": "Do human sacrums have more fused vertebrae than an Alaskan Malamute?", "answer": true, "facts": ["The human sacrum consists of five fused vertebrae.", "An Alaskan Malamute is a large domestic dog breed.", "Dogs have three fused vertebrae attached to their sacrums."], "decomposition": ["How many vertebrae are found in the human sacrum?", "What species of animal is an Alaskan Malamute?", "How many vertebrae are found in a #2's sacrum?", "Is #1 greater than #3?"], "evidence": [[[["Sacrum-1"]], [["Alaskan Malamute-1"]], [["Dog anatomy-54", "Nuchal ligament-10"], "no_evidence"], ["no_evidence", "operation"]], [[["Sacrum-1"]], [["Alaskan Malamute-1"]], [["Sacrum-4"]], ["operation"]], [[["Sacrum-1"]], [["Alaskan Malamute-1"]], [["Sacrum-4"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human?\nFacts: The average weight of a human is about 200 pounds.\nAnswer (True or False): True\nQuestion: What is the average weight of a human?\nFacts: The average weight of a human is about 200 pounds", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do human sacrums have more fused vertebrae than an Alaskan Malamute?\nFacts: The human sacrum consists of five fused vertebrae. An Alaskan Malamute is a large domestic dog breed. Dogs have three fused vertebrae attached to their sacrums.\nAnswer (True or False):", "ct_subject": "The human sacrum consists of five fused vertebrae. An Alaskan Malamute is a large domestic dog breed. Dogs have three fused vertebrae attached to their sacrums.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.8149804"], [" is", "1.0"], [" the", "1.0"], [" average", "0.9905667"], [" weight", "0.999995"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], ["?", "0.99374795"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" weight", "0.6794513"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" is", "1.0"], [" about", "0.98606575"], [" 200", "0.9990903"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "0.9849355"], [" of", "1.0"], [" a", "1.0"], [" human", "0.9994616"], ["?", "0.99985063"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" weight", "1.0"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 200", "1.0"], [" pounds", "1.0"]]}, "97": {"qid": "87c07cc6b730abde6d76", "term": "Sweet potato", "description": "species of plant", "question": "Do Sweet Potatoes prevent other plants from growing in their place?", "answer": true, "facts": ["When sweet potato plants decompose, they release a chemical that prevents germination in their soil.", "Farmers will work to ensure that all parts of a sweet potato plant are out of the field before trying to grow in it again."], "decomposition": ["What chemical is released when sweet potatoes decompose?", "Where is #1 released into?", "Does #1 prevent other plants from growing in #2?"], "evidence": [[[["Sweet potato-19"], "no_evidence"], [["Sweet potato-19"], "no_evidence"], ["no_evidence"]], [[["Sweet potato-19"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Sweet potato storage-10"], "no_evidence"], ["no_evidence"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"C\" and a \"K\"?\nFacts: The letter \"C\" is the first letter of the alphabet. The letter \"K\" is the last letter of the alphabet.\nAnswer (True or False): True\nQuestion: What is the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do Sweet Potatoes prevent other plants from growing in their place?\nFacts: When sweet potato plants decompose, they release a chemical that prevents germination in their soil. Farmers will work to ensure that all parts of a sweet potato plant are out of the field before trying to grow in it again.\nAnswer (True or False):", "ct_subject": "When sweet potato plants decompose, they release a chemical that prevents germination in their soil. Farmers will work to ensure that all parts of a sweet potato plant are out of the field before trying to grow in it again.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99992216"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9885882"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["C", "0.9916186"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["K", "0.99961805"], ["\"?", "0.99999726"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99976116"], [" letter", "0.9259811"], [" \"", "1.0"], ["C", "1.0"], ["\"", "1.0"], [" is", "0.9998367"], [" the", "0.9996481"], [" first", "0.9956695"], [" letter", "1.0"], [" of", "1.0"], [" the", "1.0"], [" alphabet", "1.0"], [".", "0.99999774"], [" The", "1.0"], [" letter", "1.0"], [" \"", "1.0"], ["K", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" the", "1.0"], [" last", "0.9999988"], [" letter", "1.0"], [" of", "1.0"], [" the", "1.0"], [" alphabet", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"]]}, "98": {"qid": "1977a974cf1b946ea1e5", "term": "Capsaicin", "description": "chemical compound", "question": "If someone loves buffalo wings do they enjoy capsaicin?", "answer": true, "facts": ["Buffalo wings are fried chicken wings covered in a spicy sauce.", "Spicy foods are provided their spice from capsaicin from peppers."], "decomposition": ["What sauce is used on buffalo wings?", "What is the flavor of #1", "Is capsaicin used to create #2?"], "evidence": [[[["Buffalo wing-10"]], [["Cayenne pepper-1"]], [["Capsicum annuum-6"], "operation"]], [[["Buffalo wing-10"]], [["Hot sauce-1"]], [["Capsaicin-1"], "operation"]], [[["Buffalo wing-1"]], [["Buffalo wing-10"]], [["Capsaicin-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"salt and pepper\" and a \"salt and vinegar\" sandwich?\nFacts: A \"salt and vinegar\" sandwich is a sandwich that is made with a salty condiment and a vinegar-based condiment. A \"salt and pepper\" sandwich is a sandwich that is made with a salty condiment and a pepper-based cond", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: If someone loves buffalo wings do they enjoy capsaicin?\nFacts: Buffalo wings are fried chicken wings covered in a spicy sauce. Spicy foods are provided their spice from capsaicin from peppers.\nAnswer (True or False):", "ct_subject": "Buffalo wings are fried chicken wings covered in a spicy sauce. Spicy foods are provided their spice from capsaicin from peppers.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.050326258"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9999908"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.9866165"], ["s", "0.43469873"], ["alt", "0.95809716"], [" and", "0.7730575"], [" pepper", "1.0"], ["\"", "0.68528837"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.99999976"], ["alt", "1.0"], [" and", "1.0"], [" vinegar", "1.0"], ["\"", "1.0"], [" sandwich", "0.9999838"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.99767274"], [" \"", "0.99899954"], ["s", "1.0"], ["alt", "1.0"], [" and", "1.0"], [" vinegar", "0.9999534"], ["\"", "1.0"], [" sandwich", "1.0"], [" is", "1.0"], [" a", "0.9999989"], [" sandwich", "1.0"], [" that", "0.97140074"], [" is", "0.1528098"], [" made", "1.0"], [" with", "1.0"], [" a", "0.9999031"], [" salty", "0.9999199"], [" cond", "0.99988306"], ["iment", "1.0"], [" and", "0.84043753"], [" a", "0.9995739"], [" vinegar", "1.0"], ["-", "1.0"], ["based", "1.0"], [" cond", "1.0"], ["iment", "1.0"], [".", "1.0"], [" A", "0.98385584"], [" \"", "1.0"], ["s", "1.0"], ["alt", "1.0"], [" and", "1.0"], [" pepper", "1.0"], ["\"", "1.0"], [" sandwich", "1.0"], [" is", "1.0"], [" a", "1.0"], [" sandwich", "1.0"], [" that", "1.0"], [" is", "1.0"], [" made", "1.0"], [" with", "1.0"], [" a", "1.0"], [" salty", "1.0"], [" cond", "1.0"], ["iment", "1.0"], [" and", "1.0"], [" a", "1.0"], [" pepper", "1.0"], ["-", "1.0"], ["based", "1.0"], [" cond", "1.0"]]}, "99": {"qid": "38a3e1117891b029cd6b", "term": "Adrenaline", "description": "hormone, neurotransmitter and medication. Epinephrine is normally produced by both the adrenal glands and certain neurons", "question": "Can cancer cause excess adrenaline production?", "answer": true, "facts": ["Adrenaline is produced by the adrenal glands.", "Cancer is a disease characterized by the formation of tumors.", "Tumors on the adrenal glands can cause them to over-express."], "decomposition": ["What is cancer cause to grow?", "Can #1 grow on Adrenal glands?", "Does #2 cause excess adrenaline production?"], "evidence": [[[["Cancer cell-5"]], [["Adrenal tumor-9"]], [["Adrenal tumor-1"]]], [[["Cancer-1"]], [["Adrenal gland-3"]], [["Adrenal tumor-10"], "operation"]], [[["Causes of cancer-1"]], [["Adrenal tumor-5"]], [["Adrenal tumor-8"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Can a person be born with a heart condition?\nFacts: A heart condition is a condition in which the heart is not working properly. A heart condition can be caused by a number of things.\nAnswer (True or False): True\nQuestion: Can a person be born with a brain condition?\nFacts: A brain condition is a condition", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can cancer cause excess adrenaline production?\nFacts: Adrenaline is produced by the adrenal glands. Cancer is a disease characterized by the formation of tumors. Tumors on the adrenal glands can cause them to over-express.\nAnswer (True or False):", "ct_subject": "Adrenaline is produced by the adrenal glands. Cancer is a disease characterized by the formation of tumors. Tumors on the adrenal glands can cause them to over-express.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Can", "0.86861897"], [" a", "0.9473805"], [" person", "1.0"], [" be", "0.896017"], [" born", "0.99966085"], [" with", "1.0"], [" a", "1.0"], [" heart", "1.0"], [" condition", "0.050116096"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999993"], [" heart", "0.88023037"], [" condition", "1.0"], [" is", "1.0"], [" a", "1.0"], [" condition", "1.0"], [" in", "0.7174883"], [" which", "1.0"], [" the", "1.0"], [" heart", "1.0"], [" is", "0.9975533"], [" not", "1.0"], [" working", "0.99995065"], [" properly", "1.0"], [".", "1.0"], [" A", "0.9989309"], [" heart", "0.91855216"], [" condition", "1.0"], [" can", "0.9999957"], [" be", "1.0"], [" caused", "1.0"], [" by", "1.0"], [" a", "1.0"], [" number", "1.0"], [" of", "1.0"], [" things", "0.99890625"], [".", "0.3384221"], ["\n", "0.9993742"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Can", "0.99996233"], [" a", "1.0"], [" person", "1.0"], [" be", "1.0"], [" born", "1.0"], [" with", "1.0"], [" a", "1.0"], [" brain", "0.9999808"], [" condition", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" brain", "1.0"], [" condition", "1.0"], [" is", "1.0"], [" a", "1.0"], [" condition", "1.0"]]}, "100": {"qid": "cc542edc9006be8d52a5", "term": "Frigatebird", "description": "A family of seabirds found across tropical and subtropical oceans", "question": "Would a Frigatebird in Ontario be a strange sight?", "answer": true, "facts": ["Ontario is a province of Canada.", "Canada is surrounded by temperate oceans."], "decomposition": ["Where are Frigatebirds usually found?", "Which oceans can be found around Ontario?", "Do all of #2 fail to fit the description of #1?"], "evidence": [[[["Frigatebird-1"]], [["Hudson Bay-1", "James Bay-4"]], [["Frigatebird-1", "Hudson Bay-1", "James Bay-4"]]], [[["Frigatebird-1"]], [["Hudson Bay-12", "Ontario-2"]], ["operation"]], [[["Frigatebird-17"]], [["Geography of Ontario-8"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city of New York?\nFacts: New York is a city in the United States of America.\nAnswer (True or False): True\nQuestion: What is the name of the city of New York?\nFacts: New York is a city in the United States of America.\nAnswer (True or False): True\nQuestion: What is the name of the city of New York?\nF", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a Frigatebird in Ontario be a strange sight?\nFacts: Ontario is a province of Canada. Canada is surrounded by temperate oceans.\nAnswer (True or False):", "ct_subject": "Ontario is a province of Canada. Canada is surrounded by temperate oceans.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999999"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999641"], [" of", "1.0"], [" the", "1.0"], [" city", "0.95351094"], [" of", "0.99890864"], [" New", "0.7976074"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" New", "1.0"], [" York", "1.0"], [" is", "1.0"], [" a", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "0.99996924"], [" United", "0.9986834"], [" States", "1.0"], [" of", "0.99290633"], [" America", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "0.3271377"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" New", "1.0"], [" York", "1.0"], [" is", "1.0"], [" a", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" of", "1.0"], [" America", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"]]}, "101": {"qid": "e1f6fead3b0070fe2142", "term": "Intel", "description": "American semiconductor chip manufacturer", "question": "Would a silicon shortage be bad for Intel's sales?", "answer": true, "facts": ["Silicon is a key material for the production of semiconductor chips.", "A silicon shortage would mean fewer semiconductor chips could be produced.", "A business that produces fewer products than normal will receive lower than normal revenue."], "decomposition": ["What kind of products does Intel make?", "What are the key materials used in the production of #1?", "Is silicon in #2?"], "evidence": [[[["Intel-1"]], [["Integrated circuit-29"]], ["operation"]], [[["Intel-1"]], [["Integrated circuit-1"]], ["operation"]], [[["Intel-1"]], [["Integrated circuit-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake with a hole in it\"?\nFacts: A snowflake is a single crystal. A snowflake with a hole in it is a cluster of crystals.\nAnswer (True or False): True\nQuestion: What is the difference", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a silicon shortage be bad for Intel's sales?\nFacts: Silicon is a key material for the production of semiconductor chips. A silicon shortage would mean fewer semiconductor chips could be produced. A business that produces fewer products than normal will receive lower than normal revenue.\nAnswer (True or False):", "ct_subject": "Silicon is a key material for the production of semiconductor chips. A silicon shortage would mean fewer semiconductor chips could be produced. A business that produces fewer products than normal will receive lower than normal revenue.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999949"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.97367066"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.96002126"], ["now", "0.66965353"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.9999999"], ["now", "1.0"], ["flake", "1.0"], [" with", "0.99999857"], [" a", "1.0"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999993"], [" snow", "0.999954"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "1.0"], [" crystal", "1.0"], [".", "0.3685915"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" with", "1.0"], [" a", "1.0"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], [" is", "1.0"], [" a", "1.0"], [" cluster", "0.9989442"], [" of", "1.0"], [" crystals", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"]]}, "102": {"qid": "ed08e2d4bbe5bbf1b01d", "term": "Oprah Winfrey", "description": "American businesswoman, talk show host, actress, producer, and philanthropist", "question": "Could Oprah Winfrey buy dozens of her staff Bugatti luxury cars?", "answer": true, "facts": ["Oprah Winfrey is a billionaire", "A new Bugatti costs a few million dollars"], "decomposition": ["How much is Oprah Winfrey worth?", "How much does a  Bugatti cost?", "Is #2 times 2 dozen less than #1?"], "evidence": [[[["Oprah Winfrey-54"]], [["Bugatti Chiron-8"]], ["operation"]], [[["Oprah Winfrey-54"]], [["Bugatti Chiron-8"]], ["operation"]], [[["Oprah Winfrey-54"]], [["Bugatti Chiron-8"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: Thomas Jefferson\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: Thomas Jefferson\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: Thomas", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could Oprah Winfrey buy dozens of her staff Bugatti luxury cars?\nFacts: Oprah Winfrey is a billionaire A new Bugatti costs a few million dollars\nAnswer (True or False):", "ct_subject": "Oprah Winfrey is a billionaire A new Bugatti costs a few million dollars", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9992268"], [" is", "1.0"], [" the", "1.0"], [" name", "0.95877475"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9848214"], [" person", "0.9601241"], [" to", "1.0"], [" be", "0.983694"], [" born", "0.8305031"], [" in", "1.0"], [" the", "1.0"], [" United", "0.99094266"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Thomas", "0.16857597"], [" Jefferson", "1.0"], ["\n", "0.9999999"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Thomas", "1.0"], [" Jefferson", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Thomas", "1.0"]]}, "103": {"qid": "0a4fa11abccd8dd84dc8", "term": "Leopard cat", "description": "Small wild cat", "question": "Are Leopard cats in less dire straits than Bornean Orangutan?", "answer": true, "facts": ["Leopard cats are classified as Least Concern on IUCN endangered list.", "Bornean Orangutan's are classified as Endangered on IUCN endangered list."], "decomposition": ["What are the recognized threats to the Bornean orangutan?", "What are the recognized threats to the leopard cat?", "Is #1 worse than #2?"], "evidence": [[[["Bornean orangutan-25"]], [["Leopard cat-28"]], [["Bornean orangutan-26", "Leopard cat-30"]]], [[["Bornean orangutan-26"]], [["Leopard-3"]], [["Bornean orangutan-25"], "no_evidence", "operation"]], [[["Bornean orangutan-2"]], [["Leopard cat-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there more people today related to Jesus Christ than to the Pope?\nFacts: Jesus Christ had three children. Pope John Paul II has had two children.\nAnswer (True or False): True\nQuestion: Are there more people today related to the Pope than to the Dalai Lama?\nFacts: Pope John Paul", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are Leopard cats in less dire straits than Bornean Orangutan?\nFacts: Leopard cats are classified as Least Concern on IUCN endangered list. Bornean Orangutan's are classified as Endangered on IUCN endangered list.\nAnswer (True or False):", "ct_subject": "Leopard cats are classified as Least Concern on IUCN endangered list. Bornean Orangutan's are classified as Endangered on IUCN endangered list.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.9988268"], [" there", "0.9999999"], [" more", "0.9972945"], [" people", "1.0"], [" today", "0.31673896"], [" related", "0.9999999"], [" to", "1.0"], [" Jesus", "0.26839983"], [" Christ", "1.0"], [" than", "1.0"], [" to", "1.0"], [" the", "0.85765207"], [" Pope", "0.999984"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Jesus", "0.9993092"], [" Christ", "1.0"], [" had", "1.0"], [" three", "0.44558433"], [" children", "1.0"], [".", "1.0"], [" Pope", "0.92796326"], [" John", "0.9996629"], [" Paul", "1.0"], [" II", "1.0"], [" has", "0.94267344"], [" had", "0.99999976"], [" two", "0.8163096"], [" children", "1.0"], [".", "1.0"], ["\n", "0.9999808"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "1.0"], [" more", "1.0"], [" people", "1.0"], [" today", "1.0"], [" related", "1.0"], [" to", "1.0"], [" the", "1.0"], [" Pope", "0.9953772"], [" than", "1.0"], [" to", "1.0"], [" the", "1.0"], [" Dalai", "0.9999509"], [" Lama", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Pope", "0.9992066"], [" John", "1.0"], [" Paul", "1.0"]]}, "104": {"qid": "6e47423b2fdc8bc3130c", "term": "Winter", "description": "one of the Earth's four temperate seasons, occurring between autumn and spring", "question": "Are there multiple American government holidays during winter?", "answer": true, "facts": ["Winter runs from about December 20 to about March 20.", "Government holidays include Christmas, New Year, King Day, and President's Day.", "Christmas is always December 25, New Year is always January 1, King Day is a Monday in the middle of January, and President's Day is a Monday in late February."], "decomposition": ["Through which period of the year does winter usually last in the US?", "How many government holidays fall within the span of #1?", "Is #2 considerably greater than one?"], "evidence": [[[["Winter-1"], "no_evidence"], [["Federal holidays in the United States-14", "Thanksgiving (United States)-1", "Veterans Day-10", "Washington's Birthday-1"], "no_evidence"], ["operation"]], [[["Northern Hemisphere-2"]], [["Christmas-28", "Federal holidays in the United States-6", "New Year's Day-12"]], ["operation"]], [[["Winter-9"]], [["Holiday-6"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there more people in the United States today than there were in the United States in the year 1776?\nFacts: There are more people in the United States today than there were in the United States in the year", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are there multiple American government holidays during winter?\nFacts: Winter runs from about December 20 to about March 20. Government holidays include Christmas, New Year, King Day, and President's Day. Christmas is always December 25, New Year is always January 1, King Day is a Monday in the middle of January, and President's Day is a Monday in late February.\nAnswer (True or False):", "ct_subject": "Winter runs from about December 20 to about March 20. Government holidays include Christmas, New Year, King Day, and President's Day. Christmas is always December 25, New Year is always January 1, King Day is a Monday in the middle of January, and President's Day is a Monday in late February.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.9983878"], [" there", "1.0"], [" more", "0.9999794"], [" people", "0.98431116"], [" in", "0.9999994"], [" the", "1.0"], [" United", "0.9902823"], [" States", "1.0"], [" today", "1.0"], [" than", "1.0"], [" there", "0.9999938"], [" were", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "0.99991083"], [" States", "1.0"], [" in", "1.0"], [" the", "0.98876053"], [" year", "1.0"], [" 17", "0.99756765"], ["76", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "0.9996815"], [" are", "0.96738136"], [" more", "1.0"], [" people", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" today", "1.0"], [" than", "1.0"], [" there", "1.0"], [" were", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" in", "1.0"], [" the", "0.97018784"], [" year", "1.0"]]}, "105": {"qid": "8e073418da1eab499775", "term": "Islamophobia", "description": "Fear, hatred of, or prejudice against the Islamic religion or Muslims generally,", "question": "Is Islamophobia against Cyprus majority religion misdirected?", "answer": true, "facts": ["Islamophobia is prejudice and fear against Muslims.", "Cyprus is a country in the Middle East, which is a predominantly Muslim region.", "Cyprus is the only Christian majority country in the Middle East, with Christians forming between 76% and 78% of the country's total population, and most of them adhere to Eastern Orthodox Christianity."], "decomposition": ["What religion is targeted by Islamophobia?", "What is the most common religion in Cyprus?", "Is #1 different than #2?"], "evidence": [[[["Islamophobia-1"]], [["Cyprus-100"]], ["operation"]], [[["Islamophobia-54"], "no_evidence"], [["Religion in Cyprus-1"], "operation"], ["no_evidence"]], [[["Islamophobia-1"]], [["Religion in Cyprus-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the United States the only country in the world that has a national holiday for the birthday of Abraham Lincoln?\nFacts: The United States is the only country in the world that has a national holiday for the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Islamophobia against Cyprus majority religion misdirected?\nFacts: Islamophobia is prejudice and fear against Muslims. Cyprus is a country in the Middle East, which is a predominantly Muslim region. Cyprus is the only Christian majority country in the Middle East, with Christians forming between 76% and 78% of the country's total population, and most of them adhere to Eastern Orthodox Christianity.\nAnswer (True or False):", "ct_subject": "Islamophobia is prejudice and fear against Muslims. Cyprus is a country in the Middle East, which is a predominantly Muslim region. Cyprus is the only Christian majority country in the Middle East, with Christians forming between 76% and 78% of the country's total population, and most of them adhere to Eastern Orthodox Christianity.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "0.99936694"], [" United", "0.9999963"], [" States", "1.0"], [" the", "0.9799518"], [" only", "1.0"], [" country", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" that", "1.0"], [" has", "1.0"], [" a", "1.0"], [" national", "0.953256"], [" holiday", "1.0"], [" for", "1.0"], [" the", "1.0"], [" birthday", "1.0"], [" of", "1.0"], [" Abraham", "0.9942984"], [" Lincoln", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "0.99999774"], [" the", "1.0"], [" only", "1.0"], [" country", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" that", "1.0"], [" has", "1.0"], [" a", "1.0"], [" national", "1.0"], [" holiday", "1.0"], [" for", "1.0"], [" the", "1.0"]]}, "106": {"qid": "9deedbba0ca784be1855", "term": "Amtrak", "description": "Intercity rail operator in the United States", "question": "Does Amtrak operate four wheel vehicles?", "answer": true, "facts": ["Amtrak is a transportation service.", "Amtrak transports people with trains and buses.", "A bus is a four wheel vehicle. "], "decomposition": ["What kinds of vehicles does Amtrak use?", "Do any of #1 have four wheels?"], "evidence": [[[["Amtrak-1"]], [["Wheelset (rail transport)-1"], "operation"]], [[["International (Amtrak train)-14"]], [["Wheelset (rail transport)-1"]]], [[["Amtrak-3"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city of New York?\nFacts: New York is the capital of the United States. New York is the largest city in the United States. New York is the largest city in the world. New York is the largest city in the world. New York is the largest city in the world. New York is the largest city in the world. New York is the largest city in the world.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Amtrak operate four wheel vehicles?\nFacts: Amtrak is a transportation service. Amtrak transports people with trains and buses. A bus is a four wheel vehicle. \nAnswer (True or False):", "ct_subject": "Amtrak is a transportation service. Amtrak transports people with trains and buses. A bus is a four wheel vehicle. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999635"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99986374"], [" of", "1.0"], [" the", "1.0"], [" city", "0.52517056"], [" of", "0.86645937"], [" New", "0.95041126"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" New", "1.0"], [" York", "1.0"], [" is", "0.99974996"], [" the", "0.99577576"], [" capital", "0.9999981"], [" of", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [".", "0.99999964"], [" New", "1.0"], [" York", "1.0"], [" is", "0.99999714"], [" the", "0.9993604"], [" largest", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [".", "1.0"], [" New", "0.97247326"], [" York", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "0.99999905"], [".", "1.0"], [" New", "0.92619324"], [" York", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [".", "1.0"], [" New", "0.98931915"], [" York", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "0.9999999"], [".", "1.0"], [" New", "1.0"], [" York", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [".", "1.0"], [" New", "1.0"], [" York", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [".", "1.0"]]}, "107": {"qid": "65d216ee031d7c2a376f", "term": "Chlorine", "description": "Chemical element with atomic number 17", "question": "Does chlorine inhibit photosynthesis?", "answer": true, "facts": ["Chlorine prevents algae from growing in pools", "Algae photosynthesize "], "decomposition": ["What does Chlorine prevent from growing in a pool?", "Does #1 do photosynthesis?"], "evidence": [[[["Swimming pool-67"]], [["Algae-1"], "operation"]], [[["Chlorine-66"]], [["Photosynthesis-6"]]], [[["Chlorine dioxide-25"]], [["Bacteria-3"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake with a hole in it\"?\nFacts: A snowflake is a single crystal of ice. A snowflake with a hole in it is a cluster of crystals.\nAnswer (True or False): True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake with a hole in it\"?\nFacts: A snowflake is a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does chlorine inhibit photosynthesis?\nFacts: Chlorine prevents algae from growing in pools Algae photosynthesize \nAnswer (True or False):", "ct_subject": "Chlorine prevents algae from growing in pools Algae photosynthesize ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.6066867"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.986696"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.9999993"], ["s", "0.9177367"], ["now", "0.9590794"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.9999641"], ["now", "1.0"], ["flake", "1.0"], [" with", "0.83835685"], [" a", "1.0"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999993"], [" snow", "1.0"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "0.5454519"], [" crystal", "0.9999989"], [" of", "0.94143"], [" ice", "1.0"], [".", "0.99802244"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" with", "1.0"], [" a", "1.0"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], [" is", "1.0"], [" a", "1.0"], [" cluster", "0.7244804"], [" of", "1.0"], [" crystals", "0.99960095"], [".", "0.99880075"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], [" with", "1.0"], [" a", "1.0"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"]]}, "108": {"qid": "54a5df08fef4e5a4c82c", "term": "Bruce Lee", "description": "Hong Kong-American actor, martial artist", "question": "Was Bruce Lee absent from the 1964 University of Washington graduation ceremony?", "answer": true, "facts": ["Bruce Lee enrolled at the University of Washington in 1961.", "Bruce Lee dropped out of college in early 1964.", "Bruce Lee moved to Oakland to live with James Yimm Lee in 1964."], "decomposition": ["When did the University of Washington graduation ceremony for the class of 1964 take place?", "What college did Bruce Lee attend?", "When did Bruce Lee drop out of #2?", "Did #1 occur after #3?"], "evidence": [[["no_evidence"], [["Bruce Lee-30"]], [["Bruce Lee-15"]], ["operation"]], [["no_evidence"], [["Bruce Lee-2"]], [["Bruce Lee-15"]], ["operation"]], [["no_evidence"], [["Bruce Lee-14"]], [["Bruce Lee-15"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Beatles have a song called \"I Want to Hold Your Hand\"?\nFacts: The Beatles had a song called \"I Want to Hold Your Hand\" in 1964.\nAnswer (True or False): True\nQuestion: Did the Beatles have a song called \"I Want to Hold Your Hand\"?\nFacts: The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Bruce Lee absent from the 1964 University of Washington graduation ceremony?\nFacts: Bruce Lee enrolled at the University of Washington in 1961. Bruce Lee dropped out of college in early 1964. Bruce Lee moved to Oakland to live with James Yimm Lee in 1964.\nAnswer (True or False):", "ct_subject": "Bruce Lee enrolled at the University of Washington in 1961. Bruce Lee dropped out of college in early 1964. Bruce Lee moved to Oakland to live with James Yimm Lee in 1964.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.86995226"], [" the", "1.0"], [" Beatles", "0.961599"], [" have", "0.9905376"], [" a", "1.0"], [" song", "0.99999666"], [" called", "1.0"], [" \"", "1.0"], ["I", "0.99997854"], [" Want", "0.99999666"], [" to", "1.0"], [" Hold", "1.0"], [" Your", "1.0"], [" Hand", "1.0"], ["\"?", "0.9998398"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Beatles", "1.0"], [" had", "0.82028687"], [" a", "1.0"], [" song", "1.0"], [" called", "1.0"], [" \"", "1.0"], ["I", "1.0"], [" Want", "1.0"], [" to", "1.0"], [" Hold", "1.0"], [" Your", "1.0"], [" Hand", "1.0"], ["\"", "0.92588216"], [" in", "1.0"], [" 1964", "0.99990916"], [".", "1.0"], ["\n", "0.9917645"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99999857"], [" the", "1.0"], [" Beatles", "1.0"], [" have", "1.0"], [" a", "1.0"], [" song", "1.0"], [" called", "1.0"], [" \"", "1.0"], ["I", "1.0"], [" Want", "1.0"], [" to", "1.0"], [" Hold", "1.0"], [" Your", "1.0"], [" Hand", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"]]}, "109": {"qid": "0a8bb20dbdb99d68127e", "term": "Clove", "description": "species of plant", "question": "Do people who smoke Djarum's like cloves?", "answer": true, "facts": ["Djarum is a brand of cigarette popular around the world.", "Djarum cigarettes are made with a blend of cloves and tobacco."], "decomposition": ["What are Djarum cigarettes made of?", "Does #1 include cloves?"], "evidence": [[[["Djarum-1"]], ["operation"]], [[["Djarum-1"]], ["operation"]], [[["Djarum-1", "Kretek-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city of Rome?\nFacts: Rome is the capital of the Roman Empire.\nAnswer (True or False): True\nQuestion: What is the name of the city of Rome?\nFacts: Rome is the capital of the Roman Empire.\nAnswer (True or False): True\nQuestion: What is the name of the city of Rome?", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do people who smoke Djarum's like cloves?\nFacts: Djarum is a brand of cigarette popular around the world. Djarum cigarettes are made with a blend of cloves and tobacco.\nAnswer (True or False):", "ct_subject": "Djarum is a brand of cigarette popular around the world. Djarum cigarettes are made with a blend of cloves and tobacco.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9445553"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9766173"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9992913"], [" of", "0.96512526"], [" Rome", "0.9999989"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "0.99999964"], [" is", "0.9999958"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], ["?", "1.0"]]}, "110": {"qid": "3ca5966b88394e62271e", "term": "University of Pennsylvania", "description": "Private Ivy League research university in Philadelphia, Pennsylvania", "question": "Could Brooke Shields succeed at University of Pennsylvania?", "answer": true, "facts": ["Brooke Shields graduated from Princeton University.", "Princeton is ranked as the number 1 national college by US news.", "University of Pennsylvania is ranked as number 6 national college by US news.", "Princeton only admits around 6 percent of applicants as of 2018.", "University of Pennsylvania accepts around 9% of applicants as of 2018."], "decomposition": ["What college did Brooke Shields go to?", "Out of all colleges in the US, how is #1 ranked?", "Is the ranking of University of Pennsylvania similar to #2?"], "evidence": [[[["Brooke Shields-6"]], [["Princeton University-59"]], [["University of Pennsylvania-48"]]], [[["Brooke Shields-6"]], [["Princeton University-59"]], [["University of Pennsylvania-48"], "operation"]], [[["Brooke Shields-6"]], [["Princeton University-3"], "operation"], [["University of Pennsylvania-47"], "no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the capital of the state of Pennsylvania?\nFacts: Philadelphia.\nAnswer (True or False): True\nQuestion: What is the name of the city in the United", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could Brooke Shields succeed at University of Pennsylvania?\nFacts: Brooke Shields graduated from Princeton University. Princeton is ranked as the number 1 national college by US news. University of Pennsylvania is ranked as number 6 national college by US news. Princeton only admits around 6 percent of applicants as of 2018. University of Pennsylvania accepts around 9% of applicants as of 2018.\nAnswer (True or False):", "ct_subject": "Brooke Shields graduated from Princeton University. Princeton is ranked as the number 1 national college by US news. University of Pennsylvania is ranked as number 6 national college by US news. Princeton only admits around 6 percent of applicants as of 2018. University of Pennsylvania accepts around 9% of applicants as of 2018.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999356"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9890099"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9754774"], [" in", "0.99068177"], [" the", "0.9999987"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "0.9798384"], [" the", "1.0"], [" capital", "0.99568594"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" Pennsylvania", "0.99999475"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Philadelphia", "0.99999774"], [".", "0.8894017"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "0.9999981"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"]]}, "111": {"qid": "45605f9dbc0cf85f668f", "term": "Atlantic salmon", "description": "species of fish", "question": "Would Atlantic Salmon be within David Duchovny's dietary guidelines?", "answer": true, "facts": ["David Duchovny is a pescatarian. ", "Pescatarians do not eat chicken, pork, or beef, but will eat fish."], "decomposition": ["What kind of diet does David Duchovny follow?", "What type of food is Atlantic Salmon?", "Do people who follow #1 diets eat #2?"], "evidence": [[[["David Duchovny-12"]], [["Atlantic salmon-1"]], [["Pescetarianism-1"]]], [[["David Duchovny-12"]], [["Atlantic salmon-1", "Seafood-1"]], [["Pescetarianism-1"]]], [[["David Duchovny-3"], "no_evidence"], [["Atlantic salmon-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human being?\nFacts: The average human being weighs about 100 pounds.\nAnswer (True or False): True\nQuestion: What is the average weight of a horse?\nFacts: The average horse weighs about 1,000 pounds.\nAnswer (True or False): True\nQuestion: What is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Atlantic Salmon be within David Duchovny's dietary guidelines?\nFacts: David Duchovny is a pescatarian.  Pescatarians do not eat chicken, pork, or beef, but will eat fish.\nAnswer (True or False):", "ct_subject": "David Duchovny is a pescatarian.  Pescatarians do not eat chicken, pork, or beef, but will eat fish.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99926895"], [" is", "1.0"], [" the", "1.0"], [" average", "0.99913836"], [" weight", "0.99999964"], [" of", "1.0"], [" a", "0.9999999"], [" human", "0.9999994"], [" being", "0.82283735"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.9999974"], [" being", "0.999936"], [" weighs", "0.4009986"], [" about", "0.916908"], [" 100", "0.91549635"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "0.8617637"], [" of", "1.0"], [" a", "1.0"], [" horse", "0.7606501"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" horse", "0.99998224"], [" weighs", "1.0"], [" about", "1.0"], [" 1", "0.9713425"], [",", "1.0"], ["000", "0.9999995"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"]]}, "112": {"qid": "b9f5942431b6ae3ef7bd", "term": "Compact disc", "description": "Optical disc for storage and playback of digital audio", "question": "Did compact discs make computer gaming more popular?", "answer": true, "facts": ["Compact discs contained significantly more storage space than the previously popular floppy disc format.", "Gaming studios were therefore able to significantly improve the graphics, sounds, and features of their games to make them more immersive.", "The better games led to a massive increase in popularity for computer gaming."], "decomposition": ["Which external storage device was previously popular before compact discs?", "What features of compact disc made them much better than #1?", "What new possibilities did #2 create for computer games?", "Did #3 lead to increased interest in computer games?"], "evidence": [[[["Floppy disk-3"]], [["CD-ROM-21"]], [["PC game-22"]], [["PC game-4"], "operation"]], [[["Floppy disk-2", "Floppy disk-22"]], [["Compact disc-3", "Video CD-2"]], [["Video CD-2"], "no_evidence"], [["Compact disc-59", "PC game-22"]]], [[["Floppy disk-1"]], [["Compact disc-2", "Compact disc-41"]], [["Fifth generation of video game consoles-4"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the first computer game ever be played on a computer?\nFacts: The first computer game was played on a computer in the late 1950s.\nAnswer (True or False): True\nQuestion: Did the first computer game ever be played on a computer?", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did compact discs make computer gaming more popular?\nFacts: Compact discs contained significantly more storage space than the previously popular floppy disc format. Gaming studios were therefore able to significantly improve the graphics, sounds, and features of their games to make them more immersive. The better games led to a massive increase in popularity for computer gaming.\nAnswer (True or False):", "ct_subject": "Compact discs contained significantly more storage space than the previously popular floppy disc format. Gaming studios were therefore able to significantly improve the graphics, sounds, and features of their games to make them more immersive. The better games led to a massive increase in popularity for computer gaming.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999856"], [" the", "1.0"], [" first", "0.99988246"], [" computer", "0.99991584"], [" game", "1.0"], [" ever", "1.0"], [" be", "1.0"], [" played", "0.99792"], [" on", "0.9998375"], [" a", "1.0"], [" computer", "0.9995914"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" computer", "1.0"], [" game", "1.0"], [" was", "0.9999721"], [" played", "1.0"], [" on", "0.9999987"], [" a", "1.0"], [" computer", "0.52668136"], [" in", "1.0"], [" the", "1.0"], [" late", "0.26907694"], [" 1950", "0.99988616"], ["s", "0.9296443"], [".", "0.9999962"], ["\n", "0.9996654"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" first", "1.0"], [" computer", "1.0"], [" game", "1.0"], [" ever", "1.0"], [" be", "1.0"], [" played", "1.0"], [" on", "1.0"], [" a", "1.0"], [" computer", "0.9999993"], ["?", "1.0"]]}, "113": {"qid": "63caa686e799fd5825e8", "term": "H", "description": "letter in the Latin alphabet", "question": "Are any letters directly before and after H missing from Roman numerals?", "answer": true, "facts": ["The Roman numerals are: I, V, X, L, C, D, and M.", "The letter H in the alphabet is preceded by G and followed by I."], "decomposition": ["What is the letter before \"H\"?", "What is the letter after \"H\"?", "What are the Roman numerals? ", "Is it the case that not both #1 and #2 are in #3?"], "evidence": [[["operation"], ["operation"], [["Roman numerals-43"]], ["operation"]], [[["G-1"]], [["I-1"]], [["1444-1"]], ["operation"]], [[["G-1", "H-1"]], [["I-1"]], [["Roman numerals-1"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the word \"H\" a contraction of \"Hieroglyphics\"?\nFacts: The word \"H\" is a contraction of the word \"Hieroglyphics\".\nAnswer (True or False): True\nQuestion: Is the word \"H\" a contraction of \"Hieroglyphics\"?\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are any letters directly before and after H missing from Roman numerals?\nFacts: The Roman numerals are: I, V, X, L, C, D, and M. The letter H in the alphabet is preceded by G and followed by I.\nAnswer (True or False):", "ct_subject": "The Roman numerals are: I, V, X, L, C, D, and M. The letter H in the alphabet is preceded by G and followed by I.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.06992264"], [" the", "0.99999857"], [" word", "0.996342"], [" \"", "1.0"], ["H", "0.99999464"], ["\"", "0.99955565"], [" a", "0.34345064"], [" contraction", "0.8247883"], [" of", "1.0"], [" \"", "0.9999212"], ["H", "0.999997"], ["ier", "0.32266006"], ["ogly", "0.9999999"], ["ph", "1.0"], ["ics", "0.99962187"], ["\"?", "0.92000103"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["H", "1.0"], ["\"", "0.92995304"], [" is", "1.0"], [" a", "0.9871859"], [" contraction", "1.0"], [" of", "1.0"], [" the", "0.94652873"], [" word", "0.9999999"], [" \"", "1.0"], ["H", "1.0"], ["ier", "1.0"], ["ogly", "1.0"], ["ph", "1.0"], ["ics", "1.0"], ["\".", "0.59630597"], ["\n", "0.99999964"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99999154"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["H", "1.0"], ["\"", "1.0"], [" a", "1.0"], [" contraction", "1.0"], [" of", "1.0"], [" \"", "1.0"], ["H", "1.0"], ["ier", "1.0"], ["ogly", "1.0"], ["ph", "1.0"], ["ics", "1.0"], ["\"?", "1.0"], ["\n", "1.0"]]}, "114": {"qid": "faf10ced482aedfe9c58", "term": "Johnny Cash", "description": "American singer-songwriter and actor", "question": "Are there enough Jonny Cash records in the world to give one to each French citizen?", "answer": true, "facts": ["Johnny Cash has sold about 90 million albums", "The population of France is around 66 million "], "decomposition": ["How many Johnny Cash records have been sold?", "What is the population of France?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Johnny Cash-1"]], [["France-1"]], ["operation"]], [[["Johnny Cash-1"]], [["France-1"]], ["operation"]], [[["Johnny Cash-1"]], [["France-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there enough Jonny Cash records in the world to give one to each French citizen?\nFacts: Johnny Cash has sold about 90 million albums The population of France is around 66 million \nAnswer (True or False): True\nQuestion: Are there enough Jonny Cash records in the world to give one to each French citizen?\nFacts: Johnny Cash has sold about 90 million albums The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are there enough Jonny Cash records in the world to give one to each French citizen?\nFacts: Johnny Cash has sold about 90 million albums The population of France is around 66 million \nAnswer (True or False):", "ct_subject": "Johnny Cash has sold about 90 million albums The population of France is around 66 million ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.8943731"], [" there", "1.0"], [" enough", "0.99558985"], [" Jon", "0.9582765"], ["ny", "1.0"], [" Cash", "1.0"], [" records", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" to", "1.0"], [" give", "1.0"], [" one", "1.0"], [" to", "1.0"], [" each", "1.0"], [" French", "0.9972921"], [" citizen", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Johnny", "1.0"], [" Cash", "1.0"], [" has", "1.0"], [" sold", "1.0"], [" about", "1.0"], [" 90", "1.0"], [" million", "1.0"], [" albums", "1.0"], [" The", "1.0"], [" population", "1.0"], [" of", "1.0"], [" France", "1.0"], [" is", "1.0"], [" around", "1.0"], [" 66", "1.0"], [" million", "1.0"], [" ", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "1.0"], [" enough", "0.99999964"], [" Jon", "1.0"], ["ny", "1.0"], [" Cash", "1.0"], [" records", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" to", "1.0"], [" give", "1.0"], [" one", "1.0"], [" to", "1.0"], [" each", "1.0"], [" French", "1.0"], [" citizen", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Johnny", "1.0"], [" Cash", "1.0"], [" has", "1.0"], [" sold", "1.0"], [" about", "1.0"], [" 90", "1.0"], [" million", "1.0"], [" albums", "1.0"], [" The", "1.0"]]}, "115": {"qid": "66a3c0af3141c7c7d215", "term": "QWERTY", "description": "keyboard layout where the first line is \"QWERTYUIOP\"", "question": "Can monkeys use QWERTY keyboards?", "answer": true, "facts": ["QWERTY keyboards are an alphabet key layout that were first used on typrwriters. ", "Monkeys can be trained to push buttons.", "Typewriter key's are buttons.", "Monkeys can press keys on keyboards."], "decomposition": ["What kind of keys are found on QWERTY keyboards?", "Can #1 be likened to buttons?", "Can monkeys be trained to push buttons?", "Are #2 and #3 positive?"], "evidence": [[[["QWERTY-17"]], ["operation"], ["no_evidence", "operation"], ["no_evidence", "operation"]], [[["QWERTY-12"]], [["Keyboard layout-3"]], [["Pet monkey-4"]], ["operation"]], [[["QWERTY-16", "QWERTY-17"]], [["Push-button-1"], "no_evidence"], [["Tool use by animals-21"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to use the word \"candy\"?\nFacts: Candy was first used in the late 1800's.\nAnswer (True or False): True\nQuestion: What is the name of the first person to use the word \"candy\"?\nFacts: Candy was first", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can monkeys use QWERTY keyboards?\nFacts: QWERTY keyboards are an alphabet key layout that were first used on typrwriters.  Monkeys can be trained to push buttons. Typewriter key's are buttons. Monkeys can press keys on keyboards.\nAnswer (True or False):", "ct_subject": "QWERTY keyboards are an alphabet key layout that were first used on typrwriters.  Monkeys can be trained to push buttons. Typewriter key's are buttons. Monkeys can press keys on keyboards.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9755857"], [" is", "1.0"], [" the", "1.0"], [" name", "0.89289945"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9967714"], [" person", "0.994487"], [" to", "1.0"], [" use", "0.9955686"], [" the", "0.99992156"], [" word", "0.9999999"], [" \"", "1.0"], ["c", "0.9985482"], ["andy", "0.9968004"], ["\"?", "0.9999993"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Candy", "0.99999976"], [" was", "0.9962863"], [" first", "0.9804897"], [" used", "1.0"], [" in", "0.99998784"], [" the", "1.0"], [" late", "0.62241423"], [" 1800", "0.99965024"], ["'s", "1.0"], [".", "0.9998299"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" use", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Candy", "1.0"], [" was", "1.0"], [" first", "1.0"]]}, "116": {"qid": "55c54e04a9446aaf6b45", "term": "Chocolate brownie", "description": "A square, baked, chocolate dessert", "question": "Should children be kept from \"special brownies\"?", "answer": true, "facts": ["\"Special brownies\" typically refer to brownies that have been laced with THC.", "THC is an active component of cannabis, a drug meant for adults only."], "decomposition": ["What are \"special brownies\" made from that makes them special?", "Who is #1 made specifically for?", "Are children allowed to have things meant for #2?"], "evidence": [[[["Cannabis edible-1"]], [["Cannabis edible-2"]], ["no_evidence"]], [[["Cannabis edible-11"]], [["Cannabis edible-2"]], [["Cannabis edible-7"], "operation"]], [[["Cannabis edible-6"]], [["Medical cannabis-30"]], [["Medical cannabis-30"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"cocaine-sniffing dog\" and a \"cocaine-sniffing cat\"?\nFacts: A \"cocaine-sniffing dog\" is a dog that is trained to detect the presence of cocaine in the urine of a person who is suspected of using cocaine. A \"cocaine-", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Should children be kept from \"special brownies\"?\nFacts: \"Special brownies\" typically refer to brownies that have been laced with THC. THC is an active component of cannabis, a drug meant for adults only.\nAnswer (True or False):", "ct_subject": "\"Special brownies\" typically refer to brownies that have been laced with THC. THC is an active component of cannabis, a drug meant for adults only.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9827958"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9999995"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "0.9811117"], ["oc", "0.77501976"], ["aine", "1.0"], ["-", "0.9615211"], ["sn", "1.0"], ["iff", "1.0"], ["ing", "1.0"], [" dog", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["oc", "1.0"], ["aine", "1.0"], ["-", "1.0"], ["sn", "1.0"], ["iff", "1.0"], ["ing", "1.0"], [" cat", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9997694"], [" \"", "0.9983791"], ["c", "1.0"], ["oc", "1.0"], ["aine", "1.0"], ["-", "1.0"], ["sn", "1.0"], ["iff", "1.0"], ["ing", "1.0"], [" dog", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" dog", "0.9994735"], [" that", "0.9998828"], [" is", "0.99997234"], [" trained", "1.0"], [" to", "1.0"], [" detect", "1.0"], [" the", "1.0"], [" presence", "0.9999268"], [" of", "1.0"], [" cocaine", "1.0"], [" in", "0.62425774"], [" the", "0.9999958"], [" urine", "0.9999796"], [" of", "1.0"], [" a", "0.9999989"], [" person", "1.0"], [" who", "0.9824523"], [" is", "0.99514115"], [" suspected", "0.99998856"], [" of", "1.0"], [" using", "1.0"], [" cocaine", "1.0"], [".", "1.0"], [" A", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["oc", "1.0"], ["aine", "1.0"], ["-", "1.0"]]}, "117": {"qid": "f66e34419f26c8027ee6", "term": "Snoopy", "description": "cartoon dog", "question": "Would Taylor Swift refer to Snoopy as oppa?", "answer": true, "facts": ["Oppa is a Korean word used by women to address a man who is 10 or more years older than her", "Snoopy is 47 years old", "Taylor Swift is 30 years old"], "decomposition": ["What is the minimum age difference that a Korean woman would use Oppa to address an older man?", "How old is Snoopy?", "How old is Taylor Swift?", "What is #2 minus #3?", "Is #4 greater than or equal to #1?"], "evidence": [[[["Korean pronouns-20"], "no_evidence"], [["Snoopy-7"]], [["Taylor Swift-1"]], ["operation"], ["operation"]], [[["Third-person pronoun-106"], "no_evidence"], [["Snoopy-1"], "no_evidence"], [["Taylor Swift-1"]], ["operation"], ["operation"]], [[["Korean honorifics-1"], "no_evidence"], [["Snoopy-1"]], [["Taylor Swift-4"]], ["operation"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: John Smith\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: John Smith\nAnswer (True or False): True\nQuestion: What is the name of", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Taylor Swift refer to Snoopy as oppa?\nFacts: Oppa is a Korean word used by women to address a man who is 10 or more years older than her Snoopy is 47 years old Taylor Swift is 30 years old\nAnswer (True or False):", "ct_subject": "Oppa is a Korean word used by women to address a man who is 10 or more years older than her Snoopy is 47 years old Taylor Swift is 30 years old", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99999905"], [" of", "1.0"], [" the", "1.0"], [" first", "0.89892703"], [" person", "0.9782628"], [" to", "1.0"], [" be", "0.99996495"], [" born", "0.99852115"], [" in", "1.0"], [" the", "1.0"], [" United", "0.9470653"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "0.96822184"], [" Smith", "0.75949025"], ["\n", "0.90970045"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "1.0"], [" Smith", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"]]}, "118": {"qid": "9c14845b31238b66baf6", "term": "Horseradish", "description": "species of plant", "question": "Can horseradish be eaten in a religious context?", "answer": true, "facts": ["A Seder is typically held during the Jewish holiday Passover.", "The Seder involves several items representing the story of the Exodus.", "Horseradish is commonly used for the traditional bitter herb item."], "decomposition": ["What are some commonly used traditional bitter herb items for the Seder held during the Passover?", "Is horseradish included in #1?"], "evidence": [[[["Maror-1"]], [["Maror-13"], "operation"]], [[["Maror-7"]], ["operation"]], [[["Passover Seder-53"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"sugar cube\" and a \"sugar cube with a hole in it\"?\nFacts: The sugar cube is a small, round, sugar cube that is used to represent the sugar cube with a hole in it.\nAnswer (True or False): True\nQuestion: What is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can horseradish be eaten in a religious context?\nFacts: A Seder is typically held during the Jewish holiday Passover. The Seder involves several items representing the story of the Exodus. Horseradish is commonly used for the traditional bitter herb item.\nAnswer (True or False):", "ct_subject": "A Seder is typically held during the Jewish holiday Passover. The Seder involves several items representing the story of the Exodus. Horseradish is commonly used for the traditional bitter herb item.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.98268616"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.99975044"], [" between", "1.0"], [" a", "0.99999154"], [" \"", "0.9999999"], ["s", "0.9970783"], ["ugar", "0.6542298"], [" cube", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["ugar", "1.0"], [" cube", "0.99999154"], [" with", "1.0"], [" a", "0.9992925"], [" hole", "1.0"], [" in", "1.0"], [" it", "0.99999976"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.046884812"], [" sugar", "1.0"], [" cube", "1.0"], [" is", "1.0"], [" a", "1.0"], [" small", "0.9999279"], [",", "0.999998"], [" round", "1.0"], [",", "1.0"], [" sugar", "1.0"], [" cube", "1.0"], [" that", "0.9059375"], [" is", "0.9833129"], [" used", "0.99883145"], [" to", "0.9999957"], [" represent", "1.0"], [" the", "1.0"], [" sugar", "0.99999976"], [" cube", "0.9999956"], [" with", "1.0"], [" a", "1.0"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], [".", "1.0"], ["\n", "0.99594986"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"]]}, "119": {"qid": "f67ebf32de48c788dfbb", "term": "Hamster", "description": "subfamily of mammals", "question": "Do hamsters provide food for any animals?", "answer": true, "facts": ["Hamsters are prey animals.", "Prey animals provide food for predators. "], "decomposition": ["What types of animal are hamsters?", "Do #1 provide food for any other animals?"], "evidence": [[[["Hamster-1"]], [["Ball python-9", "Snake-85"], "no_evidence", "operation"]], [[["Hamster-1"]], [["Cat-1", "Hamster-2"], "operation"]], [[["Hamster-1"]], [["Rodent-73"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance a person can walk in one day?\nFacts: The longest distance a person can walk in one day is about 2,000 miles.\nAnswer (True or False): True\nQuestion: What is the longest distance a person can walk in one year?\nFacts: The longest distance a person can walk in one year is about 2,000 miles.\nAnswer (True or False): True\nQuestion: What is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do hamsters provide food for any animals?\nFacts: Hamsters are prey animals. Prey animals provide food for predators. \nAnswer (True or False):", "ct_subject": "Hamsters are prey animals. Prey animals provide food for predators. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999378"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.8916967"], [" distance", "0.9999442"], [" a", "0.99548125"], [" person", "0.37115085"], [" can", "0.7683206"], [" walk", "0.9999137"], [" in", "0.99999917"], [" one", "0.9999819"], [" day", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999999"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" person", "1.0"], [" can", "1.0"], [" walk", "1.0"], [" in", "1.0"], [" one", "1.0"], [" day", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 2", "0.40941432"], [",", "0.99999964"], ["000", "0.9999975"], [" miles", "1.0"], [".", "1.0"], ["\n", "0.9999999"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" person", "1.0"], [" can", "1.0"], [" walk", "0.9999746"], [" in", "1.0"], [" one", "1.0"], [" year", "0.9998005"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" person", "1.0"], [" can", "1.0"], [" walk", "1.0"], [" in", "1.0"], [" one", "1.0"], [" year", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 2", "0.6821649"], [",", "1.0"], ["000", "1.0"], [" miles", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"]]}, "120": {"qid": "c28b918d3baf499d1191", "term": "Chick-fil-A", "description": "American fast food chain", "question": "Will Chick-fil-A hypothetically refuse to sponsor a Pride parade?", "answer": true, "facts": ["Pride parades are annual festivals held to celebrate the gay community.", "Chick-fil-A is a fast food restaurant founded by S. Truett Cathy.", "S. Truett Cathy was a devout Southern Baptist. ", "Chick-fil-A's opposition to same-sex marriage has been the subject of public controversy."], "decomposition": ["Who founded Chick-fil-A?", "What religion was #1?", "What do pride parades typically celebrate?", "Do #2's oppose #3?"], "evidence": [[[["Chick-fil-A-3"]], [["S. Truett Cathy-3"]], [["Pride parade-1"]], [["Public image of Mike Huckabee-17"]]], [[["Chick-fil-A-3"]], [["Chick-fil-A-2"]], [["Pride parade-1"]], [["Chick-fil-A-33"], "no_evidence", "operation"]], [[["S. Truett Cathy-1"]], [["S. Truett Cathy-3"]], [["Pride parade-1"]], [["Hate group-17"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: The first person to be executed in the United States was John Brown. John Brown was a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Will Chick-fil-A hypothetically refuse to sponsor a Pride parade?\nFacts: Pride parades are annual festivals held to celebrate the gay community. Chick-fil-A is a fast food restaurant founded by S. Truett Cathy. S. Truett Cathy was a devout Southern Baptist.  Chick-fil-A's opposition to same-sex marriage has been the subject of public controversy.\nAnswer (True or False):", "ct_subject": "Pride parades are annual festivals held to celebrate the gay community. Chick-fil-A is a fast food restaurant founded by S. Truett Cathy. S. Truett Cathy was a devout Southern Baptist.  Chick-fil-A's opposition to same-sex marriage has been the subject of public controversy.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.81693214"], [" is", "1.0"], [" the", "1.0"], [" name", "0.74284923"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9984617"], [" person", "0.99991226"], [" to", "1.0"], [" be", "0.9999628"], [" executed", "0.9999925"], [" in", "0.99072915"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9724339"], [" first", "1.0"], [" person", "1.0"], [" to", "0.692343"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" John", "0.9999665"], [" Brown", "0.99713326"], [".", "0.99805874"], [" John", "0.99694616"], [" Brown", "1.0"], [" was", "1.0"], [" a", "1.0"]]}, "121": {"qid": "7035f77c008349a22833", "term": "Evander Holyfield", "description": "American boxer", "question": "Does Evander Holyfield eat pork products?", "answer": true, "facts": ["Evander Holyfield is a born-again Christian", "Mainstream Christian denominations do not observe restrictions on the kinds of animals they can eat"], "decomposition": ["What religion was Evander Holyfield?", "Does #1 allow eating pork?"], "evidence": [[[["Evander Holyfield-62"], "no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], [["Christian dietary laws-1"], "operation"]], [["no_evidence"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the Bible the word of God?\nFacts: The Bible is the word of God.\nAnswer (True or False): True\nQuestion: Is the Bible the word of God?\nFacts: The Bible is the word of God.\nAnswer (True or False): True\nQuestion: Is the Bible the word of God?\nFacts: The Bible is the word of God", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Evander Holyfield eat pork products?\nFacts: Evander Holyfield is a born-again Christian Mainstream Christian denominations do not observe restrictions on the kinds of animals they can eat\nAnswer (True or False):", "ct_subject": "Evander Holyfield is a born-again Christian Mainstream Christian denominations do not observe restrictions on the kinds of animals they can eat", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.8426457"], [" the", "0.99479127"], [" Bible", "0.9687793"], [" the", "0.98673195"], [" word", "0.66137093"], [" of", "1.0"], [" God", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Bible", "1.0"], [" is", "1.0"], [" the", "1.0"], [" word", "0.99999976"], [" of", "1.0"], [" God", "1.0"], [".", "0.99997234"], ["\n", "0.9955297"], ["Answer", "0.99999976"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" Bible", "1.0"], [" the", "1.0"], [" word", "1.0"], [" of", "1.0"], [" God", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Bible", "1.0"], [" is", "1.0"], [" the", "1.0"], [" word", "1.0"], [" of", "1.0"], [" God", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" Bible", "1.0"], [" the", "1.0"], [" word", "1.0"], [" of", "1.0"], [" God", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Bible", "1.0"], [" is", "1.0"], [" the", "1.0"], [" word", "1.0"], [" of", "1.0"], [" God", "1.0"]]}, "122": {"qid": "0cb73f1ccb217757bddf", "term": "Steven Spielberg", "description": "American film director and screenwriter", "question": "Does Steven Spielberg's 1998 film take place in a period after War Horse setting?", "answer": true, "facts": ["Steven Spielberg's 1998 film, Saving Private Ryan, takes place during World War II.", "World War II took place from 1939-1945.", "The 2011 movie War Horse takes place during World War I.", "World War I took place from 1914-1918."], "decomposition": ["What time period does War Horse take place in?", "What years did #1 take place in?", "What time period does Steven Spielberg's 1998 film take place in?", "When years did #3 take place in?", "Did #4 happen after #2?"], "evidence": [[[["War Horse (film)-2"]], [["World War I-1"]], [["Saving Private Ryan-1"]], [["World War II-1"]], ["operation"]], [[["War Horse (film)-2"]], [["World War I-1"]], [["Saving Private Ryan-1"]], [["World War II-1"]], ["operation"]], [[["War Horse (film)-2"]], [["War Horse (film)-10", "War Horse (film)-5"]], [["Saving Private Ryan-1"]], [["Saving Private Ryan-6"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the character in the movie \"The Hunger Games\" based on the real-life character of Katniss Everdeen?\nFacts: The name of the character in the movie \"The Hunger Games\" is based on the real", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Steven Spielberg's 1998 film take place in a period after War Horse setting?\nFacts: Steven Spielberg's 1998 film, Saving Private Ryan, takes place during World War II. World War II took place from 1939-1945. The 2011 movie War Horse takes place during World War I. World War I took place from 1914-1918.\nAnswer (True or False):", "ct_subject": "Steven Spielberg's 1998 film, Saving Private Ryan, takes place during World War II. World War II took place from 1939-1945. The 2011 movie War Horse takes place during World War I. World War I took place from 1914-1918.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.5779379"], [" the", "0.99998975"], [" name", "0.93414694"], [" of", "0.9999993"], [" the", "1.0"], [" character", "1.0"], [" in", "1.0"], [" the", "1.0"], [" movie", "0.9999988"], [" \"", "0.9416185"], ["The", "1.0"], [" Hunger", "0.9756547"], [" Games", "1.0"], ["\"", "1.0"], [" based", "0.9579887"], [" on", "1.0"], [" the", "0.6458148"], [" real", "1.0"], ["-", "0.9470913"], ["life", "1.0"], [" character", "1.0"], [" of", "0.66509634"], [" Kat", "0.999974"], ["n", "1.0"], ["iss", "1.0"], [" Ever", "1.0"], ["deen", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9892837"], [" name", "0.99999976"], [" of", "1.0"], [" the", "1.0"], [" character", "1.0"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "1.0"], ["The", "1.0"], [" Hunger", "1.0"], [" Games", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" based", "1.0"], [" on", "1.0"], [" the", "1.0"], [" real", "1.0"]]}, "123": {"qid": "cfe1c436a761f01041dd", "term": "Plum", "description": "subgenus of plants", "question": "Is November a bad time for a photographer to take pictures of a plum tree in bloom?", "answer": true, "facts": ["A plum tree is a deciduous tree that bears fruit.", "Deciduous trees shed their leaves in the autumn.", "Autumn happens from September until the end of Deember."], "decomposition": ["What kind of tree is a plum tree?", "What season will it be in temperate regions by November?", "What do #1 do during #2", "Considering #3, will a plum tree have an unattractive appearance at that time?"], "evidence": [[[["Plum-7"]], [["Winter-10"]], [["Plum-7"]], [["Plum-7"]]], [[["Plum-5"], "no_evidence"], [["Season-3"]], [["Plum-5"], "no_evidence"], [["Plum-5", "Plum-7"]]], [[["Prunus-1"]], [["November-2"]], [["Deciduous-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in China that is the capital of the Qing Dynasty?\nFacts: The name of the city is Nanjing.\nAnswer (True or False): True\nQuestion: What is the name of the city in China that is the capital of the Qing Dynasty?\nFacts", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is November a bad time for a photographer to take pictures of a plum tree in bloom?\nFacts: A plum tree is a deciduous tree that bears fruit. Deciduous trees shed their leaves in the autumn. Autumn happens from September until the end of Deember.\nAnswer (True or False):", "ct_subject": "A plum tree is a deciduous tree that bears fruit. Deciduous trees shed their leaves in the autumn. Autumn happens from September until the end of Deember.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.63292825"], [" is", "1.0"], [" the", "1.0"], [" name", "0.8361539"], [" of", "1.0"], [" the", "1.0"], [" city", "0.97745925"], [" in", "0.99898213"], [" China", "0.846312"], [" that", "0.9999999"], [" is", "0.9999652"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Qing", "0.9822869"], [" Dynasty", "0.9933294"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999999"], [" name", "0.99919456"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" is", "0.46337596"], [" Nan", "1.0"], ["jing", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" China", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Qing", "1.0"], [" Dynasty", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"]]}, "124": {"qid": "ab44cb3353f36e75ef24", "term": "Martyr", "description": "person who suffers persecution and death for advocating, refusing to renounce, and/or refusing to advocate a belief or cause, usually a religious one", "question": "Would Jason Voorhees hypothetically fail at being a martyr?", "answer": true, "facts": ["A martyr is someone that is killed for their beliefs.", "Jason Voorhees is the horror maniac from the Friday the 13th movies.", "Jason Voorhees is immortal and cannot be killed.", "Characters in Friday the 13th thought that dismembering Jason Voorhees would kill him but Jason even survived dismemberment."], "decomposition": ["What experience must one pass through in order to be considered a matyr?", "Can Jason Voorhes be killed?", "Would #2 being negative make Jason Voorhees unable to undergo #1?"], "evidence": [[[["Martyr-1"]], [["Jason Voorhees-40"]], ["operation"]], [[["Martyr-1"]], [["Jason Voorhees-40"]], ["operation"]], [[["Martyr-1"]], [["Jason Voorhees-40"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"soul\" and a \"spirit\"?\nFacts: A soul is a part of a person that is not a physical body. A", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Jason Voorhees hypothetically fail at being a martyr?\nFacts: A martyr is someone that is killed for their beliefs. Jason Voorhees is the horror maniac from the Friday the 13th movies. Jason Voorhees is immortal and cannot be killed. Characters in Friday the 13th thought that dismembering Jason Voorhees would kill him but Jason even survived dismemberment.\nAnswer (True or False):", "ct_subject": "A martyr is someone that is killed for their beliefs. Jason Voorhees is the horror maniac from the Friday the 13th movies. Jason Voorhees is immortal and cannot be killed. Characters in Friday the 13th thought that dismembering Jason Voorhees would kill him but Jason even survived dismemberment.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99761367"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.30222028"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.99811435"], ["s", "0.89480335"], ["oul", "0.9992994"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["spirit", "0.9959716"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" soul", "1.0"], [" is", "1.0"], [" a", "0.99993753"], [" part", "0.9998216"], [" of", "1.0"], [" a", "0.9998977"], [" person", "1.0"], [" that", "0.9999938"], [" is", "1.0"], [" not", "0.99850595"], [" a", "0.9846102"], [" physical", "1.0"], [" body", "0.92994606"], [".", "1.0"], [" A", "1.0"]]}, "125": {"qid": "9e477711fe01970389d6", "term": "Alfred Nobel", "description": "Swedish chemist, innovator, and armaments manufacturer (1833\u20131896)", "question": "Did Alfred Nobel write a banned book?", "answer": true, "facts": ["Banned books are works which are not available for free access due to government restrictions.", "Alfred Nobel is the author of Nemesis, a prose tragedy in four acts about Beatrice Cenci", "The entire stock of Nemesis was destroyed immediately after Nobel's death except for three copies, being regarded as scandalous and blasphemous. "], "decomposition": ["What literary works did Alfred Nobel write?", "Have any of the works listed in #1 been banned?"], "evidence": [[[["Alfred Nobel-21"]], ["operation"]], [[["Alfred Nobel-21"]], [["Alfred Nobel-21"]]], [[["Alfred Nobel-21"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first man to be born in the year 2000?\nFacts: The first man to be born in the year 2000 was born in the year 2000.\nAnswer (True or False", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Alfred Nobel write a banned book?\nFacts: Banned books are works which are not available for free access due to government restrictions. Alfred Nobel is the author of Nemesis, a prose tragedy in four acts about Beatrice Cenci The entire stock of Nemesis was destroyed immediately after Nobel's death except for three copies, being regarded as scandalous and blasphemous. \nAnswer (True or False):", "ct_subject": "Banned books are works which are not available for free access due to government restrictions. Alfred Nobel is the author of Nemesis, a prose tragedy in four acts about Beatrice Cenci The entire stock of Nemesis was destroyed immediately after Nobel's death except for three copies, being regarded as scandalous and blasphemous. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9430778"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9998691"], [" of", "1.0"], [" the", "1.0"], [" first", "0.68439674"], [" man", "0.80399746"], [" to", "0.9999999"], [" be", "0.90184164"], [" born", "0.9999832"], [" in", "1.0"], [" the", "0.8500412"], [" year", "0.9952113"], [" 2000", "0.98945147"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "0.9999938"], [" man", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "1.0"], [" born", "0.9984919"], [" in", "0.9812866"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [".", "1.0"], ["\n", "0.9999999"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"]]}, "126": {"qid": "9224ee338a77834434c9", "term": "Kidney", "description": "internal organ in most animals, including vertebrates and some invertebrates", "question": "Can a quarter fit inside of a human kidney?", "answer": true, "facts": ["Kidney stones are hard mineral deposits that can form in the kidneys.", "The largest kidney stone ever recorded was 13 cm wide.", "The diameter of a quarter is 2.4 cm."], "decomposition": ["How big is the largest kidney stone ever recorded?", "How wide is a quarter?", "Is #1 larger than #2?"], "evidence": [[[["Kidney stone disease-46"], "no_evidence"], [["Quarter (United States coin)-1"]], ["operation"]], [["no_evidence"], [["Quarter (United States coin)-3"]], ["operation"]], [[["Kidney-1"]], [["Quarter (United States coin)-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the largest number of people that have ever lived?\nFacts: The largest number of people that have ever lived is 2.5 billion.\nAnswer (True or False): True\nQuestion: What is the largest number of people that have ever lived?\nFacts: The largest number of people that have ever lived is 2.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can a quarter fit inside of a human kidney?\nFacts: Kidney stones are hard mineral deposits that can form in the kidneys. The largest kidney stone ever recorded was 13 cm wide. The diameter of a quarter is 2.4 cm.\nAnswer (True or False):", "ct_subject": "Kidney stones are hard mineral deposits that can form in the kidneys. The largest kidney stone ever recorded was 13 cm wide. The diameter of a quarter is 2.4 cm.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99977857"], [" is", "1.0"], [" the", "1.0"], [" largest", "0.99996996"], [" number", "0.99934334"], [" of", "1.0"], [" people", "1.0"], [" that", "0.999995"], [" have", "0.9789708"], [" ever", "1.0"], [" lived", "1.0"], ["?", "0.83885306"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99965906"], [" largest", "1.0"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" that", "0.9999877"], [" have", "1.0"], [" ever", "1.0"], [" lived", "1.0"], [" is", "0.9999913"], [" 2", "0.9897301"], [".", "0.79717714"], ["5", "1.0"], [" billion", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" that", "1.0"], [" have", "1.0"], [" ever", "1.0"], [" lived", "1.0"], ["?", "0.77830195"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" largest", "1.0"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" that", "1.0"], [" have", "1.0"], [" ever", "1.0"], [" lived", "1.0"], [" is", "1.0"], [" 2", "1.0"], [".", "1.0"]]}, "127": {"qid": "f6cc5e1c7456dd41a094", "term": "Zorro", "description": "Fictional character", "question": "Are there multiple Disney Zorro?", "answer": true, "facts": ["Zorro is a man who is a famous fictional Spanish hero and crime fighter featured in a novel.", "Disney produced a 1957 TV show featuring Zorro the spanish crime fighter. ", "Zorro is spanish for \"fox\".", "\"El Zorro y El Sabueso\" is the spanish title for Disney's animated movie called \"The Fox and The Hound\"."], "decomposition": ["Which famous hero was featured in Disney's 1957 TV show?", "What does 'The Fox' in Disney's 'The Fox and the Hound' translate to in Spanish?", "Do #1 and #2 refer to Zorro?"], "evidence": [[[["Zorro-4"]], [["The Fox and the Hound-4", "Zorro-1"]], ["operation"]], [[["Zorro-4"]], [["Zorro (disambiguation)-1"]], [["Zorro-1"], "operation"]], [[["Zorro-4"]], [["The Fox and the Hound-1", "Zorro-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there more people in the world today than there were in the year 2000?\nFacts: There are more people in the world today than there were", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are there multiple Disney Zorro?\nFacts: Zorro is a man who is a famous fictional Spanish hero and crime fighter featured in a novel. Disney produced a 1957 TV show featuring Zorro the spanish crime fighter.  Zorro is spanish for \"fox\". \"El Zorro y El Sabueso\" is the spanish title for Disney's animated movie called \"The Fox and The Hound\".\nAnswer (True or False):", "ct_subject": "Zorro is a man who is a famous fictional Spanish hero and crime fighter featured in a novel. Disney produced a 1957 TV show featuring Zorro the spanish crime fighter.  Zorro is spanish for \"fox\". \"El Zorro y El Sabueso\" is the spanish title for Disney's animated movie called \"The Fox and The Hound\".", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.9999919"], [" there", "1.0"], [" more", "0.58095765"], [" people", "0.9999999"], [" in", "0.98622143"], [" the", "1.0"], [" world", "1.0"], [" today", "1.0"], [" than", "0.99999845"], [" there", "1.0"], [" were", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "0.946625"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "0.9963373"], [" are", "1.0"], [" more", "1.0"], [" people", "1.0"], [" in", "0.99995875"], [" the", "1.0"], [" world", "1.0"], [" today", "1.0"], [" than", "1.0"], [" there", "1.0"], [" were", "1.0"]]}, "128": {"qid": "f9d0edee44a2e73f9765", "term": "Lolcat", "description": "image combining a photograph of a cat with text intended to contribute humour", "question": "Could a hundred thousand lolcats fit on a first generation iPhone?", "answer": true, "facts": ["Lolcat memes are often in jpeg form", "The average size of a jpeg is 10 to 30 kb", "One gigabyte is 1000000 kb", "The first iPhone had 4GB to 16GB of storage "], "decomposition": ["What file formats are lolcats usually saved in?", "What is the typical size of #1?", "What was the storage size range of first generation iPhones?", "What is 100000 multiplied by #2 expressed in gigabytes?", "Is #4 less than or equal to the minimum value of #3?"], "evidence": [[[["Cats and the Internet-1", "Image macro-2"]], ["no_evidence"], [["IPhone (1st generation)-11", "IPhone (1st generation)-7"]], ["operation"], ["operation"]], [[["Lolcat-1"]], ["no_evidence"], [["IPhone (1st generation)-7"]], [["Gigabyte-1"], "operation"], ["operation"]], [[["Image macro-1", "Lolcat-1"]], ["no_evidence"], [["IPhone (1st generation)-10"]], ["no_evidence", "operation"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the largest number of people that have ever lived?\nFacts: The largest number of people that have ever lived is:\nAnswer (True or False): True\nQuestion: What is the largest number of people that have ever lived that are still alive today?\nFacts: The largest number of", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could a hundred thousand lolcats fit on a first generation iPhone?\nFacts: Lolcat memes are often in jpeg form The average size of a jpeg is 10 to 30 kb One gigabyte is 1000000 kb The first iPhone had 4GB to 16GB of storage \nAnswer (True or False):", "ct_subject": "Lolcat memes are often in jpeg form The average size of a jpeg is 10 to 30 kb One gigabyte is 1000000 kb The first iPhone had 4GB to 16GB of storage ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999356"], [" is", "1.0"], [" the", "1.0"], [" largest", "0.96390057"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" that", "0.99979085"], [" have", "0.9900755"], [" ever", "1.0"], [" lived", "1.0"], ["?", "0.99995816"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9729652"], [" largest", "0.99982625"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" that", "1.0"], [" have", "1.0"], [" ever", "1.0"], [" lived", "1.0"], [" is", "0.99999845"], [":", "0.13548751"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" that", "1.0"], [" have", "1.0"], [" ever", "1.0"], [" lived", "1.0"], [" that", "0.99999917"], [" are", "0.99904066"], [" still", "0.9618212"], [" alive", "1.0"], [" today", "0.9999999"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" largest", "1.0"], [" number", "1.0"], [" of", "1.0"]]}, "129": {"qid": "c5aabbea64966b4f2d42", "term": "Chevrolet Corvette", "description": "Sports car by the Chevrolet division of General Motors (GM)", "question": "Does selling a 2020 Chevrolet Corvette almost pay for a year at Columbia University?", "answer": true, "facts": ["The price of a 2020 Chevrolet Corvette is $58,900.", "Columbia University cost $59,430 during the 2018-2019 school year."], "decomposition": ["How much does a 2020 Chevrolet Corvette cost?", "How much does a year at Columbia University cost?", "Is #1 almost as much as #2?"], "evidence": [[[["Chevrolet Corvette-1"], "no_evidence"], [["Columbia University-28"]], ["no_evidence", "operation"]], [["no_evidence"], [["Columbia University-28"]], ["no_evidence", "operation"]], [["no_evidence"], [["Columbia University-28"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average age of a person who has been in the United States for the last 30 years?\nFacts: The average age of a person who has been in the United States for the last 30 years is 42.\nAnswer (True or False): True\nQuestion: What is the average age of a person who has been in the United States for", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does selling a 2020 Chevrolet Corvette almost pay for a year at Columbia University?\nFacts: The price of a 2020 Chevrolet Corvette is $58,900. Columbia University cost $59,430 during the 2018-2019 school year.\nAnswer (True or False):", "ct_subject": "The price of a 2020 Chevrolet Corvette is $58,900. Columbia University cost $59,430 during the 2018-2019 school year.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999896"], [" is", "1.0"], [" the", "1.0"], [" average", "0.9877505"], [" age", "0.9910862"], [" of", "1.0"], [" a", "1.0"], [" person", "0.695768"], [" who", "0.99999917"], [" has", "1.0"], [" been", "0.99995446"], [" in", "0.95912623"], [" the", "0.9993932"], [" United", "0.99999034"], [" States", "1.0"], [" for", "1.0"], [" the", "0.9862282"], [" last", "0.80539316"], [" 30", "0.69839996"], [" years", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" age", "1.0"], [" of", "1.0"], [" a", "1.0"], [" person", "1.0"], [" who", "1.0"], [" has", "1.0"], [" been", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" for", "1.0"], [" the", "1.0"], [" last", "1.0"], [" 30", "1.0"], [" years", "1.0"], [" is", "1.0"], [" 42", "0.9855187"], [".", "1.0"], ["\n", "0.99999976"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" age", "0.999987"], [" of", "1.0"], [" a", "1.0"], [" person", "1.0"], [" who", "1.0"], [" has", "1.0"], [" been", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" for", "1.0"]]}, "130": {"qid": "d3013f42b7ca0c0d8685", "term": "Suburb", "description": "Human settlement that is part of or near to a larger city", "question": "Does the book Revolutionary Road give a glimpse at life in a suburb?", "answer": true, "facts": ["The setting of the book is in Connecticut.", "The area of Connecticut where the book takes place is not in a major city.", "The book takes place in a suburb called Revolutionary Hill Estates."], "decomposition": ["What location is the setting of the book Revolutionary Road?", "Is #1 a suburb?"], "evidence": [[[["Revolutionary Road-1"]], [["Revolutionary Road-5"]]], [[["Revolutionary Road-4"]], ["operation"]], [[["Revolutionary Road-4"]], [["Connecticut-61"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the book that was written by the author of the book The Great Gatsby?\nFacts: The book is called The Great Gatsby.\nAnswer (True or False): True\nQuestion: What is the name of the book that was written by the author of the book The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does the book Revolutionary Road give a glimpse at life in a suburb?\nFacts: The setting of the book is in Connecticut. The area of Connecticut where the book takes place is not in a major city. The book takes place in a suburb called Revolutionary Hill Estates.\nAnswer (True or False):", "ct_subject": "The setting of the book is in Connecticut. The area of Connecticut where the book takes place is not in a major city. The book takes place in a suburb called Revolutionary Hill Estates.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999106"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999987"], [" of", "1.0"], [" the", "1.0"], [" book", "0.4881956"], [" that", "0.9999999"], [" was", "0.20072158"], [" written", "0.9999999"], [" by", "1.0"], [" the", "0.99999356"], [" author", "1.0"], [" of", "1.0"], [" the", "1.0"], [" book", "1.0"], [" The", "0.8869574"], [" Great", "0.7578224"], [" G", "1.0"], ["ats", "1.0"], ["by", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" book", "0.9999995"], [" is", "0.9999995"], [" called", "0.9999995"], [" The", "1.0"], [" Great", "1.0"], [" G", "1.0"], ["ats", "1.0"], ["by", "1.0"], [".", "0.9999999"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" book", "1.0"], [" that", "1.0"], [" was", "1.0"], [" written", "1.0"], [" by", "1.0"], [" the", "1.0"], [" author", "1.0"], [" of", "1.0"], [" the", "1.0"], [" book", "1.0"], [" The", "1.0"]]}, "131": {"qid": "a946324a5ad6d3528da9", "term": "Publishing", "description": "Process of production and dissemination of literature, music, or information", "question": "Does Buddy The Elf know anyone who works in publishing?", "answer": true, "facts": ["Buddy The Elf is a character from the movie Elf.", "Buddy The Elf's father works in a Manhattan publishing firm."], "decomposition": ["Which people are known to the movie character Buddy The Elf?", "Does any of #1 work in publishing?"], "evidence": [[[["Elf (film)-5", "Elf (film)-9"]], [["Elf (film)-6"], "no_evidence"]], [[["Elf (film)-10"]], ["operation"]], [[["Elf (film)-3"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the character in the movie Elf?\nFacts: The character in the movie Elf is named Buddy The Elf.\nAnswer (True or False): True\nQuestion: What is the name of the character in the movie Elf?\nFacts: The character in the movie Elf is named Buddy The Elf.\nAnswer (True or False): True\nQuestion: What is the name", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Buddy The Elf know anyone who works in publishing?\nFacts: Buddy The Elf is a character from the movie Elf. Buddy The Elf's father works in a Manhattan publishing firm.\nAnswer (True or False):", "ct_subject": "Buddy The Elf is a character from the movie Elf. Buddy The Elf's father works in a Manhattan publishing firm.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999416"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" character", "0.9999968"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" Elf", "0.99892443"], ["?", "0.9999999"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99995196"], [" character", "0.9994698"], [" in", "0.9999808"], [" the", "1.0"], [" movie", "1.0"], [" Elf", "1.0"], [" is", "1.0"], [" named", "1.0"], [" Buddy", "1.0"], [" The", "0.9974655"], [" Elf", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" character", "1.0"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" Elf", "0.99998176"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" character", "1.0"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" Elf", "1.0"], [" is", "1.0"], [" named", "1.0"], [" Buddy", "1.0"], [" The", "1.0"], [" Elf", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"]]}, "132": {"qid": "6a13b8cefb285c2b81cf", "term": "Mail carrier", "description": "employee of the post office or postal service, who delivers mail to residences and businesses", "question": "Is unanimously elected president's birthday a break for mail carriers?", "answer": true, "facts": ["The post office has several holidays including: New Year's Day, Washington's Birthday (President's Day), and Veterans Day.", "George Washington was the only US president elected unanimously."], "decomposition": ["Which US president was elected unanimously?", "When is #1's birthday?", "Is #2 a break or holiday for the post office?"], "evidence": [[[["1788\u201389 United States presidential election-6"]], [["George Washington-1"]], [["Washington's Birthday-1"]]], [[["George Washington-107"]], [["Washington's Birthday-1"]], [["Postal holiday-3"], "operation"]], [[["Living presidents of the United States-3"], "no_evidence"], [["Jimmy Carter-5"]], [["Public holidays in the United States-16"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city of New York City derived from the name of the city of New York?\nFacts: The name of the city of New York City is derived from the name of the city of New York.\nAnswer (True or False): True\nQuestion: Is the name of the city of New York City derived from", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is unanimously elected president's birthday a break for mail carriers?\nFacts: The post office has several holidays including: New Year's Day, Washington's Birthday (President's Day), and Veterans Day. George Washington was the only US president elected unanimously.\nAnswer (True or False):", "ct_subject": "The post office has several holidays including: New Year's Day, Washington's Birthday (President's Day), and Veterans Day. George Washington was the only US president elected unanimously.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9958014"], [" the", "0.9999584"], [" name", "0.9999671"], [" of", "1.0"], [" the", "1.0"], [" city", "0.978837"], [" of", "0.9999964"], [" New", "0.99967265"], [" York", "1.0"], [" City", "1.0"], [" derived", "0.9999361"], [" from", "1.0"], [" the", "1.0"], [" name", "0.9999832"], [" of", "1.0"], [" the", "0.9999968"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "0.99839133"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" City", "0.9999908"], [" is", "0.9999819"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [".", "1.0"], ["\n", "0.6128673"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" City", "1.0"], [" derived", "1.0"], [" from", "1.0"]]}, "133": {"qid": "1e97ab50309873ca1789", "term": "Bengal cat", "description": "Breed of cat", "question": "Could a Bengal cat hypothetically best Javier Sotomayor's record?", "answer": true, "facts": ["Javier Sotomayor is an athlete that holds the men's high jump world record of slightly over 8 feet.", "The average cat can jump from 7.5 to 9 feet.", "Bengal cats have powerful hind legs which make them jump higher than other breeds."], "decomposition": ["How high is Javier Sotomayor's world record high jump?", "Which breed of cat can jump the highest?", "If the average cat can jump up to 9 feet, then #2 can jump higher than what number?", "Is #3 greater than #1?"], "evidence": [[[["Javier Sotomayor-1"]], [["Bengal cat-21"], "no_evidence"], ["operation"], ["operation"]], [[["High jump-3"]], [["Savannah cat-1", "Savannah cat-21"]], ["operation"], ["operation"]], [[["Javier Sotomayor-1"]], [["Caracal-2"], "no_evidence"], [["Bengal cat-21"], "no_evidence", "operation"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance a cat can jump?\nFacts: The longest distance a cat can jump is about 20 feet.\nAnswer (True or False): True\nQuestion: What is the longest distance a cat can jump?\nFacts", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could a Bengal cat hypothetically best Javier Sotomayor's record?\nFacts: Javier Sotomayor is an athlete that holds the men's high jump world record of slightly over 8 feet. The average cat can jump from 7.5 to 9 feet. Bengal cats have powerful hind legs which make them jump higher than other breeds.\nAnswer (True or False):", "ct_subject": "Javier Sotomayor is an athlete that holds the men's high jump world record of slightly over 8 feet. The average cat can jump from 7.5 to 9 feet. Bengal cats have powerful hind legs which make them jump higher than other breeds.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9998734"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.9887554"], [" distance", "0.9999999"], [" a", "1.0"], [" cat", "0.6757299"], [" can", "0.9999454"], [" jump", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999999"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" cat", "1.0"], [" can", "1.0"], [" jump", "1.0"], [" is", "1.0"], [" about", "0.9999976"], [" 20", "0.9268583"], [" feet", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" cat", "1.0"], [" can", "1.0"], [" jump", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"]]}, "134": {"qid": "08db047c63681c02184f", "term": "Basil", "description": "species of plant", "question": "Would the chef at Carmine's restaurant panic if there was no basil?", "answer": true, "facts": ["Carmines is an Italian family-style restaurant.", "Basil is an essential in Italian cooking."], "decomposition": ["What kind of cuisine does Carmine's serve?", "Is basil an essential ingredient in #1?"], "evidence": [[[["Carmine Romano-2"], "no_evidence"], [["Italian cuisine-27"], "no_evidence", "operation"]], [["no_evidence"], ["no_evidence"]], [["no_evidence"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the capital of the state of New York?\nFacts: The city of New York is the capital of the state of New York.\nAnswer (True or False): True\nQuestion: What is the name of the city in the United States that is the capital of the state of New York?\nFacts: The city of New", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would the chef at Carmine's restaurant panic if there was no basil?\nFacts: Carmines is an Italian family-style restaurant. Basil is an essential in Italian cooking.\nAnswer (True or False):", "ct_subject": "Carmines is an Italian family-style restaurant. Basil is an essential in Italian cooking.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99985826"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9829847"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9986614"], [" in", "0.99999833"], [" the", "0.99375075"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "0.92869526"], [" the", "1.0"], [" capital", "0.98729914"], [" of", "1.0"], [" the", "1.0"], [" state", "0.99809927"], [" of", "1.0"], [" New", "0.99992836"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.3556525"], [" city", "0.9971468"], [" of", "0.99972874"], [" New", "1.0"], [" York", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "0.9999901"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" New", "0.99659073"], [" York", "0.9648717"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"]]}, "135": {"qid": "fcff106193bfeb59372e", "term": "Easy Rider", "description": "1969 film by Dennis Hopper", "question": "Did Easy Rider make a profit at the theater when it was released?", "answer": true, "facts": ["Easy Rider had a filming budget of about half a million dollars.", "Upon release in 1969, it earned about 60 million dollars."], "decomposition": ["What was the budget of Easy Rider?", "How much did Easy Rider earn upon its release?", "Is #2 greater than #1?"], "evidence": [[[["Easy Rider-13"]], [["Easy Rider-3"]], [["Easy Rider-3"], "operation"]], [[["Easy Rider-13"]], [["Easy Rider-32"]], ["operation"]], [[["Easy Rider-3"]], [["Easy Rider-3"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Beatles' song \"I Want to Hold Your Hand\" have a sexual meaning?\nFacts: The song was written by Paul McCartney and John Lennon. The song was written to be a love song.\nAnswer (True or False): True\nQuestion: Did the Beatles' song \"I Want to Hold Your Hand\" have a sexual meaning?\nFacts: The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Easy Rider make a profit at the theater when it was released?\nFacts: Easy Rider had a filming budget of about half a million dollars. Upon release in 1969, it earned about 60 million dollars.\nAnswer (True or False):", "ct_subject": "Easy Rider had a filming budget of about half a million dollars. Upon release in 1969, it earned about 60 million dollars.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.48623806"], [" the", "1.0"], [" Beatles", "1.0"], ["'", "0.93005306"], [" song", "0.6739429"], [" \"", "1.0"], ["I", "0.37476557"], [" Want", "0.99999905"], [" to", "1.0"], [" Hold", "1.0"], [" Your", "1.0"], [" Hand", "1.0"], ["\"", "1.0"], [" have", "0.9902203"], [" a", "1.0"], [" sexual", "0.9840333"], [" meaning", "0.99999976"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" song", "0.999967"], [" was", "1.0"], [" written", "1.0"], [" by", "0.99999964"], [" Paul", "0.99954623"], [" McCartney", "1.0"], [" and", "1.0"], [" John", "1.0"], [" Lennon", "1.0"], [".", "0.9999957"], [" The", "0.9999485"], [" song", "0.8600095"], [" was", "1.0"], [" written", "0.9999995"], [" to", "0.9996176"], [" be", "0.9958768"], [" a", "1.0"], [" love", "0.99999607"], [" song", "1.0"], [".", "0.99230874"], ["\n", "0.8294035"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" Beatles", "1.0"], ["'", "1.0"], [" song", "1.0"], [" \"", "1.0"], ["I", "1.0"], [" Want", "1.0"], [" to", "1.0"], [" Hold", "1.0"], [" Your", "1.0"], [" Hand", "1.0"], ["\"", "1.0"], [" have", "1.0"], [" a", "1.0"], [" sexual", "1.0"], [" meaning", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"]]}, "136": {"qid": "52c712384f56ec6347ad", "term": "Mail carrier", "description": "employee of the post office or postal service, who delivers mail to residences and businesses", "question": "Do mail carriers need multiple uniforms?", "answer": true, "facts": ["Mail carriers work throughout the year independent of the weather.", "Mail carriers must often leave their vehicle in various weather conditions."], "decomposition": ["What seasons do mail carriers work through?", "In order to make it through all of #1, does one need different clothing pieces?"], "evidence": [[[["United States Postal Service-145"], "no_evidence"], [["Clothing-2"]]], [[["United States Postal Service creed-1"], "no_evidence"], [["Winter clothing-2"], "operation"]], [[["Season-1"], "no_evidence"], [["Mail carrier-8"], "no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake with a hole in it\"?\nFacts: A snowflake is a single snowflake. A snowflake with a hole in it is a single snowflake with a hole in it.\nAnswer (True or False): True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake with a hole", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do mail carriers need multiple uniforms?\nFacts: Mail carriers work throughout the year independent of the weather. Mail carriers must often leave their vehicle in various weather conditions.\nAnswer (True or False):", "ct_subject": "Mail carriers work throughout the year independent of the weather. Mail carriers must often leave their vehicle in various weather conditions.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99998033"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9841777"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.82433575"], ["now", "1.0"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], [" with", "0.44564417"], [" a", "1.0"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999949"], [" snow", "1.0"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "0.9976336"], [" snow", "0.99550146"], ["flake", "1.0"], [".", "0.96202236"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" with", "1.0"], [" a", "1.0"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "0.9998087"], [" snow", "1.0"], ["flake", "1.0"], [" with", "1.0"], [" a", "1.0"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], [" with", "1.0"], [" a", "1.0"], [" hole", "1.0"]]}, "137": {"qid": "382571a1ab13a99b1f74", "term": "Shrimp", "description": "Decapod crustaceans", "question": "Is Steve Martin someone who would refuse a dish of shrimp pasta?", "answer": true, "facts": ["Steve Martin is allergic to shellfish.", "Shrimp are one of the many types of shellfish."], "decomposition": ["What types of food is Steve Martin allergic to?", "What type of food is shrimp?", "Is #2 included in #1?"], "evidence": [[["no_evidence"], [["Shrimp-36"]], ["no_evidence", "operation"]], [[["Steve Martin-1"], "no_evidence"], [["Shrimp-1", "Shrimp-34"]], ["no_evidence", "operation"]], [["no_evidence"], [["Shrimp-16"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city of New York City the same as the name of the city of New York?\nFacts: The name of the city of New York is New York. The name of the city of New York is New York.\nAnswer (True or False): True\nQuestion: Is the name of the city of New York City the same as the name of the city of New York?", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Steve Martin someone who would refuse a dish of shrimp pasta?\nFacts: Steve Martin is allergic to shellfish. Shrimp are one of the many types of shellfish.\nAnswer (True or False):", "ct_subject": "Steve Martin is allergic to shellfish. Shrimp are one of the many types of shellfish.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99988663"], [" the", "0.9954188"], [" name", "0.8912243"], [" of", "0.99999774"], [" the", "1.0"], [" city", "0.9999993"], [" of", "1.0"], [" New", "0.9926156"], [" York", "1.0"], [" City", "1.0"], [" the", "0.19787611"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "0.9959692"], [" York", "1.0"], ["?", "0.9999999"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9989844"], [" name", "0.99999976"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" is", "1.0"], [" New", "1.0"], [" York", "1.0"], [".", "0.9999964"], [" The", "0.9937355"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" is", "0.99999976"], [" New", "1.0"], [" York", "1.0"], [".", "0.96808374"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" City", "1.0"], [" the", "1.0"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], ["?", "1.0"]]}, "138": {"qid": "a3759d8aff52a52c7155", "term": "Common Era", "description": "alternative (and religiously neutral) naming of the traditional calendar era, Anno Domini", "question": "Would Methuselah hypothetically hold a record in the Common Era?", "answer": true, "facts": ["Methuselah was a biblical figure said to have lived until 969.", "The Common Era is the years after the BC era and is alternatively referred to as A.D.", "Jeanne Louise Calment 1875\u20131997 was the oldest human whose age was well-documented, with a lifespan of 122 years and 164 days."], "decomposition": ["Which period is referred to as the Common Era?", "Who is the oldest human whose age was well documented during #1?", "How old was #2 when she died?", "How old was Methuselah when he died?", "Is #4 greater than #3?"], "evidence": [[[["Common Era-1"]], ["no_evidence"], ["no_evidence"], [["Methuselah-1"]], ["operation"]], [[["Common Era-1"]], [["Oldest people-2"]], [["Oldest people-2"]], [["Methuselah-1"]], ["operation"]], [[["Common Era-1"]], [["Oldest people-2"]], [["Oldest people-2"]], [["Methuselah-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance between two points on Earth?\nFacts: The distance between the Earth and the Sun is about 93 million miles. The distance between the Earth and the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Methuselah hypothetically hold a record in the Common Era?\nFacts: Methuselah was a biblical figure said to have lived until 969. The Common Era is the years after the BC era and is alternatively referred to as A.D. Jeanne Louise Calment 1875\u20131997 was the oldest human whose age was well-documented, with a lifespan of 122 years and 164 days.\nAnswer (True or False):", "ct_subject": "Methuselah was a biblical figure said to have lived until 969. The Common Era is the years after the BC era and is alternatively referred to as A.D. Jeanne Louise Calment 1875\u20131997 was the oldest human whose age was well-documented, with a lifespan of 122 years and 164 days.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999774"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.95074"], [" distance", "0.79336315"], [" between", "0.9676473"], [" two", "1.0"], [" points", "1.0"], [" on", "0.9999894"], [" Earth", "0.99994385"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" distance", "0.99147415"], [" between", "1.0"], [" the", "1.0"], [" Earth", "1.0"], [" and", "1.0"], [" the", "1.0"], [" Sun", "0.9998864"], [" is", "1.0"], [" about", "1.0"], [" 93", "1.0"], [" million", "1.0"], [" miles", "1.0"], [".", "0.9999995"], [" The", "0.9999994"], [" distance", "1.0"], [" between", "1.0"], [" the", "1.0"], [" Earth", "1.0"], [" and", "1.0"], [" the", "0.9996922"]]}, "139": {"qid": "083292799a2e3d07abbf", "term": "Stork", "description": "family of birds", "question": "Would you be likely to see storks at a baby shower?", "answer": true, "facts": ["Storks are a symbol of the arrival of a new baby.", "Baby showers are parties held to celebrate a woman who will soon give birth."], "decomposition": ["What do storks represent?", "Do baby showers celebrate #1?"], "evidence": [[[["White stork-46"]], [["Baby shower-1"]]], [[["White stork-46"]], [["Baby shower-1"]]], [[["White stork-46"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in China that is the largest in the world?\nFacts: The largest city in the world is Beijing.\nAnswer (True or False): True\nQuestion: What is the name of the city in China that is the largest in the world?\nFacts: The largest city in the world is Beijing.\nAnswer (True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would you be likely to see storks at a baby shower?\nFacts: Storks are a symbol of the arrival of a new baby. Baby showers are parties held to celebrate a woman who will soon give birth.\nAnswer (True or False):", "ct_subject": "Storks are a symbol of the arrival of a new baby. Baby showers are parties held to celebrate a woman who will soon give birth.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9097847"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9979644"], [" in", "0.9998216"], [" China", "0.4802692"], [" that", "1.0"], [" is", "0.99889046"], [" the", "1.0"], [" largest", "0.8253054"], [" in", "0.99975365"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9991615"], [" largest", "0.7097045"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" is", "1.0"], [" Beijing", "0.99999976"], [".", "0.9999994"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" China", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "0.9999285"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" largest", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" is", "1.0"], [" Beijing", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"]]}, "140": {"qid": "4013c96b17809c27699c", "term": "National Diet", "description": "legislature of Japan", "question": "Can Viper Room concert hypothetically be held at National Diet building?", "answer": true, "facts": ["The Viper Room has a capacity of 250 people.", "The National Diet building has two wings with over 700 seats."], "decomposition": ["What is the capacity of the The Viper Room?", "What is the capacity of the National Diet Building?", "Is #2 greater than or equal to #1?"], "evidence": [[[["The Viper Room-1"], "no_evidence"], [["National Diet Building-28"]], ["operation"]], [[["The Viper Room-1"], "no_evidence"], [["National Diet Building-11"], "no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], [["National Diet Building-28"]], [["National Diet Building-28"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the song that plays during the opening credits of the movie \"The Godfather\"?\nFacts: \"The Godfather\" is a film about the life of the Italian mobster, Vito Corleone.\nAnswer (True or False): True\nQuestion: What is the name of the song that plays during the opening credits of the movie \"The Godfather\"?\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can Viper Room concert hypothetically be held at National Diet building?\nFacts: The Viper Room has a capacity of 250 people. The National Diet building has two wings with over 700 seats.\nAnswer (True or False):", "ct_subject": "The Viper Room has a capacity of 250 people. The National Diet building has two wings with over 700 seats.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.975132"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999895"], [" of", "1.0"], [" the", "1.0"], [" song", "0.22583528"], [" that", "0.61043894"], [" plays", "0.90105504"], [" during", "0.76995873"], [" the", "1.0"], [" opening", "0.99282926"], [" credits", "1.0"], [" of", "0.99999976"], [" the", "1.0"], [" movie", "0.9976428"], [" \"", "1.0"], ["The", "1.0"], [" God", "0.91175854"], ["father", "1.0"], ["\"?", "0.99999917"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" \"", "0.9999993"], ["The", "1.0"], [" God", "1.0"], ["father", "1.0"], ["\"", "0.9999889"], [" is", "1.0"], [" a", "0.99999976"], [" film", "0.97481185"], [" about", "0.99996436"], [" the", "0.99997604"], [" life", "1.0"], [" of", "1.0"], [" the", "0.99999857"], [" Italian", "0.9972357"], [" mob", "0.0872163"], ["ster", "0.7011666"], [",", "1.0"], [" V", "0.9997528"], ["ito", "1.0"], [" Cor", "1.0"], ["le", "1.0"], ["one", "1.0"], [".", "1.0"], ["\n", "0.9999906"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" song", "1.0"], [" that", "1.0"], [" plays", "1.0"], [" during", "1.0"], [" the", "1.0"], [" opening", "1.0"], [" credits", "1.0"], [" of", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "1.0"], ["The", "1.0"], [" God", "1.0"], ["father", "1.0"], ["\"?", "1.0"], ["\n", "1.0"]]}, "141": {"qid": "6274bd508f7fbca9332f", "term": "Fever", "description": "common medical sign characterized by elevated body temperature", "question": "Can you get a fever from consuming meat?", "answer": true, "facts": ["A fever is a medical symptom that elevates the core body temperature. ", "Eating under cooked meat can cause food poisoning.", "One of the symptoms of food poisoning is elevated core body temperature. "], "decomposition": ["What is a fever?", "What can consuming uncooked meat cause?", "Is #1 a symptom of #2?"], "evidence": [[[["Fever-1"]], [["Raw meat-4"]], [["Fever-1"]]], [[["Fever-1"]], [["Trichinosis-1", "Trichinosis-2"], "no_evidence"], ["no_evidence", "operation"]], [[["Fever-1"]], [["Raw meat-4"]], [["Listeria-14"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"sick\" and a \"healthy\" person?\nFacts: A sick person is one who is not well.  A healthy person is one who is well. \nAnswer (True or False): True\nQuestion: What is the difference between a \"sick\" and a \"healthy\" person", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can you get a fever from consuming meat?\nFacts: A fever is a medical symptom that elevates the core body temperature.  Eating under cooked meat can cause food poisoning. One of the symptoms of food poisoning is elevated core body temperature. \nAnswer (True or False):", "ct_subject": "A fever is a medical symptom that elevates the core body temperature.  Eating under cooked meat can cause food poisoning. One of the symptoms of food poisoning is elevated core body temperature. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.77211195"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9999722"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.9766964"], ["s", "0.99760735"], ["ick", "0.7742665"], ["\"", "1.0"], [" and", "0.9999974"], [" a", "0.99999094"], [" \"", "1.0"], ["healthy", "0.9978879"], ["\"", "1.0"], [" person", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" sick", "0.998602"], [" person", "1.0"], [" is", "1.0"], [" one", "0.99999857"], [" who", "0.9999511"], [" is", "0.9999987"], [" not", "0.6596799"], [" well", "0.99999034"], [".", "1.0"], [" ", "0.5733799"], [" A", "0.9968501"], [" healthy", "1.0"], [" person", "1.0"], [" is", "1.0"], [" one", "1.0"], [" who", "1.0"], [" is", "1.0"], [" well", "1.0"], [".", "1.0"], [" ", "0.9999659"], ["\n", "0.9655429"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["ick", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["healthy", "1.0"], ["\"", "1.0"], [" person", "1.0"]]}, "142": {"qid": "af64e22f2e17583b79cf", "term": "Byzantine calendar", "description": "The calendar used by the Eastern Orthodox Church from c. 691 to 1728", "question": "Did Ivan the Terrible use the Byzantine calendar?", "answer": true, "facts": ["Ivan the Terrible was the Tsar of Russia from 1530 to 1585. ", "The Byzantine calendar was the official calendar of the Russian government from 988 to 1700.", "The Tsar was the leader of the Russian government. "], "decomposition": ["What was Ivan the Terrible's role from 1530 to 1585?", "What country was Ivan the Terrible #1 of?", "Was the Byzantine calendar the official calendar of #2 from 1530 to 1585?"], "evidence": [[[["Ivan the Terrible-1"]], [["Ivan the Terrible-1"]], [["Byzantine calendar-1"], "operation"]], [[["Ivan the Terrible-1"]], [["Ivan the Terrible-1"]], [["Byzantine calendar-1"]]], [[["Ivan the Terrible-1"], "no_evidence"], [["Ivan the Terrible-2"]], [["Byzantine calendar-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Roman Catholic Church use the Julian calendar?\nFacts: The Roman Catholic Church used the Julian calendar from the year 1582 to the year 1582. \nAnswer (True or False): True\nQuestion: Did the Roman Catholic Church use the Gregorian calendar?\nFacts", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Ivan the Terrible use the Byzantine calendar?\nFacts: Ivan the Terrible was the Tsar of Russia from 1530 to 1585.  The Byzantine calendar was the official calendar of the Russian government from 988 to 1700. The Tsar was the leader of the Russian government. \nAnswer (True or False):", "ct_subject": "Ivan the Terrible was the Tsar of Russia from 1530 to 1585.  The Byzantine calendar was the official calendar of the Russian government from 988 to 1700. The Tsar was the leader of the Russian government. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99999964"], [" the", "1.0"], [" Roman", "0.9999999"], [" Catholic", "0.9726742"], [" Church", "1.0"], [" use", "1.0"], [" the", "1.0"], [" Julian", "0.99999964"], [" calendar", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Roman", "0.9999523"], [" Catholic", "1.0"], [" Church", "1.0"], [" used", "0.9999795"], [" the", "1.0"], [" Julian", "1.0"], [" calendar", "1.0"], [" from", "0.99992776"], [" the", "0.99859816"], [" year", "1.0"], [" 15", "1.0"], ["82", "1.0"], [" to", "0.9604882"], [" the", "1.0"], [" year", "1.0"], [" 15", "1.0"], ["82", "1.0"], [".", "1.0"], [" ", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" Catholic", "1.0"], [" Church", "1.0"], [" use", "1.0"], [" the", "1.0"], [" Greg", "1.0"], ["orian", "1.0"], [" calendar", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"]]}, "143": {"qid": "068379affae04debca2b", "term": "Nepalese Civil War", "description": "civil war in Nepal between 1996 and 2006", "question": "Did the Nepalese Civil War take place near India?", "answer": true, "facts": ["The Nepalese Civil War happened in Nepal.", "Nepal is a country that shares a border with India."], "decomposition": ["Where did the Nepalese Civil War take place?", "Is #1 near India?"], "evidence": [[[["Nepalese Civil War-1"]], [["Nepal-1"], "operation"]], [[["Nepalese Civil War-1"]], [["Nepal-1"]]], [[["Nepalese Civil War-1"]], [["Nepal-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Roman Empire conquer the world?\nFacts: The Roman Empire conquered the world.\nAnswer (True or False): True\nQuestion: Did the Roman Empire conquer the world?\nFacts: The Roman Empire conquered the world.\nAnswer (True or False): True\nQuestion: Did the Roman Empire conquer the world?\nFacts: The Roman Empire conquered the world.\nAnswer (True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did the Nepalese Civil War take place near India?\nFacts: The Nepalese Civil War happened in Nepal. Nepal is a country that shares a border with India.\nAnswer (True or False):", "ct_subject": "The Nepalese Civil War happened in Nepal. Nepal is a country that shares a border with India.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99445254"], [" the", "1.0"], [" Roman", "0.89371794"], [" Empire", "1.0"], [" conquer", "0.9999553"], [" the", "1.0"], [" world", "0.6431853"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [" conquered", "1.0"], [" the", "1.0"], [" world", "1.0"], [".", "1.0"], ["\n", "0.9936952"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999273"], [" the", "1.0"], [" Roman", "0.99999714"], [" Empire", "1.0"], [" conquer", "0.99999964"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [" conquered", "1.0"], [" the", "1.0"], [" world", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [" conquer", "1.0"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [" conquered", "1.0"], [" the", "1.0"], [" world", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"]]}, "144": {"qid": "605a31607ccb00fc8537", "term": "Byzantine calendar", "description": "The calendar used by the Eastern Orthodox Church from c. 691 to 1728", "question": "Did the Eastern Orthodox Church and the Byzantine Empire ever use the same calendar?", "answer": true, "facts": ["Eastern Orthodox Church used the Byzantine calendar from c. 691 to 1728", "The Byzantine Empire used the Byzantine calendar from c. 988 to 1453"], "decomposition": ["What calendar did the Eastern Orthodox Church use from  c. 691 to 1728?", "What calendar did the Byzantine Empire use from c. 988 to 1453?", "Is #1 and #2 the same?"], "evidence": [[[["Eastern Orthodox Church-77"]], [["Byzantine calendar-8"]], ["operation"]], [[["Julian calendar-82"], "no_evidence"], [["Julian calendar-69"], "operation"], ["operation"]], [[["Eastern Orthodox Church-207"], "no_evidence"], [["Byzantine calendar-1"], "operation"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Eastern Orthodox Church and the Byzantine Empire ever use the same calendar?\nFacts: Eastern Orthodox Church used the Byzantine calendar from c. 691 to 1728 The Byzantine Empire used the Byzantine calendar from c. 988 to 1453\nAnswer (True or False): True\nQuestion: Did the Eastern Orthodox Church and the Byzantine Empire ever use", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did the Eastern Orthodox Church and the Byzantine Empire ever use the same calendar?\nFacts: Eastern Orthodox Church used the Byzantine calendar from c. 691 to 1728 The Byzantine Empire used the Byzantine calendar from c. 988 to 1453\nAnswer (True or False):", "ct_subject": "Eastern Orthodox Church used the Byzantine calendar from c. 691 to 1728 The Byzantine Empire used the Byzantine calendar from c. 988 to 1453", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.1152306"], [" the", "1.0"], [" Eastern", "0.9999975"], [" Orthodox", "1.0"], [" Church", "1.0"], [" and", "0.9999999"], [" the", "1.0"], [" Byzantine", "1.0"], [" Empire", "1.0"], [" ever", "1.0"], [" use", "1.0"], [" the", "1.0"], [" same", "1.0"], [" calendar", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Eastern", "0.993666"], [" Orthodox", "1.0"], [" Church", "1.0"], [" used", "1.0"], [" the", "1.0"], [" Byzantine", "1.0"], [" calendar", "1.0"], [" from", "1.0"], [" c", "1.0"], [".", "1.0"], [" 6", "1.0"], ["91", "1.0"], [" to", "1.0"], [" 17", "1.0"], ["28", "1.0"], [" The", "1.0"], [" Byzantine", "1.0"], [" Empire", "1.0"], [" used", "1.0"], [" the", "1.0"], [" Byzantine", "1.0"], [" calendar", "1.0"], [" from", "1.0"], [" c", "1.0"], [".", "1.0"], [" 9", "1.0"], ["88", "1.0"], [" to", "1.0"], [" 14", "1.0"], ["53", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999999"], [" the", "1.0"], [" Eastern", "1.0"], [" Orthodox", "1.0"], [" Church", "1.0"], [" and", "1.0"], [" the", "1.0"], [" Byzantine", "1.0"], [" Empire", "1.0"], [" ever", "1.0"], [" use", "1.0"]]}, "145": {"qid": "66d56f7e42649a3329ad", "term": "Agriculture", "description": "Cultivation of plants and animals to provide useful products", "question": "Is the rise of agriculture attributed to rivers?", "answer": true, "facts": ["Some of the earliest civilizations on record are in river valleys.", "Early civilizations used water to irrigate crops, leading to the growth of agriculture."], "decomposition": ["What are the basic factors of agricultural production?", "Do rivers provide any of #1?"], "evidence": [[[["Irrigation-45"]], [["Irrigation-45"]]], [[["Agriculture-1"]], [["Agriculture-7"], "operation"]], [[["Agriculture-7"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the rise of agriculture attributed to the rise of cities?\nFacts: The rise of cities is attributed to the rise of agriculture.\nAnswer (True or False): True\nQuestion: Is the rise of agriculture attributed to the rise of cities?\nFacts: The rise of cities is attributed to the rise of agriculture.\nAnswer (True or False): True\nQuestion: Is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is the rise of agriculture attributed to rivers?\nFacts: Some of the earliest civilizations on record are in river valleys. Early civilizations used water to irrigate crops, leading to the growth of agriculture.\nAnswer (True or False):", "ct_subject": "Some of the earliest civilizations on record are in river valleys. Early civilizations used water to irrigate crops, leading to the growth of agriculture.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9968791"], [" the", "1.0"], [" rise", "1.0"], [" of", "1.0"], [" agriculture", "0.9995864"], [" attributed", "1.0"], [" to", "1.0"], [" the", "0.9999974"], [" rise", "0.99999154"], [" of", "1.0"], [" cities", "0.9970361"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.89770854"], [" rise", "1.0"], [" of", "1.0"], [" cities", "1.0"], [" is", "0.9987525"], [" attributed", "0.8965031"], [" to", "1.0"], [" the", "1.0"], [" rise", "1.0"], [" of", "1.0"], [" agriculture", "0.9959317"], [".", "1.0"], ["\n", "0.9991008"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999999"], [" the", "1.0"], [" rise", "1.0"], [" of", "1.0"], [" agriculture", "0.85749614"], [" attributed", "1.0"], [" to", "1.0"], [" the", "1.0"], [" rise", "1.0"], [" of", "1.0"], [" cities", "0.99406296"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" rise", "1.0"], [" of", "1.0"], [" cities", "1.0"], [" is", "1.0"], [" attributed", "1.0"], [" to", "1.0"], [" the", "1.0"], [" rise", "1.0"], [" of", "1.0"], [" agriculture", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"]]}, "146": {"qid": "1feaaac968a53eac2a67", "term": "Goldfish", "description": "freshwater fish, common in aquariums", "question": "Are goldfish more difficult to care for than isopods?", "answer": true, "facts": ["Isopod care is compared to that of a houseplant.", "Goldfish are notorious for making their tanks dirty quite often."], "decomposition": ["What is isopod care comparable to?", "What challenges do Goldfish pose to keepers?", "Is #1 easier to deal with than #2?"], "evidence": [[[["Isopoda-14", "Isopoda-15"]], [["Goldfish-28"]], ["operation"]], [[["Isopoda-1", "Isopoda-13"]], [["Goldfish-15"], "no_evidence"], ["operation"]], [[["Isopoda-1"]], [["Goldfish-17", "Goldfish-21", "Goldfish-22"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it true that the first person to use the word \"candy\" was a man named John Wilkes Booth?\nFacts: Yes, John Wilkes Booth was the first person to use the word \"candy\" in a political speech.\nAnswer (True or False): True\nQuestion: Is it true that the first person to use the word \"candy\" was", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are goldfish more difficult to care for than isopods?\nFacts: Isopod care is compared to that of a houseplant. Goldfish are notorious for making their tanks dirty quite often.\nAnswer (True or False):", "ct_subject": "Isopod care is compared to that of a houseplant. Goldfish are notorious for making their tanks dirty quite often.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.93556345"], [" it", "0.9704216"], [" true", "1.0"], [" that", "1.0"], [" the", "1.0"], [" first", "0.99990785"], [" person", "0.98437655"], [" to", "1.0"], [" use", "0.9999275"], [" the", "0.99999106"], [" word", "1.0"], [" \"", "1.0"], ["c", "0.9998273"], ["andy", "0.9983845"], ["\"", "1.0"], [" was", "1.0"], [" a", "1.0"], [" man", "0.9660949"], [" named", "1.0"], [" John", "0.99997056"], [" Wil", "0.45244578"], ["kes", "1.0"], [" Booth", "0.9990245"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Yes", "0.998517"], [",", "0.6303828"], [" John", "0.9969463"], [" Wil", "1.0"], ["kes", "1.0"], [" Booth", "1.0"], [" was", "1.0"], [" the", "1.0"], [" first", "0.99999964"], [" person", "1.0"], [" to", "1.0"], [" use", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"", "0.9729092"], [" in", "1.0"], [" a", "0.59188175"], [" political", "0.99988866"], [" speech", "0.99999976"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" true", "1.0"], [" that", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" use", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"", "1.0"], [" was", "0.9999993"]]}, "147": {"qid": "58be99559ef0ccdbb36c", "term": "Meatball", "description": "dish made from ground meat rolled into a small ball-like form", "question": "Do restaurants associate meatballs with the wrong country of origin?", "answer": true, "facts": ["Spaghetti and meatballs are a staple on Italian pizzeria menus in the US.", "The Olive Garden, an Italian family restaurant, has several dishes with meatballs.", "Meatballs originated in the Chinese Qin dynasty (221 BC to 207 BC)."], "decomposition": ["In what country is the oldest evidence of people eating meatballs found?", "What dish involving meatballs became popular in the United States after being invented in New York City in the 20th century?", "With which national cuisine do Americans typically associate #2?", "Are #3 and #1 different?"], "evidence": [[[["Meatball-2"]], [["Spaghetti and meatballs-2"]], [["Spaghetti and meatballs-2"]], [["Meatball-2", "Spaghetti and meatballs-2"], "operation"]], [[["Meatball-2"]], [["Meatball-8"]], [["Spaghetti and meatballs-3"]], ["operation"]], [[["Meatball-2"]], [["Spaghetti and meatballs-2"]], [["Spaghetti and meatballs-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the most popular type of beer in the US?\nFacts: Budweiser is the most popular beer in the US. It is brewed by Anheuser-Busch InBev.\nAnswer (True or False): True\nQuestion: What is the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do restaurants associate meatballs with the wrong country of origin?\nFacts: Spaghetti and meatballs are a staple on Italian pizzeria menus in the US. The Olive Garden, an Italian family restaurant, has several dishes with meatballs. Meatballs originated in the Chinese Qin dynasty (221 BC to 207 BC).\nAnswer (True or False):", "ct_subject": "Spaghetti and meatballs are a staple on Italian pizzeria menus in the US. The Olive Garden, an Italian family restaurant, has several dishes with meatballs. Meatballs originated in the Chinese Qin dynasty (221 BC to 207 BC).", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.94457424"], [" is", "1.0"], [" the", "1.0"], [" most", "0.9873685"], [" popular", "0.97380644"], [" type", "0.99779284"], [" of", "1.0"], [" beer", "0.99926394"], [" in", "1.0"], [" the", "1.0"], [" US", "0.9575092"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Bud", "0.77192813"], ["we", "1.0"], ["iser", "1.0"], [" is", "0.9999999"], [" the", "1.0"], [" most", "1.0"], [" popular", "1.0"], [" beer", "1.0"], [" in", "1.0"], [" the", "1.0"], [" US", "1.0"], [".", "1.0"], [" It", "0.9979329"], [" is", "1.0"], [" brewed", "0.99996626"], [" by", "0.99985695"], [" An", "1.0"], ["he", "1.0"], ["user", "1.0"], ["-", "1.0"], ["Bus", "1.0"], ["ch", "1.0"], [" In", "0.93524384"], ["B", "1.0"], ["ev", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"]]}, "148": {"qid": "30fcf62f58849cb4ab05", "term": "Theocracy", "description": "Form of government with religious leaders", "question": "Were the Great Pyramids built by a theocratic government?", "answer": true, "facts": ["The Great Pyramids were built by the Ancient Egyptians.", "A theocracy is a government that is led by religious leaders or who worships their leader as a god.", "The Pharaohs of Ancient Egypt were worshipped as children of the Sun god Ra."], "decomposition": ["Who were the builders of the Great Pyramids?", "How did #1 serve their leaders?", "Could #2 be described as a theocracy?"], "evidence": [[[["Great Pyramid of Giza-8"]], ["no_evidence"], [["Theocracy-1"]]], [[["Giza pyramid complex-17"]], [["Ancient Egypt-73"], "no_evidence"], [["Theocracy-1"], "operation"]], [[["Giza pyramid complex-1"]], [["Giza pyramid complex-17"]], [["Ancient Egypt-3"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Great Pyramid of Giza have a secret chamber?\nFacts: The Great Pyramid of Giza had a secret chamber. The chamber was discovered by the French archaeologist Jacques-Yves Cousteau in the 1960's. The chamber was found to be a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Were the Great Pyramids built by a theocratic government?\nFacts: The Great Pyramids were built by the Ancient Egyptians. A theocracy is a government that is led by religious leaders or who worships their leader as a god. The Pharaohs of Ancient Egypt were worshipped as children of the Sun god Ra.\nAnswer (True or False):", "ct_subject": "The Great Pyramids were built by the Ancient Egyptians. A theocracy is a government that is led by religious leaders or who worships their leader as a god. The Pharaohs of Ancient Egypt were worshipped as children of the Sun god Ra.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9987558"], [" the", "1.0"], [" Great", "1.0"], [" Pyramid", "1.0"], [" of", "1.0"], [" G", "1.0"], ["iza", "1.0"], [" have", "1.0"], [" a", "1.0"], [" secret", "1.0"], [" chamber", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Great", "1.0"], [" Pyramid", "1.0"], [" of", "1.0"], [" G", "1.0"], ["iza", "1.0"], [" had", "0.42684475"], [" a", "0.99741435"], [" secret", "1.0"], [" chamber", "1.0"], [".", "0.9999994"], [" The", "0.9999999"], [" chamber", "0.99992454"], [" was", "1.0"], [" discovered", "0.9871213"], [" by", "0.99970144"], [" the", "0.99186"], [" French", "0.9991855"], [" archae", "0.99998164"], ["ologist", "1.0"], [" Jacques", "0.99001104"], ["-", "1.0"], ["Y", "0.8067704"], ["ves", "1.0"], [" Cou", "1.0"], ["ste", "1.0"], ["au", "1.0"], [" in", "1.0"], [" the", "1.0"], [" 1960", "0.9999317"], ["'s", "0.99888927"], [".", "1.0"], [" The", "0.6327971"], [" chamber", "1.0"], [" was", "1.0"], [" found", "0.9940301"], [" to", "0.99999726"], [" be", "0.99999976"], [" a", "0.9548977"]]}, "149": {"qid": "34c35185139b0e8672ee", "term": "Snoop Dogg", "description": "American rapper", "question": "Was Snoop Dogg an adult when Tickle Me Elmo was popular?", "answer": true, "facts": ["Tickle Me Elmo was first popular in 1996.", "Snoop Dogg would have been 25 when the Tickle Me Elmo craze took off."], "decomposition": ["In what year did Tickle Me Elmo become popular?", "In what year was Snoop Dogg born?", "What is the difference between #1 and #2?", "Is #3 greater than or equal to 18?"], "evidence": [[[["Tickle Me Elmo-2"]], [["Snoop Dogg-1"]], ["operation"], ["operation"]], [[["Tickle Me Elmo-2"]], [["Snoop Dogg-1"]], ["operation"], ["operation"]], [[["Tickle Me Elmo-2"]], [["Snoop Dogg-1"]], ["operation"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Beatles have a song called \"I Want to Hold Your Hand\"?\nFacts: The Beatles did not have a song called \"I Want to Hold Your Hand\". The song was written by Paul McCartney and sung by John Lennon.\nAnswer (True or False): True\nQuestion: Did the Beatles have a song called \"I Want", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Snoop Dogg an adult when Tickle Me Elmo was popular?\nFacts: Tickle Me Elmo was first popular in 1996. Snoop Dogg would have been 25 when the Tickle Me Elmo craze took off.\nAnswer (True or False):", "ct_subject": "Tickle Me Elmo was first popular in 1996. Snoop Dogg would have been 25 when the Tickle Me Elmo craze took off.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9160643"], [" the", "1.0"], [" Beatles", "0.9961576"], [" have", "0.99999905"], [" a", "1.0"], [" song", "0.99971086"], [" called", "1.0"], [" \"", "1.0"], ["I", "1.0"], [" Want", "1.0"], [" to", "1.0"], [" Hold", "1.0"], [" Your", "1.0"], [" Hand", "1.0"], ["\"?", "0.9324687"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999976"], [" Beatles", "1.0"], [" did", "0.97122306"], [" not", "1.0"], [" have", "0.99993837"], [" a", "1.0"], [" song", "1.0"], [" called", "1.0"], [" \"", "1.0"], ["I", "1.0"], [" Want", "1.0"], [" to", "1.0"], [" Hold", "1.0"], [" Your", "1.0"], [" Hand", "1.0"], ["\".", "0.999984"], [" The", "0.99995637"], [" song", "0.9577915"], [" was", "1.0"], [" written", "1.0"], [" by", "1.0"], [" Paul", "0.9875179"], [" McCartney", "1.0"], [" and", "0.99999905"], [" sung", "0.473052"], [" by", "1.0"], [" John", "1.0"], [" Lennon", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999976"], [" the", "1.0"], [" Beatles", "1.0"], [" have", "1.0"], [" a", "1.0"], [" song", "1.0"], [" called", "1.0"], [" \"", "1.0"], ["I", "1.0"], [" Want", "1.0"]]}, "150": {"qid": "16338eaba71f146a0c40", "term": "Central processing unit", "description": "Central component of any computer system which executes input/output, arithmetical, and logical operations", "question": "Does the central processing unit usually have a dedicated fan?", "answer": true, "facts": ["The CPU is the main chip on a computer's board, and generates a lot of heat.", "Computer manufacturers generally include a dedicated cooling system over the CPU in addition to the main board fans."], "decomposition": ["What do CPUs generate as they work over time?", "Do manufacturers incorporate fans specifically for removing #1 into computer systems?"], "evidence": [[[["Central processing unit-53"]], ["no_evidence", "operation"]], [[["CPU core voltage-14"]], [["Computer fan-7"]]], [[["Central processing unit-46"]], [["Computer cooling-15"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a computer and a computer monitor?\nFacts: A computer is a device that can be used to perform calculations. A computer monitor is a device that displays information.\nAnswer (True or False): True\nQuestion: What is the difference between a computer and a computer monitor?\nFacts: A", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does the central processing unit usually have a dedicated fan?\nFacts: The CPU is the main chip on a computer's board, and generates a lot of heat. Computer manufacturers generally include a dedicated cooling system over the CPU in addition to the main board fans.\nAnswer (True or False):", "ct_subject": "The CPU is the main chip on a computer's board, and generates a lot of heat. Computer manufacturers generally include a dedicated cooling system over the CPU in addition to the main board fans.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999958"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9772563"], [" between", "1.0"], [" a", "1.0"], [" computer", "0.91515577"], [" and", "1.0"], [" a", "1.0"], [" computer", "0.42750236"], [" monitor", "0.94566625"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.99999833"], [" computer", "1.0"], [" is", "0.20836513"], [" a", "1.0"], [" device", "0.99674547"], [" that", "1.0"], [" can", "0.7818635"], [" be", "0.99769294"], [" used", "1.0"], [" to", "1.0"], [" perform", "0.80716056"], [" calculations", "1.0"], [".", "0.99982834"], [" A", "1.0"], [" computer", "1.0"], [" monitor", "1.0"], [" is", "1.0"], [" a", "1.0"], [" device", "1.0"], [" that", "1.0"], [" displays", "0.7803362"], [" information", "0.99999905"], [".", "0.99969995"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" computer", "1.0"], [" and", "1.0"], [" a", "1.0"], [" computer", "1.0"], [" monitor", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"]]}, "151": {"qid": "45cccf7bbcd884bc0af0", "term": "Led Zeppelin", "description": "English rock band", "question": "Did the band Led Zeppelin own a prime number of gilded gramophones?", "answer": true, "facts": ["5 is a prime number", "A Grammy Award trophy is a gilded gramophone", "Led Zeppelin won 5 Grammy Awards"], "decomposition": ["What award has a trophy which consists of a gilded gramophone?", "How many #1 have Led Zeppelin won?", "Is #2 a prime number?"], "evidence": [[[["Grammy Award-1"]], [["Led Zeppelin-57"], "no_evidence"], [["Prime number-14"]]], [[["Grammy Award-1"]], [["Led Zeppelin-57"]], ["operation"]], [[["Grammy Award-1"]], [["Led Zeppelin-57"]], [["Prime number-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the band Led Zeppelin have a secret song that was never released?\nFacts: Led Zeppelin did not have a secret song.\nAnswer (True or False): True\nQuestion: Did the band Led Zeppelin have a secret song that was never released?\nFacts: Led Zeppelin did not have a secret song.\nAnswer (True or False): True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did the band Led Zeppelin own a prime number of gilded gramophones?\nFacts: 5 is a prime number A Grammy Award trophy is a gilded gramophone Led Zeppelin won 5 Grammy Awards\nAnswer (True or False):", "ct_subject": "5 is a prime number A Grammy Award trophy is a gilded gramophone Led Zeppelin won 5 Grammy Awards", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9532295"], [" the", "1.0"], [" band", "1.0"], [" Led", "1.0"], [" Ze", "1.0"], ["ppelin", "1.0"], [" have", "0.99974424"], [" a", "1.0"], [" secret", "0.9999902"], [" song", "0.9999721"], [" that", "0.9950557"], [" was", "0.99996185"], [" never", "1.0"], [" released", "0.99998224"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Led", "0.97674656"], [" Ze", "1.0"], ["ppelin", "1.0"], [" did", "0.99812335"], [" not", "0.99999"], [" have", "1.0"], [" a", "1.0"], [" secret", "1.0"], [" song", "1.0"], [".", "0.99999917"], ["\n", "0.33748022"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" band", "1.0"], [" Led", "1.0"], [" Ze", "1.0"], ["ppelin", "1.0"], [" have", "1.0"], [" a", "1.0"], [" secret", "1.0"], [" song", "1.0"], [" that", "1.0"], [" was", "1.0"], [" never", "1.0"], [" released", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Led", "1.0"], [" Ze", "1.0"], ["ppelin", "1.0"], [" did", "1.0"], [" not", "1.0"], [" have", "1.0"], [" a", "1.0"], [" secret", "1.0"], [" song", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"]]}, "152": {"qid": "e239a7c6e9a2efc98ccb", "term": "Quartz", "description": "mineral composed of silicon and oxygen atoms in a continuous framework of SiO\u2084 silicon\u2013oxygen tetrahedra, with each oxygen being shared between two tetrahedra, giving an overall chemical formula of SiO\u2082", "question": "Are Doctors of Homeopathy more likely than Doctors of Internal Medicine to recommend Quartz as a treatment?", "answer": true, "facts": ["Doctors of Homeopathy are practitioners of \"alternative medicine\" ", "In alternative medicine practices, Quartz is believed to have powers.", "Doctors of Internal Medicine have completed a medical residency and do not recommend alternative medicine."], "decomposition": ["What do doctors of homeopathy practice?", "What is Quartz believed to have in #1?", "What do doctors of internal medicine study?", "Are #1 or #2 not included in #3?"], "evidence": [[[["Homeopathy-1"]], [["Crystal healing-1"]], [["Internal medicine-1"]], ["operation"]], [[["Homeopathy-1"]], [["Quartz-1"]], [["Internal medicine-1"]], ["no_evidence", "operation"]], [[["Homeopathy-1"]], [["Crystal healing-1", "Quartz-1"]], [["Internal medicine-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there more people who have been killed by lightning than by guns?\nFacts: There are more people who have been killed by lightning than by guns.\nAnswer (True or False): True\nQuestion: Are there more people who have been killed by lightning than by guns", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are Doctors of Homeopathy more likely than Doctors of Internal Medicine to recommend Quartz as a treatment?\nFacts: Doctors of Homeopathy are practitioners of \"alternative medicine\"  In alternative medicine practices, Quartz is believed to have powers. Doctors of Internal Medicine have completed a medical residency and do not recommend alternative medicine.\nAnswer (True or False):", "ct_subject": "Doctors of Homeopathy are practitioners of \"alternative medicine\"  In alternative medicine practices, Quartz is believed to have powers. Doctors of Internal Medicine have completed a medical residency and do not recommend alternative medicine.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.9999974"], [" there", "1.0"], [" more", "0.9999763"], [" people", "1.0"], [" who", "0.8875098"], [" have", "0.98179686"], [" been", "0.985846"], [" killed", "0.9381889"], [" by", "1.0"], [" lightning", "0.9998387"], [" than", "0.9999908"], [" by", "0.9999994"], [" guns", "0.9998622"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "1.0"], [" are", "1.0"], [" more", "1.0"], [" people", "1.0"], [" who", "0.9953654"], [" have", "1.0"], [" been", "1.0"], [" killed", "1.0"], [" by", "1.0"], [" lightning", "1.0"], [" than", "1.0"], [" by", "1.0"], [" guns", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "1.0"], [" more", "1.0"], [" people", "1.0"], [" who", "1.0"], [" have", "1.0"], [" been", "1.0"], [" killed", "1.0"], [" by", "1.0"], [" lightning", "1.0"], [" than", "1.0"], [" by", "1.0"], [" guns", "1.0"]]}, "153": {"qid": "73defbf5e28a5bacdb7d", "term": "Rock in Rio", "description": "Brazilian music festival", "question": "Would it be difficult to host Stanley Cup Finals at Rock in Rio?", "answer": true, "facts": ["The Stanley Cup Finals is the last series in hockey each year.", "Hockey rinks are indoors and require ice for players to skate on.", "Rock in Rio is a massive outdoor music festival in Brazil.", "Rock in Rio takes place in June each year.", "The temperature in Brazil during June averages around 80F."], "decomposition": ["Which sport has the Stanley Cup Finals?", "Which kind of surface is required to play #1?", "What time of the year does Rock in Rio take place, and where?", "Do weather conditions in #3 at that time not favor the formation of #2?"], "evidence": [[[["Stanley Cup-1"]], [["Ice hockey-1"]], [["Rock in Rio-1", "Rock in Rio-21"]], ["operation"]], [[["Stanley Cup-1"]], [["Ice hockey-1"]], [["Rock in Rio-1", "Rock in Rio-31"]], [["Brazil-47"], "operation"]], [[["Stanley Cup-1"]], [["Hockey-16"]], [["Rock in Rio-16"]], [["Rio de Janeiro-31"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in Brazil that is the home of the Rock in Rio festival?\nFacts: Rio de Janeiro is the capital of Brazil. The Rock in Rio festival is held in the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would it be difficult to host Stanley Cup Finals at Rock in Rio?\nFacts: The Stanley Cup Finals is the last series in hockey each year. Hockey rinks are indoors and require ice for players to skate on. Rock in Rio is a massive outdoor music festival in Brazil. Rock in Rio takes place in June each year. The temperature in Brazil during June averages around 80F.\nAnswer (True or False):", "ct_subject": "The Stanley Cup Finals is the last series in hockey each year. Hockey rinks are indoors and require ice for players to skate on. Rock in Rio is a massive outdoor music festival in Brazil. Rock in Rio takes place in June each year. The temperature in Brazil during June averages around 80F.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999999"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9993594"], [" of", "1.0"], [" the", "1.0"], [" city", "0.99998724"], [" in", "0.99999976"], [" Brazil", "0.7827481"], [" that", "0.97311485"], [" is", "0.89547986"], [" the", "1.0"], [" home", "0.9352287"], [" of", "1.0"], [" the", "1.0"], [" Rock", "1.0"], [" in", "1.0"], [" Rio", "1.0"], [" festival", "0.9999112"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rio", "0.8465346"], [" de", "1.0"], [" Janeiro", "1.0"], [" is", "0.99998224"], [" the", "1.0"], [" capital", "0.99995434"], [" of", "1.0"], [" Brazil", "1.0"], [".", "1.0"], [" The", "0.9608314"], [" Rock", "0.9993237"], [" in", "1.0"], [" Rio", "1.0"], [" festival", "1.0"], [" is", "0.99636"], [" held", "1.0"], [" in", "1.0"], [" the", "0.9999989"]]}, "154": {"qid": "a406e06cd0164c1c6d15", "term": "Auburn, New York", "description": "City in New York, United States", "question": "Would the trees in Auburn, New York be changing colors in September?", "answer": true, "facts": ["In New York, fall begins during the month of September. ", "When the weather gets cooler in the fall, leaves begin to change colors. "], "decomposition": ["In the month of September, what season would it be in New York?", "Do trees change color during #1?"], "evidence": [[[["Climate of New York-2"]], [["Autumn leaf color-1"]]], [[["Autumn-1"]], [["Autumn-3"], "operation"]], [[["Autumn-3"]], [["Autumn-3"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the largest in area?\nFacts: The largest city in the United States is New York City. \nAnswer (True or False): True\nQuestion: What is the name of the city in the United States that is the largest in population?\nFacts: The largest city in the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would the trees in Auburn, New York be changing colors in September?\nFacts: In New York, fall begins during the month of September.  When the weather gets cooler in the fall, leaves begin to change colors. \nAnswer (True or False):", "ct_subject": "In New York, fall begins during the month of September.  When the weather gets cooler in the fall, leaves begin to change colors. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9997906"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9999118"], [" in", "0.99998975"], [" the", "0.99999785"], [" United", "0.97643864"], [" States", "1.0"], [" that", "1.0"], [" is", "0.9499726"], [" the", "1.0"], [" largest", "0.961122"], [" in", "0.9796427"], [" area", "0.9471257"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" largest", "0.99974495"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "1.0"], [" New", "1.0"], [" York", "1.0"], [" City", "0.99999917"], [".", "1.0"], [" ", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" in", "1.0"], [" population", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" largest", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"]]}, "155": {"qid": "a6bf045651f7b6b64035", "term": "Macaque", "description": "genus of Old World monkeys", "question": "Could an elephant easily defeat a male macaque?", "answer": true, "facts": ["Male macaques range from 16 to 28 inches tall with a weight between 12.13 to 39.7 pounds.", "Elephants are between 7 to 11 feet tall and weigh several thousand pounds.", "Elephants contain large, sharp tusks that can injure or kill other animals."], "decomposition": ["How much does a male macaques weigh?", "How much can an elephant weigh?", "How tall is a male macaque?", "How tall is an elephant?", "Is #2 more than #1 and is #4 more than #3?"], "evidence": [[[["Macaque-4"]], [["Elephant-14"]], [["Macaque-4"]], [["Elephant-12"]], ["operation"]], [[["Macaque-4"]], [["Elephant-15"]], [["Macaque-4"]], [["Elephant-15"]], ["operation"]], [[["Macaque-4"]], [["Elephant-15", "Elephantidae-1"], "no_evidence"], [["Macaque-4"]], [["Elephant-15"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the largest animal in the world?\nFacts: The largest animal in the world is the blue whale. It is the largest animal in the world by weight. It is also the largest animal in the world by length. The blue", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could an elephant easily defeat a male macaque?\nFacts: Male macaques range from 16 to 28 inches tall with a weight between 12.13 to 39.7 pounds. Elephants are between 7 to 11 feet tall and weigh several thousand pounds. Elephants contain large, sharp tusks that can injure or kill other animals.\nAnswer (True or False):", "ct_subject": "Male macaques range from 16 to 28 inches tall with a weight between 12.13 to 39.7 pounds. Elephants are between 7 to 11 feet tall and weigh several thousand pounds. Elephants contain large, sharp tusks that can injure or kill other animals.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9966042"], [" is", "1.0"], [" the", "1.0"], [" largest", "0.7853531"], [" animal", "0.9020828"], [" in", "0.99945563"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" largest", "1.0"], [" animal", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" is", "1.0"], [" the", "1.0"], [" blue", "1.0"], [" whale", "1.0"], [".", "1.0"], [" It", "0.99561214"], [" is", "0.99999976"], [" the", "0.65778923"], [" largest", "1.0"], [" animal", "1.0"], [" in", "0.9998957"], [" the", "1.0"], [" world", "0.9999428"], [" by", "0.9987361"], [" weight", "0.9777755"], [".", "0.9931151"], [" It", "0.86105615"], [" is", "0.9999938"], [" also", "0.9018545"], [" the", "1.0"], [" largest", "1.0"], [" animal", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "0.9999999"], [" by", "1.0"], [" length", "1.0"], [".", "1.0"], [" The", "0.9431519"], [" blue", "0.9999151"]]}, "156": {"qid": "76d3f6c91518061deb7d", "term": "Drum", "description": "type of musical instrument of the percussion family", "question": "Would a cattle farmer be useful to a drum maker?", "answer": true, "facts": ["Cattle are often slaughtered for meat and other products, like leather.", "Drums are often made with leather."], "decomposition": ["Which animal products would a drum maker need?", "Are #1 commonly obtained from cattle?"], "evidence": [[[["Drumhead-3"], "no_evidence"], [["Leather-1"]]], [[["Drumhead-5"]], ["operation"]], [[["Drum-7"]], [["Drum-7", "Leather-5"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a drum and a cow?\nFacts: A cow is a female cow. A drum is a male cow.\nAnswer (True or False): True\nQuestion: What is the difference between a cow and a drum?\nFacts: A cow is a female cow. A drum is a male cow.\nAnswer (True or False): True\nQuestion: What is the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a cattle farmer be useful to a drum maker?\nFacts: Cattle are often slaughtered for meat and other products, like leather. Drums are often made with leather.\nAnswer (True or False):", "ct_subject": "Cattle are often slaughtered for meat and other products, like leather. Drums are often made with leather.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999924"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.999313"], [" between", "1.0"], [" a", "1.0"], [" drum", "0.8821933"], [" and", "1.0"], [" a", "1.0"], [" cow", "0.9667745"], ["?", "1.0"], ["\n", "1.0"], ["F", "0.9999999"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" cow", "0.9992478"], [" is", "1.0"], [" a", "1.0"], [" female", "1.0"], [" cow", "1.0"], [".", "1.0"], [" A", "1.0"], [" drum", "1.0"], [" is", "1.0"], [" a", "1.0"], [" male", "1.0"], [" cow", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" cow", "0.999954"], [" and", "1.0"], [" a", "1.0"], [" drum", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" cow", "1.0"], [" is", "1.0"], [" a", "1.0"], [" female", "1.0"], [" cow", "1.0"], [".", "1.0"], [" A", "1.0"], [" drum", "1.0"], [" is", "1.0"], [" a", "1.0"], [" male", "1.0"], [" cow", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"]]}, "157": {"qid": "4a974e322e3586af4c2f", "term": "Jalape\u00f1o", "description": "Hot pepper", "question": "Can children be hurt by jalapeno peppers?", "answer": true, "facts": ["Jalapeno peppers contain capsaicin. ", "Capsaicin creates a burning sensation in the eyes and can lead to surface injuries. ", "Small children do not understand how to protect themselves from peppers or how to wash their hands properly."], "decomposition": ["What do Jalapeno peppers contain?", "Can #1 cause injuries to a child if they are not careful?"], "evidence": [[[["Jalape\u00f1o-15"]], ["no_evidence"]], [[["Capsaicin-1"]], [["Capsaicin-17"], "operation"]], [[["Capsaicin-1"]], [["Capsaicin-17"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"carpet\" and a \"carpeted\" house?\nFacts: A carpet is a carpeted house. A carpeted house is a house that has been carpeted.\nAnswer (True or False): True\nQuestion: What", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can children be hurt by jalapeno peppers?\nFacts: Jalapeno peppers contain capsaicin.  Capsaicin creates a burning sensation in the eyes and can lead to surface injuries.  Small children do not understand how to protect themselves from peppers or how to wash their hands properly.\nAnswer (True or False):", "ct_subject": "Jalapeno peppers contain capsaicin.  Capsaicin creates a burning sensation in the eyes and can lead to surface injuries.  Small children do not understand how to protect themselves from peppers or how to wash their hands properly.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9970497"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.99998975"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.99999714"], ["c", "0.33652312"], ["arp", "0.89254725"], ["et", "0.9999999"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "0.98905516"], ["arp", "1.0"], ["et", "1.0"], ["ed", "1.0"], ["\"", "1.0"], [" house", "0.99999917"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" carpet", "1.0"], [" is", "0.9989292"], [" a", "1.0"], [" carpet", "0.9781927"], ["ed", "1.0"], [" house", "1.0"], [".", "1.0"], [" A", "0.97959065"], [" carpet", "1.0"], ["ed", "1.0"], [" house", "1.0"], [" is", "0.99999976"], [" a", "1.0"], [" house", "1.0"], [" that", "1.0"], [" has", "0.99999917"], [" been", "1.0"], [" carpet", "1.0"], ["ed", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"]]}, "158": {"qid": "ced05cba04173f1248b3", "term": "Brooklyn", "description": "Borough in New York City and county in New York state, United States", "question": "Is Brooklyn known for its bread products?", "answer": true, "facts": ["Brooklyn and NYC bagels are known around the world for being high quality.", "Brooklyn pizza is regarded as the best in the country. "], "decomposition": ["What food items from Brooklyn are known around the world for being high quality?", "What food item from Brooklyn is known as the best in the country?", "Are any items from #1 or #2 bread products?"], "evidence": [[[["Bagel-31"]], [["Bagel-21"]], ["operation"]], [[["New York City-137"], "no_evidence"], [["Brooklyn-68"], "no_evidence"], ["no_evidence", "operation"]], [[["Neapolitan cuisine-45"], "no_evidence"], [["Pizza-1"], "no_evidence"], [["Pizza-6"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city of Rome?\nFacts: Rome is the name of the capital of the Roman Empire.\nAnswer (True or False): True\nQuestion: What is the name of the city of Rome?\nFacts: Rome is the name of the capital of the Roman Empire.\nAnswer (True or False): True\nQuestion: What is the name of the city", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Brooklyn known for its bread products?\nFacts: Brooklyn and NYC bagels are known around the world for being high quality. Brooklyn pizza is regarded as the best in the country. \nAnswer (True or False):", "ct_subject": "Brooklyn and NYC bagels are known around the world for being high quality. Brooklyn pizza is regarded as the best in the country. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.25330648"], [" is", "1.0"], [" the", "1.0"], [" name", "0.6681771"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "0.99947625"], [" Rome", "0.7999691"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "1.0"], [" is", "0.9999976"], [" the", "1.0"], [" name", "0.99824226"], [" of", "1.0"], [" the", "1.0"], [" capital", "0.67768496"], [" of", "0.998589"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"]]}, "159": {"qid": "1af3364389f69c57f600", "term": "Vice President of the United States", "description": "Second highest executive office in United States", "question": "Can Vice President of the United States kill with impunity?", "answer": true, "facts": [" Vice President Aaron Burr fatally wounded Alexander Hamilton in a duel on July 11, 1804.", "Aaron Burr continued his term as Vice President of the United States after killing Alexander Hamilton.", "US stand-your-ground laws allow a person to defend themselves even to the point of applying lethal force."], "decomposition": ["What was the outcome of Vice President Aaron Burr's duel in July, 1804?", "Did#1 lead to loss of his opponent's life and did he continue his term afterwards?"], "evidence": [[[["Aaron Burr-1"]], [["Aaron Burr-34", "Aaron Burr-35"], "operation"]], [[["Burr\u2013Hamilton duel-35"]], [["Aaron Burr-35", "Burr\u2013Hamilton duel-35"], "no_evidence"]], [[["Aaron Burr-4"]], [["Aaron Burr-35", "Aaron Burr-4"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Can a person be a member of the United States military and a member of the United States Congress at the same time?\nFacts: Yes.\nAnswer (True or False): True\nQuestion: Can a person be a member", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can Vice President of the United States kill with impunity?\nFacts:  Vice President Aaron Burr fatally wounded Alexander Hamilton in a duel on July 11, 1804. Aaron Burr continued his term as Vice President of the United States after killing Alexander Hamilton. US stand-your-ground laws allow a person to defend themselves even to the point of applying lethal force.\nAnswer (True or False):", "ct_subject": " Vice President Aaron Burr fatally wounded Alexander Hamilton in a duel on July 11, 1804. Aaron Burr continued his term as Vice President of the United States after killing Alexander Hamilton. US stand-your-ground laws allow a person to defend themselves even to the point of applying lethal force.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Can", "0.7360272"], [" a", "0.9999974"], [" person", "0.9999999"], [" be", "0.99999905"], [" a", "0.9047101"], [" member", "0.95229334"], [" of", "1.0"], [" the", "1.0"], [" United", "0.9452021"], [" States", "1.0"], [" military", "0.8732899"], [" and", "0.9999974"], [" a", "0.99891806"], [" member", "1.0"], [" of", "1.0"], [" the", "0.9999951"], [" United", "0.9999876"], [" States", "1.0"], [" Congress", "0.9999982"], [" at", "0.99856997"], [" the", "1.0"], [" same", "1.0"], [" time", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Yes", "0.99618584"], [".", "0.9159958"], ["\n", "0.775711"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Can", "1.0"], [" a", "1.0"], [" person", "1.0"], [" be", "1.0"], [" a", "1.0"], [" member", "1.0"]]}, "160": {"qid": "ab70365febc75dcda77d", "term": "Radioactive waste", "description": "wastes that contain nuclear material", "question": "Is radioactive waste a plot device for many shows?", "answer": true, "facts": ["Radioactive isotopes in an ooze-like waste cause turtles to become the Teenage Mutant Ninja Turtles.", "In the Fox animated hit, Family Guy, radioactive waste is used to turn give the main characters superpowers. ", "The superhero 'Daredevil' encounters radioactive waste that blinds him as a child and gives him super powers."], "decomposition": ["What turned turtles into mutant turtles in Teenage Mutant Ninja Turtles?", "Which substance gives the main characters of Family Guy superpowers?", "Which substance gave Daredevil his super powers?", "Are #1. #2 and #3 radioactive waste?"], "evidence": [[[["Teenage Mutant Ninja Turtles (1990 film)-4"]], [["Griffin family-1"], "no_evidence"], [["Daredevil (Marvel Comics character)-2"]], ["no_evidence"]], [[["Teenage Mutant Ninja Turtles II: The Secret of the Ooze-6"]], ["no_evidence"], [["Alternative versions of Daredevil-33"], "no_evidence"], ["operation"]], [[["Teenage Mutant Ninja Turtles (Mirage Studios)-9"], "no_evidence"], ["no_evidence"], [["Daredevil (Marvel Comics character)-2"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the character 'Daredevil' a reference to the character 'Darth Vader'?\nFacts: The name 'Daredevil' is a reference to the character 'D", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is radioactive waste a plot device for many shows?\nFacts: Radioactive isotopes in an ooze-like waste cause turtles to become the Teenage Mutant Ninja Turtles. In the Fox animated hit, Family Guy, radioactive waste is used to turn give the main characters superpowers.  The superhero 'Daredevil' encounters radioactive waste that blinds him as a child and gives him super powers.\nAnswer (True or False):", "ct_subject": "Radioactive isotopes in an ooze-like waste cause turtles to become the Teenage Mutant Ninja Turtles. In the Fox animated hit, Family Guy, radioactive waste is used to turn give the main characters superpowers.  The superhero 'Daredevil' encounters radioactive waste that blinds him as a child and gives him super powers.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.47259963"], [" the", "0.99924743"], [" name", "0.7402644"], [" of", "0.9999999"], [" the", "1.0"], [" character", "0.9994842"], [" '", "0.99874216"], ["D", "0.5618985"], ["aredevil", "1.0"], ["'", "1.0"], [" a", "0.99669266"], [" reference", "1.0"], [" to", "1.0"], [" the", "1.0"], [" character", "0.99999654"], [" '", "1.0"], ["D", "0.8872353"], ["arth", "0.9999999"], [" Vader", "1.0"], ["'?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" name", "0.99997985"], [" '", "0.9991742"], ["D", "1.0"], ["aredevil", "1.0"], ["'", "1.0"], [" is", "1.0"], [" a", "1.0"], [" reference", "1.0"], [" to", "1.0"], [" the", "1.0"], [" character", "1.0"], [" '", "1.0"], ["D", "1.0"]]}, "161": {"qid": "7ce5f72988455dfb63da", "term": "Samsung Galaxy", "description": "series of Android mobile computing devices", "question": "Would the operating system of a Samsung Galaxy 1 sound edible?", "answer": true, "facts": ["The first Samsung Galaxy device ran a version of Android from 2009.", "In 2009, the Android edition was called \"cupcake.\" "], "decomposition": ["What are the operating systems of a Samsung Galaxy 1?", "Does #1 sound like something that is edible?"], "evidence": [[[["Samsung Galaxy S-22"]], ["operation"]], [[["Android Cupcake-1", "Samsung Galaxy (original)-1"]], [["Cupcake-1"], "operation"]], [[["Samsung Galaxy S-26"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the year 2000?\nFacts: The first person to be born in the year 2000 was a woman named Sarah.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the year 2000?\nFacts: The first person to be born in the year 2000 was", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would the operating system of a Samsung Galaxy 1 sound edible?\nFacts: The first Samsung Galaxy device ran a version of Android from 2009. In 2009, the Android edition was called \"cupcake.\" \nAnswer (True or False):", "ct_subject": "The first Samsung Galaxy device ran a version of Android from 2009. In 2009, the Android edition was called \"cupcake.\" ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999976"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99537694"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "0.99705446"], [" to", "1.0"], [" be", "0.99944717"], [" born", "0.98793375"], [" in", "1.0"], [" the", "1.0"], [" year", "0.8366756"], [" 2000", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "1.0"], [" a", "0.9999633"], [" woman", "0.99923277"], [" named", "1.0"], [" Sarah", "0.12782893"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "1.0"]]}, "162": {"qid": "9247b5afb2eef4c9f95b", "term": "Tibia", "description": "larger of the two bones of the leg below the knee for vertebrates", "question": "Can a minotaur hypothetically injure a tibia playing football?", "answer": true, "facts": ["A minotaur is a mythological creature with the head of a bull and the body of a human.", "The tibia is the bone between the knee and the ankle in humans.", "Tibia injuries are common injuries in contact sports such as football."], "decomposition": ["What is the body structure of a Minotaur?", "Where in the human body is the tibia located?", "Does #1 account for the presence of #2 in a Minotaur?", "Are injuries to #2 common during football?", "Are #3 and #4 positive"], "evidence": [[[["Minotaur-1"]], [["Tibia-1"]], ["operation"], [["Running injuries-1"], "no_evidence"], ["operation"]], [[["Minotaur-1"]], [["Tibia-1"]], ["operation"], [["Anterior cruciate ligament injury-2", "Intercondylar area-6"]], ["operation"]], [[["Minotaur-1"]], [["Tibia-1"]], ["operation"], [["Shin splints-1", "Shin splints-2"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Can a man who has a penis and testicles be a woman?\nFacts: A man who has a penis and testicles is a man. A man who has a penis and testicles is a woman.\nAnswer (True or False): True\nQuestion", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can a minotaur hypothetically injure a tibia playing football?\nFacts: A minotaur is a mythological creature with the head of a bull and the body of a human. The tibia is the bone between the knee and the ankle in humans. Tibia injuries are common injuries in contact sports such as football.\nAnswer (True or False):", "ct_subject": "A minotaur is a mythological creature with the head of a bull and the body of a human. The tibia is the bone between the knee and the ankle in humans. Tibia injuries are common injuries in contact sports such as football.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Can", "0.97096217"], [" a", "1.0"], [" man", "0.86922467"], [" who", "0.9963791"], [" has", "0.99999774"], [" a", "0.6337949"], [" penis", "0.94002277"], [" and", "0.98907286"], [" test", "1.0"], ["icles", "1.0"], [" be", "0.5317731"], [" a", "1.0"], [" woman", "0.99994695"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.99885404"], [" man", "0.99998605"], [" who", "0.56841284"], [" has", "1.0"], [" a", "1.0"], [" penis", "1.0"], [" and", "1.0"], [" test", "1.0"], ["icles", "1.0"], [" is", "0.99999917"], [" a", "1.0"], [" man", "1.0"], [".", "1.0"], [" A", "1.0"], [" man", "0.99996316"], [" who", "1.0"], [" has", "1.0"], [" a", "1.0"], [" penis", "0.86266285"], [" and", "1.0"], [" test", "1.0"], ["icles", "1.0"], [" is", "0.9999994"], [" a", "1.0"], [" woman", "0.9862548"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"]]}, "163": {"qid": "8fe1b7912f41e5653e88", "term": "Tonsure", "description": "hairstyle related to religious devotion", "question": "Would Christopher Hitchens be very unlikely to engage in tonsure?", "answer": true, "facts": ["Tonsure is the practice of cutting or shaving some or all of the hair on the scalp as a sign of religious devotion or humility.", "Christopher Hitchens was an anti-theist, and he regarded all religions as false, harmful, and authoritarian."], "decomposition": ["What were Christopher Hitchens' views on religion?", "What is the purpose of tonsure?", "Would a proponent of #1 have a negative opinion of #2?"], "evidence": [[[["Christopher Hitchens-2"]], [["Tonsure-1"]], ["operation"]], [[["Christopher Hitchens-2"]], [["Tonsure-1"]], ["operation"]], [[["Christopher Hitchens-32"]], [["Tonsure-5"]], [["Christopher Hitchens-33"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"soul\" and a \"spirit\"?\nFacts: A soul is a part of the human body that is separate from the body. A spirit is a part of the human body that is not separate from the body.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Christopher Hitchens be very unlikely to engage in tonsure?\nFacts: Tonsure is the practice of cutting or shaving some or all of the hair on the scalp as a sign of religious devotion or humility. Christopher Hitchens was an anti-theist, and he regarded all religions as false, harmful, and authoritarian.\nAnswer (True or False):", "ct_subject": "Tonsure is the practice of cutting or shaving some or all of the hair on the scalp as a sign of religious devotion or humility. Christopher Hitchens was an anti-theist, and he regarded all religions as false, harmful, and authoritarian.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.43083623"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9547549"], [" between", "1.0"], [" a", "0.9977386"], [" \"", "1.0"], ["s", "0.96555483"], ["oul", "0.98174745"], ["\"", "0.99688596"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["spirit", "0.878562"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9975268"], [" soul", "1.0"], [" is", "1.0"], [" a", "0.99984634"], [" part", "0.99954957"], [" of", "1.0"], [" the", "0.80553657"], [" human", "0.99999523"], [" body", "1.0"], [" that", "0.9994491"], [" is", "1.0"], [" separate", "0.9988607"], [" from", "1.0"], [" the", "1.0"], [" body", "1.0"], [".", "1.0"], [" A", "1.0"], [" spirit", "1.0"], [" is", "1.0"], [" a", "1.0"], [" part", "1.0"], [" of", "1.0"], [" the", "1.0"], [" human", "1.0"], [" body", "0.99998105"], [" that", "1.0"], [" is", "1.0"], [" not", "0.9999995"], [" separate", "1.0"], [" from", "1.0"], [" the", "1.0"], [" body", "1.0"], [".", "1.0"]]}, "164": {"qid": "d47120efd0f09badd848", "term": "Menthol", "description": "chemical compound", "question": "Is menthol associated with Christmas?", "answer": true, "facts": ["Menthol is the chemical in mint products that give mint its characteristic cool and tangy taste.", "Peppermint is a popular candy flavor during Christmas season."], "decomposition": ["What is a popular candy flavor during Christmas?", "Is menthol an ingredient in #1?"], "evidence": [[[["Candy cane-1"]], [["Peppermint-2"]]], [[["Candy cane-1"]], [["Menthol-1"]]], [[["Candy cane-1"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the word \"Christmas\" derived from the word \"Christ\"?\nFacts: The word \"Christmas\" is a modern invention. The word \"Christmas\" was first used in the 16th century.\nAnswer (True or False): True\nQuestion: Is the word \"Christmas\" derived from the word \"Christ\"?\nFacts: The word \"Christmas\" is a modern invention.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is menthol associated with Christmas?\nFacts: Menthol is the chemical in mint products that give mint its characteristic cool and tangy taste. Peppermint is a popular candy flavor during Christmas season.\nAnswer (True or False):", "ct_subject": "Menthol is the chemical in mint products that give mint its characteristic cool and tangy taste. Peppermint is a popular candy flavor during Christmas season.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999943"], [" the", "0.695944"], [" word", "0.9946228"], [" \"", "1.0"], ["Christmas", "0.99999523"], ["\"", "1.0"], [" derived", "0.9998573"], [" from", "1.0"], [" the", "1.0"], [" word", "0.99999976"], [" \"", "1.0"], ["Christ", "1.0"], ["\"?", "0.9999958"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999999"], [" word", "1.0"], [" \"", "1.0"], ["Christmas", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "0.99465126"], [" modern", "0.49028617"], [" invention", "1.0"], [".", "1.0"], [" The", "0.9996176"], [" word", "1.0"], [" \"", "1.0"], ["Christmas", "0.9999789"], ["\"", "1.0"], [" was", "0.9616589"], [" first", "1.0"], [" used", "1.0"], [" in", "1.0"], [" the", "1.0"], [" 16", "0.9999589"], ["th", "1.0"], [" century", "1.0"], [".", "0.99949956"], ["\n", "0.9992737"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["Christmas", "1.0"], ["\"", "1.0"], [" derived", "0.99996245"], [" from", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["Christ", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["Christmas", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" modern", "1.0"], [" invention", "1.0"], [".", "1.0"]]}, "165": {"qid": "b0ab236d19fec61c0111", "term": "Gorilla", "description": "Genus of mammals", "question": "Is it expected that Charla Nash would be anxious near a gorilla?", "answer": true, "facts": ["In 2009, Charla Nash was attacked and nearly killed by a chimpanzee. ", "While a different species, Gorillas and Chimpanzees have similar physical appearances and are both primates."], "decomposition": ["Which animal attacked Charla Nash in 2009?", "Does #1 bear significant similarity to a gorilla?"], "evidence": [[[["Travis (chimpanzee)-8"]], ["operation"]], [[["Travis (chimpanzee)-8"]], [["Hominidae-1"]]], [[["Travis (chimpanzee)-8"]], [["Gorilla-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it possible to have a baby with a human sperm donor?\nFacts: Yes. In fact, it is the most common way to have a baby.\nAnswer (True or False): True\nQuestion: Is it possible to have a baby with a human egg donor?\nFacts: Yes. In fact", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is it expected that Charla Nash would be anxious near a gorilla?\nFacts: In 2009, Charla Nash was attacked and nearly killed by a chimpanzee.  While a different species, Gorillas and Chimpanzees have similar physical appearances and are both primates.\nAnswer (True or False):", "ct_subject": "In 2009, Charla Nash was attacked and nearly killed by a chimpanzee.  While a different species, Gorillas and Chimpanzees have similar physical appearances and are both primates.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99273443"], [" it", "1.0"], [" possible", "0.9724523"], [" to", "0.99936503"], [" have", "0.9350966"], [" a", "1.0"], [" baby", "0.99998796"], [" with", "1.0"], [" a", "1.0"], [" human", "0.97488075"], [" sperm", "0.99999654"], [" donor", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Yes", "0.99558485"], [".", "0.24141672"], [" In", "0.8949439"], [" fact", "0.9999933"], [",", "1.0"], [" it", "0.54297024"], [" is", "1.0"], [" the", "0.40977353"], [" most", "1.0"], [" common", "1.0"], [" way", "0.83748674"], [" to", "0.99999976"], [" have", "0.108584136"], [" a", "1.0"], [" baby", "0.9999999"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" possible", "1.0"], [" to", "1.0"], [" have", "1.0"], [" a", "1.0"], [" baby", "1.0"], [" with", "1.0"], [" a", "1.0"], [" human", "1.0"], [" egg", "1.0"], [" donor", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Yes", "1.0"], [".", "1.0"], [" In", "1.0"], [" fact", "1.0"]]}, "166": {"qid": "f69c8412d57ad726d658", "term": "Cholera", "description": "Bacterial infection of the small intestine", "question": "Is Cholera alive?", "answer": true, "facts": ["Cholera are a type of bacteria.", "Bacteria are considered living creatures."], "decomposition": ["Is cholera a bacteria?", "Are bacteria considered to be living?", "Are the answers to #1 and #2 the same?"], "evidence": [[[["Cholera-1"]], [["Bacteria-1"]], ["operation"]], [[["Cholera-1"]], [["Bacteria-1"]], ["operation"]], [[["Vibrio cholerae-6"]], [["Evolution of bacteria-5"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the Earth round?\nFacts: The Earth is a sphere.\nAnswer (True or False): True\nQuestion: Is the Earth flat?\nFacts: The Earth is flat.\nAnswer (True or False): True\nQuestion: Is the Earth round?\nFacts: The Earth is round.\nAnswer (True or False): True\nQuestion: Is the Earth flat?\nFacts: The Earth is flat.\nAnswer (True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Cholera alive?\nFacts: Cholera are a type of bacteria. Bacteria are considered living creatures.\nAnswer (True or False):", "ct_subject": "Cholera are a type of bacteria. Bacteria are considered living creatures.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999999"], [" the", "0.99765354"], [" Earth", "0.9999987"], [" round", "0.99997056"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" a", "0.9966834"], [" sphere", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "0.9999994"], [" Earth", "1.0"], [" flat", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" flat", "0.9961863"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" Earth", "1.0"], [" round", "0.9999869"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" round", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" Earth", "1.0"], [" flat", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" flat", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"]]}, "167": {"qid": "d03daae5b38423aec74c", "term": "Mongols", "description": "ethnic group of central Asia", "question": "Would a packed Wembley stadium be likely to have a descendant of the Mongols inside?", "answer": true, "facts": ["Wembley stadium has a capacity of 90,000 people.", "The Mongols were an ethnic group that dominated the 13th and 14th centuries.", "Genghis Khan was the founder of the Mongol Empire.", "Geneticists have determined that 1 in every 200 men are descended from Genghis Khan."], "decomposition": ["What is the capacity of the Wembley stadium?", "Who is the founder of the Mongol empire?", "What is the minimum number of men within which at least one descendant of #2 is found?", "Is #1 divided by #3 greater than or equal to one?"], "evidence": [[[["Wembley Stadium-2"]], [["Mongol Empire-2"]], ["no_evidence"], ["operation"]], [[["Wembley Stadium-2"]], [["Mongol Empire-2"]], [["Descent from Genghis Khan-22"], "no_evidence"], ["no_evidence", "operation"]], [[["Wembley Stadium-2"]], [["Mongol Empire-2"]], [["Descent from Genghis Khan-22"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average height of a Mongolian man?\nFacts: The average height of a Mongolian man is 5 feet 10 inches tall.\nAnswer (True or False): True\nQuestion:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a packed Wembley stadium be likely to have a descendant of the Mongols inside?\nFacts: Wembley stadium has a capacity of 90,000 people. The Mongols were an ethnic group that dominated the 13th and 14th centuries. Genghis Khan was the founder of the Mongol Empire. Geneticists have determined that 1 in every 200 men are descended from Genghis Khan.\nAnswer (True or False):", "ct_subject": "Wembley stadium has a capacity of 90,000 people. The Mongols were an ethnic group that dominated the 13th and 14th centuries. Genghis Khan was the founder of the Mongol Empire. Geneticists have determined that 1 in every 200 men are descended from Genghis Khan.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9998234"], [" is", "1.0"], [" the", "1.0"], [" average", "0.99877745"], [" height", "1.0"], [" of", "1.0"], [" a", "0.99999976"], [" Mongol", "0.99999917"], ["ian", "0.98868513"], [" man", "0.9999994"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" height", "1.0"], [" of", "1.0"], [" a", "1.0"], [" Mongol", "1.0"], ["ian", "1.0"], [" man", "1.0"], [" is", "1.0"], [" 5", "0.99995327"], [" feet", "0.8683931"], [" 10", "0.91441816"], [" inches", "1.0"], [" tall", "0.7632908"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"]]}, "168": {"qid": "1a5d8dbceb6a7411a52b", "term": "Smooth jazz", "description": "category of music", "question": "Would James Cotton's instrument be too strident for a smooth jazz band?", "answer": true, "facts": ["Smooth jazz is an offshoot of jazz music that relies on a more melodic form.", "Smooth jazz employs the following instruments: saxophone. guitar. piano. trumpet. synthesizer. electric bass. and drums.", "James Cotton was a famous blues harmonica player."], "decomposition": ["What instument is James Cotton known for?", "What instuments are used to play Smooth Jazz?", "Is #1 not  one of #2?"], "evidence": [[[["James Cotton-1"]], [["Smooth jazz-2"], "no_evidence"], ["operation"]], [[["James Cotton-1"]], [["Smooth jazz-3"], "no_evidence"], ["operation"]], [[["James Cotton-1"]], [["Musical ensemble-15"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: The first person executed in the United States was John Brown.\nAnswer (True or False): True\nQuestion: What is the name of", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would James Cotton's instrument be too strident for a smooth jazz band?\nFacts: Smooth jazz is an offshoot of jazz music that relies on a more melodic form. Smooth jazz employs the following instruments: saxophone. guitar. piano. trumpet. synthesizer. electric bass. and drums. James Cotton was a famous blues harmonica player.\nAnswer (True or False):", "ct_subject": "Smooth jazz is an offshoot of jazz music that relies on a more melodic form. Smooth jazz employs the following instruments: saxophone. guitar. piano. trumpet. synthesizer. electric bass. and drums. James Cotton was a famous blues harmonica player.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999994"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9427026"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9898739"], [" person", "0.9957047"], [" to", "1.0"], [" be", "0.9972294"], [" executed", "0.9948925"], [" in", "0.99991083"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.97958267"], [" first", "1.0"], [" person", "1.0"], [" executed", "0.71785945"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" John", "0.9905998"], [" Brown", "1.0"], [".", "0.6939236"], ["\n", "0.98996514"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"]]}, "169": {"qid": "95b91109c6228074725b", "term": "United Airlines", "description": "Airline in the United States", "question": "Are there tearjerkers about United Airlines flights?", "answer": true, "facts": ["Tearjerkers typically refer to a genre of movie. ", "United Airlines flight 93 was involved in a terrorist attack in 2001.", "Several flights memorialize the passengers of Flight 93,."], "decomposition": ["What do tearjerkers refer to?", "Which United Airlines flight was involved in a terrorist attack in 2001?", "Are there any #1 in memory of the passengers of #2?"], "evidence": [[[["Melodrama-1"], "no_evidence"], [["September 11 attacks-2"]], [["United 93 (film)-1"], "no_evidence", "operation"]], [[["Melodrama-1"]], [["American Airlines Flight 11-1", "American Airlines Flight 77-1", "United Airlines Flight 175-1", "United Airlines Flight 93-1"]], ["no_evidence", "operation"]], [[["Tearjerker-1"]], [["United Airlines Flight 811-29"]], [["United Airlines Flight 811-29"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there tearjerkers about the death of Osama bin Laden?\nFacts: Tearjerkers typically refer to a genre of movie.  Osama bin Laden was killed by a US Navy SEAL team in 2011.\nAnswer (True or False): True\nQuestion: Are there tearjerkers about the death of Princess", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are there tearjerkers about United Airlines flights?\nFacts: Tearjerkers typically refer to a genre of movie.  United Airlines flight 93 was involved in a terrorist attack in 2001. Several flights memorialize the passengers of Flight 93,.\nAnswer (True or False):", "ct_subject": "Tearjerkers typically refer to a genre of movie.  United Airlines flight 93 was involved in a terrorist attack in 2001. Several flights memorialize the passengers of Flight 93,.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.9999745"], [" there", "1.0"], [" tear", "0.98999935"], ["j", "1.0"], ["erk", "1.0"], ["ers", "1.0"], [" about", "1.0"], [" the", "1.0"], [" death", "0.94515765"], [" of", "1.0"], [" Osama", "0.99422693"], [" bin", "0.83310837"], [" Laden", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" T", "0.99999976"], ["ear", "1.0"], ["j", "1.0"], ["erk", "1.0"], ["ers", "1.0"], [" typically", "1.0"], [" refer", "1.0"], [" to", "1.0"], [" a", "1.0"], [" genre", "1.0"], [" of", "1.0"], [" movie", "1.0"], [".", "1.0"], [" ", "1.0"], [" Osama", "1.0"], [" bin", "1.0"], [" Laden", "1.0"], [" was", "1.0"], [" killed", "1.0"], [" by", "0.98717844"], [" a", "0.9999939"], [" US", "0.99751616"], [" Navy", "1.0"], [" SEAL", "1.0"], [" team", "1.0"], [" in", "0.9999913"], [" 2011", "0.99999523"], [".", "1.0"], ["\n", "0.99999976"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "1.0"], [" tear", "1.0"], ["j", "1.0"], ["erk", "1.0"], ["ers", "1.0"], [" about", "1.0"], [" the", "1.0"], [" death", "1.0"], [" of", "1.0"], [" Princess", "0.86096764"]]}, "170": {"qid": "f3a7a8fe3a00f075e871", "term": "Mercury (element)", "description": "Chemical element with atomic number 80", "question": "Does Mercury help detect coronavirus?", "answer": true, "facts": ["Mercury is used in thermometers", "Thermometers are used in taking body temperature", "High temperature or fever is one symptom of coronavirus"], "decomposition": ["What are the basic symptoms of coronavirus?", "Which instrument is used to measure a symptom among #1 that can be measured?", "Does a type of #2 use mercury?"], "evidence": [[[["Coronavirus-26"]], [["Fever-1", "Medical thermometer-1"]], [["Medical thermometer-24"]]], [[["Coronavirus-26"]], [["Thermometer-1"]], ["operation"]], [[["Human coronavirus NL63-4"]], [["Fever-7", "Medical thermometer-12"]], [["Thermometer-44"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Does the sun have a magnetic field?\nFacts: The sun has a magnetic field The sun has a magnetic field The sun has a magnetic field The sun has a magnetic field The sun has a magnetic field The sun has a magnetic field The sun has a magnetic field The sun has a magnetic field The sun has a magnetic field The sun has a magnetic field The sun has a magnetic field The sun has a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Mercury help detect coronavirus?\nFacts: Mercury is used in thermometers Thermometers are used in taking body temperature High temperature or fever is one symptom of coronavirus\nAnswer (True or False):", "ct_subject": "Mercury is used in thermometers Thermometers are used in taking body temperature High temperature or fever is one symptom of coronavirus", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Does", "0.43407756"], [" the", "0.9999999"], [" sun", "0.9993544"], [" have", "0.99467"], [" a", "1.0"], [" magnetic", "1.0"], [" field", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" sun", "1.0"], [" has", "0.9999999"], [" a", "1.0"], [" magnetic", "1.0"], [" field", "1.0"], [" The", "0.9999995"], [" sun", "1.0"], [" has", "0.9895202"], [" a", "1.0"], [" magnetic", "1.0"], [" field", "1.0"], [" The", "1.0"], [" sun", "1.0"], [" has", "1.0"], [" a", "1.0"], [" magnetic", "1.0"], [" field", "1.0"], [" The", "1.0"], [" sun", "1.0"], [" has", "1.0"], [" a", "1.0"], [" magnetic", "1.0"], [" field", "1.0"], [" The", "1.0"], [" sun", "1.0"], [" has", "1.0"], [" a", "1.0"], [" magnetic", "1.0"], [" field", "1.0"], [" The", "1.0"], [" sun", "1.0"], [" has", "1.0"], [" a", "1.0"], [" magnetic", "1.0"], [" field", "1.0"], [" The", "1.0"], [" sun", "1.0"], [" has", "1.0"], [" a", "1.0"], [" magnetic", "1.0"], [" field", "1.0"], [" The", "1.0"], [" sun", "1.0"], [" has", "1.0"], [" a", "1.0"], [" magnetic", "1.0"], [" field", "1.0"], [" The", "1.0"], [" sun", "1.0"], [" has", "1.0"], [" a", "1.0"], [" magnetic", "1.0"], [" field", "1.0"], [" The", "1.0"], [" sun", "1.0"], [" has", "1.0"], [" a", "1.0"], [" magnetic", "1.0"], [" field", "1.0"], [" The", "1.0"], [" sun", "1.0"], [" has", "1.0"], [" a", "1.0"], [" magnetic", "1.0"], [" field", "1.0"], [" The", "1.0"], [" sun", "1.0"], [" has", "1.0"], [" a", "1.0"]]}, "171": {"qid": "aef16532a9009fd318e2", "term": "The Jackson 5", "description": "American pop music family group", "question": "Was The Jackson 5 bigger family band than The Isley Brothers?", "answer": true, "facts": ["The Jackson 5 has sold over 100 million albums worldwide.", "The Eisley Brothers have sold over 18 million albums.", "The Jackson 5 consisted of Jackie, Tito, Jermaine, Marlon and Michael.", "The Isley Brothers consisted of brothers O'Kelly, Rudolph. Ronald, and Vernon."], "decomposition": ["How many albums has the Jackson 5 sold?", "How many albums has the Eisley Brothers sold?", "How many people were in the Jackson 5?", "How many people made up the Eisley Brothers?", "Is #1 greater than #2 and is #3 greater than #4?"], "evidence": [[[["The Jackson 5-4"]], [["The Isley Brothers-5"]], [["Ronnie Rancifer-1"]], [["The Isley Brothers-4"]], ["operation"]], [[["The Jackson 5-4"]], [["The Isley Brothers-5"]], [["The Jackson 5-1"]], [["The Isley Brothers-1"]], ["operation"]], [[["The Jackson 5-4"]], [["The Isley Brothers-5"]], [["The Jackson 5-1"]], [["The Isley Brothers-4"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Beatles have a secret song that was never released?\nFacts: The Beatles had a secret song that was never released. The song was called \"I Want to Hold Your Hand\" and was written by", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was The Jackson 5 bigger family band than The Isley Brothers?\nFacts: The Jackson 5 has sold over 100 million albums worldwide. The Eisley Brothers have sold over 18 million albums. The Jackson 5 consisted of Jackie, Tito, Jermaine, Marlon and Michael. The Isley Brothers consisted of brothers O'Kelly, Rudolph. Ronald, and Vernon.\nAnswer (True or False):", "ct_subject": "The Jackson 5 has sold over 100 million albums worldwide. The Eisley Brothers have sold over 18 million albums. The Jackson 5 consisted of Jackie, Tito, Jermaine, Marlon and Michael. The Isley Brothers consisted of brothers O'Kelly, Rudolph. Ronald, and Vernon.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.60396516"], [" the", "1.0"], [" Beatles", "1.0"], [" have", "1.0"], [" a", "0.9999999"], [" secret", "0.999987"], [" song", "1.0"], [" that", "0.9995364"], [" was", "0.99920744"], [" never", "1.0"], [" released", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Beatles", "1.0"], [" had", "0.997251"], [" a", "1.0"], [" secret", "0.975449"], [" song", "1.0"], [" that", "1.0"], [" was", "1.0"], [" never", "1.0"], [" released", "1.0"], [".", "1.0"], [" The", "0.9999994"], [" song", "1.0"], [" was", "1.0"], [" called", "1.0"], [" \"", "1.0"], ["I", "0.9991861"], [" Want", "1.0"], [" to", "0.9999974"], [" Hold", "1.0"], [" Your", "1.0"], [" Hand", "1.0"], ["\"", "0.96058035"], [" and", "1.0"], [" was", "0.9999999"], [" written", "1.0"], [" by", "1.0"]]}, "172": {"qid": "25a088d9d2ce674e639a", "term": "Grapefruit", "description": "citrus fruit", "question": "Can eating grapefruit kill besides allergies or choking?", "answer": true, "facts": ["Grapefruit is a citrus fruit consumed mostly during the summer months.", "Chemicals in grapefruit can interact with medications such as statins.", "Grapefruit can lead to too much absorption of statin medicine.", "Too much statins can cause severe muscle pain, liver damage, kidney failure and death. "], "decomposition": ["What health risks associated with eating grapefruit could lead to death?", "Is #1 more than just allergy and choking?"], "evidence": [[[["Grapefruit-16"]], [["Grapefruit-16"]]], [[["Grapefruit-18"]], [["Drug overdose-1"]]], [[["Grapefruit\u2013drug interactions-3"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Can you get a cold from a cold sore?\nFacts: Cold sores are caused by a virus. The virus is transmitted by the saliva of an infected person. The virus is not contagious. The virus is not spread by", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can eating grapefruit kill besides allergies or choking?\nFacts: Grapefruit is a citrus fruit consumed mostly during the summer months. Chemicals in grapefruit can interact with medications such as statins. Grapefruit can lead to too much absorption of statin medicine. Too much statins can cause severe muscle pain, liver damage, kidney failure and death. \nAnswer (True or False):", "ct_subject": "Grapefruit is a citrus fruit consumed mostly during the summer months. Chemicals in grapefruit can interact with medications such as statins. Grapefruit can lead to too much absorption of statin medicine. Too much statins can cause severe muscle pain, liver damage, kidney failure and death. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Can", "0.33261383"], [" you", "0.9998171"], [" get", "1.0"], [" a", "0.9995859"], [" cold", "0.99858224"], [" from", "1.0"], [" a", "0.9885584"], [" cold", "0.9999908"], [" sore", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Cold", "0.8851553"], [" s", "1.0"], ["ores", "1.0"], [" are", "1.0"], [" caused", "1.0"], [" by", "1.0"], [" a", "0.93692374"], [" virus", "1.0"], [".", "0.9999622"], [" The", "0.99992085"], [" virus", "1.0"], [" is", "0.9999999"], [" transmitted", "0.015954047"], [" by", "0.9999939"], [" the", "0.9990639"], [" saliva", "1.0"], [" of", "1.0"], [" an", "0.9999881"], [" infected", "1.0"], [" person", "1.0"], [".", "1.0"], [" The", "0.99923515"], [" virus", "1.0"], [" is", "0.83963746"], [" not", "0.9999958"], [" contagious", "0.9556263"], [".", "0.9977968"], [" The", "0.75442576"], [" virus", "1.0"], [" is", "0.932785"], [" not", "0.99999785"], [" spread", "0.9999995"], [" by", "0.99986506"]]}, "173": {"qid": "8b41a17e65ef7f6e22b5", "term": "Advertising", "description": "Form of communication for marketing, typically paid for", "question": "During the pandemic, is door to door advertising considered inconsiderate?", "answer": true, "facts": ["Door to door advertising involves someone going to several homes in a residential area to make sales and leave informational packets.", "During the COVID-19 pandemic, the CDC recommends that people limit their travel to essential needs only.", "During the COVID-19 pandemic, citizens are advised to stay home and to limit their interaction with others.", "During the COVID-19 pandemic, people are encouraged to remain six feet away from each other at all times.", "The more people that someone interacts with, the higher the likelihood of them becoming a vector for the COVID-19 virus."], "decomposition": ["What does door to door advertising involve a person to do?", "During the COVID-19 pandemic, what does the CDC advise people to do in terms of traveling?", "During the COVID-19 pandemic, what does the CDC advise people to do in terms of interaction with others?", "Does doing #1 go against #2 and #3?"], "evidence": [[[["Door-to-door-1"]], [["Cloth face mask-12"]], ["no_evidence"], ["operation"]], [[["Door-to-door-1"]], [["Coronavirus recession-13"]], [["Social distancing-30"]], ["operation"]], [[["Door-to-door-1"]], [["Stay-at-home order-18"], "no_evidence"], [["Social distancing-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: During the pandemic, is door to door advertising considered inconsiderate?\nFacts: Door to door advertising involves someone going to several homes in a residential area to make sales and leave informational packets. During the COVID-19 pandemic, the CDC recommends that people limit their travel to essential needs only. During the COVID-19 pandemic, citizens are advised to stay home and to limit their interaction with others. During the COVID-19 pandemic, people are encouraged to remain six feet away from each other at all times. The more people that someone interacts with, the higher the likelihood of them becoming a vector for the COVID-19 virus.\nAnswer (True or False):", "ct_subject": "Door to door advertising involves someone going to several homes in a residential area to make sales and leave informational packets. During the COVID-19 pandemic, the CDC recommends that people limit their travel to essential needs only. During the COVID-19 pandemic, citizens are advised to stay home and to limit their interaction with others. During the COVID-19 pandemic, people are encouraged to remain six feet away from each other at all times. The more people that someone interacts with, the higher the likelihood of them becoming a vector for the COVID-19 virus.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"]]}, "174": {"qid": "09348bfb96097f3b4a99", "term": "Mickey Mouse", "description": "Disney cartoon character", "question": "Is Mickey Mouse hypothetically unlikely to make a purchase at Zazzle?", "answer": true, "facts": ["Mickey Mouse is a Disney character that has starred in numerous movies and TV specials.", "Mickey Mouse wears a pair of red pants and never wears a shirt.", "Zazzle is a website that specializes in custom T-shirts."], "decomposition": ["What clothing pieces does Micky Mouse typically wear?", "What clothing pieces does Zazzle specialize in?", "Is there no overlap between #1 and #2?"], "evidence": [[[["Mickey Mouse-49"]], [["Zazzle-1"], "no_evidence"], ["operation"]], [[["Mickey Mouse-1"]], [["Zazzle-3"]], [["Mickey Mouse-1", "Zazzle-1"]]], [[["Mickey Mouse-49"]], [["Zazzle-1"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city of New York City spelled with an \"e\" or an \"i\"?\nFacts: The name of the city of New York City is spelled with an \"e\".\nAnswer (True or False): True\nQuestion: Is the name of the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Mickey Mouse hypothetically unlikely to make a purchase at Zazzle?\nFacts: Mickey Mouse is a Disney character that has starred in numerous movies and TV specials. Mickey Mouse wears a pair of red pants and never wears a shirt. Zazzle is a website that specializes in custom T-shirts.\nAnswer (True or False):", "ct_subject": "Mickey Mouse is a Disney character that has starred in numerous movies and TV specials. Mickey Mouse wears a pair of red pants and never wears a shirt. Zazzle is a website that specializes in custom T-shirts.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.999423"], [" the", "0.4023375"], [" name", "0.89406425"], [" of", "0.9999995"], [" the", "1.0"], [" city", "0.9999987"], [" of", "0.999992"], [" New", "0.99921584"], [" York", "1.0"], [" City", "1.0"], [" spelled", "0.20635049"], [" with", "0.99943656"], [" an", "0.9971591"], [" \"", "1.0"], ["e", "0.99875164"], ["\"", "1.0"], [" or", "1.0"], [" an", "0.9912538"], [" \"", "1.0"], ["i", "0.9999999"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" name", "0.99618775"], [" of", "1.0"], [" the", "0.9998553"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" City", "0.9999999"], [" is", "1.0"], [" spelled", "1.0"], [" with", "1.0"], [" an", "1.0"], [" \"", "1.0"], ["e", "1.0"], ["\".", "0.9144365"], ["\n", "0.9999999"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"]]}, "175": {"qid": "d0ad95f36e42f85cc94d", "term": "Linus Torvalds", "description": "Creator and lead developer of Linux kernel", "question": "Is Maruti Suzuki Baleno an efficient car for Linus Torvald's family?", "answer": true, "facts": ["Linus Torvald has a family consisting of five people including his wife and children.", "The Maruti Suzuki Baleno is and Indian car that can seat five people."], "decomposition": ["How many people can sit in a  Maruti Suzuki Baleno?", "How many people are in Linus Torvald's family?", "Is #1 at least equal or greater than #2?"], "evidence": [[[["Suzuki Baleno (2015)-4"]], [["Linus Torvalds-20"]], ["operation"]], [[["Suzuki Baleno (2015)-14"], "no_evidence"], [["Linus Torvalds-20"]], ["no_evidence", "operation"]], [["no_evidence"], [["Linus Torvalds-20"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city of New York City derived from the name of the city of New York?\nFacts: The name of the city of New York City is derived from the name of the city of New York.\nAnswer (True or False): True\nQuestion: Is the name of the city of", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Maruti Suzuki Baleno an efficient car for Linus Torvald's family?\nFacts: Linus Torvald has a family consisting of five people including his wife and children. The Maruti Suzuki Baleno is and Indian car that can seat five people.\nAnswer (True or False):", "ct_subject": "Linus Torvald has a family consisting of five people including his wife and children. The Maruti Suzuki Baleno is and Indian car that can seat five people.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99999607"], [" the", "0.9999999"], [" name", "0.999995"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9979874"], [" of", "1.0"], [" New", "0.9985275"], [" York", "1.0"], [" City", "1.0"], [" derived", "0.98935026"], [" from", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "0.99999666"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99999857"], [" name", "1.0"], [" of", "1.0"], [" the", "0.9999608"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" City", "0.9992969"], [" is", "1.0"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [".", "1.0"], ["\n", "0.9999926"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"]]}, "176": {"qid": "9422300529f87f9917bf", "term": "Benjamin Franklin", "description": "American polymath and a Founding Father of the United States", "question": "Is Benjamin Franklin a prime candidate to have his statues removed by Black Lives Matter movement?", "answer": true, "facts": ["The Black Lives Matter movement is a social movement advocating for racial equality.", "Benjamin Franklin, a famous founding father, has his image on many monuments and on American currency.", "Members of the Black Lives Matter movement petitioned for statues of Christopher Columbus to be removed due to his subjugation of Native Americans.", "Benjamin Franklin's 1730s newspaper, The Philadelphia Gazette, posted ads for black slaves.", "Benjamin Franklin owned two slaves, George and King, who worked as personal servants."], "decomposition": ["What social issue motivates the Black Lives Matter movement?", "Did Benjamin Franklin act against achieving #1?"], "evidence": [[[["Black Lives Matter-10"]], [["Benjamin Franklin-135", "Benjamin Franklin-136"]]], [[["Black Lives Matter-14"], "no_evidence"], [["Benjamin Franklin-5"], "operation"]], [[["Black Lives Matter-10"]], [["Benjamin Franklin-5"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the Statue of Liberty a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Benjamin Franklin a prime candidate to have his statues removed by Black Lives Matter movement?\nFacts: The Black Lives Matter movement is a social movement advocating for racial equality. Benjamin Franklin, a famous founding father, has his image on many monuments and on American currency. Members of the Black Lives Matter movement petitioned for statues of Christopher Columbus to be removed due to his subjugation of Native Americans. Benjamin Franklin's 1730s newspaper, The Philadelphia Gazette, posted ads for black slaves. Benjamin Franklin owned two slaves, George and King, who worked as personal servants.\nAnswer (True or False):", "ct_subject": "The Black Lives Matter movement is a social movement advocating for racial equality. Benjamin Franklin, a famous founding father, has his image on many monuments and on American currency. Members of the Black Lives Matter movement petitioned for statues of Christopher Columbus to be removed due to his subjugation of Native Americans. Benjamin Franklin's 1730s newspaper, The Philadelphia Gazette, posted ads for black slaves. Benjamin Franklin owned two slaves, George and King, who worked as personal servants.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999995"], [" the", "0.99999964"], [" Statue", "0.85584337"], [" of", "1.0"], [" Liberty", "1.0"], [" a", "1.0"]]}, "177": {"qid": "99e34b51538c03fcd8bb", "term": "Fiat Chrysler Automobiles", "description": "Multinational automotive manufacturing conglomerate", "question": "Is Fiat Chrysler gaining a new overall corporate identity?", "answer": true, "facts": ["The company is renaming itself Stellantis following the completion of its merger.", "There are 14 automobile brands owned by the company, which will be keeping their names and logos."], "decomposition": ["What plans are underway as regards naming after the completion of the Fiat Chrysler merger?", "Does #1 involve a change of the collective corporate identity?"], "evidence": [[[["Fiat Chrysler Automobiles-37"], "no_evidence"], [["Corporate identity-2"], "operation"]], [[["Fiat Chrysler Automobiles-1"]], [["Fiat Chrysler Automobiles-1"], "no_evidence"]], [[["Groupe PSA-23"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the U.S. government planning to build a wall along the Mexican border?\nFacts: The U.S. government has not announced any plans to build a wall along the Mexican border.\nAnswer (True or False): True\nQuestion: Is the U.S. government planning to build a wall along the Mexican border?\nF", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Fiat Chrysler gaining a new overall corporate identity?\nFacts: The company is renaming itself Stellantis following the completion of its merger. There are 14 automobile brands owned by the company, which will be keeping their names and logos.\nAnswer (True or False):", "ct_subject": "The company is renaming itself Stellantis following the completion of its merger. There are 14 automobile brands owned by the company, which will be keeping their names and logos.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9980122"], [" the", "0.99999523"], [" U", "0.1023204"], [".", "1.0"], ["S", "1.0"], [".", "1.0"], [" government", "0.99999964"], [" planning", "0.9999248"], [" to", "1.0"], [" build", "0.9999993"], [" a", "1.0"], [" wall", "0.39102072"], [" along", "1.0"], [" the", "1.0"], [" Mexican", "0.99923337"], [" border", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" U", "0.9926339"], [".", "1.0"], ["S", "1.0"], [".", "1.0"], [" government", "1.0"], [" has", "0.9999988"], [" not", "0.99232626"], [" announced", "0.9999999"], [" any", "0.9999958"], [" plans", "1.0"], [" to", "1.0"], [" build", "1.0"], [" a", "1.0"], [" wall", "1.0"], [" along", "1.0"], [" the", "1.0"], [" Mexican", "1.0"], [" border", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" U", "1.0"], [".", "1.0"], ["S", "1.0"], [".", "1.0"], [" government", "1.0"], [" planning", "1.0"], [" to", "1.0"], [" build", "1.0"], [" a", "1.0"], [" wall", "1.0"], [" along", "1.0"], [" the", "1.0"], [" Mexican", "1.0"], [" border", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"]]}, "178": {"qid": "72c3d20cf35ba7643311", "term": "The Jungle Book", "description": "1894 children's book by Rudyard Kipling", "question": "Did Disney get most of Rudyard Kipling's The Jungle Book profits?", "answer": true, "facts": ["Rudyard Kipling's 1894 book The Jungle Book has was adapted into several Disney films.", "The 2016 film The Jungle Book grossed over 966 million dollars.", "Disney's 1967 film version of The Jungle Book grossed over 378 million dollars.", "The Jungle Book was not a worldwide phenomenon in Kipling's time."], "decomposition": ["When did Rudyard Kipling write \"The Jungle Book\"?", "The 1967 and 2016 adaptations of the book was produced by which media company?", "When did #2 produce these movies?", "Is #3 several decades after #1 and #2 the same as Disney?"], "evidence": [[[["Rudyard Kipling-2"]], [["The Jungle Book (2016 film)-1"]], [["The Jungle Book (2016 film)-1"]], ["operation"]], [[["The Jungle Book-1"]], [["The Jungle Book (franchise)-1"], "no_evidence"], [["The Jungle Book (franchise)-1"]], ["no_evidence", "operation"]], [[["The Jungle Book-1"]], [["The Jungle Book-26"]], [["Adventures of Mowgli-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the first person to walk on the moon walk on the moon?\nFacts: The first person to walk on the moon was Neil Armstrong. He was the first person to walk on", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Disney get most of Rudyard Kipling's The Jungle Book profits?\nFacts: Rudyard Kipling's 1894 book The Jungle Book has was adapted into several Disney films. The 2016 film The Jungle Book grossed over 966 million dollars. Disney's 1967 film version of The Jungle Book grossed over 378 million dollars. The Jungle Book was not a worldwide phenomenon in Kipling's time.\nAnswer (True or False):", "ct_subject": "Rudyard Kipling's 1894 book The Jungle Book has was adapted into several Disney films. The 2016 film The Jungle Book grossed over 966 million dollars. Disney's 1967 film version of The Jungle Book grossed over 378 million dollars. The Jungle Book was not a worldwide phenomenon in Kipling's time.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9925828"], [" the", "1.0"], [" first", "0.9880695"], [" person", "0.9993082"], [" to", "1.0"], [" walk", "0.091271296"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "0.9999957"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "0.99998593"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" was", "0.9999999"], [" Neil", "1.0"], [" Armstrong", "1.0"], [".", "1.0"], [" He", "0.94738895"], [" was", "0.24554166"], [" the", "1.0"], [" first", "1.0"], [" person", "0.98678315"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"]]}, "179": {"qid": "fa1e09fee5546d5b7b65", "term": "Kelly Clarkson", "description": "American singer-songwriter, actress, and television personality", "question": "Would Kelly Clarkson's voice shake glass?", "answer": true, "facts": ["Glass vibrates at its resonant frequency which is around a middle C note.", "Kelly Clarkson has an impressive three octave vocal range.", "Kelly Clarkson's Never Enough is in the key of A-flat.", "A-flat is above middle C in terms of notes."], "decomposition": ["At what note would glass start to vibrate?", "In Kelly Clarkson's song Never Enough, what key is the song sung in?", "Is #2 above #1?"], "evidence": [[[["Acoustic resonance-51"], "no_evidence"], [["Never Again (Kelly Clarkson song)-5"]], ["operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Crystallophone-2", "Resonance-1", "Resonance-8"], "no_evidence"], [["Kelly Clarkson-9"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"C\" and a \"D\"?\nFacts: A \"C\" is a \"C\" and a \"D\" is a \"D\".\nAnswer (True or False): True\nQuestion: What is the difference between a \"", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Kelly Clarkson's voice shake glass?\nFacts: Glass vibrates at its resonant frequency which is around a middle C note. Kelly Clarkson has an impressive three octave vocal range. Kelly Clarkson's Never Enough is in the key of A-flat. A-flat is above middle C in terms of notes.\nAnswer (True or False):", "ct_subject": "Glass vibrates at its resonant frequency which is around a middle C note. Kelly Clarkson has an impressive three octave vocal range. Kelly Clarkson's Never Enough is in the key of A-flat. A-flat is above middle C in terms of notes.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99996126"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9995121"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.9480667"], ["C", "0.74024165"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["D", "0.99667263"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.95201975"], [" \"", "0.6139167"], ["C", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "0.99981946"], [" \"", "0.983732"], ["C", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["D", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["D", "1.0"], ["\".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"]]}, "180": {"qid": "cd43ae9cd3bb64c45247", "term": "QWERTY", "description": "keyboard layout where the first line is \"QWERTYUIOP\"", "question": "Did the Qwerty keyboard layout predate computers?", "answer": true, "facts": ["The Qwerty layout was originally developed for mechanical typewriters in the 1870s.", "ENIAC was considered to be the first computer, built in the late 1940s."], "decomposition": ["When was the QWERTY keyboard layout developed?", "When was the modern computer invented?", "Is #1 before #2?"], "evidence": [[[["QWERTY-1"]], [["Computer-3", "QWERTY-20"]], ["operation"]], [[["QWERTY-1"]], [["Computer-25", "Computer-26"]], ["operation"]], [[["QWERTY-8"]], [["Computer-23"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance between two points on Earth?\nFacts: The distance between the Earth and the Sun is about 93 million miles. The distance between the Earth and the Moon is about 238,000 miles.\nAnswer (True or False): True\nQuestion: What is the longest distance between two points on the Moon?\nF", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did the Qwerty keyboard layout predate computers?\nFacts: The Qwerty layout was originally developed for mechanical typewriters in the 1870s. ENIAC was considered to be the first computer, built in the late 1940s.\nAnswer (True or False):", "ct_subject": "The Qwerty layout was originally developed for mechanical typewriters in the 1870s. ENIAC was considered to be the first computer, built in the late 1940s.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.70473015"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.6269212"], [" distance", "0.76447976"], [" between", "0.99999404"], [" two", "1.0"], [" points", "1.0"], [" on", "0.9999999"], [" Earth", "0.99998987"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" distance", "0.9994473"], [" between", "1.0"], [" the", "1.0"], [" Earth", "1.0"], [" and", "1.0"], [" the", "1.0"], [" Sun", "0.9999988"], [" is", "1.0"], [" about", "1.0"], [" 93", "1.0"], [" million", "1.0"], [" miles", "1.0"], [".", "1.0"], [" The", "0.9793895"], [" distance", "1.0"], [" between", "1.0"], [" the", "1.0"], [" Earth", "1.0"], [" and", "1.0"], [" the", "0.999961"], [" Moon", "0.9999999"], [" is", "1.0"], [" about", "1.0"], [" 238", "1.0"], [",", "1.0"], ["000", "1.0"], [" miles", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.9999999"], [" distance", "1.0"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"], [" on", "1.0"], [" the", "1.0"], [" Moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"]]}, "181": {"qid": "bdc60c16ac8d47491464", "term": "Spider-Man", "description": "Fictional Marvel superhero", "question": "Did Spiderman fight against Falcon in the MCU?", "answer": true, "facts": ["In Captain America: Civil War, Iron Man and Captain America became enemies following a disagreement.", "Iron Man summoned Spiderman to fight with his team of still-loyal Avengers.", "Falcon was one of Captain America's best friends and supported the Captain in the conflict.", "Therefore, Spiderman and Falcon were on opposite teams during the inter-Avenger battle in the movie."], "decomposition": ["In the marvel movie Captain America: Civil War, which factions were the avengers divided into?", "Were Spiderman and Falcon on opposing sides of #1?"], "evidence": [[[["Captain America: Civil War-1"]], ["no_evidence"]], [[["Captain America: The Winter Soldier-1"]], [["Peter Parker (Marvel Cinematic Universe)-7", "The Falcon and the Winter Soldier-5"], "operation"]], [[["Captain America: Civil War-1"]], [["Falcon (comics)-38", "Spider-Man-27"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the character that was killed in the movie?\nFacts: The character that was killed in the movie was the villainous Baron", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Spiderman fight against Falcon in the MCU?\nFacts: In Captain America: Civil War, Iron Man and Captain America became enemies following a disagreement. Iron Man summoned Spiderman to fight with his team of still-loyal Avengers. Falcon was one of Captain America's best friends and supported the Captain in the conflict. Therefore, Spiderman and Falcon were on opposite teams during the inter-Avenger battle in the movie.\nAnswer (True or False):", "ct_subject": "In Captain America: Civil War, Iron Man and Captain America became enemies following a disagreement. Iron Man summoned Spiderman to fight with his team of still-loyal Avengers. Falcon was one of Captain America's best friends and supported the Captain in the conflict. Therefore, Spiderman and Falcon were on opposite teams during the inter-Avenger battle in the movie.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.82923186"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" character", "0.9999871"], [" that", "0.99401075"], [" was", "0.998711"], [" killed", "0.9998035"], [" in", "0.99945885"], [" the", "1.0"], [" movie", "0.99994814"], ["?", "0.9999951"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9998172"], [" character", "1.0"], [" that", "1.0"], [" was", "1.0"], [" killed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" was", "0.99999917"], [" the", "0.99921656"], [" villain", "0.9998641"], ["ous", "0.5880238"], [" Baron", "0.8572289"]]}, "182": {"qid": "fa472b6db8dcec9abcff", "term": "Torso", "description": "the central part of the living body", "question": "Will the torso be safe from blows to the largest and smallest bones in body?", "answer": true, "facts": ["The three smallest bone in the body are malleus, incus, and stapes.", "Malleus, incus, and stapes are located in the ear.", "The femur is the largest bone in the body.", "The femur is located in the leg.", "The torso is located in the center of the body."], "decomposition": ["Which part of the human body is the torso?", "Which is the largest and smallest bone in the human body?", "Where are #2 located?", "Is any of #3 part of #1?"], "evidence": [[[["Torso-1"]], [["Femur-1", "Stapes-1"]], [["Human leg-1", "Stapes-1"]], [["Abdomen-1", "Perineum-1", "Thorax-1"], "operation"]], [[["Torso-1"]], [["Bone-3"]], [["Femur-7"]], [["Femur-7"], "operation"]], [[["Torso-1"]], [["Bone-3"]], [["Thigh-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the largest bone in the body?\nFacts: The largest bone in the body is the femur. The femur is located in the leg. The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Will the torso be safe from blows to the largest and smallest bones in body?\nFacts: The three smallest bone in the body are malleus, incus, and stapes. Malleus, incus, and stapes are located in the ear. The femur is the largest bone in the body. The femur is located in the leg. The torso is located in the center of the body.\nAnswer (True or False):", "ct_subject": "The three smallest bone in the body are malleus, incus, and stapes. Malleus, incus, and stapes are located in the ear. The femur is the largest bone in the body. The femur is located in the leg. The torso is located in the center of the body.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9501589"], [" is", "1.0"], [" the", "1.0"], [" largest", "0.5036321"], [" bone", "0.9866679"], [" in", "1.0"], [" the", "1.0"], [" body", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" largest", "1.0"], [" bone", "1.0"], [" in", "1.0"], [" the", "1.0"], [" body", "1.0"], [" is", "1.0"], [" the", "1.0"], [" fem", "1.0"], ["ur", "1.0"], [".", "1.0"], [" The", "1.0"], [" fem", "1.0"], ["ur", "1.0"], [" is", "1.0"], [" located", "0.9999999"], [" in", "1.0"], [" the", "1.0"], [" leg", "1.0"], [".", "1.0"], [" The", "1.0"]]}, "183": {"qid": "8d708927b1593d8b9b42", "term": "Woodrow Wilson", "description": "28th president of the United States", "question": "Would Woodrow Wilson support Plessy v. Ferguson decision?", "answer": true, "facts": ["Plessy v Ferguson was a landmark case that stated segregation did not violate the constitution.", "President Woodrow Wilson escalated the discriminatory hiring policies and segregation of government offices.", "By the end of 1913, under President Wilson, many departments, including the navy, had segregated work spaces, restrooms, and cafeterias."], "decomposition": ["What was the topic of Plessy v. Ferguson?", "Does Woodrow Wilson's veiws on #1 agree more with Plessy or Ferguson?", "Who did the court rule in favor of in Plessy v. Ferguson?", "Are #2 and #3 the same?"], "evidence": [[[["Plessy v. Ferguson-1"]], [["Woodrow Wilson-77"]], [["Plessy v. Ferguson-3"]], ["operation"]], [[["Plessy v. Ferguson-2"]], [["Woodrow Wilson-77", "Woodrow Wilson-78"], "no_evidence"], [["Plessy v. Ferguson-1"]], ["no_evidence", "operation"]], [[["Plessy v. Ferguson-9"]], [["Woodrow Wilson-78"]], [["Plessy v. Ferguson-7"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"white\" and a \"black\" person?\nFacts: A \"white\" person is a person who is born with white skin. A \"black\" person is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Woodrow Wilson support Plessy v. Ferguson decision?\nFacts: Plessy v Ferguson was a landmark case that stated segregation did not violate the constitution. President Woodrow Wilson escalated the discriminatory hiring policies and segregation of government offices. By the end of 1913, under President Wilson, many departments, including the navy, had segregated work spaces, restrooms, and cafeterias.\nAnswer (True or False):", "ct_subject": "Plessy v Ferguson was a landmark case that stated segregation did not violate the constitution. President Woodrow Wilson escalated the discriminatory hiring policies and segregation of government offices. By the end of 1913, under President Wilson, many departments, including the navy, had segregated work spaces, restrooms, and cafeterias.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9998809"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.89684534"], [" between", "1.0"], [" a", "0.99898"], [" \"", "1.0"], ["white", "0.33870718"], ["\"", "1.0"], [" and", "1.0"], [" a", "0.99999654"], [" \"", "1.0"], ["black", "1.0"], ["\"", "1.0"], [" person", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9996669"], [" \"", "0.9931276"], ["white", "1.0"], ["\"", "1.0"], [" person", "1.0"], [" is", "1.0"], [" a", "0.9989685"], [" person", "1.0"], [" who", "0.89086986"], [" is", "0.99999976"], [" born", "0.99996793"], [" with", "0.99998593"], [" white", "0.9980305"], [" skin", "1.0"], [".", "0.9999182"], [" A", "1.0"], [" \"", "1.0"], ["black", "1.0"], ["\"", "1.0"], [" person", "1.0"], [" is", "1.0"]]}, "184": {"qid": "b747938f597b09e43603", "term": "Go (game)", "description": "Abstract strategy board game for two players", "question": "Did origin dynasty of Go precede Jia Sidao?", "answer": true, "facts": ["Go is a strategy game that originated in the Zhou dynasty.", "The Zhou dynasty lasted from 1046 BC \u2013 256 BC.", "Jia Sidao was a chancellor during the late Song dynasty.", "The Song dynasty started in 960 AD and lasted until 1279 AD."], "decomposition": ["During which Chinese dynasty did the game Go originate?", "Which Chinese dynasty was Jia Sidao a part of during his lifetime?", "Did #1 precede #2?"], "evidence": [[[["Go (game)-1"]], [["Jia Sidao-1"]], ["operation"]], [[["History of Go-9"]], [["Jia Sidao-1"]], [["Government of the Han dynasty-1", "Song dynasty-1"]]], [[["History of Go-4"]], [["Jia Sidao-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Chinese emperor, Qin Shi Huang, have a son named Qin Shi Huang?\nFacts: Qin Shi Huang was the first emperor of the Qin dynasty. He was the first emperor of the Qin dynasty. He was the first emperor of the Qin", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did origin dynasty of Go precede Jia Sidao?\nFacts: Go is a strategy game that originated in the Zhou dynasty. The Zhou dynasty lasted from 1046 BC \u2013 256 BC. Jia Sidao was a chancellor during the late Song dynasty. The Song dynasty started in 960 AD and lasted until 1279 AD.\nAnswer (True or False):", "ct_subject": "Go is a strategy game that originated in the Zhou dynasty. The Zhou dynasty lasted from 1046 BC \u2013 256 BC. Jia Sidao was a chancellor during the late Song dynasty. The Song dynasty started in 960 AD and lasted until 1279 AD.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9918041"], [" the", "1.0"], [" Chinese", "0.9997111"], [" emperor", "0.9987953"], [",", "0.9948133"], [" Qin", "1.0"], [" Shi", "1.0"], [" Huang", "1.0"], [",", "1.0"], [" have", "1.0"], [" a", "1.0"], [" son", "1.0"], [" named", "1.0"], [" Qin", "1.0"], [" Shi", "1.0"], [" Huang", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Qin", "0.20670883"], [" Shi", "1.0"], [" Huang", "1.0"], [" was", "1.0"], [" the", "1.0"], [" first", "0.9999362"], [" emperor", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Qin", "1.0"], [" dynasty", "1.0"], [".", "1.0"], [" He", "1.0"], [" was", "0.999908"], [" the", "0.88827974"], [" first", "0.933334"], [" emperor", "1.0"], [" of", "0.75932854"], [" the", "1.0"], [" Qin", "1.0"], [" dynasty", "1.0"], [".", "0.99959713"], [" He", "1.0"], [" was", "1.0"], [" the", "1.0"], [" first", "1.0"], [" emperor", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Qin", "1.0"]]}, "185": {"qid": "db7ceb3da1361288eeb2", "term": "Star Wars", "description": "Epic science fantasy space opera franchise", "question": "Are there multiple Star Wars TV shows produced by Disney?", "answer": true, "facts": ["Star Wars Rebels and Star Wars Resistance were released after Disney bought ownership of Star Wars.", "Disney also produced the first live-action TV show set in the Star Wars galaxy, The Mandalorian.", "Disney produced one additional revival season of Star Wars Clone Wars which was originally produced before Disney owned Star Wars."], "decomposition": ["When did Disney acquire Star Wars?", "How many Star Wars TV shows have been produced since #1?", "Is #2 greater than one?"], "evidence": [[[["The Walt Disney Company-38"]], [["Star Wars Resistance-2", "The Mandalorian-2"]], ["operation"]], [[["The Walt Disney Company-38"]], [["Star Wars Rebels-2", "Star Wars Resistance-2"]], ["operation"]], [[["Star Wars-2"]], [["Star Wars-23"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there multiple Star Wars movies released each year?\nFacts: There are currently three Star Wars movies released each year. The first two Star Wars movies were released in 1977 and 1983. The third Star Wars movie was released in", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are there multiple Star Wars TV shows produced by Disney?\nFacts: Star Wars Rebels and Star Wars Resistance were released after Disney bought ownership of Star Wars. Disney also produced the first live-action TV show set in the Star Wars galaxy, The Mandalorian. Disney produced one additional revival season of Star Wars Clone Wars which was originally produced before Disney owned Star Wars.\nAnswer (True or False):", "ct_subject": "Star Wars Rebels and Star Wars Resistance were released after Disney bought ownership of Star Wars. Disney also produced the first live-action TV show set in the Star Wars galaxy, The Mandalorian. Disney produced one additional revival season of Star Wars Clone Wars which was originally produced before Disney owned Star Wars.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.9973484"], [" there", "1.0"], [" multiple", "0.9999788"], [" Star", "1.0"], [" Wars", "0.9999999"], [" movies", "1.0"], [" released", "0.9783821"], [" each", "0.9614652"], [" year", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "0.9265634"], [" are", "1.0"], [" currently", "0.99999166"], [" three", "0.9800239"], [" Star", "1.0"], [" Wars", "1.0"], [" movies", "1.0"], [" released", "1.0"], [" each", "1.0"], [" year", "1.0"], [".", "0.99999976"], [" The", "0.9999697"], [" first", "1.0"], [" two", "0.814626"], [" Star", "0.99862266"], [" Wars", "1.0"], [" movies", "1.0"], [" were", "1.0"], [" released", "1.0"], [" in", "1.0"], [" 1977", "1.0"], [" and", "1.0"], [" 1983", "0.0018073192"], [".", "0.87135005"], [" The", "1.0"], [" third", "1.0"], [" Star", "0.9997545"], [" Wars", "1.0"], [" movie", "1.0"], [" was", "0.999987"], [" released", "1.0"], [" in", "1.0"]]}, "186": {"qid": "25cb5d3136c997326121", "term": "Larry King", "description": "American television and radio host", "question": "Can Larry King's ex-wives form a water polo team?", "answer": true, "facts": ["Water polo is a sport played by teams of seven competitors", "Larry King has seven ex-wives"], "decomposition": ["How many ex wives does Larry King have?", "How many players  are on a water polo team?", "Is #1 equal to #2?"], "evidence": [[[["Larry King-43"]], [["Water polo-1"]], ["operation"]], [[["Larry King-37"]], [["Water polo-1"]], ["operation"]], [[["Larry King-37"]], [["Water polo-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the capital of the state of New York?\nFacts: The capital of the state of New York is Albany\nAnswer (True or False): True\nQuestion: What is the name of the city in the United States that is the capital of the state of New York?\nFacts: The capital of the state of New York is Albany", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can Larry King's ex-wives form a water polo team?\nFacts: Water polo is a sport played by teams of seven competitors Larry King has seven ex-wives\nAnswer (True or False):", "ct_subject": "Water polo is a sport played by teams of seven competitors Larry King has seven ex-wives", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9961196"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99102426"], [" of", "1.0"], [" the", "1.0"], [" city", "0.0758483"], [" in", "0.9992409"], [" the", "0.9999449"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "0.9953053"], [" the", "1.0"], [" capital", "0.99920446"], [" of", "1.0"], [" the", "1.0"], [" state", "0.99998856"], [" of", "1.0"], [" New", "0.83305675"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.5726481"], [" capital", "0.9659076"], [" of", "1.0"], [" the", "0.9355336"], [" state", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" is", "1.0"], [" Albany", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "0.9999989"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99336374"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" is", "1.0"], [" Albany", "1.0"]]}, "187": {"qid": "64689ddba750aaad911e", "term": "Amazon (company)", "description": "American electronic commerce and cloud computing company", "question": "Would Iceland lose to Amazon in a bidding war?", "answer": true, "facts": ["Iceland had a nominal GDP of $27 billion as of a 2018 estimate.", "Amazon recorded revenues of $232.887 billion in the 2018 fiscal year."], "decomposition": ["What was the nominal GDP of Iceland in 2018?", "What was Amazon's recorded revenues in 2018?", "Is #2 greater than #1?"], "evidence": [[[["Economy of Iceland-1"]], [["Amazon (company)-64"]], ["operation"]], [[["Economy of Iceland-1"]], [["Amazon (company)-64"]], ["operation"]], [[["Economy of Iceland-1"], "operation"], [["Advertising revenue-10"], "operation"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average IQ of the world's nations?\nFacts: The average IQ of the world's nations is 100.\nAnswer (True or False): True\nQuestion: What is the average height of the world's nations?\nFacts: The average height of the world's nations is 5'9\".\nAnswer (True or False): True\nQuestion:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Iceland lose to Amazon in a bidding war?\nFacts: Iceland had a nominal GDP of $27 billion as of a 2018 estimate. Amazon recorded revenues of $232.887 billion in the 2018 fiscal year.\nAnswer (True or False):", "ct_subject": "Iceland had a nominal GDP of $27 billion as of a 2018 estimate. Amazon recorded revenues of $232.887 billion in the 2018 fiscal year.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99992454"], [" is", "1.0"], [" the", "1.0"], [" average", "0.7616605"], [" IQ", "0.9031326"], [" of", "1.0"], [" the", "0.97218996"], [" world", "0.9999999"], ["'s", "1.0"], [" nations", "0.9583728"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" IQ", "1.0"], [" of", "1.0"], [" the", "1.0"], [" world", "1.0"], ["'s", "0.99999094"], [" nations", "1.0"], [" is", "1.0"], [" 100", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" height", "0.99482095"], [" of", "1.0"], [" the", "0.9999397"], [" world", "1.0"], ["'s", "1.0"], [" nations", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" height", "1.0"], [" of", "1.0"], [" the", "1.0"], [" world", "1.0"], ["'s", "1.0"], [" nations", "1.0"], [" is", "1.0"], [" 5", "0.98248565"], ["'", "1.0"], ["9", "0.8227427"], ["\".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"]]}, "188": {"qid": "6330413d518d44e68b94", "term": "Boolean algebra", "description": "Algebra involving variables containing only \"true\" and \"false\" (or 1 and 0) as values", "question": "Could boolean algebra be described as binary?", "answer": true, "facts": ["Binary options tend to have 2 instead of 10 as a base. ", "Binary directly describes something composed of 2 things. "], "decomposition": ["How many digits are used in boolean algebra?", "How many does 'binary' denote?", "Is #1 the same as #2?"], "evidence": [[[["Boolean algebra-1"]], [["Binary number-1"]], ["operation"]], [[["Boolean algebra-1"]], [["Binary number-1"]], ["operation"]], [[["Boolean algebra-1"]], [["Binary number-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake with a hole in it\"?\nFacts: A snowflake is a single snowflake. A snowflake with a hole in it is a single snowflake with a hole in it.\nAnswer (True or False): True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake with", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could boolean algebra be described as binary?\nFacts: Binary options tend to have 2 instead of 10 as a base.  Binary directly describes something composed of 2 things. \nAnswer (True or False):", "ct_subject": "Binary options tend to have 2 instead of 10 as a base.  Binary directly describes something composed of 2 things. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999957"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "0.9999999"], [" \"", "0.99999976"], ["s", "0.5235438"], ["now", "0.99995637"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.9999999"], ["now", "1.0"], ["flake", "1.0"], [" with", "0.99989855"], [" a", "1.0"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.99999976"], [" snow", "1.0"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "0.9847831"], [" snow", "0.859165"], ["flake", "1.0"], [".", "1.0"], [" A", "0.99538285"], [" snow", "1.0"], ["flake", "1.0"], [" with", "1.0"], [" a", "1.0"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "0.9999964"], [" snow", "1.0"], ["flake", "1.0"], [" with", "1.0"], [" a", "1.0"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], [" with", "1.0"]]}, "189": {"qid": "8ec0f7fd908451102838", "term": "JPEG", "description": "Lossy compression method for reducing the size of digital images", "question": "Does the JPEG acronym stand for a joint committee?", "answer": true, "facts": ["The term \"JPEG\" is an initialism/acronym for the Joint Photographic Experts Group.", "They created the standard in 1992.", "The Joint Photographic Experts Group (JPEG) is the joint committee between ISO/IEC JTC 1 and ITU-T Study Group 16 (formerly CCITT) . ", "The Joint Photographic Experts Group created and maintains the JPEG, JPEG 2000, and JPEG XR standards. "], "decomposition": ["What does the acronym JPEG represent?", "Is #1 a coalition of different groups?"], "evidence": [[[["JPEG-1"]], [["Coalition-1"]]], [[["JPEG-2"]], [["Joint Photographic Experts Group-1"], "operation"]], [[["JPEG-2"]], [["Joint Photographic Experts Group-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"JPG\" file and a \"JFIF\"", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does the JPEG acronym stand for a joint committee?\nFacts: The term \"JPEG\" is an initialism/acronym for the Joint Photographic Experts Group. They created the standard in 1992. The Joint Photographic Experts Group (JPEG) is the joint committee between ISO/IEC JTC 1 and ITU-T Study Group 16 (formerly CCITT) .  The Joint Photographic Experts Group created and maintains the JPEG, JPEG 2000, and JPEG XR standards. \nAnswer (True or False):", "ct_subject": "The term \"JPEG\" is an initialism/acronym for the Joint Photographic Experts Group. They created the standard in 1992. The Joint Photographic Experts Group (JPEG) is the joint committee between ISO/IEC JTC 1 and ITU-T Study Group 16 (formerly CCITT) .  The Joint Photographic Experts Group created and maintains the JPEG, JPEG 2000, and JPEG XR standards. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999356"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9979735"], [" between", "1.0"], [" a", "0.9999958"], [" \"", "0.99999905"], ["J", "0.998749"], ["PG", "0.99984384"], ["\"", "1.0"], [" file", "0.9999716"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["J", "1.0"], ["F", "1.0"], ["IF", "1.0"], ["\"", "1.0"]]}, "190": {"qid": "6a756a5734139bfce297", "term": "Emu", "description": "Large flightless bird endemic to Australia", "question": "Can an emu chase a bogan?", "answer": true, "facts": ["Emus are endemic to the continent of Australia", "Bogan is a pejorative term for certain citizens of Australia"], "decomposition": ["Where are emus endemic to?", "Where is a \"bogan\" found?", "Do areas #1 and #2 overlap?"], "evidence": [[[["Emu-1"]], [["Bogan-2"]], ["operation"]], [[["Emu-1"]], [["Bogan-25"]], ["operation"]], [[["Emu-1"]], [["Bogan-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a bogan and a bogan boy?\nFacts: A bogan is a person who is a member of the Australian Labor Party. A bogan boy is a person who is a member of the Australian Liberal Party\nAnswer (True or False): True\nQuestion: What is the difference between a bogan and a bogan boy?\nFacts: A bogan is a person", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can an emu chase a bogan?\nFacts: Emus are endemic to the continent of Australia Bogan is a pejorative term for certain citizens of Australia\nAnswer (True or False):", "ct_subject": "Emus are endemic to the continent of Australia Bogan is a pejorative term for certain citizens of Australia", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999776"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9846543"], [" between", "1.0"], [" a", "1.0"], [" b", "1.0"], ["ogan", "1.0"], [" and", "1.0"], [" a", "1.0"], [" b", "1.0"], ["ogan", "1.0"], [" boy", "0.99572146"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9995454"], [" b", "1.0"], ["ogan", "1.0"], [" is", "1.0"], [" a", "1.0"], [" person", "1.0"], [" who", "1.0"], [" is", "0.9974463"], [" a", "0.98306644"], [" member", "1.0"], [" of", "1.0"], [" the", "0.99898666"], [" Australian", "0.999879"], [" Labor", "1.0"], [" Party", "1.0"], [".", "0.9999962"], [" A", "1.0"], [" b", "1.0"], ["ogan", "1.0"], [" boy", "1.0"], [" is", "1.0"], [" a", "1.0"], [" person", "1.0"], [" who", "1.0"], [" is", "1.0"], [" a", "1.0"], [" member", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Australian", "1.0"], [" Liberal", "0.9690679"], [" Party", "1.0"], ["\n", "0.6110121"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" b", "1.0"], ["ogan", "1.0"], [" and", "1.0"], [" a", "1.0"], [" b", "1.0"], ["ogan", "1.0"], [" boy", "0.71591455"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" b", "1.0"], ["ogan", "1.0"], [" is", "1.0"], [" a", "1.0"], [" person", "1.0"]]}, "191": {"qid": "54128d7439105554c9e3", "term": "ABBA", "description": "Swedish pop group", "question": "Is calling ABBA the Swedish Beatles a preposterous claim?", "answer": true, "facts": ["ABBA was a Swedish band that had 1 Billboard number 1 hit and 4 top 10 hits.", "The Beatles had 20 Billboard number 1 hits and 34 top 10 hits."], "decomposition": ["How many Billboard number ones did ABBA have?", "How many Billboard number ones did the Beatles have?", "Is #1 lower than #2?"], "evidence": [[[["ABBA-38"]], [["Billboard 200-25"]], ["operation"]], [[["ABBA-120"]], [["The Beatles-111", "The Beatles-4"], "no_evidence"], ["operation"]], [[["ABBA-121"]], [["Billboard 200-26"]], [["Billboard 200-26"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it true that the first person to ever be born in the United States was a woman?\nFacts: The first person to ever be born in the United States was a woman named Mary Ann Cotton.\nAnswer (True or False): True\nQuestion: Is it true that the first person to ever be born in the United States was a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is calling ABBA the Swedish Beatles a preposterous claim?\nFacts: ABBA was a Swedish band that had 1 Billboard number 1 hit and 4 top 10 hits. The Beatles had 20 Billboard number 1 hits and 34 top 10 hits.\nAnswer (True or False):", "ct_subject": "ABBA was a Swedish band that had 1 Billboard number 1 hit and 4 top 10 hits. The Beatles had 20 Billboard number 1 hits and 34 top 10 hits.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99999774"], [" it", "0.999395"], [" true", "1.0"], [" that", "1.0"], [" the", "1.0"], [" first", "0.9999461"], [" person", "0.9992956"], [" to", "1.0"], [" ever", "0.62450755"], [" be", "0.9966214"], [" born", "0.99942434"], [" in", "0.9999999"], [" the", "0.992322"], [" United", "0.03058157"], [" States", "1.0"], [" was", "1.0"], [" a", "1.0"], [" woman", "0.9999944"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" ever", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" a", "1.0"], [" woman", "1.0"], [" named", "0.9999974"], [" Mary", "0.9830808"], [" Ann", "0.9999989"], [" Cotton", "0.99998057"], [".", "1.0"], ["\n", "0.66520673"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" true", "1.0"], [" that", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" ever", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" a", "1.0"]]}, "192": {"qid": "b3c5d591c696cda63e48", "term": "Hyena", "description": "family of mammal", "question": "Do hyenas appear in a Broadway musical?", "answer": true, "facts": ["Scar is the evil lion in Disney's Lion King.", "Scar's minions are a group of hyenas.", "There is a Broadway stage version of Lion King."], "decomposition": ["Who is the main antagonist in Disney's Lion King?", "Which animals were #1's minions?", "Has the Lion King been adapted into a Broadway musical and are #2 hyenas?"], "evidence": [[[["Scar (The Lion King)-1"]], [["Scar (The Lion King)-17"]], [["The Lion King (musical)-2"], "operation"]], [[["Scar (The Lion King)-1"]], [["Scar (The Lion King)-1"]], [["The Lion King (musical)-1"], "operation"]], [[["Scar (The Lion King)-1"]], [["Scar (The Lion King)-3"]], [["Scar (The Lion King)-3", "The Lion King (musical)-2"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first man to walk on the moon?\nFacts: Neil Armstrong.\nAnswer (True or False): True\nQuestion: What is the name of the first man to walk on the moon?\nFacts: Neil Armstrong.\nAnswer (True or False): True\nQuestion: What is the name of the first man to walk", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do hyenas appear in a Broadway musical?\nFacts: Scar is the evil lion in Disney's Lion King. Scar's minions are a group of hyenas. There is a Broadway stage version of Lion King.\nAnswer (True or False):", "ct_subject": "Scar is the evil lion in Disney's Lion King. Scar's minions are a group of hyenas. There is a Broadway stage version of Lion King.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999962"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999995"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9324792"], [" man", "0.8271966"], [" to", "0.97240114"], [" walk", "0.99951494"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Neil", "1.0"], [" Armstrong", "1.0"], [".", "0.87069696"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "0.99999976"], [" to", "1.0"], [" walk", "0.9999995"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Neil", "1.0"], [" Armstrong", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "0.9999999"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" walk", "1.0"]]}, "193": {"qid": "4125c5f4c7a536916608", "term": "Retail", "description": "Sale of goods and services from individuals or businesses to the end-user", "question": "Would a retail associate envy the retailer's CEO's pay?", "answer": true, "facts": ["The average retail CEO makes 14 million yearly.", "Retail associates typically make between $8 and $13 hourly."], "decomposition": ["How much does a retail CEO make yearly?", "How much does a retail associate make yearly?", "Is #1 greater than #2?"], "evidence": [[["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Executive compensation in the United States-5"], "no_evidence"], [["Executive compensation in the United States-5"], "no_evidence"], [["Executive compensation in the United States-5"], "operation"]], [[["Chief risk officer-15"], "no_evidence"], [["Minimum wage-87"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average age of a person who has been in the United States for the last 10 years?\nFacts: The average age of a person who has been in the United States for the last 10 years is 35.\nAnswer (True or False): True\nQuestion: What is the average age of a person who has been in the United States for the last 10 years?\nFacts: The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a retail associate envy the retailer's CEO's pay?\nFacts: The average retail CEO makes 14 million yearly. Retail associates typically make between $8 and $13 hourly.\nAnswer (True or False):", "ct_subject": "The average retail CEO makes 14 million yearly. Retail associates typically make between $8 and $13 hourly.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99676657"], [" is", "1.0"], [" the", "1.0"], [" average", "0.99996865"], [" age", "0.9950113"], [" of", "1.0"], [" a", "1.0"], [" person", "0.9767146"], [" who", "1.0"], [" has", "1.0"], [" been", "0.99999213"], [" in", "0.97193044"], [" the", "0.92517245"], [" United", "0.99630237"], [" States", "1.0"], [" for", "1.0"], [" the", "0.99926966"], [" last", "0.5563845"], [" 10", "0.9611398"], [" years", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" age", "1.0"], [" of", "1.0"], [" a", "1.0"], [" person", "1.0"], [" who", "1.0"], [" has", "1.0"], [" been", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" for", "1.0"], [" the", "1.0"], [" last", "1.0"], [" 10", "1.0"], [" years", "1.0"], [" is", "1.0"], [" 35", "0.9691498"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" age", "0.99999666"], [" of", "1.0"], [" a", "1.0"], [" person", "1.0"], [" who", "1.0"], [" has", "1.0"], [" been", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" for", "1.0"], [" the", "1.0"], [" last", "1.0"], [" 10", "1.0"], [" years", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"]]}, "194": {"qid": "a7e8f2cbec209b317b8f", "term": "Menstruation", "description": "Regular discharge of blood and tissue from the inner lining of the uterus through the vagina", "question": "Are there people who are men who experience menstruation?", "answer": true, "facts": ["Menstruation can occur in any human being who has a uterus and vagina. ", "People who are born with a vagina may transition socially and/or medically to being male. ", "Someone with a vagina who has transitioned to being male is a man. "], "decomposition": ["What body organs are involved in menstruation?", "Do some men possess #1? "], "evidence": [[[["Menstruation-9"]], [["Transgender pregnancy-2"], "operation"]], [[["Menstruation-1"]], [["Male menstruation-1"], "no_evidence", "operation"]], [[["Menstruation-1"]], [["Sex and gender distinction-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there people who are women who experience menstruation?\nFacts: Women can experience menstruation.  Women who are born with a uterus and vagina may transition socially and/or medically to being female.  Someone with a vagina who has transitioned to being female is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are there people who are men who experience menstruation?\nFacts: Menstruation can occur in any human being who has a uterus and vagina.  People who are born with a vagina may transition socially and/or medically to being male.  Someone with a vagina who has transitioned to being male is a man. \nAnswer (True or False):", "ct_subject": "Menstruation can occur in any human being who has a uterus and vagina.  People who are born with a vagina may transition socially and/or medically to being male.  Someone with a vagina who has transitioned to being male is a man. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "1.0"], [" people", "1.0"], [" who", "1.0"], [" are", "0.9999181"], [" women", "1.0"], [" who", "1.0"], [" experience", "1.0"], [" menstru", "1.0"], ["ation", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Women", "0.9999987"], [" can", "0.44625714"], [" experience", "0.98746604"], [" menstru", "1.0"], ["ation", "1.0"], [".", "0.5558497"], [" ", "1.0"], [" Women", "0.99996614"], [" who", "0.9999684"], [" are", "0.99998844"], [" born", "1.0"], [" with", "1.0"], [" a", "1.0"], [" uterus", "0.99992037"], [" and", "0.9845013"], [" vagina", "1.0"], [" may", "1.0"], [" transition", "1.0"], [" socially", "1.0"], [" and", "1.0"], ["/", "1.0"], ["or", "1.0"], [" medically", "1.0"], [" to", "1.0"], [" being", "1.0"], [" female", "0.60835415"], [".", "1.0"], [" ", "1.0"], [" Someone", "1.0"], [" with", "1.0"], [" a", "1.0"], [" vagina", "1.0"], [" who", "1.0"], [" has", "1.0"], [" transitioned", "1.0"], [" to", "1.0"], [" being", "1.0"], [" female", "1.0"], [" is", "1.0"]]}, "195": {"qid": "af2d3c137bd3f5230012", "term": "United States Department of Education", "description": "United States government department", "question": "Does the United States Department of Education oversee services benefiting undocumented migrants? ", "answer": true, "facts": ["The United States Department of Education oversees public education across the United States.", "Public education is a service.", "Public education services are given to students of migrant families that may be undocumented."], "decomposition": ["Which service does the United States Department of Education oversee?", "Which services could children from undocumented migrant families benefit from?", "Is #1 included in #2?"], "evidence": [[[["United States Department of Education-3", "United States Department of Education-4"]], [["Office of Migrant Education-1"]], ["operation"]], [[["United States Department of Education-3"]], ["no_evidence"], ["no_evidence", "operation"]], [[["United States Department of Education-3"]], [["Office of Migrant Education-1"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the capital of the state of Texas?\nFacts: The capital of the state of Texas is Austin.\nAnswer (True or False): True\nQuestion: What is the name of the city in the United States that is the capital of the state of California?", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does the United States Department of Education oversee services benefiting undocumented migrants? \nFacts: The United States Department of Education oversees public education across the United States. Public education is a service. Public education services are given to students of migrant families that may be undocumented.\nAnswer (True or False):", "ct_subject": "The United States Department of Education oversees public education across the United States. Public education is a service. Public education services are given to students of migrant families that may be undocumented.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.7010401"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99997807"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9826476"], [" in", "0.99999785"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "0.9634009"], [" the", "1.0"], [" capital", "0.9911628"], [" of", "1.0"], [" the", "1.0"], [" state", "0.9985934"], [" of", "1.0"], [" Texas", "0.9999678"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99999976"], [" capital", "0.9773481"], [" of", "1.0"], [" the", "0.9983205"], [" state", "1.0"], [" of", "1.0"], [" Texas", "1.0"], [" is", "1.0"], [" Austin", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "0.99999905"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" California", "1.0"], ["?", "1.0"]]}, "196": {"qid": "d8ef42c2c54f93d5eb78", "term": "Strawberry", "description": "edible fruit", "question": "Would an owl monkey enjoy a strawberry?", "answer": true, "facts": ["Owl monkeys are frugivores, and they prefer small, ripe fruit when available.", "Strawberries vary in size but are generally under 2 inches across and an inch in diameter.", "Strawberries are a kind of fruit."], "decomposition": ["What food group does an owl monkey's diet mainly consist of?", "Is a strawberry a #1?"], "evidence": [[[["Night monkey-8"]], [["Strawberry-1"]]], [[["Night monkey-8"]], [["Strawberry-1"]]], [[["Night monkey-1", "Night monkey-8"]], [["Strawberry-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance a person can walk in one day?\nFacts: The longest distance a person can walk in one day is about 2,000 miles.\nAnswer (True or False): True\nQuestion: What is the longest distance a person can walk in one year?\nFacts: The longest", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would an owl monkey enjoy a strawberry?\nFacts: Owl monkeys are frugivores, and they prefer small, ripe fruit when available. Strawberries vary in size but are generally under 2 inches across and an inch in diameter. Strawberries are a kind of fruit.\nAnswer (True or False):", "ct_subject": "Owl monkeys are frugivores, and they prefer small, ripe fruit when available. Strawberries vary in size but are generally under 2 inches across and an inch in diameter. Strawberries are a kind of fruit.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999969"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.67574036"], [" distance", "0.9471429"], [" a", "0.99703634"], [" person", "0.9347407"], [" can", "0.99975055"], [" walk", "0.9999974"], [" in", "1.0"], [" one", "0.9715375"], [" day", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99999905"], [" longest", "0.9999999"], [" distance", "1.0"], [" a", "1.0"], [" person", "1.0"], [" can", "1.0"], [" walk", "1.0"], [" in", "1.0"], [" one", "1.0"], [" day", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 2", "0.89033836"], [",", "1.0"], ["000", "0.99999857"], [" miles", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" person", "1.0"], [" can", "1.0"], [" walk", "0.9999627"], [" in", "1.0"], [" one", "1.0"], [" year", "0.8288003"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"]]}, "197": {"qid": "6d2b70f4dd9eec8ef932", "term": "Doctorate", "description": "academic or professional degree", "question": "Should you be skeptical of a 21 year old claiming to have a doctorate?", "answer": true, "facts": ["The average age that someone gets their doctorate at is 33. ", "A doctorate takes an average of 8.5 years."], "decomposition": ["What is the average age at which people get their doctorate?", "Is 21 very much less than #1 ?"], "evidence": [[[["Graduate science education in the United States-6"]], ["operation"]], [[["Doctorate-1", "Graduate science education in the United States-6"], "no_evidence"], ["operation"]], [[["Doctorate-1", "Doctorate-18"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average age of a woman who has a baby?\nFacts: The average age of a woman who has a baby is 27.  The average age of a woman who has a baby is 27. \nAnswer (True or False): True\nQuestion: What is the average age of a man who has a baby?\nFacts: The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Should you be skeptical of a 21 year old claiming to have a doctorate?\nFacts: The average age that someone gets their doctorate at is 33.  A doctorate takes an average of 8.5 years.\nAnswer (True or False):", "ct_subject": "The average age that someone gets their doctorate at is 33.  A doctorate takes an average of 8.5 years.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999981"], [" is", "1.0"], [" the", "1.0"], [" average", "0.99997187"], [" age", "0.99071056"], [" of", "1.0"], [" a", "1.0"], [" woman", "0.93636173"], [" who", "0.9999882"], [" has", "1.0"], [" a", "0.9999634"], [" baby", "0.9970186"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" age", "1.0"], [" of", "1.0"], [" a", "1.0"], [" woman", "1.0"], [" who", "1.0"], [" has", "1.0"], [" a", "1.0"], [" baby", "1.0"], [" is", "1.0"], [" 27", "0.9505799"], [".", "1.0"], [" ", "0.9999989"], [" The", "0.7203528"], [" average", "1.0"], [" age", "1.0"], [" of", "1.0"], [" a", "1.0"], [" woman", "1.0"], [" who", "1.0"], [" has", "1.0"], [" a", "1.0"], [" baby", "1.0"], [" is", "1.0"], [" 27", "1.0"], [".", "1.0"], [" ", "0.9997011"], ["\n", "0.87940913"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" age", "1.0"], [" of", "1.0"], [" a", "1.0"], [" man", "1.0"], [" who", "1.0"], [" has", "1.0"], [" a", "1.0"], [" baby", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"]]}, "198": {"qid": "24a3aff40ef7fd04e208", "term": "Richard Dawkins", "description": "English ethologist, evolutionary biologist and author", "question": "Would Richard Dawkins hypothetically refuse an offering of the Last rites?", "answer": true, "facts": ["Richard Dawkins is known as an outspoken atheist, well known for his criticism of creationism and intelligent design.", "The Last rites, in Catholicism, are the last prayers and ministrations given to an individual of the faith, when possible, shortly before death."], "decomposition": ["Which religious beliefs are the Last rites affiliated with?", "What was Richard Dawkins' belief regarding religion?", "Would a #2 refuse to practice #1?"], "evidence": [[[["Last rites-1"]], [["Richard Dawkins-31", "Richard Dawkins-32"]], ["operation"]], [[["Last rites-1"]], [["Richard Dawkins-3"]], [["Atheism-1", "Richard Dawkins-24"]]], [[["Last rites-1"]], [["Richard Dawkins-3"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: The first person to be born in the United States was a woman named Sarah Hensley.\nAnswer (True or False): True\nQuestion: What is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Richard Dawkins hypothetically refuse an offering of the Last rites?\nFacts: Richard Dawkins is known as an outspoken atheist, well known for his criticism of creationism and intelligent design. The Last rites, in Catholicism, are the last prayers and ministrations given to an individual of the faith, when possible, shortly before death.\nAnswer (True or False):", "ct_subject": "Richard Dawkins is known as an outspoken atheist, well known for his criticism of creationism and intelligent design. The Last rites, in Catholicism, are the last prayers and ministrations given to an individual of the faith, when possible, shortly before death.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99989724"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9676958"], [" of", "1.0"], [" the", "1.0"], [" first", "0.90109634"], [" person", "0.9013658"], [" to", "1.0"], [" be", "0.99991775"], [" born", "0.90407234"], [" in", "1.0"], [" the", "1.0"], [" United", "0.9999877"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999709"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" a", "0.7754424"], [" woman", "0.99999964"], [" named", "1.0"], [" Sarah", "0.8369756"], [" H", "0.99996805"], ["ens", "1.0"], ["ley", "1.0"], [".", "1.0"], ["\n", "0.9999889"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"]]}, "199": {"qid": "9332b9062a1b5c8109b0", "term": "Christopher Columbus", "description": "Italian explorer, navigator, and colonizer", "question": "Did Christopher Columbus condone multiple deadly sins?", "answer": true, "facts": ["The seven deadly sins are:  pride, greed, wrath, envy, lust, gluttony, and sloth.", "Under Columbus, every native of fourteen years of age or upward was to pay a large hawk's bell of gold dust or cotton and those who could not pay were punished.", " in just two years under Columbus's governorship, over 125,000 of the 250,000\u2013300,000 natives in Haiti were dead."], "decomposition": ["What are the deadly sins?", "What were Christopher Columbus's actions in the New World?", "Did #2 include more than one of #1?"], "evidence": [[[["Seven deadly sins-1"]], [["Christopher Columbus-56", "Christopher Columbus-68"]], ["operation"]], [[["Seven deadly sins-1"]], [["Christopher Columbus-93"], "no_evidence"], ["no_evidence", "operation"]], [[["Seven deadly sins-1"]], [["Christopher Columbus-43", "Christopher Columbus-98"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Bible say that the earth was flat?\nFacts: The Bible does not say that", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Christopher Columbus condone multiple deadly sins?\nFacts: The seven deadly sins are: pride, greed, wrath, envy, lust, gluttony, and sloth. Under Columbus, every native of fourteen years of age or upward was to pay a large hawk's bell of gold dust or cotton and those who could not pay were punished.  in just two years under Columbus's governorship, over 125,000 of the 250,000\u2013300,000 natives in Haiti were dead.\nAnswer (True or False):", "ct_subject": "The seven deadly sins are: pride, greed, wrath, envy, lust, gluttony, and sloth. Under Columbus, every native of fourteen years of age or upward was to pay a large hawk's bell of gold dust or cotton and those who could not pay were punished.  in just two years under Columbus's governorship, over 125,000 of the 250,000\u2013300,000 natives in Haiti were dead.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" Bible", "0.9988011"], [" say", "0.99596316"], [" that", "1.0"], [" the", "0.99990106"], [" earth", "0.99996483"], [" was", "1.0"], [" flat", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Bible", "1.0"], [" does", "0.9487847"], [" not", "1.0"], [" say", "1.0"], [" that", "1.0"]]}, "200": {"qid": "dd31908b73e958cfd678", "term": "Rurouni Kenshin", "description": "1994 Japanese manga series written and illustrated by Nobuhiro Watsuki", "question": "Is Rurouni Kenshin from same country as lead character in Nobunaga's Ambition?", "answer": true, "facts": ["Rurouni Kenshin is a manga series that comes from Japan.", "Nobunaga's Ambition is a video game series based on the experiences of Oda Nobunaga.", "Oda Nobunaga was a Japanese feudal lord."], "decomposition": ["Where is Rurouni Kenshin from?", "Where was Oda Nobunaga from?", "Is #1 the same as #2?"], "evidence": [[[["Rurouni Kenshin-1"]], [["Oda Nobunaga-1"]], ["operation"]], [[["Rurouni Kenshin-1"]], [["Nobunaga's Ambition-1", "Oda Nobunaga-4"]], ["operation"]], [[["Rurouni Kenshin-1"]], [["Oda Nobunaga-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the character in the movie \"The Avengers\" \"The Vision\" or \"Vision\"?\nFacts: The name of the character in the movie \"The Avengers\" is \"The Vision\". The name of the character", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Rurouni Kenshin from same country as lead character in Nobunaga's Ambition?\nFacts: Rurouni Kenshin is a manga series that comes from Japan. Nobunaga's Ambition is a video game series based on the experiences of Oda Nobunaga. Oda Nobunaga was a Japanese feudal lord.\nAnswer (True or False):", "ct_subject": "Rurouni Kenshin is a manga series that comes from Japan. Nobunaga's Ambition is a video game series based on the experiences of Oda Nobunaga. Oda Nobunaga was a Japanese feudal lord.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "0.99998856"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" character", "0.9999721"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "1.0"], ["The", "1.0"], [" Avengers", "0.17729348"], ["\"", "1.0"], [" \"", "0.55912864"], ["The", "0.8807906"], [" Vision", "0.59064573"], ["\"", "0.94988704"], [" or", "1.0"], [" \"", "1.0"], ["Vision", "0.007180797"], ["\"?", "0.99999964"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" name", "0.99999917"], [" of", "1.0"], [" the", "1.0"], [" character", "1.0"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "1.0"], ["The", "1.0"], [" Avengers", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" \"", "1.0"], ["The", "0.9996724"], [" Vision", "1.0"], ["\".", "1.0"], [" The", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" character", "1.0"]]}, "201": {"qid": "85a77e70dd0e86d50995", "term": "Aldi", "description": "Germany-based supermarket chain", "question": "Should you bring your own bags to Aldi?", "answer": true, "facts": ["Unlike most grocery stores, Aldi charges customers for use of paper bags.", "Aldi does not supply shopping carts without a deposit, so shopping bags are a good alternative."], "decomposition": ["In US Aldi stores, how do customers get shopping bags?", "How do customers get shopping carts?", "Do #1 and #2 cost money or value?"], "evidence": [[["no_evidence"], ["no_evidence"], ["no_evidence"]], [[["Aldi-23"], "no_evidence"], [["Aldi-23"], "no_evidence"], ["operation"]], [[["Aldi-1"], "no_evidence"], [["Aldi-32"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake-shaped\" snowflake?\nFacts: A snowflake is a single crystal of ice. A snowflake-shaped snowflake is a cluster of crystals.\nAnswer (True or False): True\nQuestion: What is the difference between a \"snowflake\"", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Should you bring your own bags to Aldi?\nFacts: Unlike most grocery stores, Aldi charges customers for use of paper bags. Aldi does not supply shopping carts without a deposit, so shopping bags are a good alternative.\nAnswer (True or False):", "ct_subject": "Unlike most grocery stores, Aldi charges customers for use of paper bags. Aldi does not supply shopping carts without a deposit, so shopping bags are a good alternative.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999999"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.99999416"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.42860058"], ["now", "0.80456704"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["-", "0.9595803"], ["shaped", "1.0"], ["\"", "1.0"], [" snow", "0.99999547"], ["flake", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9974848"], [" snow", "0.99991333"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "0.99999595"], [" crystal", "0.99990463"], [" of", "0.99176276"], [" ice", "1.0"], [".", "0.63302004"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], ["-", "1.0"], ["shaped", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" cluster", "0.9993394"], [" of", "1.0"], [" crystals", "1.0"], [".", "0.9967039"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\"", "1.0"]]}, "202": {"qid": "2075b087e620fb920439", "term": "Conan O'Brien", "description": "American television show host and comedian", "question": "Would most children be up past their bedtime if they were watching Conan O'Brien?", "answer": true, "facts": ["Conan O'Brien airs at 11 PM. ", "It is recommended that children are in bed before 10PM."], "decomposition": ["When does Conan O' Brian air?", "What is the recommended bedtime for children?", "Does #1 occur after #2?"], "evidence": [[[["Conan (talk show)-1"]], ["no_evidence"], [["Conan (talk show)-1"]]], [[["Conan O'Brien-34"]], [["Bedtime-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Conan (talk show)-1"]], ["no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the most popular color in the world?\nFacts: Red\nAnswer (True or False): True\nQuestion: What is the most popular color in the world?\nFacts: Red\nAnswer (True or False): True\nQuestion: What is the most popular color in the world?\nFacts: Red\nAnswer (True or False): True\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would most children be up past their bedtime if they were watching Conan O'Brien?\nFacts: Conan O'Brien airs at 11 PM.  It is recommended that children are in bed before 10PM.\nAnswer (True or False):", "ct_subject": "Conan O'Brien airs at 11 PM.  It is recommended that children are in bed before 10PM.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999995"], [" is", "1.0"], [" the", "1.0"], [" most", "0.7485242"], [" popular", "0.9793195"], [" color", "0.85375273"], [" in", "1.0"], [" the", "1.0"], [" world", "0.99994624"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Red", "0.8864802"], ["\n", "0.94815457"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" most", "1.0"], [" popular", "1.0"], [" color", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "0.9999269"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Red", "0.9999887"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" most", "1.0"], [" popular", "1.0"], [" color", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Red", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"]]}, "203": {"qid": "250bb60b384ff1d91891", "term": "Glucose", "description": "A simple form of sugar", "question": "4 Krispy Kreme glazed doughnuts exceed AHA  daily sugar allowance?", "answer": true, "facts": ["Glucose is a form of sugar that humans need in order to live.", "The AHA (American Heart Association) recommends no more than 38g of sugar a day.", "One Krispy Kreme glazed doughnut has 10g of sugar."], "decomposition": ["What does the AHA recommend as the maximum amount of sugar a day?", "How much sugar is in a Krispy Kreme glazed doghnut?", "What is #2 multiplied by 4?", "Is #3 greater than #1?"], "evidence": [[[["Healthy diet-8"]], [["Doughnut-1", "Junk food-6"], "no_evidence"], ["operation"], ["operation"]], [[["American Heart Association-19"], "no_evidence"], [["Doughnut-3"], "no_evidence"], ["no_evidence", "operation"], ["no_evidence", "operation"]], [[["Sugar-54"]], ["no_evidence"], ["no_evidence", "operation"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human?\nFacts: The average human is about 200 pounds.\nAnswer (True or False): True\nQuestion: What is the average weight of a dog?\nFacts: The average dog is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: 4 Krispy Kreme glazed doughnuts exceed AHA  daily sugar allowance?\nFacts: Glucose is a form of sugar that humans need in order to live. The AHA (American Heart Association) recommends no more than 38g of sugar a day. One Krispy Kreme glazed doughnut has 10g of sugar.\nAnswer (True or False):", "ct_subject": "Glucose is a form of sugar that humans need in order to live. The AHA (American Heart Association) recommends no more than 38g of sugar a day. One Krispy Kreme glazed doughnut has 10g of sugar.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9171634"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "1.0"], [" of", "1.0"], [" a", "0.99999976"], [" human", "1.0"], ["?", "0.999777"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.99982303"], [" is", "0.838137"], [" about", "0.9999999"], [" 200", "0.055069245"], [" pounds", "0.9999982"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "0.967993"], [" of", "1.0"], [" a", "1.0"], [" dog", "0.9985776"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" dog", "1.0"], [" is", "0.9999995"]]}, "204": {"qid": "1463354ede2204c8a171", "term": "Mexican Revolution", "description": "major nationwide armed struggle in Mexico between 1910 and 1920", "question": "Could Barron Trump have lived through the Mexican Revolution?", "answer": true, "facts": ["The Mexican Revolution took place over a period of ten years", "Barron Trump is 14 years old"], "decomposition": ["How long did the Mexican Revolution last?", "How old has Barron Trump already lived as of 2020?", "Is #2 greater than or equal to #1?"], "evidence": [[[["Mexican Revolution-1"]], [["Family of Donald Trump-11"]], ["operation"]], [[["Mexican Revolution-1"]], [["Family of Donald Trump-11"]], ["operation"]], [[["Mexican Revolution-1"]], [["Family of Donald Trump-11"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Could the American Revolution have been fought by a group of people who were not Americans?\nFacts: The American Revolution was fought by a group of people who were not Americans.\nAnswer (True or False): True\nQuestion: Could the American Revolution have been fought by a group of people who were not Americans?\nFacts: The American Revolution was fought by a group of people who were not Americans.\nAnswer (True or False", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could Barron Trump have lived through the Mexican Revolution?\nFacts: The Mexican Revolution took place over a period of ten years Barron Trump is 14 years old\nAnswer (True or False):", "ct_subject": "The Mexican Revolution took place over a period of ten years Barron Trump is 14 years old", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Could", "0.4776986"], [" the", "0.17572807"], [" American", "0.9621127"], [" Revolution", "1.0"], [" have", "1.0"], [" been", "0.9999993"], [" fought", "0.9548903"], [" by", "0.9999914"], [" a", "0.99999666"], [" group", "0.99999857"], [" of", "1.0"], [" people", "1.0"], [" who", "0.99996066"], [" were", "1.0"], [" not", "1.0"], [" Americans", "0.52393246"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" American", "1.0"], [" Revolution", "1.0"], [" was", "1.0"], [" fought", "1.0"], [" by", "1.0"], [" a", "0.9614197"], [" group", "1.0"], [" of", "1.0"], [" people", "1.0"], [" who", "1.0"], [" were", "1.0"], [" not", "0.999987"], [" Americans", "1.0"], [".", "0.9999957"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Could", "0.99999917"], [" the", "1.0"], [" American", "1.0"], [" Revolution", "1.0"], [" have", "1.0"], [" been", "1.0"], [" fought", "1.0"], [" by", "1.0"], [" a", "1.0"], [" group", "1.0"], [" of", "1.0"], [" people", "1.0"], [" who", "1.0"], [" were", "1.0"], [" not", "1.0"], [" Americans", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" American", "1.0"], [" Revolution", "1.0"], [" was", "1.0"], [" fought", "1.0"], [" by", "1.0"], [" a", "1.0"], [" group", "1.0"], [" of", "1.0"], [" people", "1.0"], [" who", "1.0"], [" were", "1.0"], [" not", "1.0"], [" Americans", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"]]}, "205": {"qid": "e1f10b57579fa6a92aa9", "term": "Martin Luther", "description": "Saxon priest, monk and theologian, seminal figure in Protestant Reformation", "question": "Did Martin Luther believe in Satan?", "answer": true, "facts": ["Martin Luther was a Protestant.", "Satan is also known as the devil.", "Protestants traditionally have believed in the devil as a being. "], "decomposition": ["What religion was Martin Luther?", "Do #1's believe in the existence of a non-human evil being (Satan, Beelzebub, the devil, etc)?"], "evidence": [[[["Martin Luther-1"]], [["Antichrist-1"], "no_evidence", "operation"]], [[["Martin Luther-12"]], [["Augustinians-1", "Devil-9"], "operation"]], [[["Martin Luther-111"]], [["Satan-32"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Bible say that the earth was flat?\nFacts: The Bible does not say that the earth was flat. The Bible says that the earth is round.\nAnswer (True or False): True\nQuestion: Did the Bible say that the earth was flat?\nFacts: The Bible does not say that the earth was flat. The Bible says that the earth is round.\nAnswer (True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Martin Luther believe in Satan?\nFacts: Martin Luther was a Protestant. Satan is also known as the devil. Protestants traditionally have believed in the devil as a being. \nAnswer (True or False):", "ct_subject": "Martin Luther was a Protestant. Satan is also known as the devil. Protestants traditionally have believed in the devil as a being. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999999"], [" the", "1.0"], [" Bible", "0.5725271"], [" say", "1.0"], [" that", "1.0"], [" the", "0.99948823"], [" earth", "0.9939476"], [" was", "0.99999964"], [" flat", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Bible", "1.0"], [" does", "0.9997837"], [" not", "1.0"], [" say", "1.0"], [" that", "1.0"], [" the", "1.0"], [" earth", "1.0"], [" was", "0.99864286"], [" flat", "1.0"], [".", "1.0"], [" The", "0.99999774"], [" Bible", "0.99999976"], [" says", "0.99996233"], [" that", "1.0"], [" the", "1.0"], [" earth", "1.0"], [" is", "1.0"], [" round", "0.99999833"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" Bible", "1.0"], [" say", "1.0"], [" that", "1.0"], [" the", "1.0"], [" earth", "1.0"], [" was", "0.99999976"], [" flat", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Bible", "1.0"], [" does", "1.0"], [" not", "1.0"], [" say", "1.0"], [" that", "1.0"], [" the", "1.0"], [" earth", "1.0"], [" was", "1.0"], [" flat", "1.0"], [".", "1.0"], [" The", "1.0"], [" Bible", "1.0"], [" says", "1.0"], [" that", "1.0"], [" the", "1.0"], [" earth", "1.0"], [" is", "1.0"], [" round", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"]]}, "206": {"qid": "3c8d24e1b6e89a1534bb", "term": "Canidae", "description": "family of mammals", "question": "Were any members of Canidae in Aesop's Fables?", "answer": true, "facts": ["Canidae is a family of mammals that includes dogs, foxes, and coyotes.", "Aesop's Fables was a collection of stories with animals as the main characters.", "One of the most famous stories involves a fox and a lion."], "decomposition": ["Which animals were typical characters in Aesop's Fables?", "Do any of #1 belong to the family Canidae?"], "evidence": [[[["Aesop's Fables-53"]], [["Canidae-1"], "operation"]], [[["Aesop-19", "The Boy Who Cried Wolf-1"], "no_evidence"], ["operation"]], [[["Aesop's Fables-65"]], [["Canidae-1", "Dog-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Romans have a word for \"camel\"?\nFacts: The word \"camel\" is a Latin word meaning \"a large animal.\" The word \"camel\" is also used in English to refer to a horse.\nAnswer (True or", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Were any members of Canidae in Aesop's Fables?\nFacts: Canidae is a family of mammals that includes dogs, foxes, and coyotes. Aesop's Fables was a collection of stories with animals as the main characters. One of the most famous stories involves a fox and a lion.\nAnswer (True or False):", "ct_subject": "Canidae is a family of mammals that includes dogs, foxes, and coyotes. Aesop's Fables was a collection of stories with animals as the main characters. One of the most famous stories involves a fox and a lion.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99989843"], [" the", "1.0"], [" Romans", "0.99691963"], [" have", "1.0"], [" a", "1.0"], [" word", "0.58111423"], [" for", "1.0"], [" \"", "1.0"], ["c", "0.91157097"], ["amel", "0.99928087"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" word", "0.99735534"], [" \"", "0.9999999"], ["c", "1.0"], ["amel", "1.0"], ["\"", "1.0"], [" is", "0.9999974"], [" a", "0.9941413"], [" Latin", "0.9999112"], [" word", "1.0"], [" meaning", "0.99706024"], [" \"", "1.0"], ["a", "0.999998"], [" large", "0.99999976"], [" animal", "0.9998889"], [".\"", "0.89037865"], [" The", "0.9999715"], [" word", "1.0"], [" \"", "0.9999999"], ["c", "1.0"], ["amel", "1.0"], ["\"", "1.0"], [" is", "0.9295065"], [" also", "0.8318415"], [" used", "0.9999995"], [" in", "0.42071116"], [" English", "0.93098366"], [" to", "1.0"], [" refer", "0.9999919"], [" to", "1.0"], [" a", "1.0"], [" horse", "0.99946576"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"]]}, "207": {"qid": "cdc5da16ca1bb2a4edf7", "term": "United States Air Force", "description": "Air and space warfare branch of the United States Armed Forces", "question": "Would United States Air Force consider Return of the Jedi's Han Solo bad hypothetical candidate?", "answer": true, "facts": ["Han Solo is an ace pilot ally in the Star Wars universe.", "The US Air Force requires candidates to be between 18 and 35 years old.", "Return of the Jedi's Han Solo is 36 years of age.", "The US Air Force requires a candidate to be an American citizen.", "Han Solo is from the planet Corellia in a galaxy far, far, away."], "decomposition": ["What requirements does the US Air Force demand of potential candidates?", "What are the characteristics of character Han Solo as featured in Return of the Jedi?", "Does #2 fail to satisfy all of #1?"], "evidence": [[[["United States Air Force-60"], "no_evidence"], [["Han Solo-12"]], ["no_evidence"]], [[["United States Air Force Basic Military Training-34"]], [["Han Solo-36"]], ["operation"]], [[["United States Air Force Fitness Assessment-1"], "no_evidence"], [["Han Solo-11", "Han Solo-12"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the planet where the first Star Wars film was filmed?\nFacts: Tatooine.\nAnswer (", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would United States Air Force consider Return of the Jedi's Han Solo bad hypothetical candidate?\nFacts: Han Solo is an ace pilot ally in the Star Wars universe. The US Air Force requires candidates to be between 18 and 35 years old. Return of the Jedi's Han Solo is 36 years of age. The US Air Force requires a candidate to be an American citizen. Han Solo is from the planet Corellia in a galaxy far, far, away.\nAnswer (True or False):", "ct_subject": "Han Solo is an ace pilot ally in the Star Wars universe. The US Air Force requires candidates to be between 18 and 35 years old. Return of the Jedi's Han Solo is 36 years of age. The US Air Force requires a candidate to be an American citizen. Han Solo is from the planet Corellia in a galaxy far, far, away.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.98618627"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99419653"], [" of", "1.0"], [" the", "1.0"], [" planet", "0.99999905"], [" where", "0.6588093"], [" the", "1.0"], [" first", "0.9999883"], [" Star", "0.9998696"], [" Wars", "1.0"], [" film", "0.8161501"], [" was", "1.0"], [" filmed", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Tat", "0.9999554"], ["oo", "1.0"], ["ine", "1.0"], [".", "0.99978536"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"]]}, "208": {"qid": "e32529b0074a1b857a85", "term": "DC Comics", "description": "U.S. comic book publisher", "question": "Would Avengers Comics be out of place in a DC Comics store?", "answer": true, "facts": ["The Avengers are a comic produced by Marvel.", "Marvel and DC are rival companies, each having their own line of products and merchandise. "], "decomposition": ["Who produces the Avengers Comics?", "Are #1 and DC Comics rival companies?"], "evidence": [[[["Marvel Avengers Alliance-15"]], [["DC vs. Marvel-7"]]], [[["Avengers (comics)-1"]], [["DC Comics-18", "Marvel Comics-17"], "operation"]], [[["Avengers (comics)-1"]], [["Marvel Comics-25"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the character that appeared in the movie \"The Avengers\"?\nFacts: The character that appeared in the movie \"The Avengers\" is named Nick Fury.\nAnswer (True or False): True\nQuestion: What is the name of the character that appeared in the movie \"The Avengers\"?\nFacts: The character that appeared in the movie \"The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Avengers Comics be out of place in a DC Comics store?\nFacts: The Avengers are a comic produced by Marvel. Marvel and DC are rival companies, each having their own line of products and merchandise. \nAnswer (True or False):", "ct_subject": "The Avengers are a comic produced by Marvel. Marvel and DC are rival companies, each having their own line of products and merchandise. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9990396"], [" of", "1.0"], [" the", "1.0"], [" character", "0.99904746"], [" that", "0.8552103"], [" appeared", "0.65128547"], [" in", "1.0"], [" the", "1.0"], [" movie", "0.99999046"], [" \"", "1.0"], ["The", "1.0"], [" Avengers", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" character", "1.0"], [" that", "1.0"], [" appeared", "1.0"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "1.0"], ["The", "1.0"], [" Avengers", "1.0"], ["\"", "1.0"], [" is", "0.9999951"], [" named", "0.9999031"], [" Nick", "0.57664996"], [" Fury", "1.0"], [".", "1.0"], ["\n", "0.99998295"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" character", "1.0"], [" that", "1.0"], [" appeared", "1.0"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "1.0"], ["The", "1.0"], [" Avengers", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" character", "1.0"], [" that", "1.0"], [" appeared", "1.0"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "1.0"], ["The", "1.0"]]}, "209": {"qid": "0d22526961c82ad6ef4a", "term": "Fairy", "description": "mythical being or legendary creature", "question": "Is a fairy more prevalent in world myths than a valkyrie?", "answer": true, "facts": ["Valkyries are female figures that choose heroes to bring to Valhalla.", "Valkyries are exclusive to Norse mythology.", "A fairy is a mystical magical being that can be found in Celtic, Slavic, German, English, and French folklore."], "decomposition": ["In what myths do the Valkyries appear?", "Do fairies appear in more myths than #1?"], "evidence": [[[["Valkyrie-1"]], [["Fairy-12"]]], [[["Valkyrie-2"], "no_evidence"], [["Fairy-2"], "no_evidence", "operation"]], [[["Valkyrie-1"]], [["Fairyland-1", "Fairyland-3"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is a unicorn more common in the Bible than a unicorn?\nFacts: Unicorns are mythical creatures that are found in many cultures. The Bible does not mention unicorns.\nAnswer (True or False): True\nQuestion: Is a unicorn more common in the Bible", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is a fairy more prevalent in world myths than a valkyrie?\nFacts: Valkyries are female figures that choose heroes to bring to Valhalla. Valkyries are exclusive to Norse mythology. A fairy is a mystical magical being that can be found in Celtic, Slavic, German, English, and French folklore.\nAnswer (True or False):", "ct_subject": "Valkyries are female figures that choose heroes to bring to Valhalla. Valkyries are exclusive to Norse mythology. A fairy is a mystical magical being that can be found in Celtic, Slavic, German, English, and French folklore.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99984574"], [" a", "0.7704616"], [" unicorn", "0.96429706"], [" more", "1.0"], [" common", "0.27942583"], [" in", "1.0"], [" the", "0.3670467"], [" Bible", "0.84266365"], [" than", "1.0"], [" a", "0.94508433"], [" unicorn", "0.99493045"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Unic", "0.99992394"], ["orns", "1.0"], [" are", "1.0"], [" mythical", "1.0"], [" creatures", "1.0"], [" that", "1.0"], [" are", "1.0"], [" found", "0.9216382"], [" in", "1.0"], [" many", "0.9055952"], [" cultures", "0.97430575"], [".", "1.0"], [" The", "0.9940858"], [" Bible", "1.0"], [" does", "1.0"], [" not", "1.0"], [" mention", "1.0"], [" unic", "1.0"], ["orns", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" a", "1.0"], [" unicorn", "1.0"], [" more", "1.0"], [" common", "1.0"], [" in", "1.0"], [" the", "1.0"], [" Bible", "1.0"]]}, "210": {"qid": "4589898e4852e5389728", "term": "YMCA", "description": "Worldwide organization founded in 1844 on principles of muscular Christianity", "question": "Can you get Raclette in YMCA headquarters city?", "answer": true, "facts": ["YMCA is headquartered in Geneva, Switzerland.", "Raclette is a melted cheese and potato dish.", "Raclette is one of several foods Geneva, Switzerland is famous for."], "decomposition": ["Where is the YMCA headquartered?", "What foods is #1 famous for?", "Is raclette in #2?"], "evidence": [[[["YMCA-1"]], [["Swiss Cheese Union-9"], "no_evidence"], [["Raclette-2"], "operation"]], [[["YMCA-1"]], [["Swiss cuisine-2"]], ["operation"]], [[["YMCA-53"]], [["Lincoln Park, Chicago-37"]], [["Raclette-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the capital of the state of New York?\nFacts: New York City is the capital of the state of New York.\nAnswer (True or False): True\nQuestion: What is the name of the city in the United States that is the capital of the state of New", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can you get Raclette in YMCA headquarters city?\nFacts: YMCA is headquartered in Geneva, Switzerland. Raclette is a melted cheese and potato dish. Raclette is one of several foods Geneva, Switzerland is famous for.\nAnswer (True or False):", "ct_subject": "YMCA is headquartered in Geneva, Switzerland. Raclette is a melted cheese and potato dish. Raclette is one of several foods Geneva, Switzerland is famous for.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99952507"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "0.99999595"], [" in", "0.9925207"], [" the", "0.4731965"], [" United", "1.0"], [" States", "1.0"], [" that", "0.99980813"], [" is", "0.9680351"], [" the", "1.0"], [" capital", "0.9999995"], [" of", "1.0"], [" the", "1.0"], [" state", "0.9847707"], [" of", "1.0"], [" New", "0.99927694"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" New", "0.50061035"], [" York", "1.0"], [" City", "0.94500107"], [" is", "1.0"], [" the", "1.0"], [" capital", "0.9997409"], [" of", "1.0"], [" the", "0.9999367"], [" state", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" New", "0.97199833"]]}, "211": {"qid": "fa7b5c8e022bb8bec2d6", "term": "Diarrhea", "description": "Loose or liquid bowel movements", "question": "Can too many oranges cause diarrhea?", "answer": true, "facts": ["Oranges are very high in fiber and sugar.", "Too much fiber can cause diarrhea."], "decomposition": ["What high-level nutritional values do oranges have?", "Can excess of any of #1 cause diarrhea?"], "evidence": [[[["Mandarin orange-12"]], [["Vitamin C-21"], "operation"]], [[["Orange (fruit)-40", "Orange (fruit)-41"], "no_evidence"], ["no_evidence", "operation"]], [[["Orange (fruit)-20"]], [["Vitamin C-21"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"sugar pill\" and a \"sugar pill with a sugar pill\"?\nFacts: A sugar pill is a sugar pill that contains no sugar. A sugar pill with a sugar pill is a sugar pill that contains sugar.\nAnswer (True or False): True\nQuestion: What is the difference between a \"sugar pill\" and a \"sugar pill with a sugar pill\"?\nFacts: A sugar", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can too many oranges cause diarrhea?\nFacts: Oranges are very high in fiber and sugar. Too much fiber can cause diarrhea.\nAnswer (True or False):", "ct_subject": "Oranges are very high in fiber and sugar. Too much fiber can cause diarrhea.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99939716"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.99973136"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.9999901"], ["s", "0.9130115"], ["ugar", "0.264588"], [" pill", "0.66625506"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["ugar", "1.0"], [" pill", "1.0"], [" with", "1.0"], [" a", "0.9995029"], [" sugar", "0.9999969"], [" pill", "0.9974132"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999993"], [" sugar", "1.0"], [" pill", "1.0"], [" is", "1.0"], [" a", "1.0"], [" sugar", "0.9999932"], [" pill", "1.0"], [" that", "0.9999691"], [" contains", "0.9999043"], [" no", "0.9999989"], [" sugar", "1.0"], [".", "1.0"], [" A", "1.0"], [" sugar", "1.0"], [" pill", "1.0"], [" with", "1.0"], [" a", "1.0"], [" sugar", "1.0"], [" pill", "1.0"], [" is", "1.0"], [" a", "1.0"], [" sugar", "1.0"], [" pill", "1.0"], [" that", "1.0"], [" contains", "1.0"], [" sugar", "0.42508307"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["ugar", "1.0"], [" pill", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["ugar", "1.0"], [" pill", "1.0"], [" with", "1.0"], [" a", "1.0"], [" sugar", "1.0"], [" pill", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" sugar", "1.0"]]}, "212": {"qid": "ae723d6788b21ccdd2d7", "term": "Central Park Zoo", "description": "Zoo in Central Park, Manhattan, New York City", "question": "Is Central Park Zoo located on an island?", "answer": true, "facts": ["Central Park Zoo is located in Manhattan.", "Manhattan is an island."], "decomposition": ["Where is the Central Park Zoo located?", "Is #1 an island?"], "evidence": [[[["Central Park Zoo-1", "Central Park-1"]], [["Manhattan-1"], "operation"]], [[["Central Park Zoo-4"]], [["Central Park-2", "Manhattan-34"]]], [[["Central Park Zoo-1", "Central Park-1"]], [["Manhattan-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the Statue of Liberty a woman?\nFacts: The Statue of Liberty is a woman.\nAnswer (True or False): True\nQuestion: Is the Statue of Liberty a woman?\nFacts: The Statue of Liberty is a woman.\nAnswer (True or False): True\nQuestion: Is the Statue of Liberty a woman?\nFacts: The Statue of Liberty is a woman.\nAnswer (True or False): True\nQuestion: Is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Central Park Zoo located on an island?\nFacts: Central Park Zoo is located in Manhattan. Manhattan is an island.\nAnswer (True or False):", "ct_subject": "Central Park Zoo is located in Manhattan. Manhattan is an island.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9988707"], [" the", "0.99960226"], [" Statue", "1.0"], [" of", "1.0"], [" Liberty", "1.0"], [" a", "0.8627912"], [" woman", "0.99801886"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Statue", "1.0"], [" of", "1.0"], [" Liberty", "1.0"], [" is", "1.0"], [" a", "1.0"], [" woman", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" Statue", "1.0"], [" of", "1.0"], [" Liberty", "1.0"], [" a", "1.0"], [" woman", "0.99928445"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Statue", "1.0"], [" of", "1.0"], [" Liberty", "1.0"], [" is", "1.0"], [" a", "1.0"], [" woman", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" Statue", "1.0"], [" of", "1.0"], [" Liberty", "1.0"], [" a", "1.0"], [" woman", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Statue", "1.0"], [" of", "1.0"], [" Liberty", "1.0"], [" is", "1.0"], [" a", "1.0"], [" woman", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"]]}, "213": {"qid": "81af1391cbdbf67be3f7", "term": "Parachuting", "description": "action sport of exiting an aircraft and returning to Earth using a parachute", "question": "Is coal needed to practice parachuting?", "answer": true, "facts": ["Parachuting requires a parachute.", "Parachutes are made from nylon.", "Nylon is made from coal. "], "decomposition": ["What is one of the most important items that you need to go parachuting?", "What is #1 made out of?", "Is #2 originally made from coal?"], "evidence": [[[["Parachute-1"]], [["Nylon riots-3"]], [["Nylon-21"]]], [[["Parachute-1"]], [["Gerard B\u00e9rchet-2"]], [["Nylon-16"]]], [[["Parachute-1"]], [["Parachute-1"]], [["Nylon-16"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it true that the first person to jump out of a plane was a woman?\nFacts: The first person to jump out of a plane was a man. The first person to jump out of a plane was a woman. The first person to jump out of a plane was a man. The first person to jump out of a plane was a woman. The first person to jump out of a plane was a man", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is coal needed to practice parachuting?\nFacts: Parachuting requires a parachute. Parachutes are made from nylon. Nylon is made from coal. \nAnswer (True or False):", "ct_subject": "Parachuting requires a parachute. Parachutes are made from nylon. Nylon is made from coal. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99893314"], [" it", "0.99140036"], [" true", "0.99999964"], [" that", "1.0"], [" the", "1.0"], [" first", "0.9999994"], [" person", "0.73748916"], [" to", "1.0"], [" jump", "0.999998"], [" out", "0.21349242"], [" of", "1.0"], [" a", "1.0"], [" plane", "1.0"], [" was", "1.0"], [" a", "1.0"], [" woman", "0.99999964"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99991703"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" jump", "1.0"], [" out", "1.0"], [" of", "1.0"], [" a", "1.0"], [" plane", "1.0"], [" was", "1.0"], [" a", "1.0"], [" man", "0.9999999"], [".", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "0.8881781"], [" to", "1.0"], [" jump", "1.0"], [" out", "1.0"], [" of", "1.0"], [" a", "1.0"], [" plane", "1.0"], [" was", "1.0"], [" a", "1.0"], [" woman", "1.0"], [".", "1.0"], [" The", "0.9922127"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" jump", "1.0"], [" out", "1.0"], [" of", "1.0"], [" a", "1.0"], [" plane", "1.0"], [" was", "1.0"], [" a", "1.0"], [" man", "1.0"], [".", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" jump", "1.0"], [" out", "1.0"], [" of", "1.0"], [" a", "1.0"], [" plane", "1.0"], [" was", "1.0"], [" a", "1.0"], [" woman", "0.9386041"], [".", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" jump", "1.0"], [" out", "1.0"], [" of", "1.0"], [" a", "1.0"], [" plane", "1.0"], [" was", "1.0"], [" a", "1.0"], [" man", "0.9997795"]]}, "214": {"qid": "5db17a562505d84c0c74", "term": "Cactus", "description": "Family of mostly succulent plants, adapted to dry environments", "question": "Would an oil painter avoid reds from scale insects that live on a cactus?", "answer": true, "facts": ["Carmine is the product of an insect that lives on some cacti", "Carmine is not stable in oil paints and its usage has been discontinued", "Carmine is red"], "decomposition": ["What red pigments are made from insects?", "What scale insects live on cacti?", "What pigments overlap with #1 and #2?", "Is #3 unstable in oil paint?"], "evidence": [[[["Carmine-1"]], [["Cochineal-10"]], [["Cochineal-10"]], [["Carmine-9"]]], [[["Cochineal-1"], "no_evidence"], [["Scale insect-15"], "no_evidence"], [["Carmine-1"]], [["Carmine-9"], "operation"]], [[["Red-60"]], [["Cochineal-1", "Opuntia-31"]], ["operation"], [["Oil paint-18"], "no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"carpet\" and a \"carpeted\" house?\nFacts: A carpet is a carpeted house A carpet is a carpeted house A carpet is a carpeted house A carpet is a carpeted house\nAnswer (True or False): True\nQuestion: What is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would an oil painter avoid reds from scale insects that live on a cactus?\nFacts: Carmine is the product of an insect that lives on some cacti Carmine is not stable in oil paints and its usage has been discontinued Carmine is red\nAnswer (True or False):", "ct_subject": "Carmine is the product of an insect that lives on some cacti Carmine is not stable in oil paints and its usage has been discontinued Carmine is red", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999962"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9992144"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.99999976"], ["c", "0.9990214"], ["arp", "0.7777354"], ["et", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "0.9999988"], ["arp", "1.0"], ["et", "1.0"], ["ed", "1.0"], ["\"", "0.99988663"], [" house", "0.99998176"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" carpet", "1.0"], [" is", "0.9999999"], [" a", "1.0"], [" carpet", "1.0"], ["ed", "1.0"], [" house", "1.0"], [" A", "0.4385297"], [" carpet", "1.0"], [" is", "0.9999833"], [" a", "1.0"], [" carpet", "1.0"], ["ed", "1.0"], [" house", "1.0"], [" A", "0.9992403"], [" carpet", "1.0"], [" is", "1.0"], [" a", "1.0"], [" carpet", "1.0"], ["ed", "1.0"], [" house", "1.0"], [" A", "0.94146526"], [" carpet", "1.0"], [" is", "1.0"], [" a", "1.0"], [" carpet", "1.0"], ["ed", "1.0"], [" house", "1.0"], ["\n", "0.99819297"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"]]}, "215": {"qid": "4d20bb8dc217f39ee929", "term": "New York Public Library", "description": "Public library system in New York City", "question": "Could you go to New York Public Library and the Six Flags Great Escape in the same day?", "answer": true, "facts": ["Six Flags Great Escape is located in Lake George, NY.", "New York Public Library is located in New York City.", "Lake George is 3.5 driving hours from New York City."], "decomposition": ["Where is Six Flags Great Escape located?", "Where is The New York Public Library located?", "How long does it take to drive from #1 to #2?", "Is #3 less than 24 hours?"], "evidence": [[[["The Great Escape and Hurricane Harbor-1"]], [["New York Public Library-1"]], ["no_evidence"], ["no_evidence"]], [[["The Great Escape and Hurricane Harbor-1"]], [["New York Public Library Main Branch-1"]], ["no_evidence"], ["operation"]], [[["The Great Escape and Hurricane Harbor-1"]], [["New York Public Library-1"]], ["no_evidence", "operation"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the largest city without a subway system?\nFacts: New York City.\nAnswer (True or False): True\nQuestion: What is the name of the city in the United States that has the largest population of people who are not", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could you go to New York Public Library and the Six Flags Great Escape in the same day?\nFacts: Six Flags Great Escape is located in Lake George, NY. New York Public Library is located in New York City. Lake George is 3.5 driving hours from New York City.\nAnswer (True or False):", "ct_subject": "Six Flags Great Escape is located in Lake George, NY. New York Public Library is located in New York City. Lake George is 3.5 driving hours from New York City.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99994886"], [" is", "1.0"], [" the", "1.0"], [" name", "0.8218053"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9999753"], [" in", "0.97884536"], [" the", "0.99982196"], [" United", "0.99999845"], [" States", "1.0"], [" that", "1.0"], [" is", "0.37916645"], [" the", "1.0"], [" largest", "0.9784856"], [" city", "0.4080616"], [" without", "0.9953963"], [" a", "1.0"], [" subway", "0.999081"], [" system", "0.99988496"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" New", "0.5177689"], [" York", "1.0"], [" City", "1.0"], [".", "0.9998776"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "0.99998355"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" has", "0.10419213"], [" the", "1.0"], [" largest", "0.9991059"], [" population", "0.9999728"], [" of", "1.0"], [" people", "1.0"], [" who", "0.99999833"], [" are", "0.99804115"], [" not", "1.0"]]}, "216": {"qid": "ec6b6fdf9c72f77dac67", "term": "Goofy", "description": "Disney cartoon character", "question": "Did brother of Goofy creator's employer commit an excommunicable offense?", "answer": true, "facts": ["Goofy was created by Art Babbitt who worked for Walt Disney.", "Walt Disney's brother, Roy Disney, was a member of the Freemasons.", "The Catholic Church has a litany of offenses that can get someone excommunicated.", "Being initiated to Freemasonry, is listed as an excommunicable offense."], "decomposition": ["Who created the character Goofy?", "Who did #1 work for?", "Who are #2's siblings?", "Are any of #3 Freemasons?", "Is #4 a reason for excommunication?"], "evidence": [[[["Goofy-4"]], [["Pinto Colvig-8"]], [["Roy O. Disney-1"]], ["no_evidence"], [["Freemasonry-62"], "operation"]], [[["Goofy-2"]], [["Goofy-2", "Goofy-43"], "no_evidence"], ["no_evidence"], ["no_evidence"], ["no_evidence"]], [[["Goofy-4"], "no_evidence"], [["Walt Disney-1"]], [["Flora Call Disney-4"], "no_evidence"], ["no_evidence"], [["Papal ban of Freemasonry-1"], "no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the first man on the moon land on the moon?\nFacts: The first man on the moon was Neil Armstrong. The first man on the moon was Neil Armstrong.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did brother of Goofy creator's employer commit an excommunicable offense?\nFacts: Goofy was created by Art Babbitt who worked for Walt Disney. Walt Disney's brother, Roy Disney, was a member of the Freemasons. The Catholic Church has a litany of offenses that can get someone excommunicated. Being initiated to Freemasonry, is listed as an excommunicable offense.\nAnswer (True or False):", "ct_subject": "Goofy was created by Art Babbitt who worked for Walt Disney. Walt Disney's brother, Roy Disney, was a member of the Freemasons. The Catholic Church has a litany of offenses that can get someone excommunicated. Being initiated to Freemasonry, is listed as an excommunicable offense.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.977439"], [" the", "1.0"], [" first", "0.9995041"], [" man", "0.8492986"], [" on", "0.9999988"], [" the", "1.0"], [" moon", "1.0"], [" land", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" man", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" was", "0.9719701"], [" Neil", "1.0"], [" Armstrong", "1.0"], [".", "1.0"], [" The", "0.91080326"], [" first", "1.0"], [" man", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" was", "0.99996936"], [" Neil", "0.2627118"], [" Armstrong", "1.0"], [".", "1.0"]]}, "217": {"qid": "5fd0dc99fdf46de79b6a", "term": "Citrus", "description": "genus of fruit-bearing plants (source of fruit such as lemons and oranges)", "question": "Would someone on antidepressants need to be cautious of some citrus fruits?", "answer": true, "facts": ["Grapefruit is a citrus fruit.", "Grapefruit can cause some medications to reach unintentionally high levels in the body. ", "SSRI's are a medication type that can be affected by grapefruit."], "decomposition": ["Which fruits can affect antidepressant medications?", "Is #1 a citrus fruit?"], "evidence": [[["no_evidence"], ["no_evidence"]], [[["Antidepressant-30"]], ["no_evidence", "operation"]], [[["Grapefruit\u2013drug interactions-1", "Grapefruit\u2013drug interactions-2"]], [["Grapefruit\u2013drug interactions-2"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"sugar pill\" and a \"sugar pill with a sugar substitute\"?\nFacts: A sugar pill is a medication that is taken to treat a specific condition. A sugar substitute is a medication that is taken to treat a specific condition.\nAnswer (True or False): True\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would someone on antidepressants need to be cautious of some citrus fruits?\nFacts: Grapefruit is a citrus fruit. Grapefruit can cause some medications to reach unintentionally high levels in the body.  SSRI's are a medication type that can be affected by grapefruit.\nAnswer (True or False):", "ct_subject": "Grapefruit is a citrus fruit. Grapefruit can cause some medications to reach unintentionally high levels in the body.  SSRI's are a medication type that can be affected by grapefruit.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9990119"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9999449"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.50688213"], ["ugar", "0.9922118"], [" pill", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.9999608"], ["ugar", "1.0"], [" pill", "1.0"], [" with", "1.0"], [" a", "0.99999976"], [" sugar", "0.3923919"], [" substitute", "0.98284894"], ["\"?", "0.9997527"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.8851019"], [" sugar", "1.0"], [" pill", "1.0"], [" is", "1.0"], [" a", "1.0"], [" medication", "0.26580402"], [" that", "1.0"], [" is", "0.9738254"], [" taken", "0.99999905"], [" to", "0.9994535"], [" treat", "0.99932015"], [" a", "1.0"], [" specific", "0.7210112"], [" condition", "0.9892992"], [".", "1.0"], [" A", "1.0"], [" sugar", "1.0"], [" substitute", "0.97729915"], [" is", "1.0"], [" a", "1.0"], [" medication", "1.0"], [" that", "1.0"], [" is", "1.0"], [" taken", "1.0"], [" to", "1.0"], [" treat", "0.47916853"], [" a", "1.0"], [" specific", "1.0"], [" condition", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"]]}, "218": {"qid": "cf0a1e2e39a6af9a79a9", "term": "Durian", "description": "genus of plants", "question": "Could Durian cause someone's stomach to feel unwell?", "answer": true, "facts": ["Durian has a pungent odor that many people describe as being similar to feet and onions.", "Unpleasant smells can make people feel nauseous. "], "decomposition": ["What would some people describe durian's smell as?", "Would #1  cause some people to feel unwell?"], "evidence": [[[["Durian-3"]], [["Durian-50"]]], [[["Durian-3"]], ["operation"]], [[["Durian-29"]], [["Durian-50"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake with a hole in it\"?\nFacts: A snowflake is a single crystal of ice that has been frozen. A snowflake with a hole in it is a single crystal of ice that has been melted.\nAnswer (True or False): True\nQuestion: What is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could Durian cause someone's stomach to feel unwell?\nFacts: Durian has a pungent odor that many people describe as being similar to feet and onions. Unpleasant smells can make people feel nauseous. \nAnswer (True or False):", "ct_subject": "Durian has a pungent odor that many people describe as being similar to feet and onions. Unpleasant smells can make people feel nauseous. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9993369"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.95487344"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.9918719"], ["s", "0.9562119"], ["now", "0.9971625"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.9993629"], ["now", "1.0"], ["flake", "1.0"], [" with", "0.7287604"], [" a", "1.0"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.99999785"], [" snow", "0.9999995"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "0.964818"], [" crystal", "0.9998054"], [" of", "0.5783945"], [" ice", "1.0"], [" that", "0.9783155"], [" has", "0.9105205"], [" been", "0.99999964"], [" frozen", "0.8411619"], [".", "1.0"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" with", "1.0"], [" a", "1.0"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "0.6114024"], [" crystal", "1.0"], [" of", "1.0"], [" ice", "1.0"], [" that", "1.0"], [" has", "1.0"], [" been", "1.0"], [" melted", "0.913335"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"]]}, "219": {"qid": "947a089ce6992869815a", "term": "Swallow", "description": "family of birds", "question": "Did the swallow play a role in a famous film about King Arthur?", "answer": true, "facts": ["Monty Python and the Holy Grail was a famous film about King Arthur", "In Monty Python and the Holy Grail, swallows are mentioned several times"], "decomposition": ["What Monty Python film is about King Arthur?", "Are swallows mentioned several times in #1?"], "evidence": [[[["Monty Python and the Holy Grail-1"]], ["no_evidence", "operation"]], [[["Monty Python and the Holy Grail-1"]], [["Monty Python and the Holy Grail-4"]]], [[["Monty Python and the Holy Grail-2", "Monty Python and the Holy Grail-4", "Monty Python and the Holy Grail-9"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the swallow play a role in a famous film about King Arthur?\nFacts: Monty Python and the Holy Grail was a famous film about King Arthur In Monty Python and the Holy Grail, swallows are mentioned several times\nAnswer (True or False): True\nQuestion: Did the swallow play a role in a famous film about King Arthur?\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did the swallow play a role in a famous film about King Arthur?\nFacts: Monty Python and the Holy Grail was a famous film about King Arthur In Monty Python and the Holy Grail, swallows are mentioned several times\nAnswer (True or False):", "ct_subject": "Monty Python and the Holy Grail was a famous film about King Arthur In Monty Python and the Holy Grail, swallows are mentioned several times", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.98192316"], [" the", "1.0"], [" swallow", "0.9999292"], [" play", "1.0"], [" a", "1.0"], [" role", "1.0"], [" in", "1.0"], [" a", "0.9999788"], [" famous", "1.0"], [" film", "0.9999709"], [" about", "1.0"], [" King", "1.0"], [" Arthur", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Mont", "1.0"], ["y", "1.0"], [" Python", "1.0"], [" and", "1.0"], [" the", "1.0"], [" Holy", "1.0"], [" Grail", "1.0"], [" was", "1.0"], [" a", "1.0"], [" famous", "1.0"], [" film", "1.0"], [" about", "1.0"], [" King", "1.0"], [" Arthur", "1.0"], [" In", "1.0"], [" Mont", "1.0"], ["y", "1.0"], [" Python", "1.0"], [" and", "1.0"], [" the", "1.0"], [" Holy", "1.0"], [" Grail", "1.0"], [",", "1.0"], [" swall", "1.0"], ["ows", "1.0"], [" are", "1.0"], [" mentioned", "1.0"], [" several", "1.0"], [" times", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" swallow", "1.0"], [" play", "1.0"], [" a", "1.0"], [" role", "1.0"], [" in", "1.0"], [" a", "1.0"], [" famous", "1.0"], [" film", "1.0"], [" about", "1.0"], [" King", "1.0"], [" Arthur", "1.0"], ["?", "1.0"], ["\n", "1.0"]]}, "220": {"qid": "e175b012fc9b5db8da3f", "term": "Pan (god)", "description": "Ancient Greek god of the wilds, shepherds, and flocks", "question": "Does the Boy Who Cried Wolf hypothetically have reason to pray to Pan?", "answer": true, "facts": ["Pan is the ancient Greek god of the wild, shepherds and flocks.", "The Boy Who Cried Wolf, from Aesop's Fables, was a shepherd boy."], "decomposition": ["What is the profession of The Boy Who Cried Wolf?", "What profession is Pan the god of?", "Is #1 the same as #2?"], "evidence": [[[["The Boy Who Cried Wolf-2"]], [["Pan (god)-1"]], ["operation"]], [[["The Boy Who Cried Wolf-2"]], [["Pan (god)-1"]], ["operation"]], [[["The Boy Who Cried Wolf-2"]], [["Pan (god)-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the Boy Who Cried Wolf a real person?\nFacts: The Boy Who Cried Wolf is a fictional character.\nAnswer (True or False): True\nQuestion: Is the Boy Who Cried Wolf a real person?\nFacts: The Boy Who Cried Wolf is a fictional character.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does the Boy Who Cried Wolf hypothetically have reason to pray to Pan?\nFacts: Pan is the ancient Greek god of the wild, shepherds and flocks. The Boy Who Cried Wolf, from Aesop's Fables, was a shepherd boy.\nAnswer (True or False):", "ct_subject": "Pan is the ancient Greek god of the wild, shepherds and flocks. The Boy Who Cried Wolf, from Aesop's Fables, was a shepherd boy.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.21728049"], [" the", "0.9999733"], [" Boy", "0.99999833"], [" Who", "1.0"], [" C", "1.0"], ["ried", "1.0"], [" Wolf", "1.0"], [" a", "0.9999826"], [" real", "0.33627996"], [" person", "0.9999949"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Boy", "1.0"], [" Who", "1.0"], [" C", "1.0"], ["ried", "1.0"], [" Wolf", "1.0"], [" is", "1.0"], [" a", "1.0"], [" fictional", "1.0"], [" character", "1.0"], [".", "0.9595979"], ["\n", "0.9999994"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99982494"], [" the", "1.0"], [" Boy", "1.0"], [" Who", "1.0"], [" C", "1.0"], ["ried", "1.0"], [" Wolf", "1.0"], [" a", "1.0"], [" real", "1.0"], [" person", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Boy", "1.0"], [" Who", "1.0"], [" C", "1.0"], ["ried", "1.0"], [" Wolf", "1.0"], [" is", "1.0"], [" a", "1.0"], [" fictional", "1.0"], [" character", "1.0"], [".", "1.0"]]}, "221": {"qid": "2930044a09f13c95fb91", "term": "Conducting", "description": "Directing a musical performance by way of visible gestures", "question": "Is a paraplegic suitable for conducting an orchestra?", "answer": true, "facts": ["Musical conductors use their hands to wave a baton and guide the orchestra.", "Paraplegics are people that are paralyzed from the waist down.", "Paraplegics are able to play sports that involve their upper body, such as basketball."], "decomposition": ["What part(s) of the body is/are needed to conduct an orchestra?", "Which portion of a paraplegic's body is paralyzed?", "Are #1 separate from #2?"], "evidence": [[[["Conducting-1"]], [["Paraplegia-1"]], ["operation"]], [[["Conducting-1"]], [["Paraplegia-2"]], [["Conducting-1", "Paraplegia-2"]]], [[["Conducting-1"]], [["Paraplegia-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a man and a woman?\nFacts: A man is a male human being. A woman is a female human being.\nAnswer (True or False): True\nQuestion: What is the difference between a man and a woman?\nFacts", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is a paraplegic suitable for conducting an orchestra?\nFacts: Musical conductors use their hands to wave a baton and guide the orchestra. Paraplegics are people that are paralyzed from the waist down. Paraplegics are able to play sports that involve their upper body, such as basketball.\nAnswer (True or False):", "ct_subject": "Musical conductors use their hands to wave a baton and guide the orchestra. Paraplegics are people that are paralyzed from the waist down. Paraplegics are able to play sports that involve their upper body, such as basketball.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.961774"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.99959916"], [" between", "1.0"], [" a", "1.0"], [" man", "0.23987956"], [" and", "1.0"], [" a", "1.0"], [" woman", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" man", "1.0"], [" is", "1.0"], [" a", "1.0"], [" male", "0.9997317"], [" human", "0.99999857"], [" being", "1.0"], [".", "1.0"], [" A", "1.0"], [" woman", "1.0"], [" is", "1.0"], [" a", "1.0"], [" female", "1.0"], [" human", "1.0"], [" being", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" man", "1.0"], [" and", "1.0"], [" a", "1.0"], [" woman", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"]]}, "222": {"qid": "b0ee9781a2840b582d9d", "term": "Nicole Kidman", "description": "Australian-American actress and film producer", "question": "Is Nicole Kidman ideal choice to play Psylocke based on height and weight?", "answer": true, "facts": ["Psylocke is a Marvel super hero whose real name is Betsy Braddock.", "Betsy Braddock is 5'11 and 155 lbs.", "Actress Nicole Kidman is 5'11 and weighs 137 lbs.", "Actresses gain weight all the time for roles, such as Charlize Theron who gained 30 pounds for the movie Monster."], "decomposition": ["What is Psylocke's height?", "What is Psylocke's wieght?", "Does Nicole Kidman have similar attributes as #1 and #2?"], "evidence": [[[["Psylocke-2"], "no_evidence"], ["no_evidence"], [["Nicole Kidman-1"], "no_evidence", "operation"]], [[["Psylocke-2"], "no_evidence"], [["Psylocke-2"], "no_evidence"], ["no_evidence", "operation"]], [[["Psylocke-4"], "no_evidence"], [["Psylocke-4"], "no_evidence"], [["Model (person)-19", "Model (person)-20", "Nicole Kidman-1"], "no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is there a difference between the two types of people who are called \"The Avengers\"?\nFacts: The Avengers are a group of superheroes who are all members of the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Nicole Kidman ideal choice to play Psylocke based on height and weight?\nFacts: Psylocke is a Marvel super hero whose real name is Betsy Braddock. Betsy Braddock is 5'11 and 155 lbs. Actress Nicole Kidman is 5'11 and weighs 137 lbs. Actresses gain weight all the time for roles, such as Charlize Theron who gained 30 pounds for the movie Monster.\nAnswer (True or False):", "ct_subject": "Psylocke is a Marvel super hero whose real name is Betsy Braddock. Betsy Braddock is 5'11 and 155 lbs. Actress Nicole Kidman is 5'11 and weighs 137 lbs. Actresses gain weight all the time for roles, such as Charlize Theron who gained 30 pounds for the movie Monster.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9972984"], [" there", "0.20165183"], [" a", "1.0"], [" difference", "0.9999534"], [" between", "0.99999976"], [" the", "0.9746294"], [" two", "0.95841306"], [" types", "0.9999999"], [" of", "1.0"], [" people", "0.999801"], [" who", "0.9995696"], [" are", "0.9658216"], [" called", "0.99938035"], [" \"", "1.0"], ["The", "0.93620276"], [" Avengers", "0.14853814"], ["\"?", "0.88999575"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Avengers", "1.0"], [" are", "1.0"], [" a", "0.9999995"], [" group", "0.96122324"], [" of", "1.0"], [" superheroes", "1.0"], [" who", "1.0"], [" are", "0.99999964"], [" all", "0.99540424"], [" members", "0.87896997"], [" of", "1.0"], [" the", "1.0"]]}, "223": {"qid": "35a5b27d84294ebf1fe0", "term": "Satanism", "description": "group of ideological and philosophical beliefs based on Satan", "question": "Is Capricorn the hypothetical zodiac sign of Satanism?", "answer": true, "facts": ["Satanism is a group of beliefs based on Satan and has numerous symbols.", "Satan (The Devil) is often depicted as the deity Baphomet, a Sabbatic Goat with a human body and goat head.", "The Capricorn is the zodiac sign symbolized by the goat.", "The Tarot card attributed to the Zodiac sign Capricorn is The Devil."], "decomposition": ["What animal represents the zodiac sign Capricorn?", "What are some symbols in Satanism?", "Is #1 among #2?"], "evidence": [[[["Capricorn (astrology)-1"]], [["Baphomet-1", "Baphomet-3"]], ["operation"]], [[["Capricorn (astrology)-1"]], [["Sigil of Baphomet-2"]], ["operation"]], [[["Capricorn (astrology)-2"]], [["Baphomet-3", "LaVeyan Satanism-40"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the Roman god of the underworld, Pluto, a reference to the Greek god of the underworld, Hades?\nFacts:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Capricorn the hypothetical zodiac sign of Satanism?\nFacts: Satanism is a group of beliefs based on Satan and has numerous symbols. Satan (The Devil) is often depicted as the deity Baphomet, a Sabbatic Goat with a human body and goat head. The Capricorn is the zodiac sign symbolized by the goat. The Tarot card attributed to the Zodiac sign Capricorn is The Devil.\nAnswer (True or False):", "ct_subject": "Satanism is a group of beliefs based on Satan and has numerous symbols. Satan (The Devil) is often depicted as the deity Baphomet, a Sabbatic Goat with a human body and goat head. The Capricorn is the zodiac sign symbolized by the goat. The Tarot card attributed to the Zodiac sign Capricorn is The Devil.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99963915"], [" the", "1.0"], [" name", "0.48482433"], [" of", "0.9999485"], [" the", "1.0"], [" Roman", "0.99835736"], [" god", "1.0"], [" of", "0.9999968"], [" the", "1.0"], [" underworld", "1.0"], [",", "1.0"], [" Pluto", "0.8384169"], [",", "1.0"], [" a", "0.9999993"], [" reference", "0.99999654"], [" to", "1.0"], [" the", "1.0"], [" Greek", "0.71934354"], [" god", "1.0"], [" of", "1.0"], [" the", "1.0"], [" underworld", "1.0"], [",", "1.0"], [" Hades", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"]]}, "224": {"qid": "cdc41c9bae19c441248d", "term": "Easter", "description": "Major Christian festival celebrating the resurrection of Jesus", "question": "Does Adam Sandler skip celebrating Easter?", "answer": true, "facts": ["Adam Sandler is Jewish.", "Jewish religious people do not celebrate Easter."], "decomposition": ["Easter is usually celebrated by people of which religion?", "What is Adam Sandler's religion?", "Is #1 different from #2?"], "evidence": [[[["Easter-6"]], [["Adam Sandler-26"]], ["operation"]], [[["Easter-58"]], [["Adam Sandler-26"]], ["operation"]], [[["Easter-2"]], [["Adam Sandler-5"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city of Rome derived from the name of the Roman god?\nFacts: The name of the city of Rome is derived from the name of the Roman god.\nAnswer (True or False): True\nQuestion: Is the name of the city of Rome derived from the name of the Roman god?\nFacts: The name of the city of Rome is derived from the name of the Roman god.\nAnswer (True or False):", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Adam Sandler skip celebrating Easter?\nFacts: Adam Sandler is Jewish. Jewish religious people do not celebrate Easter.\nAnswer (True or False):", "ct_subject": "Adam Sandler is Jewish. Jewish religious people do not celebrate Easter.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.46986833"], [" the", "0.9997749"], [" name", "0.4173463"], [" of", "0.9930703"], [" the", "1.0"], [" city", "0.99834275"], [" of", "1.0"], [" Rome", "0.99980706"], [" derived", "0.9950618"], [" from", "1.0"], [" the", "1.0"], [" name", "0.95143884"], [" of", "1.0"], [" the", "1.0"], [" Roman", "0.9999206"], [" god", "0.6229899"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "0.057081256"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], [" is", "1.0"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" god", "1.0"], [".", "0.7998897"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9997218"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" god", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], [" is", "1.0"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" god", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"]]}, "225": {"qid": "077cce9150f60c6576ea", "term": "Alcatraz Island", "description": "United States historic place", "question": "Is there historic graffiti on Alcatraz?", "answer": true, "facts": ["Native Americans occupied the island of Alcatraz in 1969.", "Wall writings from the Native American occupation has been preserved and in some cases, restored completely."], "decomposition": ["Who were the occupants of the island of Alcatraz in 1969?", "Did #1 make wall writings?"], "evidence": [[[["Alcatraz Island-1"]], [["Alcatraz Island-23"], "operation"]], [[["Alcatraz Island-1"]], ["no_evidence", "operation"]], [[["Alcatraz Island-1"]], [["Alcatraz Island-23"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is there a statue of Genghis Khan in the United States?\nFacts: There is a statue of Genghis Khan in the United States.\nAnswer (True or False): True\nQuestion: Is there a statue of Genghis Khan in the United Kingdom?\nFacts: There is a statue of Genghis Khan in the United Kingdom.\nAnswer", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is there historic graffiti on Alcatraz?\nFacts: Native Americans occupied the island of Alcatraz in 1969. Wall writings from the Native American occupation has been preserved and in some cases, restored completely.\nAnswer (True or False):", "ct_subject": "Native Americans occupied the island of Alcatraz in 1969. Wall writings from the Native American occupation has been preserved and in some cases, restored completely.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999821"], [" there", "1.0"], [" a", "1.0"], [" statue", "0.9999901"], [" of", "1.0"], [" Gen", "0.9999988"], ["gh", "1.0"], ["is", "1.0"], [" Khan", "1.0"], [" in", "0.9999608"], [" the", "0.9999708"], [" United", "0.99999654"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "1.0"], [" is", "1.0"], [" a", "0.9999323"], [" statue", "1.0"], [" of", "1.0"], [" Gen", "1.0"], ["gh", "1.0"], ["is", "1.0"], [" Khan", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [".", "0.9999443"], ["\n", "0.99982256"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" there", "1.0"], [" a", "1.0"], [" statue", "1.0"], [" of", "1.0"], [" Gen", "1.0"], ["gh", "1.0"], ["is", "1.0"], [" Khan", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" Kingdom", "0.84521323"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "1.0"], [" is", "1.0"], [" a", "1.0"], [" statue", "1.0"], [" of", "1.0"], [" Gen", "1.0"], ["gh", "1.0"], ["is", "1.0"], [" Khan", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" Kingdom", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"]]}, "226": {"qid": "be5ba924348f83b5df96", "term": "United Airlines", "description": "Airline in the United States", "question": "Was United Airlines blameless in worst crash in history?", "answer": true, "facts": ["The Tenerife Airport disaster is the deadliest crash in aviation history.", "The Tenerife Airport disaster involved a Boeing plane and a Pan Am plane.", "Pan Am airlines competed with United Airlines and other US companies.", "Boeing is an American multinational corporation that designs and sells airplanes, rockets, satellites,and missiles."], "decomposition": ["Which aviation accident is considered the worst in aviation history?", "Is United Airlines excluded from #1?"], "evidence": [[[["Tenerife airport disaster-1"]], ["operation"]], [[["Tenerife airport disaster-1"]], [["United Airlines-1"], "operation"]], [[["Tenerife-36"]], [["Tenerife airport disaster-3"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the largest number of people that have ever lived on Earth?\nFacts: The largest number of people that have ever lived on Earth is estimated to be between 10,000,000 and 100,000,", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was United Airlines blameless in worst crash in history?\nFacts: The Tenerife Airport disaster is the deadliest crash in aviation history. The Tenerife Airport disaster involved a Boeing plane and a Pan Am plane. Pan Am airlines competed with United Airlines and other US companies. Boeing is an American multinational corporation that designs and sells airplanes, rockets, satellites,and missiles.\nAnswer (True or False):", "ct_subject": "The Tenerife Airport disaster is the deadliest crash in aviation history. The Tenerife Airport disaster involved a Boeing plane and a Pan Am plane. Pan Am airlines competed with United Airlines and other US companies. Boeing is an American multinational corporation that designs and sells airplanes, rockets, satellites,and missiles.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9876117"], [" is", "1.0"], [" the", "1.0"], [" largest", "0.7805755"], [" number", "0.81177866"], [" of", "1.0"], [" people", "1.0"], [" that", "0.99998033"], [" have", "0.5896721"], [" ever", "1.0"], [" lived", "1.0"], [" on", "0.9999999"], [" Earth", "0.99999976"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99924135"], [" largest", "0.9698785"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" that", "1.0"], [" have", "1.0"], [" ever", "1.0"], [" lived", "1.0"], [" on", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" estimated", "0.99427795"], [" to", "0.9999988"], [" be", "1.0"], [" between", "0.8170292"], [" 10", "0.9999994"], [",", "0.97148365"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [" and", "1.0"], [" 100", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"]]}, "227": {"qid": "625080d6e74261c523f6", "term": "Bohai Sea", "description": "The innermost gulf of the Yellow Sea and Korea Bay on the coast of Northeastern and North China", "question": "Would Statue of Liberty be visible if submerged in Bohai Sea?", "answer": true, "facts": ["The Bohai Sea is 230 feet deep.", "The Statue of Liberty is 305 feet tall."], "decomposition": ["How deep is the Bohai Sea?", "How tall is the Statue of Liberty?", "Is #2 greater than #1?"], "evidence": [[["no_evidence"], [["Statue of Liberty-18"]], ["operation"]], [["no_evidence"], [["Statue of Liberty-18"]], ["no_evidence"]], [["no_evidence"], [["Statue of Liberty-18"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the largest number of people that can fit into a single room?\nFacts: The largest room in the world is the Great Pyramid of Giza. The room is 1,976 feet long, 1,848 feet wide, and 1,848 feet high.\nAnswer (True or False): True\nQuestion: What is the largest number of people that can fit into a single room?\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Statue of Liberty be visible if submerged in Bohai Sea?\nFacts: The Bohai Sea is 230 feet deep. The Statue of Liberty is 305 feet tall.\nAnswer (True or False):", "ct_subject": "The Bohai Sea is 230 feet deep. The Statue of Liberty is 305 feet tall.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999974"], [" is", "1.0"], [" the", "1.0"], [" largest", "0.6416063"], [" number", "0.99613816"], [" of", "1.0"], [" people", "1.0"], [" that", "0.99984074"], [" can", "0.99999857"], [" fit", "1.0"], [" into", "0.092172466"], [" a", "0.9999999"], [" single", "0.9999999"], [" room", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999753"], [" largest", "1.0"], [" room", "0.9447396"], [" in", "0.99985754"], [" the", "1.0"], [" world", "1.0"], [" is", "1.0"], [" the", "1.0"], [" Great", "0.9999784"], [" Pyramid", "1.0"], [" of", "1.0"], [" G", "1.0"], ["iza", "1.0"], [".", "1.0"], [" The", "0.9999119"], [" room", "1.0"], [" is", "0.99999976"], [" 1", "0.08391319"], [",", "1.0"], ["9", "0.9757313"], ["76", "0.96145564"], [" feet", "1.0"], [" long", "1.0"], [",", "0.90554184"], [" 1", "0.9996568"], [",", "1.0"], ["8", "0.99998224"], ["48", "0.98351675"], [" feet", "1.0"], [" wide", "1.0"], [",", "0.99481887"], [" and", "1.0"], [" 1", "1.0"], [",", "1.0"], ["8", "0.99927276"], ["48", "1.0"], [" feet", "1.0"], [" high", "1.0"], [".", "1.0"], ["\n", "0.9998859"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" that", "1.0"], [" can", "1.0"], [" fit", "1.0"], [" into", "1.0"], [" a", "1.0"], [" single", "1.0"], [" room", "1.0"], ["?", "1.0"], ["\n", "1.0"]]}, "228": {"qid": "f56ebcf7b0a06bcb7d97", "term": "Fake news", "description": "Hoax or deliberate spread of misinformation", "question": "Have Jamie Lee Curtis been the subject of fake news?", "answer": true, "facts": ["Fake news is a hoax that is circulated and spreads to get people to believe a falsehood.", "Jamie Lee Curtis is an American actress known for the Halloween series.", "Rumors have been spread about Jamie Lee Curtis that she was born a hermaphrodite."], "decomposition": ["What is fake news?", "Has Jamie Lee Curtis ever been the victim of #1?"], "evidence": [[[["Fake news in India-1"]], ["no_evidence", "operation"]], [[["Fake news-7"]], [["Jamie Lee Curtis-25"], "no_evidence"]], [[["Fake news-1"]], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the country that is the largest in the world?\nFacts: The United States of America is the largest country in the world.\nAnswer (True or False): True\nQuestion: What is the name of the country that is the largest", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Have Jamie Lee Curtis been the subject of fake news?\nFacts: Fake news is a hoax that is circulated and spreads to get people to believe a falsehood. Jamie Lee Curtis is an American actress known for the Halloween series. Rumors have been spread about Jamie Lee Curtis that she was born a hermaphrodite.\nAnswer (True or False):", "ct_subject": "Fake news is a hoax that is circulated and spreads to get people to believe a falsehood. Jamie Lee Curtis is an American actress known for the Halloween series. Rumors have been spread about Jamie Lee Curtis that she was born a hermaphrodite.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99987435"], [" is", "1.0"], [" the", "1.0"], [" name", "0.93970203"], [" of", "1.0"], [" the", "1.0"], [" country", "0.04378682"], [" that", "0.9986971"], [" is", "0.99986196"], [" the", "1.0"], [" largest", "0.99960726"], [" in", "0.99987936"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "0.99977857"], [" States", "1.0"], [" of", "1.0"], [" America", "1.0"], [" is", "0.82216746"], [" the", "1.0"], [" largest", "1.0"], [" country", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [".", "1.0"], ["\n", "0.9999988"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" country", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"]]}, "229": {"qid": "4ad7adb3122739af2458", "term": "JPMorgan Chase", "description": "American multinational banking and financial services holding company", "question": "Could JPMorgan Chase give every American $10?", "answer": true, "facts": ["JPMorgan Chase has total assets of US$2.687 trillion.", "As of November 8, 2018, the United States is estimated to have a population of 328,953,020.", "One trillion is equal to 1,000 billions.", "One billion is equal to 1,000 millions."], "decomposition": ["How much are the total assets of JPMorgan Chase?", "What is the population of the United States?", "Is #2 times $10 less than #1?"], "evidence": [[[["JPMorgan Chase-1"]], [["Demographics of the United States-1"]], ["operation"]], [[["JPMorgan Chase-1"]], [["United States-1"]], ["operation"]], [[["JPMorgan Chase-1"]], [["United States-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average height of a man in the United States?\nFacts: The average height of a man in the United States is 5'9\".\nAnswer (True or False): True\nQuestion: What is the average weight of a man", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could JPMorgan Chase give every American $10?\nFacts: JPMorgan Chase has total assets of US$2.687 trillion. As of November 8, 2018, the United States is estimated to have a population of 328,953,020. One trillion is equal to 1,000 billions. One billion is equal to 1,000 millions.\nAnswer (True or False):", "ct_subject": "JPMorgan Chase has total assets of US$2.687 trillion. As of November 8, 2018, the United States is estimated to have a population of 328,953,020. One trillion is equal to 1,000 billions. One billion is equal to 1,000 millions.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999201"], [" is", "1.0"], [" the", "1.0"], [" average", "0.97410214"], [" height", "0.15325974"], [" of", "1.0"], [" a", "0.9999982"], [" man", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" height", "1.0"], [" of", "1.0"], [" a", "1.0"], [" man", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "1.0"], [" 5", "0.99999964"], ["'", "0.9999275"], ["9", "0.08478656"], ["\".", "0.988684"], ["\n", "0.9999995"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "1.0"], [" of", "1.0"], [" a", "1.0"], [" man", "1.0"]]}, "230": {"qid": "a50ae7ec93a5ac585694", "term": "Noah's Ark", "description": "the vessel in the Genesis flood narrative", "question": "Is Noah's Ark an upgrade for Golden Age of Piracy pirates?", "answer": true, "facts": ["The Golden Age of Piracy took place between 1650 and 1720 and had many famous pirates including Blackbeard.", "Noah's Ark had dimensions in feet of 450 x 75 x 45.", "Blackbeard's ship, Queen Anne's Revenge, had a length of 103 feet.", "William Kidd's ship, Adventure Galley, was 124 feet long."], "decomposition": ["Which pirates were famously known during the Golden Age of Piracy?", "What were the dimensions of Noah's Ark?", "Is #2 greater than the dimensions of the ships of most of #1?"], "evidence": [[[["William Kidd-1"]], [["Noah's Ark-3"]], [["Noah's Ark-3", "William Kidd-9"], "no_evidence"]], [[["Whydah Gally-6"], "no_evidence"], [["Noah's Ark-3"]], ["no_evidence"]], [[["Blackbeard-1", "Henry Morgan-1", "Samuel Bellamy-1"], "no_evidence"], [["Noah's Ark-3"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Was the Great Pyramid built by the Egyptians?\nFacts: The Great Pyramid was built by the Egyptians in the 13th century BC. The Great Pyramid was built in", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Noah's Ark an upgrade for Golden Age of Piracy pirates?\nFacts: The Golden Age of Piracy took place between 1650 and 1720 and had many famous pirates including Blackbeard. Noah's Ark had dimensions in feet of 450 x 75 x 45. Blackbeard's ship, Queen Anne's Revenge, had a length of 103 feet. William Kidd's ship, Adventure Galley, was 124 feet long.\nAnswer (True or False):", "ct_subject": "The Golden Age of Piracy took place between 1650 and 1720 and had many famous pirates including Blackbeard. Noah's Ark had dimensions in feet of 450 x 75 x 45. Blackbeard's ship, Queen Anne's Revenge, had a length of 103 feet. William Kidd's ship, Adventure Galley, was 124 feet long.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "0.7934565"], [" the", "1.0"], [" Great", "0.9935049"], [" Pyramid", "1.0"], [" built", "0.9999945"], [" by", "0.9501606"], [" the", "0.9999999"], [" Egyptians", "0.93306947"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Great", "1.0"], [" Pyramid", "1.0"], [" was", "0.9999987"], [" built", "1.0"], [" by", "0.9999957"], [" the", "1.0"], [" Egyptians", "1.0"], [" in", "0.7343496"], [" the", "1.0"], [" 13", "0.99950266"], ["th", "1.0"], [" century", "0.9748122"], [" BC", "0.9998084"], [".", "1.0"], [" The", "0.999848"], [" Great", "0.9962656"], [" Pyramid", "1.0"], [" was", "0.9999999"], [" built", "1.0"], [" in", "0.99855095"]]}, "231": {"qid": "abd771e9a20b107e7e56", "term": "Nikola Tesla", "description": "Serbian American inventor", "question": "Has Nikola Tesla's name inspired multiple brands?", "answer": true, "facts": ["Nikola Tesla was a famous inventor born in 1856.", "The electric car company Tesla was named after Nikola Tesla.", "The hard rock band Tesla is named after Nikola Tesla."], "decomposition": ["How many brands are named after Nikola Tesla?", "Is #1 much more than one?"], "evidence": [[[["Tesla, Inc.-4"]], ["operation"]], [[["Nikola Motor Company-1", "Tesla, Inc.-4"]], ["operation"]], [[["Nikola Motor Company-1", "Tesla Electric Light and Manufacturing-2", "Tesla, Inc.-1", "Tesla-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first man to fly?\nFacts: The first man to fly was a man named Charles Lindbergh. He flew in 1927.\nAnswer (True or False): True\nQuestion: What is the name of the first man to fly?\nFacts: The first man to fly was a man named Charles Lindbergh.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Has Nikola Tesla's name inspired multiple brands?\nFacts: Nikola Tesla was a famous inventor born in 1856. The electric car company Tesla was named after Nikola Tesla. The hard rock band Tesla is named after Nikola Tesla.\nAnswer (True or False):", "ct_subject": "Nikola Tesla was a famous inventor born in 1856. The electric car company Tesla was named after Nikola Tesla. The hard rock band Tesla is named after Nikola Tesla.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.96537024"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99999976"], [" of", "1.0"], [" the", "1.0"], [" first", "0.87816554"], [" man", "0.44315654"], [" to", "1.0"], [" fly", "0.98652565"], ["?", "0.99999607"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" fly", "1.0"], [" was", "1.0"], [" a", "0.9999982"], [" man", "1.0"], [" named", "1.0"], [" Charles", "1.0"], [" Lind", "1.0"], ["ber", "1.0"], ["gh", "1.0"], [".", "1.0"], [" He", "0.55502063"], [" flew", "1.0"], [" in", "0.96057034"], [" 1927", "0.99998164"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "0.99981374"], [" to", "1.0"], [" fly", "0.99369675"], ["?", "0.7499638"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" fly", "1.0"], [" was", "1.0"], [" a", "1.0"], [" man", "1.0"], [" named", "1.0"], [" Charles", "1.0"], [" Lind", "1.0"], ["ber", "1.0"], ["gh", "1.0"], [".", "1.0"]]}, "232": {"qid": "2b45a624da84a288d37e", "term": "Wednesday", "description": "Day of the week", "question": "Are all Wednesdays in a year enough to read Bible 15 times?", "answer": true, "facts": ["There are 52 Wednesdays in a year.", "There are 1,248 hours over all the Wednesdays in a year.", "The Old Testament of the Bible takes an average of 52 hours to read.", "The New Testament of the Bible takes an average of 18 hours to read."], "decomposition": ["How many Wednesdays are there in a year?", "What is #1 multiplied by 24?", "How long does it take to read the old testament?", "How long does it take to read the new testament?", "Is #2 greater than or equal to: #3 plus #4?"], "evidence": [[[["Year-57"]], ["operation"], [["Old Testament-2"], "no_evidence"], [["New Testament-2"], "no_evidence"], ["no_evidence", "operation"]], [[["Week-10"]], ["operation"], [["New Testament-11", "Old Testament-2"], "no_evidence"], [["New Testament-9"], "no_evidence"], ["operation"]], [[["Year-66"], "no_evidence"], ["operation"], [["Protestant Bible-15"], "no_evidence"], [["Protestant Bible-15"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the Bible the only book that is true?\nFacts: The Bible is the only book that is true. The Bible is the only book that is true. The Bible is the only book that is true. The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are all Wednesdays in a year enough to read Bible 15 times?\nFacts: There are 52 Wednesdays in a year. There are 1,248 hours over all the Wednesdays in a year. The Old Testament of the Bible takes an average of 52 hours to read. The New Testament of the Bible takes an average of 18 hours to read.\nAnswer (True or False):", "ct_subject": "There are 52 Wednesdays in a year. There are 1,248 hours over all the Wednesdays in a year. The Old Testament of the Bible takes an average of 52 hours to read. The New Testament of the Bible takes an average of 18 hours to read.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.87973166"], [" the", "0.7543293"], [" Bible", "0.99999046"], [" the", "0.92254543"], [" only", "1.0"], [" book", "1.0"], [" that", "0.99972504"], [" is", "0.99768245"], [" true", "0.9999999"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Bible", "1.0"], [" is", "1.0"], [" the", "1.0"], [" only", "1.0"], [" book", "1.0"], [" that", "1.0"], [" is", "1.0"], [" true", "1.0"], [".", "1.0"], [" The", "0.82973874"], [" Bible", "1.0"], [" is", "1.0"], [" the", "1.0"], [" only", "1.0"], [" book", "1.0"], [" that", "1.0"], [" is", "1.0"], [" true", "1.0"], [".", "1.0"], [" The", "1.0"], [" Bible", "1.0"], [" is", "1.0"], [" the", "1.0"], [" only", "1.0"], [" book", "1.0"], [" that", "1.0"], [" is", "1.0"], [" true", "1.0"], [".", "1.0"], [" The", "0.9999987"]]}, "233": {"qid": "c6b927da57fb0b45b830", "term": "Bicycle", "description": "Pedal-driven two-wheel vehicle", "question": "Do children's bicycles often have extra wheels?", "answer": true, "facts": ["Training wheels are a set of two wheels to attach to bicycles of new bike riders for additional support.", "Training wheels are marketed primarily at children."], "decomposition": ["What types of bicycles have more than two wheels?", "Are any of #1 customarily bought for children?"], "evidence": [[[["Training wheels-1"]], [["Training wheels-1"]]], [[["Tricycle-3"]], ["operation"]], [[["Tricycle-1"]], [["Tricycle-3"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"carpet\" and a \"carpeted\" floor?\nFacts: A carpet is a floor that is made of a single layer of material. A carpeted floor is a floor that is made of two or more layers of material.\nAnswer (True or False): True\nQuestion: What is the difference between a \"carpet", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do children's bicycles often have extra wheels?\nFacts: Training wheels are a set of two wheels to attach to bicycles of new bike riders for additional support. Training wheels are marketed primarily at children.\nAnswer (True or False):", "ct_subject": "Training wheels are a set of two wheels to attach to bicycles of new bike riders for additional support. Training wheels are marketed primarily at children.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999932"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9745489"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "0.5013582"], ["arp", "0.9772335"], ["et", "0.9999815"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "0.99763715"], ["arp", "1.0"], ["et", "1.0"], ["ed", "1.0"], ["\"", "0.99999976"], [" floor", "0.999985"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" carpet", "1.0"], [" is", "0.99737144"], [" a", "1.0"], [" floor", "0.7418306"], [" that", "0.97138256"], [" is", "0.9678651"], [" made", "0.7956848"], [" of", "1.0"], [" a", "1.0"], [" single", "1.0"], [" layer", "1.0"], [" of", "1.0"], [" material", "0.8497083"], [".", "0.9999999"], [" A", "1.0"], [" carpet", "1.0"], ["ed", "1.0"], [" floor", "1.0"], [" is", "1.0"], [" a", "0.9999163"], [" floor", "1.0"], [" that", "1.0"], [" is", "1.0"], [" made", "1.0"], [" of", "1.0"], [" two", "0.99937516"], [" or", "1.0"], [" more", "1.0"], [" layers", "1.0"], [" of", "1.0"], [" material", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["arp", "1.0"], ["et", "1.0"]]}, "234": {"qid": "5fa29655003d347cb493", "term": "Groundhog Day", "description": "Traditional method of weather prediction", "question": "Would most school children in New York be wearing jackets on groundhog day?", "answer": true, "facts": ["Groundhog day takes place on February second.", "New York is typically very cold in February."], "decomposition": ["What month does Groundhog day occur?", "What is the season in #1?", "Do people typically wear jackets during #2?"], "evidence": [[[["Groundhog Day-1"]], [["Groundhog Day-1"]], [["Winter clothing-2"]]], [[["Groundhog Day-1"]], [["February-2"], "no_evidence"], ["operation"]], [[["Groundhog Day-1"]], [["Groundhog Day-1"]], [["Jacket-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the largest?\nFacts: The largest city in the United States is New York.\nAnswer (True or False): True\nQuestion: What is the name of the city in the United States that is the largest?\nFacts: The largest city in the United States is New York.\nAnswer (True or False): True\nQuestion", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would most school children in New York be wearing jackets on groundhog day?\nFacts: Groundhog day takes place on February second. New York is typically very cold in February.\nAnswer (True or False):", "ct_subject": "Groundhog day takes place on February second. New York is typically very cold in February.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999999"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99981123"], [" of", "1.0"], [" the", "1.0"], [" city", "0.5224893"], [" in", "0.9999777"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "0.79943496"], [" the", "1.0"], [" largest", "0.79387486"], ["?", "0.97677726"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99576914"], [" largest", "0.9999999"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "1.0"], [" New", "0.99999964"], [" York", "1.0"], [".", "0.99028826"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" largest", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "1.0"], [" New", "1.0"], [" York", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"]]}, "235": {"qid": "8c550764a42b5810b430", "term": "Thirty Years' War", "description": "War between 1618 and 1648; with over 8 million fatalities", "question": "Could a white cockatoo have lived through the entire Thirty Years' War?", "answer": true, "facts": ["The Thirty Years' War lasted 30 years", "White cockatoos have been reported to live between 40-60 years in captivity"], "decomposition": ["How long did the Thirty Years' War last?", "How long can white cockatoos live?", "Is #2 greater than or equal to #1?"], "evidence": [[[["Thirty Years' War-1"]], [["White cockatoo-9"]], ["operation"]], [[["Thirty Years' War-1"]], [["White cockatoo-9"]], ["operation"]], [[["Germany in the early modern period-9"]], [["White cockatoo-9"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest time a human has been in space?\nFacts: The longest time a human has been in space is about 6 days.\nAnswer (True or False): True\nQuestion: What is the longest time a human has been in space?\nFacts: The longest time a human has been in space is about 6 days.\nAnswer (True or", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could a white cockatoo have lived through the entire Thirty Years' War?\nFacts: The Thirty Years' War lasted 30 years White cockatoos have been reported to live between 40-60 years in captivity\nAnswer (True or False):", "ct_subject": "The Thirty Years' War lasted 30 years White cockatoos have been reported to live between 40-60 years in captivity", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.89437103"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.999637"], [" time", "0.9819952"], [" a", "0.9999999"], [" human", "0.97447824"], [" has", "0.99818486"], [" been", "0.9800657"], [" in", "0.6373473"], [" space", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" time", "1.0"], [" a", "1.0"], [" human", "1.0"], [" has", "1.0"], [" been", "1.0"], [" in", "1.0"], [" space", "1.0"], [" is", "0.9953779"], [" about", "0.99999976"], [" 6", "0.9996406"], [" days", "0.40040702"], [".", "1.0"], ["\n", "0.9999995"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" time", "1.0"], [" a", "1.0"], [" human", "1.0"], [" has", "1.0"], [" been", "1.0"], [" in", "1.0"], [" space", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" time", "1.0"], [" a", "1.0"], [" human", "1.0"], [" has", "1.0"], [" been", "1.0"], [" in", "1.0"], [" space", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 6", "1.0"], [" days", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"]]}, "236": {"qid": "8de3fccde94a5f8448bb", "term": "Palace of Westminster", "description": "Meeting place of the Parliament of the United Kingdom,", "question": "Can the Palace of Westminster tell time in the dark?", "answer": true, "facts": ["The Palace of Westminster has Big Ben, a striking clock tower", "Big Ben communicates the time via bells"], "decomposition": ["What is the clock tower of the Palace of Westminster?", "What does #1 use to communicate time?", "Can #2 work without light?"], "evidence": [[[["Palace of Westminster-29"]], [["Palace of Westminster-30"]], ["operation"]], [[["Big Ben-1"]], [["Big Ben-50"]], ["no_evidence", "operation"]], [[["Palace of Westminster-29"]], [["Palace of Westminster-30"]], [["Palace of Westminster-30"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city of Rome?\nFacts: Rome is the name of the city in which the Roman Empire was founded.\nAnswer (True or False): True\nQuestion: What is the name of the city of Rome?\nFacts: Rome is the name of the city in which the Roman Empire was founded.\nAnswer (True or False): True\nQuestion: What is the name of the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can the Palace of Westminster tell time in the dark?\nFacts: The Palace of Westminster has Big Ben, a striking clock tower Big Ben communicates the time via bells\nAnswer (True or False):", "ct_subject": "The Palace of Westminster has Big Ben, a striking clock tower Big Ben communicates the time via bells", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999046"], [" is", "1.0"], [" the", "1.0"], [" name", "0.7451063"], [" of", "1.0"], [" the", "1.0"], [" city", "0.99718046"], [" of", "0.9999769"], [" Rome", "0.9998215"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "0.99999976"], [" is", "0.9999999"], [" the", "1.0"], [" name", "0.9252733"], [" of", "1.0"], [" the", "1.0"], [" city", "0.44552863"], [" in", "0.9712324"], [" which", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [" was", "1.0"], [" founded", "1.0"], [".", "0.9999975"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "0.9999993"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" which", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [" was", "1.0"], [" founded", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"]]}, "237": {"qid": "300927fd135e8d94af16", "term": "Prime number", "description": "Integer greater than 1 that has no positive integer divisors other than itself and 1", "question": "Can a prime number be represented by the number of days in a week?", "answer": true, "facts": ["There are seven days in a week.", "Seven is a prime number."], "decomposition": ["How many days are there in a week?", "Is #1 a prime number?"], "evidence": [[[["Week-1"]], [["Prime number-1", "Prime number-13"], "operation"]], [[["Week-1"]], [["7-1"]]], [[["Week-8"]], [["Prime number-7"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the largest number that can be written in base 10?\nFacts: The largest number that can be written in base 10 is 10,000,000.\nAnswer (True or False): True\nQuestion: What is the largest number that can be written in base 2?\nFacts: The largest number that can be written in base 2 is 2,000,000.\nAnswer (True or False): True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can a prime number be represented by the number of days in a week?\nFacts: There are seven days in a week. Seven is a prime number.\nAnswer (True or False):", "ct_subject": "There are seven days in a week. Seven is a prime number.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9962507"], [" is", "1.0"], [" the", "1.0"], [" largest", "0.9988059"], [" number", "0.6410819"], [" that", "1.0"], [" can", "1.0"], [" be", "1.0"], [" written", "0.9988129"], [" in", "0.99088013"], [" base", "0.98630255"], [" 10", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.90097713"], [" largest", "1.0"], [" number", "1.0"], [" that", "1.0"], [" can", "1.0"], [" be", "1.0"], [" written", "1.0"], [" in", "1.0"], [" base", "1.0"], [" 10", "1.0"], [" is", "1.0"], [" 10", "0.90930355"], [",", "0.9794851"], ["000", "1.0"], [",", "0.99887985"], ["000", "1.0"], [".", "0.9724262"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "0.9998903"], [" number", "1.0"], [" that", "1.0"], [" can", "1.0"], [" be", "1.0"], [" written", "1.0"], [" in", "1.0"], [" base", "1.0"], [" 2", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" largest", "1.0"], [" number", "1.0"], [" that", "1.0"], [" can", "1.0"], [" be", "1.0"], [" written", "1.0"], [" in", "1.0"], [" base", "1.0"], [" 2", "1.0"], [" is", "1.0"], [" 2", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "0.99999976"], ["000", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"]]}, "238": {"qid": "9c0111d03557271a54fc", "term": "Reformation", "description": "Schism within the Christian Church in the 16th century", "question": "Would a tool used for Martin Luther's Reformation opening salvo aid in a crucifixion?", "answer": true, "facts": ["Martin Luther began the Reformation with the defiant act of nailing 95 grievances to the door of the Wittenberg church.", "Roman crucifixions required several tools including nails and wooden beams."], "decomposition": ["What did Martin Luther begin his Reformation with?", "What tools were used in #1?", "What are the tools required to preform Roman crucifixions?", "Is there any overlap between #2 and #3?"], "evidence": [[[["Ninety-five Theses-1"]], [["Wittenberg-7"]], [["Crucifixion of Jesus-44"]], ["operation"]], [[["Martin Luther-19"]], [["Nail (fastener)-1", "Nail (fastener)-2"]], [["Crucifixion-1"]], ["operation"]], [[["Martin Luther-19"]], [["Nail (fastener)-2"]], [["Crucifixion-1", "Nail (fastener)-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: John Brown was hanged in 1859.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: John", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a tool used for Martin Luther's Reformation opening salvo aid in a crucifixion?\nFacts: Martin Luther began the Reformation with the defiant act of nailing 95 grievances to the door of the Wittenberg church. Roman crucifixions required several tools including nails and wooden beams.\nAnswer (True or False):", "ct_subject": "Martin Luther began the Reformation with the defiant act of nailing 95 grievances to the door of the Wittenberg church. Roman crucifixions required several tools including nails and wooden beams.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99109733"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99868137"], [" of", "1.0"], [" the", "1.0"], [" first", "0.6124168"], [" person", "0.99747723"], [" to", "1.0"], [" be", "0.9997079"], [" executed", "0.99999404"], [" in", "0.69883066"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "0.11860823"], [" Brown", "0.9999449"], [" was", "0.9999999"], [" hanged", "0.6885364"], [" in", "0.98908305"], [" 18", "1.0"], ["59", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "1.0"]]}, "239": {"qid": "7b15ca4cc2a7bf3fe1b9", "term": "Ahura Mazda", "description": "highest deity of Zoroastrianism", "question": "Will Ahura Mazda have to look down to see Abaddon's dwelling??", "answer": true, "facts": ["Abaddon is a demon that is said to dwell in a bottomless pit below the earth that is a realm of the dead.", "Ahura Mazda is the chief deity of Zoroastrianism.", "Ahura Mazda is the Supreme Being located in heaven, high above the sky."], "decomposition": ["Where does Abaddon dwell?", "Where is Ahura Mazda known to reside?", "Is #2 located physically above #1?"], "evidence": [[[["Abaddon-1"]], [["Ahura Mazda-1"], "no_evidence"], ["operation"]], [[["Abaddon-1"]], [["Ahura Mazda-1"], "no_evidence"], ["operation"]], [[["Abaddon in popular culture-1"]], [["Ahura Mazda-1"], "no_evidence"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Will the sun rise in the east tomorrow?\nFacts: The sun rises in the east because the sun is the center of the universe. The sun rises in the east because the sun is the center of the universe.\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Will Ahura Mazda have to look down to see Abaddon's dwelling??\nFacts: Abaddon is a demon that is said to dwell in a bottomless pit below the earth that is a realm of the dead. Ahura Mazda is the chief deity of Zoroastrianism. Ahura Mazda is the Supreme Being located in heaven, high above the sky.\nAnswer (True or False):", "ct_subject": "Abaddon is a demon that is said to dwell in a bottomless pit below the earth that is a realm of the dead. Ahura Mazda is the chief deity of Zoroastrianism. Ahura Mazda is the Supreme Being located in heaven, high above the sky.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Will", "0.7255976"], [" the", "1.0"], [" sun", "0.9996866"], [" rise", "0.9999895"], [" in", "0.7485092"], [" the", "1.0"], [" east", "1.0"], [" tomorrow", "0.99704486"], ["?", "0.99934214"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" sun", "1.0"], [" rises", "1.0"], [" in", "1.0"], [" the", "1.0"], [" east", "1.0"], [" because", "0.30365247"], [" the", "0.98822534"], [" sun", "0.7213505"], [" is", "1.0"], [" the", "1.0"], [" center", "0.99999833"], [" of", "1.0"], [" the", "1.0"], [" universe", "1.0"], [".", "1.0"], [" The", "0.9997416"], [" sun", "0.99999595"], [" rises", "0.9960431"], [" in", "1.0"], [" the", "1.0"], [" east", "1.0"], [" because", "1.0"], [" the", "0.9999906"], [" sun", "1.0"], [" is", "1.0"], [" the", "1.0"], [" center", "1.0"], [" of", "1.0"], [" the", "1.0"], [" universe", "1.0"], [".", "1.0"], ["\n", "1.0"]]}, "240": {"qid": "18586794a7000980c6a8", "term": "Bing (search engine)", "description": "Web search engine from Microsoft", "question": "Do Bing (search engine) searches earn the searcher more than competitors do?", "answer": true, "facts": ["Bing (search engine) has a search rewards program that gives the user points, from conducting searches, to redeem for prizes.", "Bing (search engine) has several competitors such as Google, and DuckDuckGo.", "Google and DuckDuckGo do not have search rewards programs."], "decomposition": ["What does Bing give to people who use the search engine?", "Who are Bing's major competitors?", "What do the companies in #2 give people for using their service?", "Is #1 of greater value than #3?"], "evidence": [[[["Bing (search engine)-10", "Bing (search engine)-26"]], [["Bing (search engine)-52"]], ["no_evidence"], ["operation"]], [[["Bing (search engine)-57"]], [["Bing (search engine)-67"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Bing (search engine)-57"]], [["Bing (search engine)-54"]], ["no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"bachelor\" and a \"bachelorette\"?\nFacts: A \"bachelor\" is a man who has not yet married. A \"bachelorette\" is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do Bing (search engine) searches earn the searcher more than competitors do?\nFacts: Bing (search engine) has a search rewards program that gives the user points, from conducting searches, to redeem for prizes. Bing (search engine) has several competitors such as Google, and DuckDuckGo. Google and DuckDuckGo do not have search rewards programs.\nAnswer (True or False):", "ct_subject": "Bing (search engine) has a search rewards program that gives the user points, from conducting searches, to redeem for prizes. Bing (search engine) has several competitors such as Google, and DuckDuckGo. Google and DuckDuckGo do not have search rewards programs.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9982717"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.8845121"], [" between", "1.0"], [" a", "0.99987817"], [" \"", "1.0"], ["b", "0.39974374"], ["achelor", "0.91309416"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["b", "1.0"], ["achel", "1.0"], ["oret", "1.0"], ["te", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999999"], [" \"", "0.82196367"], ["b", "1.0"], ["achelor", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "0.9997166"], [" man", "0.9999957"], [" who", "1.0"], [" has", "0.99996793"], [" not", "0.9404237"], [" yet", "0.99762887"], [" married", "0.99630004"], [".", "1.0"], [" A", "1.0"], [" \"", "1.0"], ["b", "1.0"], ["achel", "1.0"], ["oret", "1.0"], ["te", "1.0"], ["\"", "1.0"], [" is", "1.0"]]}, "241": {"qid": "653e7f9907cc581803b6", "term": "Amtrak", "description": "Intercity rail operator in the United States", "question": "Would three newborn kittens fit on a standard Amtrak coach seat?", "answer": true, "facts": ["Newborn kittens are small enough to fit in an average human hand.", "The average human hand is 7 inches.", "An Amtrak coach seat is 39\" x 23\"."], "decomposition": ["What is the size of a newborn kitten?", "How big would #1 times three kittens be?", "How large is an Amtrak coach seat?", "Is #2 smaller than #3?"], "evidence": [[[["Cat-28"], "no_evidence"], ["operation"], [["Airline seat-29"], "no_evidence"], ["operation"]], [[["Kitten-4"], "no_evidence"], ["no_evidence"], [["Amtrak-43"], "no_evidence"], ["no_evidence", "operation"]], [[["Cat-28"], "no_evidence"], ["no_evidence", "operation"], [["Amtrak-48"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human?\nFacts: The average human weight is about 150 pounds.\nAnswer (True or False): True\nQuestion: What is the average height of a human?\nFacts: The average human height is about 5'10\".\nAnswer (True or False): True\nQuestion: What is the average weight", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would three newborn kittens fit on a standard Amtrak coach seat?\nFacts: Newborn kittens are small enough to fit in an average human hand. The average human hand is 7 inches. An Amtrak coach seat is 39\" x 23\".\nAnswer (True or False):", "ct_subject": "Newborn kittens are small enough to fit in an average human hand. The average human hand is 7 inches. An Amtrak coach seat is 39\" x 23\".", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99997485"], [" is", "1.0"], [" the", "1.0"], [" average", "0.94390297"], [" weight", "0.99999225"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], ["?", "0.9999994"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.82965165"], [" weight", "0.9999895"], [" is", "1.0"], [" about", "0.9998529"], [" 150", "0.2289645"], [" pounds", "0.99999845"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" height", "0.9557576"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.47892484"], [" height", "1.0"], [" is", "1.0"], [" about", "0.99999034"], [" 5", "1.0"], ["'", "0.40588287"], ["10", "0.9363338"], ["\".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "1.0"]]}, "242": {"qid": "29d785535d6462d9e711", "term": "Red Sea", "description": "Arm of the Indian Ocean between Arabia and Africa", "question": "Does the Red Sea have biblical significance? ", "answer": true, "facts": ["During the biblical Exodus, the Israelite had to cross the Red Sea.", "Moses parted the Red Sea to allow the Israelite group to escape from the Egyptians. "], "decomposition": ["What bodies of water are important to Biblical stories?", "Is the Red Sea among #1?"], "evidence": [[[["Red Sea-8"]], [["Red Sea-8"], "operation"]], [[["Jordan River-2", "Mediterranean Sea-11", "Red Sea-8", "Sea of Galilee-4"]], ["operation"]], [[["Crossing the Red Sea-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city of Jerusalem?\nFacts: Jerusalem is the capital of the Kingdom of Israel. The city was founded by King David.\nAnswer (True or False): True\nQuestion: What is the name of the city of Rome?\nFacts: Rome is the capital of the Roman Empire. The city was founded by Emperor Augustus.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does the Red Sea have biblical significance? \nFacts: During the biblical Exodus, the Israelite had to cross the Red Sea. Moses parted the Red Sea to allow the Israelite group to escape from the Egyptians. \nAnswer (True or False):", "ct_subject": "During the biblical Exodus, the Israelite had to cross the Red Sea. Moses parted the Red Sea to allow the Israelite group to escape from the Egyptians. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.5380126"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99989724"], [" of", "1.0"], [" the", "1.0"], [" city", "0.99999976"], [" of", "0.9999999"], [" Jerusalem", "0.97702116"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Jerusalem", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "0.9004205"], [" of", "1.0"], [" the", "0.9999944"], [" Kingdom", "0.99978846"], [" of", "1.0"], [" Israel", "0.9999995"], [".", "1.0"], [" The", "0.9170801"], [" city", "0.7717951"], [" was", "0.5342312"], [" founded", "0.9986921"], [" by", "0.99997973"], [" King", "1.0"], [" David", "1.0"], [".", "0.9901748"], ["\n", "0.97415257"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "0.99051535"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "1.0"], [" is", "0.99997294"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [".", "1.0"], [" The", "0.9999969"], [" city", "1.0"], [" was", "1.0"], [" founded", "1.0"], [" by", "1.0"], [" Emperor", "0.99997616"], [" Augustus", "1.0"], [".", "1.0"]]}, "243": {"qid": "c8bf91bdcde3eb7501e2", "term": "Chinatown, Manhattan", "description": "Neighborhood of Manhattan in New York City", "question": "Would moon cakes be easy to find in Chinatown, Manhattan?", "answer": true, "facts": ["Moon cakes are a Chinese traditional desert.", "Chinatown offers many 'tastes of home' to Chinese locals and travelers. "], "decomposition": ["What cuisine are moon cakes from?", "Does Chinatown, Manhattan sell food products of #1?"], "evidence": [[[["Mooncake-1"]], [["Chinatown-14"], "no_evidence", "operation"]], [[["Moon shrimp cake-1"]], [["Moon shrimp cake-1"]]], [[["Mooncake-1"]], [["Chinatown-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the most common type of food that is served at Chinese restaurants?\nFacts: Chinese restaurants are known for their large portions and large portions of food. \nAnswer (True or False): True\nQuestion: What is the most common type of food that is served at Chinese restaurants?\nFacts: Chinese restaurants are known for their large portions and large portions of food. ", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would moon cakes be easy to find in Chinatown, Manhattan?\nFacts: Moon cakes are a Chinese traditional desert. Chinatown offers many 'tastes of home' to Chinese locals and travelers. \nAnswer (True or False):", "ct_subject": "Moon cakes are a Chinese traditional desert. Chinatown offers many 'tastes of home' to Chinese locals and travelers. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999976"], [" is", "1.0"], [" the", "1.0"], [" most", "0.013942354"], [" common", "0.28447816"], [" type", "0.99918073"], [" of", "1.0"], [" food", "0.9343014"], [" that", "0.9898218"], [" is", "0.988729"], [" served", "1.0"], [" at", "0.9996259"], [" Chinese", "0.076076046"], [" restaurants", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Chinese", "1.0"], [" restaurants", "1.0"], [" are", "0.99999464"], [" known", "1.0"], [" for", "1.0"], [" their", "1.0"], [" large", "0.030634021"], [" portions", "1.0"], [" and", "0.9411557"], [" large", "0.88119817"], [" portions", "1.0"], [" of", "0.99999106"], [" food", "1.0"], [".", "1.0"], [" ", "0.9941421"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" most", "1.0"], [" common", "0.99999726"], [" type", "1.0"], [" of", "1.0"], [" food", "1.0"], [" that", "1.0"], [" is", "1.0"], [" served", "1.0"], [" at", "1.0"], [" Chinese", "1.0"], [" restaurants", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Chinese", "1.0"], [" restaurants", "1.0"], [" are", "1.0"], [" known", "1.0"], [" for", "1.0"], [" their", "1.0"], [" large", "1.0"], [" portions", "1.0"], [" and", "1.0"], [" large", "1.0"], [" portions", "1.0"], [" of", "1.0"], [" food", "1.0"], [".", "1.0"], [" ", "1.0"]]}, "244": {"qid": "f046ac1a972a64c13501", "term": "Bugs Bunny", "description": "Warner Bros. cartoon character", "question": "Is Bugs Bunny known for carrying a root vegetable around with him?", "answer": true, "facts": ["Bugs Bunny carries a carrot around with him.", "Carrots are considered root vegetables."], "decomposition": ["What does Bugs Bunny carry around with him?", "Is #1 a root vegetable?"], "evidence": [[[["Bugs Bunny-20"]], [["Carrot-31"]]], [[["Bugs Bunny-20"]], [["Carrot-1"]]], [[["Bugs Bunny-37"]], [["Carrot-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the word \"candy\" derived from the word \"candy\"?\nFacts: The word \"candy\" is derived from the word \"candy\" and the word \"candy\" is derived from the word \"candy\".\nAnswer (True or False): True\nQuestion: Is the word \"candy\" derived from the word \"candy\"?\nFacts: The word \"candy\" is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Bugs Bunny known for carrying a root vegetable around with him?\nFacts: Bugs Bunny carries a carrot around with him. Carrots are considered root vegetables.\nAnswer (True or False):", "ct_subject": "Bugs Bunny carries a carrot around with him. Carrots are considered root vegetables.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9599596"], [" the", "0.99992585"], [" word", "0.9566836"], [" \"", "1.0"], ["c", "0.99996257"], ["andy", "1.0"], ["\"", "1.0"], [" derived", "0.99999595"], [" from", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"?", "0.99999607"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99999976"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"", "0.90590787"], [" and", "0.99999905"], [" the", "0.9668347"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"", "1.0"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"", "1.0"], [" is", "1.0"]]}, "245": {"qid": "e9e2b22b193fcca2a976", "term": "Cricket (insect)", "description": "small insects of the family Gryllidae", "question": "Would someone buying crickets be likely to own pets?", "answer": true, "facts": ["Reptiles are a popular pet for people.", "Reptiles enjoy eating crickets. ", "Crickets are sold at many pet stores."], "decomposition": ["What are some common animal classes that people keep as pets?", "Do any of #1 usually eat crickets?"], "evidence": [[[["Pet-22"]], [["Crickets as pets-4"]]], [[["Crickets as pets-26", "Pet-2"], "no_evidence"], [["Cricket (insect)-3"], "no_evidence", "operation"]], [[["Pet-2"]], [["Lizard-27"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a crickets?\nFacts: The average weight of a crickets is about 1.5 ounces.\nAnswer (True or False): True\nQuestion: What is the average lifespan of a crickets?\nFacts: Crickets live for about 3 years.\nAnswer (True or False): True\nQuestion: What is the average lifespan of a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would someone buying crickets be likely to own pets?\nFacts: Reptiles are a popular pet for people. Reptiles enjoy eating crickets.  Crickets are sold at many pet stores.\nAnswer (True or False):", "ct_subject": "Reptiles are a popular pet for people. Reptiles enjoy eating crickets.  Crickets are sold at many pet stores.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99997723"], [" is", "1.0"], [" the", "1.0"], [" average", "0.53671646"], [" weight", "0.9728502"], [" of", "1.0"], [" a", "1.0"], [" cr", "0.99978536"], ["ickets", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99805814"], [" average", "1.0"], [" weight", "1.0"], [" of", "1.0"], [" a", "1.0"], [" cr", "0.99630105"], ["ickets", "1.0"], [" is", "1.0"], [" about", "0.9999999"], [" 1", "0.9998883"], [".", "0.999997"], ["5", "1.0"], [" ounces", "0.98717695"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99976677"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" lifespan", "0.99824727"], [" of", "1.0"], [" a", "1.0"], [" cr", "1.0"], ["ickets", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Cr", "0.90505254"], ["ickets", "1.0"], [" live", "0.99996245"], [" for", "1.0"], [" about", "1.0"], [" 3", "0.99993587"], [" years", "0.12588845"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999496"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" lifespan", "0.9936598"], [" of", "1.0"], [" a", "1.0"]]}, "246": {"qid": "2f49d601ea00411ef3d5", "term": "Christianity in China", "description": "Religious community", "question": "Are some adherents to Christianity in China historic enemies of Catholic Church?", "answer": true, "facts": ["Christianity in China is comprised of several different groups including: Protestants, Catholics, Evangelicals, and Orthodox Christians.", "Catholics have been at war with Protestants throughout history.", "The 1572 St. Bartholomew's Day Massacre saw thousands of Protestants killed by Catholic mobs.", "English Protestant rulers killed many Irish Catholics during the Reformation."], "decomposition": ["Which Christian denominations are historic enemies of the Catholic Church?", "Do any of the denominations in #1 currently have adherents in China?"], "evidence": [[[["Catholic Church-119"]], [["Protestantism by country-3"]]], [[["St. Bartholomew's Day massacre-1"]], [["Christianity in China-35", "Robert Morrison (missionary)-50"], "operation"]], [[["Protestantism-1"]], [["Protestantism by country-3"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any people in the world today who have a higher IQ than Einstein?\nFacts: Einstein was a genius. He was the first person to discover the laws of physics. He was", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are some adherents to Christianity in China historic enemies of Catholic Church?\nFacts: Christianity in China is comprised of several different groups including: Protestants, Catholics, Evangelicals, and Orthodox Christians. Catholics have been at war with Protestants throughout history. The 1572 St. Bartholomew's Day Massacre saw thousands of Protestants killed by Catholic mobs. English Protestant rulers killed many Irish Catholics during the Reformation.\nAnswer (True or False):", "ct_subject": "Christianity in China is comprised of several different groups including: Protestants, Catholics, Evangelicals, and Orthodox Christians. Catholics have been at war with Protestants throughout history. The 1572 St. Bartholomew's Day Massacre saw thousands of Protestants killed by Catholic mobs. English Protestant rulers killed many Irish Catholics during the Reformation.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.98826593"], [" there", "0.99877447"], [" any", "1.0"], [" people", "0.79268724"], [" in", "0.9881308"], [" the", "1.0"], [" world", "1.0"], [" today", "0.99999964"], [" who", "0.99949455"], [" have", "0.09813063"], [" a", "0.95420426"], [" higher", "1.0"], [" IQ", "1.0"], [" than", "1.0"], [" Einstein", "0.99754924"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Einstein", "1.0"], [" was", "0.27414483"], [" a", "0.99118984"], [" genius", "1.0"], [".", "0.9997898"], [" He", "0.99950397"], [" was", "1.0"], [" the", "0.96193147"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" discover", "0.9609134"], [" the", "1.0"], [" laws", "0.94434255"], [" of", "1.0"], [" physics", "0.8895"], [".", "0.99999917"], [" He", "1.0"], [" was", "1.0"]]}, "247": {"qid": "e681e7f094a45ab8f72d", "term": "Oval Office", "description": "office of the U.S. President", "question": "Can a Kia Rio fit inside the Oval Office?", "answer": true, "facts": ["The Oval Office is 35' long and 29' wide.", "A Kia Rio is 14.3' long and 5.6' wide. "], "decomposition": ["How large is the Oval Office?", "How large is a Kia Rio?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Oval Office-2"], "no_evidence"], [["Kia Rio-28"], "no_evidence"], ["operation"]], [[["Eric Gugler-9"], "no_evidence"], [["Kia Rio-1"], "no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first president of the United States?\nFacts: George Washington.\nAnswer (True or False): True\nQuestion: What is the name of the first president of the United States?\nFacts: George Washington.\nAnswer (True or False): True\nQuestion: What is the name of the first president of the United States?", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can a Kia Rio fit inside the Oval Office?\nFacts: The Oval Office is 35' long and 29' wide. A Kia Rio is 14.3' long and 5.6' wide. \nAnswer (True or False):", "ct_subject": "The Oval Office is 35' long and 29' wide. A Kia Rio is 14.3' long and 5.6' wide. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999964"], [" is", "1.0"], [" the", "1.0"], [" name", "0.09233855"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9905862"], [" president", "0.9997123"], [" of", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" George", "0.9774057"], [" Washington", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" president", "1.0"], [" of", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" George", "1.0"], [" Washington", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" president", "1.0"], [" of", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"]]}, "248": {"qid": "eb7c254ac7ae82656aee", "term": "Rhinoceros", "description": "family of mammals", "question": "Have rhinoceroses been killed to improve human sex lives?", "answer": true, "facts": ["Rhinoceros horns are used for folk treatment of sexual impotency.", "Rhinoceroses are killed to remove their horns."], "decomposition": ["Which part of the Rhinoceros do most poachers hunt and kill it for?", "What are some common traditional uses of #1?", "Is treatment of sexual impotency included in #2?"], "evidence": [[[["Rhinoceros-31"]], [["Rhinoceros-32"]], ["operation"]], [[["Rhinoceros-3"]], [["Rhinoceros-32", "Rhinoceros-34"]], ["operation"]], [[["Rhinoceros-3"]], [["Rhinoceros-32"]], [["Aphrodisiac-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any animals that are not endangered?\nFacts: There are no animals that are not endangered.\nAnswer (True or False): True\nQuestion: Are there any animals that are not endangered?\nFacts: There are no animals that are not endangered.\nAnswer (True or False): True\nQuestion: Are there any animals that are not endangered", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Have rhinoceroses been killed to improve human sex lives?\nFacts: Rhinoceros horns are used for folk treatment of sexual impotency. Rhinoceroses are killed to remove their horns.\nAnswer (True or False):", "ct_subject": "Rhinoceros horns are used for folk treatment of sexual impotency. Rhinoceroses are killed to remove their horns.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.6693319"], [" there", "0.9999999"], [" any", "0.99996185"], [" animals", "0.96572644"], [" that", "1.0"], [" are", "0.9340313"], [" not", "1.0"], [" endangered", "0.9998568"], ["?", "0.999972"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "1.0"], [" are", "1.0"], [" no", "0.66536057"], [" animals", "1.0"], [" that", "1.0"], [" are", "1.0"], [" not", "1.0"], [" endangered", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.97709197"], [" there", "1.0"], [" any", "1.0"], [" animals", "1.0"], [" that", "1.0"], [" are", "1.0"], [" not", "0.9999999"], [" endangered", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "1.0"], [" are", "1.0"], [" no", "1.0"], [" animals", "1.0"], [" that", "1.0"], [" are", "1.0"], [" not", "1.0"], [" endangered", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "1.0"], [" any", "1.0"], [" animals", "1.0"], [" that", "1.0"], [" are", "1.0"], [" not", "1.0"], [" endangered", "1.0"]]}, "249": {"qid": "32b82c383b3a92966d07", "term": "United States Army Rangers", "description": "Elite military formation of the United States Army", "question": "Is Mozambique Drill an easy shot for United States Army Ranger?", "answer": true, "facts": ["The Mozambique Drill is a close quarters combat technique involving firing two shots to the body and one to the head.", "United States Army Rangers are equipped with M4A1 guns.", "M4A1 guns can fire up to 600 meters."], "decomposition": ["What is the The Mozambique Drill?", "What guns are United States Army Rangers equipped with?", "What is the shooting range of #2?", "Would a gun with the range of #3 be helpful in #1?"], "evidence": [[[["Mozambique Drill-1"]], [["75th Ranger Regiment-2", "United States Army-68"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Mozambique Drill-1"]], ["no_evidence"], ["no_evidence"], ["operation"]], [[["Mozambique Drill-1"]], [["M4 carbine-31"]], [["M4 carbine-31"]], [["Close-quarters combat-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the United States Army Ranger a member of the United States Army?\nFacts: The United States Army Ranger is a member of the United States Army.\nAnswer (True or False): True\nQuestion: Is the United States Army Ranger a member of the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Mozambique Drill an easy shot for United States Army Ranger?\nFacts: The Mozambique Drill is a close quarters combat technique involving firing two shots to the body and one to the head. United States Army Rangers are equipped with M4A1 guns. M4A1 guns can fire up to 600 meters.\nAnswer (True or False):", "ct_subject": "The Mozambique Drill is a close quarters combat technique involving firing two shots to the body and one to the head. United States Army Rangers are equipped with M4A1 guns. M4A1 guns can fire up to 600 meters.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9977549"], [" the", "0.99996936"], [" United", "0.9999918"], [" States", "1.0"], [" Army", "1.0"], [" Ranger", "1.0"], [" a", "0.9645424"], [" member", "0.9997619"], [" of", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" Army", "1.0"], ["?", "0.9999999"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "1.0"], [" States", "1.0"], [" Army", "1.0"], [" Ranger", "1.0"], [" is", "1.0"], [" a", "1.0"], [" member", "1.0"], [" of", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" Army", "1.0"], [".", "1.0"], ["\n", "0.9982906"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99999833"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" Army", "1.0"], [" Ranger", "1.0"], [" a", "1.0"], [" member", "1.0"], [" of", "1.0"], [" the", "1.0"]]}, "250": {"qid": "3428fcbe9ffdcd50d234", "term": "ABBA", "description": "Swedish pop group", "question": "Does ABBA have similar gender configuration to The Mamas & The Papas?", "answer": true, "facts": ["ABBA was a Swedish pop group composed of four members.", "The Mamas & The Papas was an American band composed of four members.", "The members of ABBA are Bj\u00f6rn Ulvaeus (male), Benny Andersson (male), Agnetha F\u00e4ltskog (female), and Anni-Frid Lyngstad (female)..", "The members of The Mamas & The Papas are John Phillips (male), Denny Doherty (male), Cass Elliot (female), and Michelle Phillips (female)."], "decomposition": ["How many men and women each make up the Mamas and the Papas?", "How many men and women each make up ABBA?", "Is #1 equal to #2?"], "evidence": [[[["The Mamas and the Papas-1"]], [["ABBA-1"]], ["operation"]], [[["The Mamas and the Papas-1"]], [["ABBA-1"]], ["operation"]], [[["The Mamas and the Papas-1"]], [["ABBA-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does ABBA have similar gender configuration to The Mamas & The Papas?\nFacts: ABBA was a Swedish pop group composed of four members. The Mamas & The Papas was an American band composed of four members. The members of ABBA are Bj\u00f6rn Ulvaeus (male), Benny Andersson (male), Agnetha F\u00e4ltskog (female), and Anni-Frid Lyngstad (female).. The members of The Mamas & The Papas are John Phillips (male), Denny Doherty (male), Cass Elliot (female), and Michelle Phillips (female).\nAnswer (True or False):", "ct_subject": "ABBA was a Swedish pop group composed of four members. The Mamas & The Papas was an American band composed of four members. The members of ABBA are Bj\u00f6rn Ulvaeus (male), Benny Andersson (male), Agnetha F\u00e4ltskog (female), and Anni-Frid Lyngstad (female).. The members of The Mamas & The Papas are John Phillips (male), Denny Doherty (male), Cass Elliot (female), and Michelle Phillips (female).", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"]]}, "251": {"qid": "ac543a4e99396d4f8132", "term": "Mona Lisa", "description": "Painting by Leonardo da Vinci", "question": "Is the Mona Lisa based on a real person?", "answer": true, "facts": ["There are two main theories about the origin of the Mona Lisa.", "The first is that a wealthy noblewoman, the wife of one of Leonardo's friends, sat as the model.", "Another popular theory is that Leonardo painted her as a cross-gendered self portrait."], "decomposition": ["Who was the Mona Lisa painting based on?", "Is #1 a real person?"], "evidence": [[[["Mona Lisa-2"]], [["Lisa del Giocondo-1"]]], [[["Mona Lisa-2"]], [["Lisa del Giocondo-1"], "operation"]], [[["Mona Lisa-12"]], [["Isabella of Aragon, Queen of Germany-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first man to walk on the moon?\nFacts: The first man to walk on the moon was Neil Armstrong.\nAnswer (True or False): True\nQuestion: What is the name of the first man to walk", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is the Mona Lisa based on a real person?\nFacts: There are two main theories about the origin of the Mona Lisa. The first is that a wealthy noblewoman, the wife of one of Leonardo's friends, sat as the model. Another popular theory is that Leonardo painted her as a cross-gendered self portrait.\nAnswer (True or False):", "ct_subject": "There are two main theories about the origin of the Mona Lisa. The first is that a wealthy noblewoman, the wife of one of Leonardo's friends, sat as the model. Another popular theory is that Leonardo painted her as a cross-gendered self portrait.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.49767166"], [" is", "1.0"], [" the", "1.0"], [" name", "0.999962"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9871148"], [" man", "0.9044899"], [" to", "0.9999995"], [" walk", "0.99994636"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9991623"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" was", "1.0"], [" Neil", "1.0"], [" Armstrong", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "0.9999901"], [" to", "1.0"], [" walk", "0.9994394"]]}, "252": {"qid": "c234a28480510591980d", "term": "Justin Timberlake", "description": "American singer, record producer, and actor", "question": "Has Justin Timberlake ever written a song about Britney Spears?", "answer": true, "facts": ["Justin Timberlake and Britney Spears dated in 1999.", "In 2002, Justin Timberlake released a music video for his breakup song 'Cry Me A River' and featured an actress who resembled his then ex Britney Spears."], "decomposition": ["Who did Justin Timberlake date in 1999?", "Who was the song 'Cry Me A River' by Justin timberlake about?", "Is #1 and #2 the same?"], "evidence": [[[["Justin Timberlake-32"]], [["Cry Me a River (Justin Timberlake song)-1"]], ["operation"]], [[["Justin Timberlake-32"]], [["Cry Me a River (Justin Timberlake song)-1"]], ["operation"]], [[["Justin Timberlake-32"]], [["Cry Me a River (Justin Timberlake song)-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the year 2000?\nFacts: The year 2000 was the first year that the year zero was used.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the year", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Has Justin Timberlake ever written a song about Britney Spears?\nFacts: Justin Timberlake and Britney Spears dated in 1999. In 2002, Justin Timberlake released a music video for his breakup song 'Cry Me A River' and featured an actress who resembled his then ex Britney Spears.\nAnswer (True or False):", "ct_subject": "Justin Timberlake and Britney Spears dated in 1999. In 2002, Justin Timberlake released a music video for his breakup song 'Cry Me A River' and featured an actress who resembled his then ex Britney Spears.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999871"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9470028"], [" person", "0.99704796"], [" to", "1.0"], [" be", "0.9993254"], [" born", "0.79679745"], [" in", "1.0"], [" the", "1.0"], [" year", "0.954689"], [" 2000", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" year", "0.9992987"], [" 2000", "1.0"], [" was", "0.9999999"], [" the", "1.0"], [" first", "0.9999999"], [" year", "1.0"], [" that", "1.0"], [" the", "0.99881613"], [" year", "1.0"], [" zero", "0.99623626"], [" was", "1.0"], [" used", "0.96407914"], [".", "0.9999995"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"]]}, "253": {"qid": "2915baf6fba0c1b5100f", "term": "Christopher Columbus", "description": "Italian explorer, navigator, and colonizer", "question": "Did Christopher Columbus break the fifth commandment in Christianity?", "answer": true, "facts": ["The fifth commandment states that Thou Shalt Not Kill.", "Christopher Columbus ordered a brutal crackdown in which many natives were killed, and then paraded their dismembered bodies through the streets."], "decomposition": ["What is the fifth commandment in Christianity?", "What activities did Christopher Columbus subject Native Americans too?", "Is the activity in #1 also in #2?"], "evidence": [[[["Thou shalt not kill-29"]], [["Christopher Columbus-48"]], [["Christopher Columbus-48"], "operation"]], [[["Ten Commandments-16"]], [["Christopher Columbus-93"], "no_evidence"], ["no_evidence", "operation"]], [[["Thou shalt not kill-36"], "no_evidence"], [["Christopher Columbus-48"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the first man on the moon land on the moon?\nFacts: The first man on the moon landed on the moon on July 20, 1969.\nAnswer (True or False): True\nQuestion: Did the first man on the moon land on the moon?\nFacts: The first man on the moon landed on", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Christopher Columbus break the fifth commandment in Christianity?\nFacts: The fifth commandment states that Thou Shalt Not Kill. Christopher Columbus ordered a brutal crackdown in which many natives were killed, and then paraded their dismembered bodies through the streets.\nAnswer (True or False):", "ct_subject": "The fifth commandment states that Thou Shalt Not Kill. Christopher Columbus ordered a brutal crackdown in which many natives were killed, and then paraded their dismembered bodies through the streets.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" first", "0.7494884"], [" man", "0.40824452"], [" on", "0.99920255"], [" the", "1.0"], [" moon", "1.0"], [" land", "0.9999999"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999999"], [" first", "1.0"], [" man", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" landed", "0.99995816"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" on", "0.99999976"], [" July", "1.0"], [" 20", "1.0"], [",", "1.0"], [" 1969", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "0.999997"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" land", "0.9954821"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" man", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" landed", "1.0"], [" on", "1.0"]]}, "254": {"qid": "8c0614f44911693572b5", "term": "Transport", "description": "Human-directed movement of things or people between locations", "question": "Can you transport a primate in a backpack?", "answer": true, "facts": ["Primates include lemurs, monkeys, apes, and humans.", "A Capuchin is a type of monkey that are an average height of twelve inches and weighs nine pounds.", "A school ruler is twelve inches.", "One school textbook can weigh up to six pounds."], "decomposition": ["What is the average size and weight capacity of a backpack?", "What are some common primates?", "Is any of #2 such that its size and weight is less than or equal to #1?"], "evidence": [[[["Backpack-3"]], [["Primate-1"]], [["Madame Berthe's mouse lemur-1"]]], [[["Backpack-3"]], [["Primate-1"], "no_evidence"], ["operation"]], [[["Backpack-18"], "no_evidence"], [["Primates (journal)-1"], "no_evidence"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance a human has ever traveled?\nFacts: The longest distance a human has ever traveled is about 1,000 miles.\nAnswer (True or False): True\nQuestion: What is the longest distance a human has ever traveled?\nF", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can you transport a primate in a backpack?\nFacts: Primates include lemurs, monkeys, apes, and humans. A Capuchin is a type of monkey that are an average height of twelve inches and weighs nine pounds. A school ruler is twelve inches. One school textbook can weigh up to six pounds.\nAnswer (True or False):", "ct_subject": "Primates include lemurs, monkeys, apes, and humans. A Capuchin is a type of monkey that are an average height of twelve inches and weighs nine pounds. A school ruler is twelve inches. One school textbook can weigh up to six pounds.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999763"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.81516784"], [" distance", "0.99999905"], [" a", "0.8561558"], [" human", "0.97525716"], [" has", "0.45734015"], [" ever", "0.86204517"], [" traveled", "0.99999976"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" human", "1.0"], [" has", "1.0"], [" ever", "1.0"], [" traveled", "1.0"], [" is", "1.0"], [" about", "0.9982122"], [" 1", "0.9999207"], [",", "1.0"], ["000", "0.99997616"], [" miles", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "0.99999857"], [" a", "1.0"], [" human", "1.0"], [" has", "1.0"], [" ever", "1.0"], [" traveled", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"]]}, "255": {"qid": "6f0b33d71d2e65d3b376", "term": "Sonnet", "description": "form of poetry with fourteen lines; by the thirteenth century it signified a poem of fourteen lines that follows a strict rhyme scheme and specific structure", "question": "Did a Polish poet write sonnets about Islamic religion?", "answer": true, "facts": ["Adam Mickiewicz was a Polish poet. ", "Adam Mickiewicz 's sonnet sequence focuses heavily on the culture and Islamic religion of the Crimean Tatars."], "decomposition": ["What were the major focus of Adam Mickiewicz's sonnets?", "Is #1 about Islamic religion?", "Was Adam Mickiewicz a Polish poet?", "Are #2 and #3 positive?"], "evidence": [[[["The Crimean Sonnets-1"]], [["Orientalism-1", "The Crimean Sonnets-2"]], [["Adam Mickiewicz-1"]], ["operation"]], [[["The Crimean Sonnets-2"]], ["no_evidence"], [["Adam Mickiewicz-1"]], ["operation"]], [[["The Crimean Sonnets-1"], "no_evidence"], [["Adam Mickiewicz-23"], "no_evidence", "operation"], [["Adam Mickiewicz-1"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the first person to use the word \"gay\" in English was a man?\nFacts: The first person to use the word \"gay\" in English was a man. The word \"gay\" was first used in English in the 17th century.\nAnswer (True or False): True\nQuestion: Did the first person to use the word \"gay", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did a Polish poet write sonnets about Islamic religion?\nFacts: Adam Mickiewicz was a Polish poet.  Adam Mickiewicz 's sonnet sequence focuses heavily on the culture and Islamic religion of the Crimean Tatars.\nAnswer (True or False):", "ct_subject": "Adam Mickiewicz was a Polish poet.  Adam Mickiewicz 's sonnet sequence focuses heavily on the culture and Islamic religion of the Crimean Tatars.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999964"], [" the", "0.99990654"], [" first", "0.9737975"], [" person", "0.96841085"], [" to", "1.0"], [" use", "0.9922226"], [" the", "1.0"], [" word", "0.99999917"], [" \"", "1.0"], ["gay", "0.9979765"], ["\"", "1.0"], [" in", "1.0"], [" English", "0.9999987"], [" was", "0.44609088"], [" a", "1.0"], [" man", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "0.8320171"], [" person", "0.9999976"], [" to", "1.0"], [" use", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["gay", "1.0"], ["\"", "1.0"], [" in", "1.0"], [" English", "1.0"], [" was", "1.0"], [" a", "1.0"], [" man", "1.0"], [".", "0.9999988"], [" The", "0.9989785"], [" word", "0.9999937"], [" \"", "0.99999845"], ["gay", "1.0"], ["\"", "1.0"], [" was", "1.0"], [" first", "0.94838923"], [" used", "1.0"], [" in", "1.0"], [" English", "0.95116127"], [" in", "0.9996877"], [" the", "0.99999917"], [" 17", "0.8571884"], ["th", "1.0"], [" century", "1.0"], [".", "0.9995684"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" use", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["gay", "1.0"]]}, "256": {"qid": "c4a9e56df5b83e483769", "term": "Brazilian Navy", "description": "Naval warfare branch of Brazil's military forces", "question": "Could modern Brazilian Navy have hypothetically turned the tide in Battle of Actium?", "answer": true, "facts": ["The Battle of Actium saw Mark Antony's army lose to Octavian.", "Octavian's army had 400 ships, 16000 infantry, and 3,000 archers.", "The Brazilian Navy has over 80,000 personnel, including 16,000 marines.", "Several Brazilian Navy ships are armed with explosive torpedoes. "], "decomposition": ["What was the result of the Battle of Actium?", "In #1, how many resources did the Octavian's army have?", " How many resources does the Brazilian Navy have? ", "Is #3 significantly more than #2?"], "evidence": [[[["Battle of Actium-26"]], [["Battle of Actium-14"]], [["Brazilian Navy-55", "Brazilian Navy-56"]], ["operation"]], [[["Battle of Actium-2"]], [["Battle of Actium-12"]], [["Brazilian Navy-55"]], ["operation"]], [[["Battle of Actium-2"]], [["Battle of Actium-12"]], [["Brazilian Navy-56"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the largest number of people that can fit into a single room?\nFacts: The largest number of people that can fit into a single room is 2,000", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could modern Brazilian Navy have hypothetically turned the tide in Battle of Actium?\nFacts: The Battle of Actium saw Mark Antony's army lose to Octavian. Octavian's army had 400 ships, 16000 infantry, and 3,000 archers. The Brazilian Navy has over 80,000 personnel, including 16,000 marines. Several Brazilian Navy ships are armed with explosive torpedoes. \nAnswer (True or False):", "ct_subject": "The Battle of Actium saw Mark Antony's army lose to Octavian. Octavian's army had 400 ships, 16000 infantry, and 3,000 archers. The Brazilian Navy has over 80,000 personnel, including 16,000 marines. Several Brazilian Navy ships are armed with explosive torpedoes. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9990677"], [" is", "1.0"], [" the", "1.0"], [" largest", "0.595723"], [" number", "0.99951816"], [" of", "1.0"], [" people", "1.0"], [" that", "0.99998987"], [" can", "0.7346404"], [" fit", "1.0"], [" into", "0.93379164"], [" a", "1.0"], [" single", "1.0"], [" room", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" largest", "1.0"], [" number", "0.9999536"], [" of", "1.0"], [" people", "1.0"], [" that", "1.0"], [" can", "1.0"], [" fit", "1.0"], [" into", "1.0"], [" a", "1.0"], [" single", "1.0"], [" room", "1.0"], [" is", "1.0"], [" 2", "0.99850625"], [",", "1.0"], ["000", "0.99999666"]]}, "257": {"qid": "347eea662abc9467ed18", "term": "Jason", "description": "Greek mythological hero", "question": "Could the children of Greek hero Jason hypothetically fill a polo team?", "answer": true, "facts": ["The Greek mythological hero is known for his quest to obtain the Golden Fleece.", "The Greek mythological hero had four children: Euneus, Nebrophonus, Mermerus, and Pheres.", "Polo is a sport played between two teams of 4 players."], "decomposition": ["How many children did Greek mythological hero Jason have?", "How many people are needed to make a polo team?", "Is #1 equal to or more than #2?"], "evidence": [[["no_evidence"], [["Polo-51"]], ["operation"]], [[["Medea-10"]], [["Polo-4"]], ["operation"]], [[["Medea-10"]], [["Water polo-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Could the children of the Greek hero Jason hypothetically fill a polo team?\nFacts: The Greek mythological hero is known for his quest to obtain the Golden Fleece. The Greek mythological hero had four", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could the children of Greek hero Jason hypothetically fill a polo team?\nFacts: The Greek mythological hero is known for his quest to obtain the Golden Fleece. The Greek mythological hero had four children: Euneus, Nebrophonus, Mermerus, and Pheres. Polo is a sport played between two teams of 4 players.\nAnswer (True or False):", "ct_subject": "The Greek mythological hero is known for his quest to obtain the Golden Fleece. The Greek mythological hero had four children: Euneus, Nebrophonus, Mermerus, and Pheres. Polo is a sport played between two teams of 4 players.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Could", "0.8179866"], [" the", "1.0"], [" children", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Greek", "0.8780751"], [" hero", "1.0"], [" Jason", "1.0"], [" hypot", "1.0"], ["hetically", "1.0"], [" fill", "1.0"], [" a", "1.0"], [" pol", "1.0"], ["o", "1.0"], [" team", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Greek", "1.0"], [" myth", "1.0"], ["ological", "1.0"], [" hero", "1.0"], [" is", "1.0"], [" known", "1.0"], [" for", "1.0"], [" his", "1.0"], [" quest", "1.0"], [" to", "1.0"], [" obtain", "1.0"], [" the", "1.0"], [" Golden", "1.0"], [" F", "1.0"], ["lee", "1.0"], ["ce", "1.0"], [".", "1.0"], [" The", "1.0"], [" Greek", "1.0"], [" myth", "1.0"], ["ological", "1.0"], [" hero", "1.0"], [" had", "1.0"], [" four", "1.0"]]}, "258": {"qid": "0a32d7cfde6cec332fd6", "term": "ABBA", "description": "Swedish pop group", "question": "Could ABBA play a mixed doubles tennis game against each other?", "answer": true, "facts": ["ABBA contained two male and two female members.", "Mixed doubles tennis games consist of two teams of one man and one woman on each."], "decomposition": ["How many men and women are required to participate in a mixed doubles tennis game?", "How many men and women are members of the ABBA group?", "Is #2 at least equal to #1?"], "evidence": [[[["Mixed doubles-1", "Mixed-sex sports-12"]], [["ABBA-1", "ABBA-2"]], ["operation"]], [[["Types of tennis match-4"]], [["ABBA-2"]], ["operation"]], [[["Mixed doubles-1"]], [["ABBA-1", "Agnetha F\u00e4ltskog-11"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: John Adams was born on July 4, 1735.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: John Adams was born on July 4, 1735.\nAnswer (", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could ABBA play a mixed doubles tennis game against each other?\nFacts: ABBA contained two male and two female members. Mixed doubles tennis games consist of two teams of one man and one woman on each.\nAnswer (True or False):", "ct_subject": "ABBA contained two male and two female members. Mixed doubles tennis games consist of two teams of one man and one woman on each.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999889"], [" is", "1.0"], [" the", "1.0"], [" name", "0.98584414"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9979101"], [" person", "0.9566109"], [" to", "1.0"], [" be", "0.99338293"], [" born", "0.9997253"], [" in", "1.0"], [" the", "1.0"], [" United", "0.9667359"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "0.97658545"], [" Adams", "0.9999709"], [" was", "0.9999989"], [" born", "0.99939585"], [" on", "0.99983835"], [" July", "0.8561078"], [" 4", "1.0"], [",", "1.0"], [" 17", "1.0"], ["35", "0.5466717"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "0.9138257"], [" Adams", "1.0"], [" was", "1.0"], [" born", "1.0"], [" on", "1.0"], [" July", "1.0"], [" 4", "1.0"], [",", "1.0"], [" 17", "1.0"], ["35", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"]]}, "259": {"qid": "65247fed8b1775f3b91c", "term": "Aloe", "description": "genus of plants", "question": "Is material from an aloe plant sometimes enclosed in petroleum-derived products?", "answer": true, "facts": ["Aloe vera gel is sometimes kept in plastic packaging.", "Plastic packaging is derived from petroleum."], "decomposition": ["What kind of products are derived from petroleum?", "What products are made from aloe plants?", "Is #2 ever be packaged inside #1?"], "evidence": [[[["Petroleum product-4"]], [["Aloe vera-19"]], ["no_evidence", "operation"]], [[["Plastic-3"], "no_evidence"], [["Aloe vera-15"], "no_evidence"], ["operation"]], [[["Petroleum-2"]], [["Petroleum-2"]], [["Petroleum-2"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the word \"candy\" derived from the word \"candy\"?\nFacts: The word \"candy\" is derived from the word \"candy\" and the word \"candy\" is derived from the word \"candy\".\nAnswer (True or False): True\nQuestion: Is the word \"candy\" derived from the word \"candy\"?\nFacts: The word", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is material from an aloe plant sometimes enclosed in petroleum-derived products?\nFacts: Aloe vera gel is sometimes kept in plastic packaging. Plastic packaging is derived from petroleum.\nAnswer (True or False):", "ct_subject": "Aloe vera gel is sometimes kept in plastic packaging. Plastic packaging is derived from petroleum.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "0.7188842"], [" word", "0.99950445"], [" \"", "1.0"], ["c", "0.9996295"], ["andy", "0.9991178"], ["\"", "1.0"], [" derived", "0.9999995"], [" from", "1.0"], [" the", "1.0"], [" word", "0.99993885"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"?", "0.9957825"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" derived", "0.9999126"], [" from", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "0.99999833"], ["\"", "0.9998945"], [" and", "0.99992704"], [" the", "0.6815068"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"", "1.0"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" word", "1.0"]]}, "260": {"qid": "e183f97699de5a056823", "term": "Hanging", "description": "execution or suicide method involving suspension of a person by a ligature", "question": "Do bodies movie during hanging?", "answer": true, "facts": ["Electrochemical nerve signals are fired after death that can cause a body to twitch.", "If death by hanging is accomplished due to asphyxia, the victim may attempt to free themselves or may appear to struggle."], "decomposition": ["What does death by hanging usually induce in victims?", "What processes could occur in the nervous system immediately after death?", "Do #1 or #2 result in body movement?"], "evidence": [[[["Hanging-25"]], [["Hanging-26"]], ["operation"]], [[["Hanging-19"]], ["no_evidence"], ["operation"]], [[["Hanging-1"]], [["Hanging-25"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"skeleton\" and a \"body\"?\nFacts: A skeleton is a solid mass of tissue that is not alive. A body is a living organism.\nAnswer (True or False): True\nQuestion: What is the difference between a \"skeleton\" and a \"body\"?\nFacts:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do bodies movie during hanging?\nFacts: Electrochemical nerve signals are fired after death that can cause a body to twitch. If death by hanging is accomplished due to asphyxia, the victim may attempt to free themselves or may appear to struggle.\nAnswer (True or False):", "ct_subject": "Electrochemical nerve signals are fired after death that can cause a body to twitch. If death by hanging is accomplished due to asphyxia, the victim may attempt to free themselves or may appear to struggle.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9986755"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "0.99999964"], [" \"", "0.9755107"], ["s", "0.99327564"], ["keleton", "0.60726607"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["body", "0.13256392"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.99999857"], [" skeleton", "1.0"], [" is", "1.0"], [" a", "1.0"], [" solid", "0.9986725"], [" mass", "0.9999993"], [" of", "1.0"], [" tissue", "0.9983083"], [" that", "0.9999567"], [" is", "0.9999335"], [" not", "0.87358254"], [" alive", "1.0"], [".", "1.0"], [" A", "1.0"], [" body", "1.0"], [" is", "1.0"], [" a", "1.0"], [" living", "1.0"], [" organism", "0.9921508"], [".", "0.17901643"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["keleton", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["body", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"]]}, "261": {"qid": "61dd8e3c0e1e6aa4422f", "term": "Little Women", "description": "1860s novel by Louisa May Alcott", "question": "Would the author of Little Women have remembered the ratification of the 13th Amendment?", "answer": true, "facts": ["The 13th Amendment was ratified in 1865.", "Louisa May Alcott died in 1888."], "decomposition": ["When was the 13th Amendment ratified?", "Who wrote Little Women?", "What years was #2 alive?", "Did #1 occur sometime during #3?"], "evidence": [[[["Thirteenth Amendment to the United States Constitution-1"]], [["Little Women-1"]], [["Louisa May Alcott-1"]], ["operation"]], [[["Thirteenth Amendment to the United States Constitution-1"]], [["Little Women-1"]], [["Little Women-1"]], ["operation"]], [[["Thirteenth Amendment to the United States Constitution-1"]], [["Little Women-1"]], [["Little Women-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the author of Little Women have a son named John?\nFacts: The author of Little Women had a son named John.\nAnswer (True or False): True\nQuestion: Did the author of Little Women have a daughter named Mary?\nFacts: The author of Little Women had a daughter named Mary.\nAnswer (True or False): True\nQuestion: Did the author of Little Women", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would the author of Little Women have remembered the ratification of the 13th Amendment?\nFacts: The 13th Amendment was ratified in 1865. Louisa May Alcott died in 1888.\nAnswer (True or False):", "ct_subject": "The 13th Amendment was ratified in 1865. Louisa May Alcott died in 1888.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.40543032"], [" the", "1.0"], [" author", "1.0"], [" of", "1.0"], [" Little", "1.0"], [" Women", "1.0"], [" have", "0.9045676"], [" a", "1.0"], [" son", "0.99984384"], [" named", "0.9999927"], [" John", "0.9936818"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99989724"], [" author", "1.0"], [" of", "1.0"], [" Little", "1.0"], [" Women", "1.0"], [" had", "1.0"], [" a", "1.0"], [" son", "1.0"], [" named", "1.0"], [" John", "1.0"], [".", "1.0"], ["\n", "0.9999999"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" author", "1.0"], [" of", "1.0"], [" Little", "1.0"], [" Women", "1.0"], [" have", "1.0"], [" a", "1.0"], [" daughter", "1.0"], [" named", "1.0"], [" Mary", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" author", "1.0"], [" of", "1.0"], [" Little", "1.0"], [" Women", "1.0"], [" had", "1.0"], [" a", "1.0"], [" daughter", "1.0"], [" named", "1.0"], [" Mary", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" author", "1.0"], [" of", "1.0"], [" Little", "1.0"], [" Women", "1.0"]]}, "262": {"qid": "b53d6f0058183ba7cefe", "term": "Western honey bee", "description": "Species of insect", "question": "Does US brand Nice depend on Western honey bee?", "answer": true, "facts": ["US brand Nice creates a number of products including honey.", "The Western honey bee can be found on every continent except Antarctica.", "The Western honey bee is the most common pollinator in US, making it the most important bee to domestic agriculture."], "decomposition": ["Which insect does US brand Nice need to obtain the honey they sell from?", "Is the Western honey bee a common example of #1 that can be found in the US?"], "evidence": [[[["Honey-1"], "no_evidence"], [["Honey bee-20", "Western honey bee-1"], "operation"]], [[["Honey-5"]], [["Western honey bee-5"], "operation"]], [[["Walgreens-13"], "no_evidence"], [["Western honey bee-5"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"C\" and a \"K\"?\nFacts: The \"C\" stands for \"Civil\" and the \"K\" stands for \"King\".\nAnswer (True or False): True\nQuestion: What is the difference between a \"C\" and a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does US brand Nice depend on Western honey bee?\nFacts: US brand Nice creates a number of products including honey. The Western honey bee can be found on every continent except Antarctica. The Western honey bee is the most common pollinator in US, making it the most important bee to domestic agriculture.\nAnswer (True or False):", "ct_subject": "US brand Nice creates a number of products including honey. The Western honey bee can be found on every continent except Antarctica. The Western honey bee is the most common pollinator in US, making it the most important bee to domestic agriculture.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99205184"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9999906"], [" between", "1.0"], [" a", "0.99998116"], [" \"", "0.9999417"], ["C", "0.8548357"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["K", "0.95220834"], ["\"?", "0.9999119"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.95077"], [" \"", "0.9695413"], ["C", "1.0"], ["\"", "1.0"], [" stands", "0.9998987"], [" for", "1.0"], [" \"", "1.0"], ["Civil", "0.9751534"], ["\"", "0.6124386"], [" and", "0.9999999"], [" the", "1.0"], [" \"", "1.0"], ["K", "1.0"], ["\"", "1.0"], [" stands", "1.0"], [" for", "1.0"], [" \"", "1.0"], ["King", "1.0"], ["\".", "0.99997747"], ["\n", "0.99999905"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["C", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"]]}, "263": {"qid": "e91ba24305ae9f8850ed", "term": "Hulk", "description": "Superhero appearing in Marvel Comics publications and related media", "question": "Can Hulk's alter ego explain atomic events?", "answer": true, "facts": ["Hulk's alter ego is Dr. Robert Bruce Banner", "Dr. Robert Bruce Banner is a nuclear physicist. ", "Nuclear physics is the field of physics that studies atomic nuclei and their constituents and interactions. "], "decomposition": ["Who is the Hulk's alter ego?", "What is the profession of #1?", "What do people in #2 have a knowledge of?", "Is atomic events included in #3?"], "evidence": [[[["Hulk-1"]], [["Hulk-45"]], [["Physicist-1"]], [["Elementary event-1"]]], [[["Hulk-9"]], [["Hulk-1"]], [["Scientist-1"]], [["Atomic Age (design)-1"], "operation"]], [[["Hulk-1"]], [["Hulk-57"]], ["operation"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the planet that the sun orbits?\nFacts: The planet that the sun orbits is called the Sun.  The Sun is a star that is the center of our solar system.  The Sun is a star that is the center of our solar system. \nAnswer (True or False): True\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can Hulk's alter ego explain atomic events?\nFacts: Hulk's alter ego is Dr. Robert Bruce Banner Dr. Robert Bruce Banner is a nuclear physicist.  Nuclear physics is the field of physics that studies atomic nuclei and their constituents and interactions. \nAnswer (True or False):", "ct_subject": "Hulk's alter ego is Dr. Robert Bruce Banner Dr. Robert Bruce Banner is a nuclear physicist.  Nuclear physics is the field of physics that studies atomic nuclei and their constituents and interactions. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9996126"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9433934"], [" of", "1.0"], [" the", "1.0"], [" planet", "0.9250371"], [" that", "0.99865085"], [" the", "0.9999851"], [" sun", "0.9999999"], [" orbits", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" planet", "0.99942744"], [" that", "1.0"], [" the", "1.0"], [" sun", "1.0"], [" orbits", "1.0"], [" is", "1.0"], [" called", "1.0"], [" the", "0.99648994"], [" Sun", "0.95606023"], [".", "1.0"], [" ", "1.0"], [" The", "0.99782085"], [" Sun", "1.0"], [" is", "1.0"], [" a", "0.9997967"], [" star", "1.0"], [" that", "0.9936214"], [" is", "1.0"], [" the", "0.9999975"], [" center", "0.9999478"], [" of", "1.0"], [" our", "0.9995571"], [" solar", "1.0"], [" system", "1.0"], [".", "1.0"], [" ", "1.0"], [" The", "0.9997665"], [" Sun", "1.0"], [" is", "1.0"], [" a", "0.9997738"], [" star", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" center", "1.0"], [" of", "1.0"], [" our", "1.0"], [" solar", "1.0"], [" system", "1.0"], [".", "1.0"], [" ", "1.0"], ["\n", "0.018380921"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"]]}, "264": {"qid": "f41b809dcfb764234ce8", "term": "Reddit", "description": "Online news aggregator", "question": "Are the majority of Reddit users familiar with the Pledge of Allegiance?", "answer": true, "facts": ["55% of the Reddit user base comes from the United States.", "Congressional sessions open with the recital of the Pledge, as do many government meetings at local levels, and meetings held by many private organizations.", "All states except California, Hawaii, Iowa, Vermont, and Wyoming require a regularly scheduled recitation of the pledge in public schools."], "decomposition": ["What country do most Reddit users come from?", "What country is the Pledge of Allegiance associated with?", "Is #1 the same as #2?"], "evidence": [[[["Reddit-2"]], [["Pledge of Allegiance-1"]], ["operation"]], [[["Reddit-2"]], [["Pledge of Allegiance-1"]], ["operation"]], [[["Reddit-2"]], [["Pledge of Allegiance-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are the majority of Reddit users familiar with the Pledge of Allegiance?\nFacts: 55% of the Reddit user base comes from the United States. Congressional sessions open with", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are the majority of Reddit users familiar with the Pledge of Allegiance?\nFacts: 55% of the Reddit user base comes from the United States. Congressional sessions open with the recital of the Pledge, as do many government meetings at local levels, and meetings held by many private organizations. All states except California, Hawaii, Iowa, Vermont, and Wyoming require a regularly scheduled recitation of the pledge in public schools.\nAnswer (True or False):", "ct_subject": "55% of the Reddit user base comes from the United States. Congressional sessions open with the recital of the Pledge, as do many government meetings at local levels, and meetings held by many private organizations. All states except California, Hawaii, Iowa, Vermont, and Wyoming require a regularly scheduled recitation of the pledge in public schools.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.99999416"], [" the", "0.5916053"], [" majority", "1.0"], [" of", "1.0"], [" Reddit", "0.9944982"], [" users", "1.0"], [" familiar", "1.0"], [" with", "1.0"], [" the", "1.0"], [" Pledge", "1.0"], [" of", "1.0"], [" Alleg", "1.0"], ["iance", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" 55", "1.0"], ["%", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Reddit", "1.0"], [" user", "1.0"], [" base", "1.0"], [" comes", "1.0"], [" from", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [".", "1.0"], [" Congressional", "1.0"], [" sessions", "1.0"], [" open", "1.0"], [" with", "1.0"]]}, "265": {"qid": "353bf34cd9457f69b870", "term": "YMCA", "description": "Worldwide organization founded in 1844 on principles of muscular Christianity", "question": "Is it normal for people to sing when the YMCA is mentioned?", "answer": true, "facts": ["The YMCA was written about in a widely popular song by \"The Village People\"", "The Village People's song \"YMCA\" had an easy to do and very popular dance routine that went with it. ", "The song \"YMCA\" is extremely well known."], "decomposition": ["In what song was the YMCA written about?", "Is #1 a very popular song?"], "evidence": [[[["Y.M.C.A. (song)-1"]], [["Y.M.C.A. (song)-2"]]], [[["Y.M.C.A. (song)-1"]], [["Y.M.C.A. (song)-4"], "no_evidence"]], [[["Y.M.C.A. (song)-1"]], [["Y.M.C.A. (song)-1", "Y.M.C.A. (song)-2"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it normal for people to have a \"Y\" in their name?\nFacts: The Y is the first letter of the alphabet. The first letter of the alphabet is \"A\". The first letter of the alphabet", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is it normal for people to sing when the YMCA is mentioned?\nFacts: The YMCA was written about in a widely popular song by \"The Village People\" The Village People's song \"YMCA\" had an easy to do and very popular dance routine that went with it.  The song \"YMCA\" is extremely well known.\nAnswer (True or False):", "ct_subject": "The YMCA was written about in a widely popular song by \"The Village People\" The Village People's song \"YMCA\" had an easy to do and very popular dance routine that went with it.  The song \"YMCA\" is extremely well known.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9994825"], [" it", "1.0"], [" normal", "0.99999654"], [" for", "1.0"], [" people", "1.0"], [" to", "1.0"], [" have", "0.9999516"], [" a", "1.0"], [" \"", "0.9895169"], ["Y", "1.0"], ["\"", "1.0"], [" in", "0.94187444"], [" their", "1.0"], [" name", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999558"], [" Y", "0.9999777"], [" is", "0.9999963"], [" the", "0.9522956"], [" first", "0.99981064"], [" letter", "1.0"], [" of", "0.99999964"], [" the", "1.0"], [" alphabet", "1.0"], [".", "0.9999999"], [" The", "0.9837008"], [" first", "0.99096733"], [" letter", "1.0"], [" of", "1.0"], [" the", "1.0"], [" alphabet", "1.0"], [" is", "1.0"], [" \"", "0.99999833"], ["A", "1.0"], ["\".", "0.9999925"], [" The", "0.9999937"], [" first", "1.0"], [" letter", "1.0"], [" of", "1.0"], [" the", "1.0"], [" alphabet", "1.0"]]}, "266": {"qid": "0c6e530c3581d6cf0193", "term": "Porsche", "description": "automotive brand manufacturing subsidiary of Volkswagen", "question": "Can Billie Eilish afford a Porsche?", "answer": true, "facts": ["Billie Eilish is a famous female singer.", "Billie Eilish is 18 years old and has a net worth of $25 Million. ", "A Porsche Boxster is a car that starts at $59,000.", "$25,000,000 is greater than $59,000."], "decomposition": ["What is Billie Eilish's net worth?", "How much does a Porsche cost?", "Is #1 greater than #2?"], "evidence": [[[["Billie Eilish-3"], "no_evidence"], [["Porsche Carrera GT-5", "Porsche-16"], "no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], [["Porsche-16"]], ["no_evidence", "operation"]], [[["Billie Eilish-23"], "no_evidence"], [["Porsche Panamera-19"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average IQ of a man?\nFacts: The average IQ of a man is 100. The average IQ of a woman is 100.\nAnswer (True or False): True\nQuestion: What is the average", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can Billie Eilish afford a Porsche?\nFacts: Billie Eilish is a famous female singer. Billie Eilish is 18 years old and has a net worth of $25 Million.  A Porsche Boxster is a car that starts at $59,000. $25,000,000 is greater than $59,000.\nAnswer (True or False):", "ct_subject": "Billie Eilish is a famous female singer. Billie Eilish is 18 years old and has a net worth of $25 Million.  A Porsche Boxster is a car that starts at $59,000. $25,000,000 is greater than $59,000.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.999969"], [" is", "1.0"], [" the", "1.0"], [" average", "0.8846253"], [" IQ", "0.7009205"], [" of", "1.0"], [" a", "0.9992242"], [" man", "0.9979352"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" IQ", "1.0"], [" of", "1.0"], [" a", "1.0"], [" man", "1.0"], [" is", "1.0"], [" 100", "1.0"], [".", "1.0"], [" The", "0.99010974"], [" average", "1.0"], [" IQ", "1.0"], [" of", "1.0"], [" a", "1.0"], [" woman", "1.0"], [" is", "1.0"], [" 100", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"]]}, "267": {"qid": "0b7782218af2a65a6094", "term": "Astronomer", "description": "Scientist who studies celestial bodies", "question": "Does James Webb Space Telescope fail astronomer in locating planet Krypton?", "answer": true, "facts": ["The James Webb Space Telescope is the most powerful telescope created.", "Krypton is a planet in the fictional Superman comic book series."], "decomposition": ["Which universe does the planet Krypton exist in?", "Does the James Webb Space Telescope as we know it exist in a universe different from #1?"], "evidence": [[[["Krypton (comics)-1"]], [["James Webb Space Telescope-1"], "no_evidence"]], [[["Krypton (comics)-1"]], [["James Webb Space Telescope-1"]]], [[["Krypton (comics)-1"]], [["James Webb Space Telescope-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the Earth flat?\nFacts: The Earth is round.\nAnswer (True or False): True\nQuestion: Is the Earth flat?\nFacts: The Earth is round.\nAnswer (True or False): True\nQuestion: Is the Earth flat?\nFacts: The Earth is round.\nAnswer (True or False): True\nQuestion: Is the Earth", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does James Webb Space Telescope fail astronomer in locating planet Krypton?\nFacts: The James Webb Space Telescope is the most powerful telescope created. Krypton is a planet in the fictional Superman comic book series.\nAnswer (True or False):", "ct_subject": "The James Webb Space Telescope is the most powerful telescope created. Krypton is a planet in the fictional Superman comic book series.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.8371347"], [" the", "0.9995815"], [" Earth", "0.27183858"], [" flat", "0.9999645"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" round", "0.99999964"], [".", "1.0"], ["\n", "0.99954695"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" Earth", "1.0"], [" flat", "0.9208131"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" round", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" Earth", "1.0"], [" flat", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" round", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" Earth", "1.0"]]}, "268": {"qid": "a8088fd4d533c2eaa365", "term": "Eggplant", "description": "plant species Solanum melongena", "question": "Can spiders help eggplant farmers control parasites?", "answer": true, "facts": ["The potato tuber moth is a parasite that targets the plant family Solanaceae, including eggplant ", "Selenops radiatus is a spider genus in South Africa that effectively controls the potato tuber moth"], "decomposition": ["Which major parasite insect are eggplants host plants to?", "What are the natural enemies of #1 that farmers can use to control them?", "Is any of #2 a spider?"], "evidence": [[[["Eggplant-50"]], [["Spider behavior-2"]], ["operation"]], [[["Eggplant-46"]], [["Spider-4"]], ["operation"]], [[["Eggplant-46", "Eggplant-47"]], [["Aphid-36", "Spider-4"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Can you tell the difference between a man and a woman by looking at their genitals?\nFacts: The male and female genitals are very similar. The only difference is that the male has a penis and the female has a vagina.\nAnswer (True or False): True\nQuestion: Can you tell the difference between a man and a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can spiders help eggplant farmers control parasites?\nFacts: The potato tuber moth is a parasite that targets the plant family Solanaceae, including eggplant  Selenops radiatus is a spider genus in South Africa that effectively controls the potato tuber moth\nAnswer (True or False):", "ct_subject": "The potato tuber moth is a parasite that targets the plant family Solanaceae, including eggplant  Selenops radiatus is a spider genus in South Africa that effectively controls the potato tuber moth", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Can", "0.21504845"], [" you", "0.82739"], [" tell", "0.9997589"], [" the", "0.99999845"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" man", "0.8003949"], [" and", "1.0"], [" a", "1.0"], [" woman", "1.0"], [" by", "0.9999999"], [" looking", "0.9999975"], [" at", "1.0"], [" their", "1.0"], [" genitals", "0.9999517"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" male", "0.999788"], [" and", "0.4019178"], [" female", "1.0"], [" genitals", "0.99999857"], [" are", "1.0"], [" very", "0.9999825"], [" similar", "0.8969586"], [".", "0.9998697"], [" The", "0.9999999"], [" only", "0.31794998"], [" difference", "0.99995875"], [" is", "0.99998283"], [" that", "0.999546"], [" the", "1.0"], [" male", "0.99978906"], [" has", "0.92520845"], [" a", "1.0"], [" penis", "1.0"], [" and", "0.99999964"], [" the", "1.0"], [" female", "1.0"], [" has", "1.0"], [" a", "1.0"], [" vagina", "0.9999304"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Can", "0.8307414"], [" you", "1.0"], [" tell", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" man", "1.0"], [" and", "1.0"], [" a", "1.0"]]}, "269": {"qid": "48648780e56f136fdae5", "term": "Pandora", "description": "Mythological figure", "question": "Were items released from Pandora's box at least two of the names of Four Horsemen?", "answer": true, "facts": ["Pandora was a mythical figure that opened a box and released several ills on the world including famine, sickness, and death.", "The Four Horsemen of the Apocalypse are: Pestilence, War, Famine, and Death."], "decomposition": ["What items were released from Pandora's box?", "What were the names of the Four Horsemen", "Is there any overlap between #1 and #2?"], "evidence": [[[["Pandora's box-2"]], [["Four Horsemen of the Apocalypse-3"]], [["Four Horsemen of the Apocalypse-3", "Pandora's box-2"]]], [[["Pandora's box-2", "Pandora's box-31"]], [["Four Horsemen of the Apocalypse-5", "Horsemen of Apocalypse-7"], "no_evidence"], ["no_evidence", "operation"]], [[["Pandora's box-2"]], [["Four Horsemen of the Apocalypse-3"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: John Adams was the first person born in the United States.\nAnswer (True or False): True\nQuestion: What is the name of the first person", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Were items released from Pandora's box at least two of the names of Four Horsemen?\nFacts: Pandora was a mythical figure that opened a box and released several ills on the world including famine, sickness, and death. The Four Horsemen of the Apocalypse are: Pestilence, War, Famine, and Death.\nAnswer (True or False):", "ct_subject": "Pandora was a mythical figure that opened a box and released several ills on the world including famine, sickness, and death. The Four Horsemen of the Apocalypse are: Pestilence, War, Famine, and Death.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9996978"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "0.99997604"], [" person", "0.599074"], [" to", "1.0"], [" be", "0.99999666"], [" born", "0.6100997"], [" in", "1.0"], [" the", "1.0"], [" United", "0.6305214"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "0.9939277"], [" Adams", "0.99992895"], [" was", "1.0"], [" the", "0.99495715"], [" first", "1.0"], [" person", "1.0"], [" born", "0.45549378"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"]]}, "270": {"qid": "37f33f40f86886a334d8", "term": "Amoeba", "description": "polyphyletic group including different eucariot taxons", "question": "Could amoebas have played a part in the Black Death?", "answer": true, "facts": ["The Black Death is a bacterial disease called bubonic plague.", "Yersinia pestis has been found to grow and thrive inside amoebas.", "Bubonic plague is caused by Yersinia pestis."], "decomposition": ["Which disease is referred to as the Black Death?", "Which specific organism is responsible for #1?", "Could #2 be hosted by amoeba?"], "evidence": [[[["Black Death-1"]], [["Black Death-3"]], [["Amoeba-1"]]], [[["Black Death-1"]], [["Yersinia pestis-1"]], [["Amoeba-10"], "operation"]], [[["Black Death-1"]], [["Black Death-3"]], [["Amoeba-1"], "no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Black Death kill more people than the Spanish Flu?\nFacts: The Black Death killed more people than the Spanish Flu. The Spanish Flu killed more people than the Black Death.\nAnswer (True or False): True\nQuestion: Did the Black Death kill more people than the Spanish Flu", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could amoebas have played a part in the Black Death?\nFacts: The Black Death is a bacterial disease called bubonic plague. Yersinia pestis has been found to grow and thrive inside amoebas. Bubonic plague is caused by Yersinia pestis.\nAnswer (True or False):", "ct_subject": "The Black Death is a bacterial disease called bubonic plague. Yersinia pestis has been found to grow and thrive inside amoebas. Bubonic plague is caused by Yersinia pestis.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9174761"], [" the", "1.0"], [" Black", "0.27120543"], [" Death", "1.0"], [" kill", "0.9999988"], [" more", "0.9999573"], [" people", "1.0"], [" than", "1.0"], [" the", "1.0"], [" Spanish", "0.84649545"], [" Flu", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Black", "0.9992267"], [" Death", "1.0"], [" killed", "1.0"], [" more", "0.99999976"], [" people", "1.0"], [" than", "1.0"], [" the", "1.0"], [" Spanish", "1.0"], [" Flu", "1.0"], [".", "1.0"], [" The", "0.99999833"], [" Spanish", "0.99999976"], [" Flu", "1.0"], [" killed", "0.9999999"], [" more", "0.9921674"], [" people", "1.0"], [" than", "1.0"], [" the", "1.0"], [" Black", "1.0"], [" Death", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99028647"], [" the", "1.0"], [" Black", "1.0"], [" Death", "1.0"], [" kill", "1.0"], [" more", "1.0"], [" people", "1.0"], [" than", "1.0"], [" the", "1.0"], [" Spanish", "1.0"], [" Flu", "1.0"]]}, "271": {"qid": "1337be5d470d68393db7", "term": "The Invisible Man", "description": "1897 science fiction novella by H. G. Wells", "question": " Is The Invisible Man more prevalent in films than Picnic at Hanging Rock?", "answer": true, "facts": ["H.G. Wells's book The Invisible Man has been adapted into more than 7 films.", "Joan Lindsay's book Picnic at Hanging Rock was adapted into one film and one TV series."], "decomposition": ["How many films have been made of \"The Invisible Man\"", "How many films have been made of Picnic at Hanging Rock?", "Is #1 larger than #2?"], "evidence": [[[["Griffin (The Invisible Man)-11"]], [["Picnic at Hanging Rock (novel)-25"]], ["operation"]], [[["The Invisible Man-2"]], [["Picnic at Hanging Rock (film)-11"]], ["operation"]], [[["The Invisible Man-13"], "no_evidence"], [["Picnic at Hanging Rock (film)-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the character in the movie \"The Wizard of Oz\" based on the real-life character of \"The Tin Man\"?\nFacts: The Tin Man was a character in the book of the same name by L. Frank Baum. The Tin Man was a character in the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion:  Is The Invisible Man more prevalent in films than Picnic at Hanging Rock?\nFacts: H.G. Wells's book The Invisible Man has been adapted into more than 7 films. Joan Lindsay's book Picnic at Hanging Rock was adapted into one film and one TV series.\nAnswer (True or False):", "ct_subject": "H.G. Wells's book The Invisible Man has been adapted into more than 7 films. Joan Lindsay's book Picnic at Hanging Rock was adapted into one film and one TV series.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99917054"], [" the", "0.9961935"], [" name", "0.98077446"], [" of", "0.99986625"], [" the", "1.0"], [" character", "0.98351914"], [" in", "1.0"], [" the", "0.9466769"], [" movie", "1.0"], [" \"", "0.9984902"], ["The", "1.0"], [" Wizard", "0.9930387"], [" of", "1.0"], [" Oz", "1.0"], ["\"", "1.0"], [" based", "0.41175073"], [" on", "1.0"], [" the", "0.98960555"], [" real", "0.9999939"], ["-", "0.9998994"], ["life", "1.0"], [" character", "0.99999535"], [" of", "1.0"], [" \"", "0.99880946"], ["The", "1.0"], [" Tin", "0.9999975"], [" Man", "1.0"], ["\"?", "0.99999547"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Tin", "0.9999999"], [" Man", "1.0"], [" was", "0.99964833"], [" a", "0.99999523"], [" character", "0.95816433"], [" in", "1.0"], [" the", "1.0"], [" book", "1.0"], [" of", "0.9991642"], [" the", "1.0"], [" same", "1.0"], [" name", "1.0"], [" by", "0.99936384"], [" L", "1.0"], [".", "1.0"], [" Frank", "1.0"], [" Ba", "1.0"], ["um", "1.0"], [".", "1.0"], [" The", "0.81990683"], [" Tin", "0.9990325"], [" Man", "1.0"], [" was", "1.0"], [" a", "0.9999951"], [" character", "1.0"], [" in", "1.0"], [" the", "1.0"]]}, "272": {"qid": "7a1c9d5ad2a66bf90240", "term": "Disco", "description": "music genre", "question": "Is ABBA's 1970's genre still relevant today?", "answer": true, "facts": ["ABBA was a 1970's music group that specialized in Disco music.", "Pop artist Dua Lipa's 2020 album, Future Nostalgia, was described by Rolling Stone as , \"The Disco Liberation We Need.\"", "Magnetic Magazine released an article in 2020 entitled, \"The Best Disco and Funk Tracks of 2020.\""], "decomposition": ["What genre of music did music group ABBA specialize in in the 1970's?", "Are #1 still relevant today?"], "evidence": [[[["ABBA-1"]], [["Mamma Mia! Here We Go Again-1", "Mamma Mia! Here We Go Again-21"]]], [[["ABBA-1"]], [["Popular music-1"], "operation"]], [[["ABBA-43"]], [["Disco-5"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"C\" and a \"B\"?\nFacts: A \"C\" is a letter, while a \"B\" is a letter.\nAnswer (True or False): True\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is ABBA's 1970's genre still relevant today?\nFacts: ABBA was a 1970's music group that specialized in Disco music. Pop artist Dua Lipa's 2020 album, Future Nostalgia, was described by Rolling Stone as , \"The Disco Liberation We Need.\" Magnetic Magazine released an article in 2020 entitled, \"The Best Disco and Funk Tracks of 2020.\"\nAnswer (True or False):", "ct_subject": "ABBA was a 1970's music group that specialized in Disco music. Pop artist Dua Lipa's 2020 album, Future Nostalgia, was described by Rolling Stone as , \"The Disco Liberation We Need.\" Magnetic Magazine released an article in 2020 entitled, \"The Best Disco and Funk Tracks of 2020.\"", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.97965664"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.0818142"], [" between", "1.0"], [" a", "0.99999917"], [" \"", "1.0"], ["C", "0.9119622"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["B", "0.9998275"], ["\"?", "0.9999871"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.8311473"], [" \"", "1.0"], ["C", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "0.9999987"], [" letter", "0.9789467"], [",", "0.96980727"], [" while", "0.89550334"], [" a", "1.0"], [" \"", "1.0"], ["B", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" letter", "0.9994822"], [".", "0.8310478"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"]]}, "273": {"qid": "a9238d2bba6ea325fb41", "term": "Cactus", "description": "Family of mostly succulent plants, adapted to dry environments", "question": " Is cactus fruit an important menu item for a restaurant based on Cuauht\u00e9moc?", "answer": true, "facts": ["The Aztecs cultivated cacti for the fruit", "Tenochtitlan was the capital of the Aztec empire", "Cuauht\u00e9moc was the last king of Tenochtitlan "], "decomposition": ["Where city was Cuauht\u00e9moc the king of?", "What empire was #1 the capital of?", "Did people in #2 eat cacti?"], "evidence": [[[["Cuauht\u00e9moc-1"]], [["Tenochtitlan-1"]], [["Aztec Empire-8"], "no_evidence"]], [[["Cuauht\u00e9moc-1"]], [["Tenochtitlan-1"]], [["Cactus-61"], "operation"]], [[["Cuauht\u00e9moc-1"]], [["Cuauht\u00e9moc-1"]], [["Aztec cuisine-4"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city of Mexico City?\nFacts: Mexico City was the capital of the Aztec empire. It was named after the Aztec god of war, Huitzilopochtli.\nAnswer (True or False): True\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion:  Is cactus fruit an important menu item for a restaurant based on Cuauht\u00e9moc?\nFacts: The Aztecs cultivated cacti for the fruit Tenochtitlan was the capital of the Aztec empire Cuauht\u00e9moc was the last king of Tenochtitlan \nAnswer (True or False):", "ct_subject": "The Aztecs cultivated cacti for the fruit Tenochtitlan was the capital of the Aztec empire Cuauht\u00e9moc was the last king of Tenochtitlan ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.667745"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Mexico", "0.5123751"], [" City", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Mexico", "1.0"], [" City", "1.0"], [" was", "0.0229134"], [" the", "0.99709415"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Az", "1.0"], ["tec", "1.0"], [" empire", "1.0"], [".", "0.5322716"], [" It", "0.97152305"], [" was", "1.0"], [" named", "0.82312995"], [" after", "1.0"], [" the", "1.0"], [" Az", "1.0"], ["tec", "1.0"], [" god", "1.0"], [" of", "1.0"], [" war", "1.0"], [",", "1.0"], [" Hu", "0.9892202"], ["itz", "1.0"], ["il", "1.0"], ["op", "1.0"], ["och", "1.0"], ["t", "1.0"], ["li", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"]]}, "274": {"qid": "b24d019c3f205bafdea5", "term": "Euphoria", "description": "mental and emotional condition in which a person experiences intense feelings of well-being, elation, happiness and excitement", "question": "Did Rumi spend his time in a state of euphoria?", "answer": true, "facts": ["Euphoria is a state in which people experience intense feelings that overwhelm their body.", "Rumi was a 13th century Persian poet that was also a dervish.", "Dervishes participated in ceremonies in which they experienced religious ecstasy.", "Religious ecstasy is an altered state of consciousness characterized by visions and emotional (and sometimes physical) euphoria."], "decomposition": ["What religious practices did Rumi engage in?", "What emotional experiences are associated with #1?", "Is euphoria among #2?"], "evidence": [[[["Rumi-1", "Sufi whirling-1"]], [["Sufi whirling-5"]], ["operation"]], [[["Rumi-1"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Rumi-1"]], [["Sufism-75"]], [["Ecstasy (emotion)-8"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the first person to use the word \"gay\" in English be a man?\nFacts: The first person to use the word \"gay\" in English was a man", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Rumi spend his time in a state of euphoria?\nFacts: Euphoria is a state in which people experience intense feelings that overwhelm their body. Rumi was a 13th century Persian poet that was also a dervish. Dervishes participated in ceremonies in which they experienced religious ecstasy. Religious ecstasy is an altered state of consciousness characterized by visions and emotional (and sometimes physical) euphoria.\nAnswer (True or False):", "ct_subject": "Euphoria is a state in which people experience intense feelings that overwhelm their body. Rumi was a 13th century Persian poet that was also a dervish. Dervishes participated in ceremonies in which they experienced religious ecstasy. Religious ecstasy is an altered state of consciousness characterized by visions and emotional (and sometimes physical) euphoria.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.98439896"], [" the", "1.0"], [" first", "0.6713917"], [" person", "0.9633765"], [" to", "1.0"], [" use", "0.99385554"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["gay", "0.033362947"], ["\"", "1.0"], [" in", "0.9999995"], [" English", "0.9999981"], [" be", "0.56129426"], [" a", "1.0"], [" man", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "0.45865557"], [" person", "1.0"], [" to", "1.0"], [" use", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["gay", "1.0"], ["\"", "1.0"], [" in", "1.0"], [" English", "1.0"], [" was", "1.0"], [" a", "1.0"], [" man", "1.0"]]}, "275": {"qid": "43c636907c6b92d251d5", "term": "Silicon", "description": "Chemical element with atomic number 14", "question": "Is silicon important in California?", "answer": true, "facts": ["There is a region in California called the Silicon Valley.", "Silicon Valley is home to a large number of technology corporations.", "Silicon Valley was originally named after the large number of corporations there that manufactured silicon-based circuit chips."], "decomposition": ["Which industrial area in California is named after silicon?", "What kind of companies are prevalent in #1?", "What kind of products do #2 make?", "Is silicon an important raw material for #3?"], "evidence": [[[["Silicon Valley-1"]], [["Silicon Valley-38"]], [["Silicon Valley-29"]], [["Silicon Valley-2"]]], [[["Silicon Valley-1"]], [["Silicon Valley-2"]], [["Silicon Valley-2"]], [["Integrated circuit-1", "Transistor-48"]]], [[["Silicon Valley-1"]], [["Silicon Valley-2"]], [["Transistor-48"]], [["Semiconductor-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city of San Francisco?\nFacts: San Francisco is the capital of California. San Francisco is the largest city in the United States.\nAnswer (True or False): True\nQuestion: What is the name of the city of Los Angeles?\nFacts: Los Angeles is the capital of California", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is silicon important in California?\nFacts: There is a region in California called the Silicon Valley. Silicon Valley is home to a large number of technology corporations. Silicon Valley was originally named after the large number of corporations there that manufactured silicon-based circuit chips.\nAnswer (True or False):", "ct_subject": "There is a region in California called the Silicon Valley. Silicon Valley is home to a large number of technology corporations. Silicon Valley was originally named after the large number of corporations there that manufactured silicon-based circuit chips.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.7529249"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999918"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9999685"], [" of", "0.9949981"], [" San", "0.9989478"], [" Francisco", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" San", "1.0"], [" Francisco", "1.0"], [" is", "1.0"], [" the", "0.99999547"], [" capital", "0.16895705"], [" of", "1.0"], [" California", "1.0"], [".", "1.0"], [" San", "0.9824692"], [" Francisco", "1.0"], [" is", "1.0"], [" the", "0.9999993"], [" largest", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "0.99981874"], [" United", "1.0"], [" States", "1.0"], [".", "1.0"], ["\n", "0.9871502"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Los", "0.9999887"], [" Angeles", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Los", "1.0"], [" Angeles", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "0.99998736"], [" of", "1.0"], [" California", "1.0"]]}, "276": {"qid": "f81316ea85357f58284b", "term": "Lord Voldemort", "description": "Fictional character of Harry Potter series", "question": "Would Lord Voldemort have been barred from Hogwarts under his own rules?", "answer": true, "facts": ["Lord Voldemort wanted to rid the wizarding world of half blood wizards.", "Lord Volemort was born a half blood, part muggle part wizard."], "decomposition": ["What kinds of people did Lord Voldemort want to prohibit from Hogwarts?", "What was Lord Voldemort born as?", "Is #1 the same as #2?"], "evidence": [[[["Lord Voldemort-4"]], [["Lord Voldemort-4"]], ["operation"]], [[["Lord Voldemort-2"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Lord Voldemort-2"]], [["Lord Voldemort-33"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the wizarding world's most powerful wizard?\nFacts: The most powerful wizard in the wizarding world is known as the Dark Lord Voldemort.\nAnswer (True or False): True\nQuestion: What is the name of the wizarding world's most powerful wizard?\nFacts: The most powerful wizard in the wizarding world", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Lord Voldemort have been barred from Hogwarts under his own rules?\nFacts: Lord Voldemort wanted to rid the wizarding world of half blood wizards. Lord Volemort was born a half blood, part muggle part wizard.\nAnswer (True or False):", "ct_subject": "Lord Voldemort wanted to rid the wizarding world of half blood wizards. Lord Volemort was born a half blood, part muggle part wizard.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9993436"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999647"], [" of", "1.0"], [" the", "1.0"], [" wizard", "0.86333245"], ["ing", "1.0"], [" world", "0.98601556"], ["'s", "1.0"], [" most", "0.9999999"], [" powerful", "0.848404"], [" wizard", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9162666"], [" most", "1.0"], [" powerful", "1.0"], [" wizard", "1.0"], [" in", "0.9998673"], [" the", "1.0"], [" wizard", "1.0"], ["ing", "1.0"], [" world", "1.0"], [" is", "1.0"], [" known", "0.9988493"], [" as", "1.0"], [" the", "0.99999833"], [" Dark", "1.0"], [" Lord", "1.0"], [" Voldemort", "0.91511214"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" wizard", "1.0"], ["ing", "1.0"], [" world", "1.0"], ["'s", "1.0"], [" most", "1.0"], [" powerful", "1.0"], [" wizard", "0.80604047"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" most", "1.0"], [" powerful", "1.0"], [" wizard", "1.0"], [" in", "1.0"], [" the", "1.0"], [" wizard", "1.0"], ["ing", "1.0"], [" world", "1.0"]]}, "277": {"qid": "be3458c61d6f0deee9d8", "term": "Pancake", "description": "Thin, round cake made of eggs, milk and flour", "question": "Are pancakes a bad snack for cats?", "answer": true, "facts": ["Pancakes contain the dairy product milk as one of the main ingredients.", "After 6 months cats lose the enzyme lactase that breaks down lactose, which makes them lactose intolerant.", "Cats that drink milk can suffer from upset stomach and vomiting."], "decomposition": ["What are the three major ingredients of pancake?", "Which substance do cats lose the ability to break down after six months?", "Does any of #1 contain #2?"], "evidence": [[[["Pancake-1"]], [["Cat-59"]], ["operation"]], [[["Pancake-64"]], [["Cat-59"]], [["Cat-59"], "operation"]], [[["Pancake-1"]], [["Cat-59"]], [["Cat-59"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a cat and a dog?\nFacts: Cats are mammals and dogs are reptiles.\nAnswer (True or False): True\nQuestion: What is the difference between a cat and a dog?\nFacts: Cats are mammals and dogs are reptiles.\nAnswer", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are pancakes a bad snack for cats?\nFacts: Pancakes contain the dairy product milk as one of the main ingredients. After 6 months cats lose the enzyme lactase that breaks down lactose, which makes them lactose intolerant. Cats that drink milk can suffer from upset stomach and vomiting.\nAnswer (True or False):", "ct_subject": "Pancakes contain the dairy product milk as one of the main ingredients. After 6 months cats lose the enzyme lactase that breaks down lactose, which makes them lactose intolerant. Cats that drink milk can suffer from upset stomach and vomiting.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.7388078"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9984767"], [" between", "1.0"], [" a", "1.0"], [" cat", "0.9296074"], [" and", "1.0"], [" a", "1.0"], [" dog", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Cats", "0.9999956"], [" are", "0.9999981"], [" mammals", "1.0"], [" and", "0.030299906"], [" dogs", "1.0"], [" are", "1.0"], [" reptiles", "0.1288742"], [".", "1.0"], ["\n", "0.58931756"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" cat", "1.0"], [" and", "1.0"], [" a", "1.0"], [" dog", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Cats", "1.0"], [" are", "1.0"], [" mammals", "1.0"], [" and", "1.0"], [" dogs", "1.0"], [" are", "1.0"], [" reptiles", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"]]}, "278": {"qid": "913c1014198f2e584dc7", "term": "Ice", "description": "water frozen into the solid state", "question": "Is there a popular Disney character made from living ice?", "answer": true, "facts": ["Olaf is a popular character in Disney's Frozen series.", "Olaf is a snowman, accidentally enchanted to life by Elsa while she magically builds her ice tower."], "decomposition": ["Which popular Disney character did Elsa accidentally enchant to life while building her ice tower?", "Was #1 made of snow/ice?"], "evidence": [[[["Frozen (2013 film)-7"]], [["Snowman-4"], "operation"]], [[["Olaf (Frozen)-1"]], [["Snowman-1"]]], [[["Olaf (Frozen)-5"]], [["Olaf (Frozen)-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is there a popular Disney character made from living ice?\nFacts: Olaf is a popular character in Disney's Frozen series. Olaf is a snowman, accidentally enchanted to life by Elsa while she magically builds her ice tower.\nAnswer (True or False): True\nQuestion: Is there a popular Disney character made from living ice?\nF", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is there a popular Disney character made from living ice?\nFacts: Olaf is a popular character in Disney's Frozen series. Olaf is a snowman, accidentally enchanted to life by Elsa while she magically builds her ice tower.\nAnswer (True or False):", "ct_subject": "Olaf is a popular character in Disney's Frozen series. Olaf is a snowman, accidentally enchanted to life by Elsa while she magically builds her ice tower.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.41211236"], [" there", "1.0"], [" a", "1.0"], [" popular", "1.0"], [" Disney", "1.0"], [" character", "1.0"], [" made", "1.0"], [" from", "1.0"], [" living", "1.0"], [" ice", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Ol", "1.0"], ["af", "1.0"], [" is", "1.0"], [" a", "1.0"], [" popular", "1.0"], [" character", "1.0"], [" in", "1.0"], [" Disney", "1.0"], ["'s", "1.0"], [" Frozen", "1.0"], [" series", "1.0"], [".", "1.0"], [" Ol", "1.0"], ["af", "1.0"], [" is", "1.0"], [" a", "1.0"], [" snow", "1.0"], ["man", "1.0"], [",", "1.0"], [" accidentally", "1.0"], [" enchanted", "1.0"], [" to", "1.0"], [" life", "1.0"], [" by", "1.0"], [" Elsa", "1.0"], [" while", "1.0"], [" she", "1.0"], [" magically", "1.0"], [" builds", "1.0"], [" her", "1.0"], [" ice", "1.0"], [" tower", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" there", "1.0"], [" a", "1.0"], [" popular", "1.0"], [" Disney", "1.0"], [" character", "1.0"], [" made", "1.0"], [" from", "1.0"], [" living", "1.0"], [" ice", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"]]}, "279": {"qid": "674c5dac91798a609ec8", "term": "Monarch", "description": "Person at the head of a monarchy", "question": "Would Hapshetsut be considered a monarch?", "answer": true, "facts": ["A monarch is the head of a monarchy and was appointed for life.", "Hapshetsut was the second known female pharaoh.", "Pharaohs ruled for life and their wealth was even buried with them to take into the afterlife."], "decomposition": ["What kind of leader was Hatshepsut?", "What was the nature of the leadership of a #1?", "What is the nature of a monarch's rule?", "Is #2 very similar to #3?"], "evidence": [[[["Hatshepsut-1"]], [["Pharaoh-1"]], [["Monarch-1"]], [["Monarch-1", "Pharaoh-1"]]], [[["Hatshepsut-1"]], [["Pharaoh-1", "Pharaoh-2"]], [["Monarch-1"]], ["operation"]], [[["Hatshepsut-1"]], [["Pharaoh-5"]], [["Monarch-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a pharaoh and a king?\nFacts: A pharaoh is the head of a monarchy and was appointed for life. A king is the head of a kingdom and is appointed for life.\nAnswer (True or False): True\nQuestion: What is the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Hapshetsut be considered a monarch?\nFacts: A monarch is the head of a monarchy and was appointed for life. Hapshetsut was the second known female pharaoh. Pharaohs ruled for life and their wealth was even buried with them to take into the afterlife.\nAnswer (True or False):", "ct_subject": "A monarch is the head of a monarchy and was appointed for life. Hapshetsut was the second known female pharaoh. Pharaohs ruled for life and their wealth was even buried with them to take into the afterlife.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999154"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9883631"], [" between", "1.0"], [" a", "1.0"], [" ph", "0.087639526"], ["araoh", "1.0"], [" and", "1.0"], [" a", "1.0"], [" king", "0.9999932"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" ph", "1.0"], ["araoh", "1.0"], [" is", "0.27312887"], [" the", "1.0"], [" head", "1.0"], [" of", "1.0"], [" a", "1.0"], [" monarchy", "1.0"], [" and", "1.0"], [" was", "1.0"], [" appointed", "1.0"], [" for", "1.0"], [" life", "1.0"], [".", "1.0"], [" A", "1.0"], [" king", "1.0"], [" is", "1.0"], [" the", "0.9999993"], [" head", "0.7288237"], [" of", "1.0"], [" a", "1.0"], [" kingdom", "0.86781466"], [" and", "1.0"], [" is", "0.9843907"], [" appointed", "1.0"], [" for", "1.0"], [" life", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"]]}, "280": {"qid": "7730e9a15ff9315c24fe", "term": "Spanish\u2013American War", "description": "Conflict in 1898 between Spain and the United States", "question": "Did US President during Spanish-American War suffer similar demise to Abraham Lincoln?", "answer": true, "facts": ["The Spanish-American War lasted from April 21, 1898 to August 13, 1898.", "William McKinley was President of the United States from March 4, 1897 to September 14, 1901.", "William McKinley died from gun related injuries after an assassination attempt.", "Abraham Lincoln died shortly after being shot by John Wilkes Booth."], "decomposition": ["What years were the Spanish\u2013American War?", "Who was the US President during #1?", "How was #2 killed?", "How was Abraham Lincoln killed?", "Is #3 the same as #4?"], "evidence": [[[["Spanish\u2013American War-1"]], [["Spanish\u2013American War-2"]], [["William McKinley-1"]], [["Abraham Lincoln-4"]], ["operation"]], [[["Spanish\u2013American War-1"]], [["Spanish\u2013American War-2"]], [["William McKinley-1"]], [["Maryland in the American Civil War-55"]], ["operation"]], [[["Spanish\u2013American War-1"]], [["William McKinley-1"]], [["Assassination of William McKinley-1"]], [["Assassination of Abraham Lincoln-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the US President during the Spanish-American War suffer similar demise to Abraham Lincoln?\nFacts: The Spanish-American War lasted from April 21, 1898 to August 13, 1898.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did US President during Spanish-American War suffer similar demise to Abraham Lincoln?\nFacts: The Spanish-American War lasted from April 21, 1898 to August 13, 1898. William McKinley was President of the United States from March 4, 1897 to September 14, 1901. William McKinley died from gun related injuries after an assassination attempt. Abraham Lincoln died shortly after being shot by John Wilkes Booth.\nAnswer (True or False):", "ct_subject": "The Spanish-American War lasted from April 21, 1898 to August 13, 1898. William McKinley was President of the United States from March 4, 1897 to September 14, 1901. William McKinley died from gun related injuries after an assassination attempt. Abraham Lincoln died shortly after being shot by John Wilkes Booth.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99950325"], [" the", "1.0"], [" US", "0.8379575"], [" President", "1.0"], [" during", "1.0"], [" the", "0.99915946"], [" Spanish", "1.0"], ["-", "1.0"], ["American", "1.0"], [" War", "1.0"], [" suffer", "1.0"], [" similar", "1.0"], [" demise", "1.0"], [" to", "1.0"], [" Abraham", "1.0"], [" Lincoln", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Spanish", "1.0"], ["-", "1.0"], ["American", "1.0"], [" War", "1.0"], [" lasted", "1.0"], [" from", "1.0"], [" April", "1.0"], [" 21", "1.0"], [",", "1.0"], [" 1898", "1.0"], [" to", "1.0"], [" August", "1.0"], [" 13", "1.0"], [",", "1.0"], [" 1898", "1.0"], [".", "1.0"]]}, "281": {"qid": "63fb8f40ca6c2226dd01", "term": "Dr. Seuss", "description": "American children's writer and illustrator", "question": "Did the death of Helen Palmer have a significant effect on Dr. Seuss?", "answer": true, "facts": ["Dr. Seuss's real name was Theodor Geisel.", "Theodor Geisel was married to Helen Palmer at the time of her suicide.", "Theodor Geisel is quoted having said he considered suicide after the death of his wife."], "decomposition": ["What relatives did Helen Palmer have when she died?", "What is Dr. Suess's real name?", "Is #2 one of #1?"], "evidence": [[[["Helen Palmer (author)-1"]], [["Dr. Seuss-1"]], ["operation"]], [[["Helen Palmer (author)-1", "Helen Palmer (author)-8"], "no_evidence"], [["Dr. Seuss-1"]], ["operation"]], [[["Helen Palmer (author)-1", "Helen Palmer (author)-8"]], [["Helen Palmer (author)-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the death of Helen Palmer have a significant effect on Dr. Seuss?\nFacts: Dr. Seuss's real name was Theodor Geisel. Theodor Geisel was married to Helen Palmer at the time of her suicide. Theodor Geisel", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did the death of Helen Palmer have a significant effect on Dr. Seuss?\nFacts: Dr. Seuss's real name was Theodor Geisel. Theodor Geisel was married to Helen Palmer at the time of her suicide. Theodor Geisel is quoted having said he considered suicide after the death of his wife.\nAnswer (True or False):", "ct_subject": "Dr. Seuss's real name was Theodor Geisel. Theodor Geisel was married to Helen Palmer at the time of her suicide. Theodor Geisel is quoted having said he considered suicide after the death of his wife.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9576369"], [" the", "1.0"], [" death", "1.0"], [" of", "1.0"], [" Helen", "0.30455458"], [" Palmer", "1.0"], [" have", "1.0"], [" a", "1.0"], [" significant", "1.0"], [" effect", "1.0"], [" on", "1.0"], [" Dr", "1.0"], [".", "1.0"], [" Se", "1.0"], ["uss", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Dr", "1.0"], [".", "1.0"], [" Se", "1.0"], ["uss", "1.0"], ["'s", "1.0"], [" real", "1.0"], [" name", "1.0"], [" was", "1.0"], [" The", "1.0"], ["odor", "1.0"], [" Ge", "1.0"], ["isel", "1.0"], [".", "1.0"], [" The", "1.0"], ["odor", "1.0"], [" Ge", "1.0"], ["isel", "1.0"], [" was", "1.0"], [" married", "1.0"], [" to", "1.0"], [" Helen", "1.0"], [" Palmer", "1.0"], [" at", "1.0"], [" the", "1.0"], [" time", "1.0"], [" of", "1.0"], [" her", "1.0"], [" suicide", "1.0"], [".", "1.0"], [" The", "1.0"], ["odor", "1.0"], [" Ge", "1.0"], ["isel", "1.0"]]}, "282": {"qid": "46428dac0ceefa1ad73b", "term": "Eric Clapton", "description": "English musician, singer, songwriter, and guitarist", "question": "Would Eric Clapton's mother hypothetically be unable to legally purchase cigarettes in the USA at his birth?", "answer": true, "facts": ["Eric Clapton's mother was 16 years old at the time of his birth.", "As of 2020, federal law required states comply with a minimum age of 21 years for sale/purchase of tobacco products."], "decomposition": ["How old was Eric Clapton's mom when he was born?", "How old must you be to legally buy cigarettes in the USA?", "Is #2 greater than #1?"], "evidence": [[[["Eric Clapton-4"]], [["Cigarette-19"]], [["Cigarette-19", "Eric Clapton-4"], "operation"]], [[["Eric Clapton-4"]], [["Tobacco 21-16"]], ["operation"]], [[["Eric Clapton-4"]], [["Cigarette-22"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: If you were to die today, would you be buried in a cemetery or cremated?\nFacts: Cremation is the preferred method of burial.\nAnswer (True or False): True\nQuestion: If you were to die today, would you", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Eric Clapton's mother hypothetically be unable to legally purchase cigarettes in the USA at his birth?\nFacts: Eric Clapton's mother was 16 years old at the time of his birth. As of 2020, federal law required states comply with a minimum age of 21 years for sale/purchase of tobacco products.\nAnswer (True or False):", "ct_subject": "Eric Clapton's mother was 16 years old at the time of his birth. As of 2020, federal law required states comply with a minimum age of 21 years for sale/purchase of tobacco products.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" If", "0.6442847"], [" you", "0.9996674"], [" were", "0.9999999"], [" to", "0.99999964"], [" die", "0.21783525"], [" today", "0.9837419"], [",", "1.0"], [" would", "1.0"], [" you", "1.0"], [" be", "1.0"], [" buried", "0.9999999"], [" in", "1.0"], [" a", "0.99997294"], [" cemetery", "0.76884073"], [" or", "0.9998915"], [" crem", "0.98572206"], ["ated", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" C", "0.97733206"], ["rem", "1.0"], ["ation", "1.0"], [" is", "1.0"], [" the", "1.0"], [" preferred", "0.90880525"], [" method", "1.0"], [" of", "1.0"], [" burial", "0.99301237"], [".", "0.8374515"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" If", "0.99996626"], [" you", "1.0"], [" were", "1.0"], [" to", "1.0"], [" die", "1.0"], [" today", "1.0"], [",", "1.0"], [" would", "1.0"], [" you", "1.0"]]}, "283": {"qid": "de287a3a87a5ac5197ff", "term": "Toronto Star", "description": "Newspaper in Toronto, Ontario, Canada", "question": "Can someone sell their time through the Toronto Star?", "answer": true, "facts": ["The Toronto Star has a classifieds section", "Readers can advertise their own labor or services and thus their time "], "decomposition": ["What section of the Toronto Star lists things for sale?", "Can someone's services or labor be sold in #1?"], "evidence": [[[["Toronto Star-31"]], [["Classified advertising-1"]]], [[["Toronto Star-31"]], [["Classified advertising-1"], "operation"]], [[["Toronto Star-31"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the capital of the United States?\nFacts: Washington, D.C.\nAnswer (True or False): Washington, D.C.\nQuestion: What is the name of the city in the United States that is the capital of the United States?\nFacts: Washington, D.C.\nAnswer (True or False): Washington,", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can someone sell their time through the Toronto Star?\nFacts: The Toronto Star has a classifieds section Readers can advertise their own labor or services and thus their time \nAnswer (True or False):", "ct_subject": "The Toronto Star has a classifieds section Readers can advertise their own labor or services and thus their time ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9998586"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9343183"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9999807"], [" in", "0.9978358"], [" the", "0.9999825"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "0.4369899"], [" the", "1.0"], [" capital", "0.9543609"], [" of", "1.0"], [" the", "1.0"], [" United", "0.841823"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Washington", "0.9999924"], [",", "0.98448586"], [" D", "0.99999547"], [".", "1.0"], ["C", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" Washington", "1.0"], [",", "1.0"], [" D", "1.0"], [".", "1.0"], ["C", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Washington", "1.0"], [",", "1.0"], [" D", "1.0"], [".", "1.0"], ["C", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" Washington", "1.0"], [",", "1.0"]]}, "284": {"qid": "131abc946e1ea9ebc87c", "term": "Parachuting", "description": "action sport of exiting an aircraft and returning to Earth using a parachute", "question": "Would Matt Damon be afraid of parachuting?", "answer": true, "facts": ["Parachuting involves jumping from high places or airplanes.", "Matt Damon is afraid of heights. "], "decomposition": ["What is Matt Damon afraid of?", "Does parachuting involve #1?"], "evidence": [[[["Matt Damon-1"], "no_evidence"], [["Parachuting-1"], "no_evidence", "operation"]], [["no_evidence"], ["no_evidence"]], [["no_evidence"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: Thomas Jefferson.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: Thomas Jefferson.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the United States?\nFacts:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Matt Damon be afraid of parachuting?\nFacts: Parachuting involves jumping from high places or airplanes. Matt Damon is afraid of heights. \nAnswer (True or False):", "ct_subject": "Parachuting involves jumping from high places or airplanes. Matt Damon is afraid of heights. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99143046"], [" of", "1.0"], [" the", "1.0"], [" first", "0.99253595"], [" person", "0.99059373"], [" to", "1.0"], [" be", "0.99892247"], [" born", "0.9844801"], [" in", "1.0"], [" the", "1.0"], [" United", "0.9993722"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Thomas", "0.24650145"], [" Jefferson", "1.0"], [".", "0.9999951"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Thomas", "1.0"], [" Jefferson", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"]]}, "285": {"qid": "dae25ebc5462a52b5c17", "term": "Harry Potter and the Philosopher's Stone", "description": "1997 fantasy novel by J. K. Rowling", "question": "Did children read Harry Potter and the Philosopher's Stone during the Albanian Civil War?", "answer": true, "facts": ["Harry Potter and the Philosopher's Stone was a 1997 children's fantasy book.", "The Albanian Civil War, also called the Albanian Civil Unrest, happened in 1997."], "decomposition": ["What year was Harry Potter and the Philosopher's Stone published?", "What year was the Albanian Civil War?", "Did #1 not after #2?"], "evidence": [[[["Harry Potter and the Philosopher's Stone-35"]], [["Albanian Civil War-6"], "no_evidence"], [["Albanian Civil War-6", "Harry Potter and the Philosopher's Stone-35"], "operation"]], [[["Harry Potter and the Philosopher's Stone-2"]], [["Albanian Civil War-1"]], ["operation"]], [[["Harry Potter and the Philosopher's Stone-2"]], [["Albanian Civil War-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Beatles have a song called \"I Want to Hold Your Hand\" in the early days of the Beatles?\nFacts: The Beatles did not have a song called \"I Want to Hold Your Hand\" in the early days of the Beatles. The Beatles did not have a song called \"I Want to", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did children read Harry Potter and the Philosopher's Stone during the Albanian Civil War?\nFacts: Harry Potter and the Philosopher's Stone was a 1997 children's fantasy book. The Albanian Civil War, also called the Albanian Civil Unrest, happened in 1997.\nAnswer (True or False):", "ct_subject": "Harry Potter and the Philosopher's Stone was a 1997 children's fantasy book. The Albanian Civil War, also called the Albanian Civil Unrest, happened in 1997.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999995"], [" the", "1.0"], [" Beatles", "0.98186654"], [" have", "0.038240775"], [" a", "1.0"], [" song", "0.9990023"], [" called", "1.0"], [" \"", "1.0"], ["I", "0.99999714"], [" Want", "1.0"], [" to", "1.0"], [" Hold", "1.0"], [" Your", "1.0"], [" Hand", "1.0"], ["\"", "0.5594254"], [" in", "0.99992096"], [" the", "1.0"], [" early", "0.99999833"], [" days", "0.99963975"], [" of", "1.0"], [" the", "0.99999535"], [" Beatles", "0.99993324"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999999"], [" Beatles", "1.0"], [" did", "0.86899644"], [" not", "1.0"], [" have", "1.0"], [" a", "1.0"], [" song", "1.0"], [" called", "1.0"], [" \"", "1.0"], ["I", "1.0"], [" Want", "1.0"], [" to", "1.0"], [" Hold", "1.0"], [" Your", "1.0"], [" Hand", "1.0"], ["\"", "0.9999999"], [" in", "1.0"], [" the", "1.0"], [" early", "1.0"], [" days", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Beatles", "1.0"], [".", "1.0"], [" The", "0.99867463"], [" Beatles", "0.99999964"], [" did", "0.99999976"], [" not", "0.98970556"], [" have", "1.0"], [" a", "1.0"], [" song", "1.0"], [" called", "1.0"], [" \"", "1.0"], ["I", "1.0"], [" Want", "1.0"], [" to", "1.0"]]}, "286": {"qid": "497501e6d21674257ca5", "term": "Six-Day War", "description": "1967 war between Israel and Egypt, Jordan, and Syria", "question": "Could an NBA game be completed within the span of the Six-Day War?", "answer": true, "facts": ["The Six-Day War took place between June 5th-June 10th, 1967.", "There are 24 hours in a day.", "An NBA game consists of four quarters that are 12 minutes long and a 15 minute long halftime.", "There are 60 minutes in an hour."], "decomposition": ["How long did the Six-day War last?", "How long does a basketball game last?", "Is #1 longer than #2?"], "evidence": [[[["Six-Day War-8"]], [["Rules of basketball-3"], "no_evidence"], ["operation"]], [[["Six-Day War-1"]], [["Basketball-32"]], ["operation"]], [[["Six-Day War-1"]], [["Basketball-32"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance between two points in the world?\nFacts: The longest distance between two points in the world is the 2,890 mile (4,890 km) distance between the two points in the United", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could an NBA game be completed within the span of the Six-Day War?\nFacts: The Six-Day War took place between June 5th-June 10th, 1967. There are 24 hours in a day. An NBA game consists of four quarters that are 12 minutes long and a 15 minute long halftime. There are 60 minutes in an hour.\nAnswer (True or False):", "ct_subject": "The Six-Day War took place between June 5th-June 10th, 1967. There are 24 hours in a day. An NBA game consists of four quarters that are 12 minutes long and a 15 minute long halftime. There are 60 minutes in an hour.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999925"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.99872273"], [" distance", "0.9999925"], [" between", "0.96754074"], [" two", "1.0"], [" points", "1.0"], [" in", "0.9761647"], [" the", "1.0"], [" world", "0.99999964"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" is", "1.0"], [" the", "0.9999982"], [" 2", "0.9999989"], [",", "1.0"], ["8", "0.99578494"], ["90", "0.5943467"], [" mile", "0.999046"], [" (", "0.9999697"], ["4", "1.0"], [",", "1.0"], ["8", "0.9449924"], ["90", "1.0"], [" km", "1.0"], [")", "1.0"], [" distance", "0.64515924"], [" between", "1.0"], [" the", "1.0"], [" two", "1.0"], [" points", "1.0"], [" in", "0.9911883"], [" the", "1.0"], [" United", "0.9998778"]]}, "287": {"qid": "6f4c8a2789c0305c4f63", "term": "Maya Angelou", "description": "American poet, author, and civil rights activist", "question": "Did any of Maya Angelou's children follow in her footsteps?", "answer": true, "facts": ["Maya Angelou was a civil rights activist and author.", "Maya Angelou had a son named Guy Johnson in 1945.", "Guy Johnson is an author that has written over twenty books and essays.", "Guy Johnson's books explore many civil rights themes."], "decomposition": ["What was Maya angelou's profession?", "Who is Maya Angelou's son?", "Did #2 do #1?"], "evidence": [[[["Maya Angelou-1"]], [["Maya Angelou-25"]], [["Maya Angelou-1", "Maya Angelou-25"], "no_evidence"]], [[["Maya Angelou-1"]], [["Maya Angelou-8"]], ["no_evidence", "operation"]], [[["Maya Angelou-1"]], [["Maya Angelou-8"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did any of the Beatles follow in the footsteps of John Lennon?\nFacts: John Lennon was a musician and songwriter. John Lennon had a son named Sean Lennon in 1970. Sean Lennon is an author and musician.\nAnswer (True or False): True\nQuestion:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did any of Maya Angelou's children follow in her footsteps?\nFacts: Maya Angelou was a civil rights activist and author. Maya Angelou had a son named Guy Johnson in 1945. Guy Johnson is an author that has written over twenty books and essays. Guy Johnson's books explore many civil rights themes.\nAnswer (True or False):", "ct_subject": "Maya Angelou was a civil rights activist and author. Maya Angelou had a son named Guy Johnson in 1945. Guy Johnson is an author that has written over twenty books and essays. Guy Johnson's books explore many civil rights themes.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99998844"], [" any", "0.9916905"], [" of", "1.0"], [" the", "1.0"], [" Beatles", "0.99987113"], [" follow", "0.9999752"], [" in", "1.0"], [" the", "0.99999726"], [" footsteps", "1.0"], [" of", "1.0"], [" John", "0.9999893"], [" Lennon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "0.9999956"], [" Lennon", "1.0"], [" was", "0.9999976"], [" a", "1.0"], [" musician", "0.9999968"], [" and", "0.99998593"], [" song", "0.99998724"], ["writer", "1.0"], [".", "1.0"], [" John", "1.0"], [" Lennon", "1.0"], [" had", "0.99996305"], [" a", "1.0"], [" son", "1.0"], [" named", "1.0"], [" Sean", "0.9999993"], [" Lennon", "1.0"], [" in", "1.0"], [" 1970", "0.7131036"], [".", "1.0"], [" Sean", "1.0"], [" Lennon", "1.0"], [" is", "1.0"], [" an", "0.9999919"], [" author", "0.9999796"], [" and", "0.99055976"], [" musician", "0.9930555"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"]]}, "288": {"qid": "f740fed84da8bd24301a", "term": "Sarah", "description": "Biblical character", "question": "Did Methuselah live at least 800 years as long as Sarah?", "answer": true, "facts": ["The biblical Sarah lived to the age of 127.", "The biblical Methuselah lived to 969 years of age."], "decomposition": ["At what age did Methuselah die?", "At what age did Sarah die?", "What is the difference between #1 and #2?", "Is #3 at least 800?"], "evidence": [[[["Methuselah-1"]], [["Sarah-11"]], ["operation"], ["operation"]], [[["Methuselah-1"]], [["Sarah-11"]], ["operation"], ["operation"]], [[["Methuselah-1"]], [["Sarah-11"]], ["operation"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Bible say that the sun goes around the earth?\nFacts: The Bible says that the sun goes around the earth.\nAnswer (True or False): True\nQuestion: Did the Bible say that the earth is flat?\nFacts: The Bible says that the earth is flat.\nAnswer (True or False): True\nQuestion: Did the Bible say that", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Methuselah live at least 800 years as long as Sarah?\nFacts: The biblical Sarah lived to the age of 127. The biblical Methuselah lived to 969 years of age.\nAnswer (True or False):", "ct_subject": "The biblical Sarah lived to the age of 127. The biblical Methuselah lived to 969 years of age.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99999046"], [" the", "0.99999964"], [" Bible", "0.9712352"], [" say", "1.0"], [" that", "1.0"], [" the", "1.0"], [" sun", "0.5174911"], [" goes", "0.99999976"], [" around", "1.0"], [" the", "1.0"], [" earth", "1.0"], ["?", "0.9997515"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Bible", "1.0"], [" says", "0.99294597"], [" that", "1.0"], [" the", "1.0"], [" sun", "1.0"], [" goes", "1.0"], [" around", "1.0"], [" the", "1.0"], [" earth", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" Bible", "1.0"], [" say", "1.0"], [" that", "1.0"], [" the", "1.0"], [" earth", "0.9972121"], [" is", "0.999998"], [" flat", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Bible", "1.0"], [" says", "1.0"], [" that", "1.0"], [" the", "1.0"], [" earth", "1.0"], [" is", "1.0"], [" flat", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" Bible", "1.0"], [" say", "1.0"], [" that", "1.0"]]}, "289": {"qid": "e6d8397b258ac4006210", "term": "Rosemary", "description": "species of plant, rosemary", "question": "Is Rosemary outclassed as plant found in most song titles?", "answer": true, "facts": ["Rosemary appears in a few popular song titles such as Love Grows (Where My Rosemary Goes) and Randy Newman's Rosemary.", "Rose appears in many song titles including: Kiss From a Rose, The Rose, Desert Rose, Beauty of the Rose, and I Never Promised You a Rose Garden."], "decomposition": ["How many songs have \"rosemary\" in the title?", "How many songs have the plant \"rose\" in the title?", "Is #1 fewer than #2?"], "evidence": [[[["Love Grows (Where My Rosemary Goes)-3"], "no_evidence"], [["Blue Rose (song)-4", "Kiss from a Rose-1", "Lady Rose (song)-1", "The Rose (song)-2"], "no_evidence"], ["operation"]], [[["Love Grows (Where My Rosemary Goes)-1", "Rosemary Lane (song)-1"]], [["Desert Rose (Sting song)-1", "Every Rose Has Its Thorn-1", "Kiss from a Rose-1"]], ["operation"]], [[["Rosemary-23"], "no_evidence"], [["Rose-25"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the word \"rose\" a reference to the rose bush?\nFacts: The word \"rose\" is a reference to the rose bush.\nAnswer (True or False): True\nQuestion", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Rosemary outclassed as plant found in most song titles?\nFacts: Rosemary appears in a few popular song titles such as Love Grows (Where My Rosemary Goes) and Randy Newman's Rosemary. Rose appears in many song titles including: Kiss From a Rose, The Rose, Desert Rose, Beauty of the Rose, and I Never Promised You a Rose Garden.\nAnswer (True or False):", "ct_subject": "Rosemary appears in a few popular song titles such as Love Grows (Where My Rosemary Goes) and Randy Newman's Rosemary. Rose appears in many song titles including: Kiss From a Rose, The Rose, Desert Rose, Beauty of the Rose, and I Never Promised You a Rose Garden.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999815"], [" the", "0.99780744"], [" word", "0.9412542"], [" \"", "1.0"], ["rose", "0.99888116"], ["\"", "0.9999969"], [" a", "0.98916"], [" reference", "0.5415703"], [" to", "1.0"], [" the", "1.0"], [" rose", "0.9999902"], [" bush", "0.99991727"], ["?", "0.9991635"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["rose", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "0.9998754"], [" reference", "1.0"], [" to", "1.0"], [" the", "1.0"], [" rose", "1.0"], [" bush", "1.0"], [".", "1.0"], ["\n", "0.99982256"], ["Answer", "0.99999714"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"]]}, "290": {"qid": "e48be839783682257024", "term": "Tony Bennett", "description": "American singer", "question": "Did Tony Bennett have more children than he had wives?", "answer": true, "facts": ["Tony Bennett had four children.", "Tony Bennet has had three wives."], "decomposition": ["How many children has Tony Bennett had?", "How many wives has Tony Bennett had?", "Is #1 greater than #2?"], "evidence": [[[["Tony Bennett-13", "Tony Bennett-27"]], [["Tony Bennett-13", "Tony Bennett-27", "Tony Bennett-43"]], ["operation"]], [[["Tony Bennett-13", "Tony Bennett-27"]], [["Tony Bennett-29", "Tony Bennett-43"]], ["operation"]], [[["Sandra Grant Bennett-2"], "no_evidence"], [["Tony Bennett-27"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Beatles have more children than they had wives?\nFacts: The Beatles had four children. The Beatles had three wives.\nAnswer (True or False): True\nQuestion: Did the Beatles have more children than they had wives?\nFacts: The Beatles had four children. The Beatles had three wives.\nAnswer (True or False): True\nQuestion: Did the Beatles have more children than they had wives?\nFacts:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Tony Bennett have more children than he had wives?\nFacts: Tony Bennett had four children. Tony Bennet has had three wives.\nAnswer (True or False):", "ct_subject": "Tony Bennett had four children. Tony Bennet has had three wives.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" Beatles", "0.99993205"], [" have", "1.0"], [" more", "1.0"], [" children", "1.0"], [" than", "1.0"], [" they", "0.99738854"], [" had", "1.0"], [" wives", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Beatles", "1.0"], [" had", "1.0"], [" four", "0.99842334"], [" children", "1.0"], [".", "1.0"], [" The", "1.0"], [" Beatles", "1.0"], [" had", "0.9960653"], [" three", "0.99998033"], [" wives", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" Beatles", "1.0"], [" have", "1.0"], [" more", "1.0"], [" children", "1.0"], [" than", "1.0"], [" they", "1.0"], [" had", "1.0"], [" wives", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Beatles", "1.0"], [" had", "1.0"], [" four", "1.0"], [" children", "1.0"], [".", "1.0"], [" The", "1.0"], [" Beatles", "1.0"], [" had", "1.0"], [" three", "1.0"], [" wives", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" Beatles", "1.0"], [" have", "1.0"], [" more", "1.0"], [" children", "1.0"], [" than", "1.0"], [" they", "1.0"], [" had", "1.0"], [" wives", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"]]}, "291": {"qid": "ca296dafd5943d07b01c", "term": "Ariana Grande", "description": "American singer, songwriter, and actress", "question": "Was Ariana Grande inspired by Imogen Heap?", "answer": true, "facts": ["Ariana Grande's song 'Goodnight And Go' uses a sample from a track of the same name.", "\"Goodnight and Go\" is originally an Imogen Heap song."], "decomposition": ["Who was the original singer of Ariana Grande's cover 'Goodnight and Go'", "Is #1 Imogen Heap?"], "evidence": [[[["Goodnight and Go-1", "Sweetener (song)-1"]], ["operation"]], [[["Goodnight and Go-1"]], ["operation"]], [[["Goodnight and Go-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Beatles have a secret song?\nFacts: The Beatles had a secret song. The song was called 'A Day In The Life'. The song was written by John Lennon and Paul McCartney.\nAnswer (True or False): True\nQuestion: Did the Beatles have a secret song?\nFacts: The Beatles had a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Ariana Grande inspired by Imogen Heap?\nFacts: Ariana Grande's song 'Goodnight And Go' uses a sample from a track of the same name. \"Goodnight and Go\" is originally an Imogen Heap song.\nAnswer (True or False):", "ct_subject": "Ariana Grande's song 'Goodnight And Go' uses a sample from a track of the same name. \"Goodnight and Go\" is originally an Imogen Heap song.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.96588326"], [" the", "1.0"], [" Beatles", "1.0"], [" have", "0.9890075"], [" a", "1.0"], [" secret", "1.0"], [" song", "0.99999785"], ["?", "0.9999969"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Beatles", "1.0"], [" had", "0.89531106"], [" a", "1.0"], [" secret", "0.9929535"], [" song", "1.0"], [".", "1.0"], [" The", "0.9999993"], [" song", "1.0"], [" was", "1.0"], [" called", "0.9998099"], [" '", "1.0"], ["A", "0.98269075"], [" Day", "1.0"], [" In", "1.0"], [" The", "1.0"], [" Life", "1.0"], ["'.", "0.9923643"], [" The", "0.9192971"], [" song", "1.0"], [" was", "1.0"], [" written", "0.99999976"], [" by", "1.0"], [" John", "0.2923711"], [" Lennon", "1.0"], [" and", "1.0"], [" Paul", "1.0"], [" McCartney", "1.0"], [".", "0.99702233"], ["\n", "0.92879313"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99998474"], [" the", "1.0"], [" Beatles", "1.0"], [" have", "1.0"], [" a", "1.0"], [" secret", "1.0"], [" song", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Beatles", "1.0"], [" had", "1.0"], [" a", "1.0"]]}, "292": {"qid": "517614ab4adf946dbe20", "term": "Eve", "description": "Biblical figure", "question": "Was Eve involved in an incestuous relationship?", "answer": true, "facts": ["God made Eve from a bone he removed from Adam.", "Since Eve was made from Adam, they would have had similar DNA and been considered twins or at least siblings.", "As the only humans at the time, they ended up starting a family together."], "decomposition": ["Who did Eve have intercourse with?", "How was Eve related to #1?", "Can it be concluded that they are family based on #2?"], "evidence": [[[["Incest-18"]], [["Incest-18"]], ["operation"]], [[["Eve-13"]], [["Eve-2"]], ["operation"]], [[["Adam and Eve-2"]], [["Adam and Eve-2"]], [["Adam and Eve-2"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Was Jesus Christ a polygamist?\nFacts: Jesus Christ was married to Mary Magdalene and had a son named Joseph. He was also married to other women.\nAnswer (True or False): True\nQuestion: Was Jesus Christ a homosexual?\nFacts: Jesus", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Eve involved in an incestuous relationship?\nFacts: God made Eve from a bone he removed from Adam. Since Eve was made from Adam, they would have had similar DNA and been considered twins or at least siblings. As the only humans at the time, they ended up starting a family together.\nAnswer (True or False):", "ct_subject": "God made Eve from a bone he removed from Adam. Since Eve was made from Adam, they would have had similar DNA and been considered twins or at least siblings. As the only humans at the time, they ended up starting a family together.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "0.46103954"], [" Jesus", "0.9933055"], [" Christ", "0.99999917"], [" a", "0.6679552"], [" polyg", "0.9545603"], ["am", "1.0"], ["ist", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Jesus", "1.0"], [" Christ", "0.9999989"], [" was", "1.0"], [" married", "1.0"], [" to", "1.0"], [" Mary", "1.0"], [" Mag", "1.0"], ["dal", "1.0"], ["ene", "1.0"], [" and", "0.49671376"], [" had", "0.9999713"], [" a", "0.98543084"], [" son", "1.0"], [" named", "1.0"], [" Joseph", "1.0"], [".", "1.0"], [" He", "0.9722707"], [" was", "0.93037254"], [" also", "1.0"], [" married", "0.9999988"], [" to", "1.0"], [" other", "0.9997913"], [" women", "1.0"], [".", "0.99999857"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "0.9991374"], [" Jesus", "0.99997306"], [" Christ", "1.0"], [" a", "1.0"], [" homosexual", "0.9999999"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Jesus", "1.0"]]}, "293": {"qid": "1331f2020fafcdb794c6", "term": "Thesis", "description": "document submitted in support of candidature for an academic degree", "question": "Would a thesis paper be unusual to assign to kindergartners? ", "answer": true, "facts": ["Kindergartners are usually between 4 and 6 years of age.", "Kindergartners are tasked with learning the alphabet and how to write their own names."], "decomposition": ["What skill set is required to create a thesis paper?", "What skill set do kindergartners possess?", "Are all the skills in #1 also found in #2?"], "evidence": [[[["Thesis-1"], "no_evidence"], [["Kindergarten-89"], "no_evidence"], ["operation"]], [[["Thesis-15"]], [["Kindergarten-29"]], [["Kindergarten-29"], "operation"]], [[["Thesis-17"]], [["Cognitive development-23", "Kindergarten-29"]], [["Cognitive development-23"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the most common type of paper used in a classroom?\nFacts: The most common type of paper used in a classroom is a whiteboard.\nAnswer (True or False): True\nQuestion: What is the most common type of paper used in a classroom?\nFacts: The most common type of paper used in a classroom is a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a thesis paper be unusual to assign to kindergartners? \nFacts: Kindergartners are usually between 4 and 6 years of age. Kindergartners are tasked with learning the alphabet and how to write their own names.\nAnswer (True or False):", "ct_subject": "Kindergartners are usually between 4 and 6 years of age. Kindergartners are tasked with learning the alphabet and how to write their own names.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999999"], [" is", "1.0"], [" the", "1.0"], [" most", "0.72881263"], [" common", "0.9999999"], [" type", "0.99987435"], [" of", "1.0"], [" paper", "1.0"], [" used", "1.0"], [" in", "1.0"], [" a", "0.9371832"], [" classroom", "0.99889296"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.951354"], [" most", "1.0"], [" common", "1.0"], [" type", "1.0"], [" of", "1.0"], [" paper", "1.0"], [" used", "1.0"], [" in", "1.0"], [" a", "1.0"], [" classroom", "1.0"], [" is", "1.0"], [" a", "1.0"], [" white", "1.0"], ["board", "0.9632899"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" most", "1.0"], [" common", "1.0"], [" type", "1.0"], [" of", "1.0"], [" paper", "1.0"], [" used", "1.0"], [" in", "1.0"], [" a", "1.0"], [" classroom", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" most", "1.0"], [" common", "1.0"], [" type", "1.0"], [" of", "1.0"], [" paper", "1.0"], [" used", "1.0"], [" in", "1.0"], [" a", "1.0"], [" classroom", "1.0"], [" is", "1.0"], [" a", "1.0"]]}, "294": {"qid": "133364fbf7e41b0f2b38", "term": "Henry Ford", "description": "American businessperson", "question": "Do people still see Henry Ford's last name often?", "answer": true, "facts": ["Henry Ford is the founder of Ford Motor Company.", "Every Ford vehicle still bears Henry's last name on the brand logo."], "decomposition": ["What company did Henry Ford create?", "What does #1 produce? ", "Does #2 have Henry's name on it?"], "evidence": [[[["Henry Ford-1"]], [["Ford Motor Company-1"]], ["operation"]], [[["Henry Ford Company-1"]], [["Henry Ford Company-1"]], [["Henry Ford Company-1"]]], [[["Henry Ford-14"]], [["Henry Ford-14"]], [["Henry Ford-14"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the capital of the state of Michigan?\nFacts: Detroit is the capital of Michigan.\nAnswer (True or False): True\nQuestion: What is the name of the city in the United States that is the capital of the state of Ohio?\nFacts: Columbus is the capital of Ohio.\nAnswer (True or False", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do people still see Henry Ford's last name often?\nFacts: Henry Ford is the founder of Ford Motor Company. Every Ford vehicle still bears Henry's last name on the brand logo.\nAnswer (True or False):", "ct_subject": "Henry Ford is the founder of Ford Motor Company. Every Ford vehicle still bears Henry's last name on the brand logo.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9981268"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9996855"], [" of", "1.0"], [" the", "1.0"], [" city", "0.95596325"], [" in", "0.95174134"], [" the", "0.9999989"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "0.99976224"], [" the", "1.0"], [" capital", "0.90491205"], [" of", "1.0"], [" the", "1.0"], [" state", "0.9999999"], [" of", "1.0"], [" Michigan", "0.3882141"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Detroit", "0.999933"], [" is", "1.0"], [" the", "1.0"], [" capital", "0.9994173"], [" of", "1.0"], [" Michigan", "0.93014556"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" Ohio", "0.9999924"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Columbus", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" Ohio", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"]]}, "295": {"qid": "7571100f05bc56919c78", "term": "Funeral", "description": "ceremony for a person who has died", "question": "Is it unusual to play Happy hardcore music at a funeral?", "answer": true, "facts": ["Happy hardcore is a music genre of hard dance.", "Happy hardcore emerged both from the UK breakbeat hardcore rave scene, and Belgian, German and Dutch hardcore techno scenes.", "A funeral is traditionally a somber event.", "Funerals typically do not involve dancing.", "Raves are typically energetic and upbeat places and are not somber like a funeral."], "decomposition": ["What type of music is usually played at funerals?", "What are the characteristics of Happy Hardcore music?", "Do any of #1 have the characteristics of #2?"], "evidence": [[[["Funeral-8"]], [["Happy hardcore-1"]], ["operation"]], [[["Dirge-1"]], [["Happy hardcore-1", "Happy hardcore-7"], "no_evidence"], ["no_evidence", "operation"]], [[["Funeral march-1"]], [["Happy hardcore-2"]], [["Funeral march-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it unusual to have a funeral in a church?\nFacts: A funeral is typically held in a church. Funerals are typically held in a church.\nAnswer", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is it unusual to play Happy hardcore music at a funeral?\nFacts: Happy hardcore is a music genre of hard dance. Happy hardcore emerged both from the UK breakbeat hardcore rave scene, and Belgian, German and Dutch hardcore techno scenes. A funeral is traditionally a somber event. Funerals typically do not involve dancing. Raves are typically energetic and upbeat places and are not somber like a funeral.\nAnswer (True or False):", "ct_subject": "Happy hardcore is a music genre of hard dance. Happy hardcore emerged both from the UK breakbeat hardcore rave scene, and Belgian, German and Dutch hardcore techno scenes. A funeral is traditionally a somber event. Funerals typically do not involve dancing. Raves are typically energetic and upbeat places and are not somber like a funeral.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9959621"], [" it", "1.0"], [" unusual", "1.0"], [" to", "1.0"], [" have", "1.0"], [" a", "1.0"], [" funeral", "0.8789838"], [" in", "0.9999939"], [" a", "0.99991703"], [" church", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9815028"], [" funeral", "0.99967146"], [" is", "1.0"], [" typically", "0.9963644"], [" held", "0.97559106"], [" in", "1.0"], [" a", "1.0"], [" church", "1.0"], [".", "1.0"], [" Fun", "0.9999999"], ["er", "1.0"], ["als", "1.0"], [" are", "1.0"], [" typically", "0.9999999"], [" held", "1.0"], [" in", "1.0"], [" a", "1.0"], [" church", "1.0"], [".", "1.0"], ["\n", "0.40934992"], ["Answer", "1.0"]]}, "296": {"qid": "fcc9fc36ed71bd5d5723", "term": "Lil Wayne", "description": "American rapper, record executive and actor from Louisiana", "question": "Lil Wayne similar real name rapper has over quadruple Wayne's Grammy awards?", "answer": true, "facts": ["Lil Wayne was born Dwayne Michael Carter.", "Jay-Z was born Shawn Corey Carter.", "Lil Wayne has won 5 Grammy awards.", "Jay-Z has won 22 Grammy awards."], "decomposition": ["What is Lil Wayne's real name?", "What rapper has a real name that is similar to #1?", "How many Grammy awards does Lil Wayne have?", "How many Grammy awards does #2 have?", "Is #4 divided by #3 greater than 4?"], "evidence": [[[["Lil Wayne-1"]], [["Jay-Z-1"]], [["Lil Wayne-4"]], [["Jay-Z-4"]], ["operation"]], [[["Lil Wayne-1"]], [["Jay-Z-1"]], [["Lil Wayne-4"]], [["Jay-Z-4"]], ["operation"]], [[["Lil Wayne-1"]], [["Jay-Z-1"]], [["Lil Wayne-4"]], [["Jay-Z-4"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the year 2000?\nFacts: The year 2000 was the year of the birth of the first person to be born in the year 2000.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the year", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Lil Wayne similar real name rapper has over quadruple Wayne's Grammy awards?\nFacts: Lil Wayne was born Dwayne Michael Carter. Jay-Z was born Shawn Corey Carter. Lil Wayne has won 5 Grammy awards. Jay-Z has won 22 Grammy awards.\nAnswer (True or False):", "ct_subject": "Lil Wayne was born Dwayne Michael Carter. Jay-Z was born Shawn Corey Carter. Lil Wayne has won 5 Grammy awards. Jay-Z has won 22 Grammy awards.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9994547"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9860977"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9998135"], [" person", "0.6684529"], [" to", "1.0"], [" be", "0.99625754"], [" born", "0.99997747"], [" in", "1.0"], [" the", "1.0"], [" year", "0.9954852"], [" 2000", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" year", "0.8949718"], [" 2000", "1.0"], [" was", "0.9999999"], [" the", "1.0"], [" year", "0.95756453"], [" of", "0.9999151"], [" the", "1.0"], [" birth", "0.999204"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"]]}, "297": {"qid": "5562e0c2c63d85105bab", "term": "Dr. Seuss", "description": "American children's writer and illustrator", "question": "Was Dr. Seuss a liar?", "answer": true, "facts": ["Dr. Seuss was a writer and illustrator of children's books", "Dr. Seuss first published a children's book under the name of Dr. Seuss in 1937", "Dr. Seuss did not actually have a doctorate or equivalent degree until 1956"], "decomposition": ["When did Dr. Seuss first use the title \"Dr.\"?", "When did he get his doctorate (or equivalent)?", "Is #1 before #2?"], "evidence": [[[["Dr. Seuss-2"]], [["Dr. Seuss-18"]], ["operation"]], [[["Dr. Seuss-8"]], [["Dr. Seuss-2"]], ["operation"]], [[["Dr. Seuss-2"]], [["Dr. Seuss-18"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: The first person to be born in the United States was a woman named Sarah Hensley. She was born on January 1, 1776.\nAnswer (True or False): True\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Dr. Seuss a liar?\nFacts: Dr. Seuss was a writer and illustrator of children's books Dr. Seuss first published a children's book under the name of Dr. Seuss in 1937 Dr. Seuss did not actually have a doctorate or equivalent degree until 1956\nAnswer (True or False):", "ct_subject": "Dr. Seuss was a writer and illustrator of children's books Dr. Seuss first published a children's book under the name of Dr. Seuss in 1937 Dr. Seuss did not actually have a doctorate or equivalent degree until 1956", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.63898855"], [" is", "1.0"], [" the", "1.0"], [" name", "0.83071136"], [" of", "1.0"], [" the", "1.0"], [" first", "0.98011786"], [" person", "0.33546075"], [" to", "1.0"], [" be", "0.9988072"], [" born", "0.9999219"], [" in", "1.0"], [" the", "0.88371986"], [" United", "0.9999995"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.8253471"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" a", "0.801811"], [" woman", "1.0"], [" named", "1.0"], [" Sarah", "0.97558093"], [" H", "0.9991677"], ["ens", "1.0"], ["ley", "0.9994266"], [".", "0.9996555"], [" She", "0.9990345"], [" was", "1.0"], [" born", "1.0"], [" on", "0.98238647"], [" January", "0.9359211"], [" 1", "0.79226416"], [",", "1.0"], [" 17", "1.0"], ["76", "0.3099505"], [".", "0.99802667"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"]]}, "298": {"qid": "fba624d8c01833419760", "term": "August", "description": "eighth month in the Julian and Gregorian calendars", "question": "Is August a winter month for part of the world?", "answer": true, "facts": ["August is a summer month in the northern hemisphere.", "However, the seasons are opposite south of the Equator.", "August is in the middle of winter for Australia, Antarctica, and parts of Africa and South America."], "decomposition": ["What season is August a part of in the northern hemisphere?", "Does #1 correspond to winter south of the Equator?"], "evidence": [[[["Summer-2"]], [["Summer-1"]]], [[["Summer-1"]], [["Summer-1"]]], [[["August-3"]], [["Winter-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the month of August the same as the name of the month of January?\nFacts: The name of the month of August is the same as the name of the month of January.\nAnswer (True or False): True\nQuestion: Is the name of the month of August the same", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is August a winter month for part of the world?\nFacts: August is a summer month in the northern hemisphere. However, the seasons are opposite south of the Equator. August is in the middle of winter for Australia, Antarctica, and parts of Africa and South America.\nAnswer (True or False):", "ct_subject": "August is a summer month in the northern hemisphere. However, the seasons are opposite south of the Equator. August is in the middle of winter for Australia, Antarctica, and parts of Africa and South America.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.98753905"], [" the", "0.9999999"], [" name", "0.9976972"], [" of", "1.0"], [" the", "1.0"], [" month", "0.9995751"], [" of", "1.0"], [" August", "1.0"], [" the", "0.97950554"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" name", "0.9999993"], [" of", "1.0"], [" the", "1.0"], [" month", "1.0"], [" of", "1.0"], [" January", "0.89003843"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" month", "1.0"], [" of", "1.0"], [" August", "1.0"], [" is", "1.0"], [" the", "0.9999999"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" month", "1.0"], [" of", "1.0"], [" January", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99999666"], [" the", "1.0"], [" name", "0.9897674"], [" of", "1.0"], [" the", "1.0"], [" month", "1.0"], [" of", "1.0"], [" August", "1.0"], [" the", "1.0"], [" same", "1.0"]]}, "299": {"qid": "13043933149b7952472f", "term": "Saddam Hussein", "description": "Iraqi politician and President", "question": "Would Saddam Hussein hypothetically choose Saladin as ally over Idris I?", "answer": true, "facts": ["Saddam Hussein, President of Iraq, was a Sunni Muslim that brutalized many Shiite Muslims.", "Saladin was the first Sultan of Egypt and was a Sunni Muslim.", "Idris I was called the founder of Morocco and was a Shiite Muslim."], "decomposition": ["Which denomination of Islam did Saddam Hussein identify with?", "Which Islamic denomination did Saladin belong to?", "Which Islamic denomination did Idris I belong to?", "Does #1 match #2 and contrast with #3?"], "evidence": [[[["Saddam Hussein-2"]], [["Saladin-1"]], [["Idris I of Morocco-1"], "no_evidence"], ["no_evidence"]], [[["Saddam Hussein-1", "Saddam Hussein-54"]], [["Saladin-1"]], [["Idris I of Morocco-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Saddam Hussein-2"]], [["Saladin-1"]], [["Zaidiyyah-1", "Zaidiyyah-22"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Would the United States have invaded Iraq if Saddam Hussein had not been in power?\nFacts: The United States invaded Iraq because Saddam Hussein was a threat to the United States. Saddam Hussein was a threat to the United States because he was a threat to the United States", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Saddam Hussein hypothetically choose Saladin as ally over Idris I?\nFacts: Saddam Hussein, President of Iraq, was a Sunni Muslim that brutalized many Shiite Muslims. Saladin was the first Sultan of Egypt and was a Sunni Muslim. Idris I was called the founder of Morocco and was a Shiite Muslim.\nAnswer (True or False):", "ct_subject": "Saddam Hussein, President of Iraq, was a Sunni Muslim that brutalized many Shiite Muslims. Saladin was the first Sultan of Egypt and was a Sunni Muslim. Idris I was called the founder of Morocco and was a Shiite Muslim.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Would", "0.95078516"], [" the", "0.99183065"], [" United", "0.99999785"], [" States", "1.0"], [" have", "0.99999964"], [" invaded", "0.9999995"], [" Iraq", "1.0"], [" if", "1.0"], [" Saddam", "1.0"], [" Hussein", "1.0"], [" had", "1.0"], [" not", "0.9998037"], [" been", "1.0"], [" in", "0.9999863"], [" power", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999999"], [" United", "1.0"], [" States", "1.0"], [" invaded", "0.9988078"], [" Iraq", "1.0"], [" because", "0.9501196"], [" Saddam", "1.0"], [" Hussein", "1.0"], [" was", "0.999918"], [" a", "0.99916553"], [" threat", "1.0"], [" to", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [".", "0.99999976"], [" Saddam", "0.99980587"], [" Hussein", "1.0"], [" was", "1.0"], [" a", "1.0"], [" threat", "0.9990274"], [" to", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" because", "1.0"], [" he", "1.0"], [" was", "1.0"], [" a", "1.0"], [" threat", "1.0"], [" to", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"]]}, "300": {"qid": "007e8eb724fccca37c36", "term": "Apollo", "description": "God in Greek mythology", "question": "Do Apollo and Baldur share similar interests?", "answer": true, "facts": ["Apollo is a Greek god of light.", "Baldur is a Norse god of light.", "They are both interested in light."], "decomposition": ["Apollo is the Greek god of what object?", "What is Baldur the Norse god of?", "Is the item in #2 the same as #1?"], "evidence": [[[["Apollo-1"]], ["no_evidence"], ["operation"]], [[["Apollo-1"]], [["Baldr-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Apollo-1"]], [["Baldr-7"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city of Rome?\nFacts: Rome is the capital of the Roman Empire.\nAnswer (True or False): True\nQuestion: What is the name of the city of Rome?\nFacts: Rome is the capital of the Roman Empire.\nAnswer (True or False): True\nQuestion: What is the name of the city of Rome?\nFacts: Rome", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do Apollo and Baldur share similar interests?\nFacts: Apollo is a Greek god of light. Baldur is a Norse god of light. They are both interested in light.\nAnswer (True or False):", "ct_subject": "Apollo is a Greek god of light. Baldur is a Norse god of light. They are both interested in light.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99939156"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "0.12267854"], [" of", "0.99992335"], [" Rome", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "0.9999999"], [" is", "0.99999225"], [" the", "0.9985904"], [" capital", "0.2212271"], [" of", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [".", "1.0"], ["\n", "0.9999012"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "1.0"]]}, "301": {"qid": "3157a0f428531ebed7b9", "term": "Minor League Baseball", "description": "hierarchy of professional baseball leagues affiliated with Major League Baseball", "question": "Are any minor league baseball teams named after felines?", "answer": true, "facts": ["Felines include cats, tigers, and lions.", "The Sacramento River Cats are a minor league baseball affiliate of the San Francisco Giants.", "The Tri-City Valley Cats are a minor league baseball affiliate of the Houston Astros.", "The Lakeland Flying Tigers are a minor league baseball affiliate of the Detroit Tigers."], "decomposition": ["What are the names of teams in Minor League Baseball?", "Which animals are regarded as felines?", "Does any of #1 include any of #2?"], "evidence": [[[["Lynchburg Hillcats-1"], "no_evidence"], [["Felinae-1"]], ["operation"]], [[["Minor League Baseball-1"], "no_evidence"], [["Felidae-1"]], [["New Hampshire Fisher Cats-1"], "operation"]], [[["New Hampshire Fisher Cats-1", "Sacramento River Cats-1"], "no_evidence"], [["Felidae-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any major league baseball teams named after animals?\nFacts: Yes. The New York Yankees are named after the Greek god Zeus. The Chicago Cubs are named after the Greek god of the sky, the sun, and", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are any minor league baseball teams named after felines?\nFacts: Felines include cats, tigers, and lions. The Sacramento River Cats are a minor league baseball affiliate of the San Francisco Giants. The Tri-City Valley Cats are a minor league baseball affiliate of the Houston Astros. The Lakeland Flying Tigers are a minor league baseball affiliate of the Detroit Tigers.\nAnswer (True or False):", "ct_subject": "Felines include cats, tigers, and lions. The Sacramento River Cats are a minor league baseball affiliate of the San Francisco Giants. The Tri-City Valley Cats are a minor league baseball affiliate of the Houston Astros. The Lakeland Flying Tigers are a minor league baseball affiliate of the Detroit Tigers.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.99381155"], [" there", "1.0"], [" any", "1.0"], [" major", "0.998235"], [" league", "1.0"], [" baseball", "1.0"], [" teams", "0.9999999"], [" named", "1.0"], [" after", "1.0"], [" animals", "0.7758326"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Yes", "0.99998116"], [".", "0.9999932"], [" The", "1.0"], [" New", "0.9999976"], [" York", "1.0"], [" Yankees", "0.99999714"], [" are", "0.9999964"], [" named", "0.999913"], [" after", "1.0"], [" the", "0.9999865"], [" Greek", "0.91331136"], [" god", "1.0"], [" Zeus", "1.0"], [".", "1.0"], [" The", "1.0"], [" Chicago", "0.92747766"], [" Cubs", "0.9999999"], [" are", "1.0"], [" named", "1.0"], [" after", "1.0"], [" the", "1.0"], [" Greek", "1.0"], [" god", "1.0"], [" of", "0.8900974"], [" the", "0.99989593"], [" sky", "1.0"], [",", "1.0"], [" the", "0.6816172"], [" sun", "0.99993074"], [",", "1.0"], [" and", "1.0"]]}, "302": {"qid": "49321f6bd311e10c2634", "term": "Rupert Murdoch", "description": "Australian-born American media mogul", "question": "Would Dante Alighieri hypothetically place Rupert Murdoch in 8th Circle of Hell?", "answer": true, "facts": ["Dante Alighieri was an Italian poet that wrote Inferno.", "Inferno depicts several layers of Hell.", "The 8th Circle of Hell is reserved for liars, bribers, flatterers, and false prophets.", "Rupert Murdoch was involved in a News International scandal in which police were bribed and phones were hacked.", "Rupert Murdoch refused to take any responsibility for the actions of his employees in the News International scandal."], "decomposition": ["Which of Dante Alighieri's works describes hell?", "According to #1, which sins would cause one to be placed in the 8th circle of hell?", "Has Rupert Murdoch come under allegations of any of #2?"], "evidence": [[[["Divine Comedy-2"]], [["Malebolge-2"]], [["Rupert Murdoch-31"], "no_evidence"]], [[["Inferno (Dante)-1"]], [["Inferno (Dante)-57"]], [["News International phone hacking scandal-1"], "no_evidence", "operation"]], [[["Inferno (Dante)-45"], "operation"], ["no_evidence"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city of Rome?\nFacts: Rome is the capital", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Dante Alighieri hypothetically place Rupert Murdoch in 8th Circle of Hell?\nFacts: Dante Alighieri was an Italian poet that wrote Inferno. Inferno depicts several layers of Hell. The 8th Circle of Hell is reserved for liars, bribers, flatterers, and false prophets. Rupert Murdoch was involved in a News International scandal in which police were bribed and phones were hacked. Rupert Murdoch refused to take any responsibility for the actions of his employees in the News International scandal.\nAnswer (True or False):", "ct_subject": "Dante Alighieri was an Italian poet that wrote Inferno. Inferno depicts several layers of Hell. The 8th Circle of Hell is reserved for liars, bribers, flatterers, and false prophets. Rupert Murdoch was involved in a News International scandal in which police were bribed and phones were hacked. Rupert Murdoch refused to take any responsibility for the actions of his employees in the News International scandal.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99997365"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9998416"], [" of", "1.0"], [" the", "1.0"], [" city", "0.96802014"], [" of", "0.9896374"], [" Rome", "0.99999535"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"]]}, "303": {"qid": "4e1b65e81ec09397b26e", "term": "Giant squid", "description": "Deep-ocean dwelling squid in the family Architeuthidae", "question": "Is capturing giant squid in natural habitat impossible with no gear?", "answer": true, "facts": ["Giant squids live between 1,000 and 3,800 feet in the ocean.", "With a dry dive suit, a scuba tank, gloves, and so on, divers can reach depths of around 1000 feet.", "Without scuba gear people can safely dive a maximum of 60 feet without feeling the negative risks associated with diving beyond the limit."], "decomposition": ["At what depths do giant squid live?", "What is the max depth a person can safely dive without gear?", "Is #2 less than #1?"], "evidence": [[[["Giant squid-5"]], [["Freediving-47"]], ["operation"]], [[["Giant squid-5"]], [["Scuba diving-65"]], ["operation"]], [[["Giant squid-5"]], [["Underwater diving-5"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it possible to get a tattoo on your body that will last forever?\nFacts: Tattoos are permanent. The ink is made of a substance called pigments. Pig", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is capturing giant squid in natural habitat impossible with no gear?\nFacts: Giant squids live between 1,000 and 3,800 feet in the ocean. With a dry dive suit, a scuba tank, gloves, and so on, divers can reach depths of around 1000 feet. Without scuba gear people can safely dive a maximum of 60 feet without feeling the negative risks associated with diving beyond the limit.\nAnswer (True or False):", "ct_subject": "Giant squids live between 1,000 and 3,800 feet in the ocean. With a dry dive suit, a scuba tank, gloves, and so on, divers can reach depths of around 1000 feet. Without scuba gear people can safely dive a maximum of 60 feet without feeling the negative risks associated with diving beyond the limit.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99971014"], [" it", "0.99930215"], [" possible", "0.9999989"], [" to", "1.0"], [" get", "0.48602277"], [" a", "1.0"], [" tattoo", "1.0"], [" on", "0.9781066"], [" your", "1.0"], [" body", "0.944098"], [" that", "0.99998975"], [" will", "0.6030648"], [" last", "0.9998536"], [" forever", "0.9999908"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Tatt", "1.0"], ["oos", "1.0"], [" are", "1.0"], [" permanent", "1.0"], [".", "0.9999999"], [" The", "0.78009444"], [" ink", "1.0"], [" is", "0.9939202"], [" made", "0.94652766"], [" of", "0.9990889"], [" a", "0.9422366"], [" substance", "0.99999964"], [" called", "0.99990535"], [" pig", "0.9945332"], ["ments", "1.0"], [".", "0.99999785"], [" Pig", "0.99868196"]]}, "304": {"qid": "ce2336a5272765f263c4", "term": "Harry Potter and the Philosopher's Stone", "description": "1997 fantasy novel by J. K. Rowling", "question": "Would characters in Harry Potter and the Philosopher's Stone be persecuted as pagans?", "answer": true, "facts": ["Pagans are defined as people that hold beliefs other than those of the major world religions (Christianity, Islam, and Judaism).", "The characters in Harry Potter and the Philosopher's Stone practice magic.", "Islam explicitly forbid the practice of magic and has harsh consequences for it.", "Jezebel in Hebrew scripture was a worshiper of pagan Baal and was thrown from a window for her beliefs.", "Women accused of being witches were burned alive by Christians during the Salem Witch Trials."], "decomposition": ["What are the characters in Harry Potter and the Philosopher's Stone known to perform?", "What would performers of #1 be considered by Christians?", "What have Christians done to #2 in the past?", "Are #2 pagans and #3 a form of persecution?"], "evidence": [[[["Harry Potter and the Philosopher's Stone-1"]], [["Witchcraft-1"]], [["Witchcraft-5"]], [["Persecution-1"]]], [[["Harry Potter and the Philosopher's Stone-1"]], [["Magic (supernatural)-2", "Paganism-24"]], [["Witch trials in the early modern period-1"], "no_evidence"], ["operation"]], [[["Harry Potter-1"]], [["Witchcraft-2"]], [["Death by burning-18"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Would the characters in Harry Potter", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would characters in Harry Potter and the Philosopher's Stone be persecuted as pagans?\nFacts: Pagans are defined as people that hold beliefs other than those of the major world religions (Christianity, Islam, and Judaism). The characters in Harry Potter and the Philosopher's Stone practice magic. Islam explicitly forbid the practice of magic and has harsh consequences for it. Jezebel in Hebrew scripture was a worshiper of pagan Baal and was thrown from a window for her beliefs. Women accused of being witches were burned alive by Christians during the Salem Witch Trials.\nAnswer (True or False):", "ct_subject": "Pagans are defined as people that hold beliefs other than those of the major world religions (Christianity, Islam, and Judaism). The characters in Harry Potter and the Philosopher's Stone practice magic. Islam explicitly forbid the practice of magic and has harsh consequences for it. Jezebel in Hebrew scripture was a worshiper of pagan Baal and was thrown from a window for her beliefs. Women accused of being witches were burned alive by Christians during the Salem Witch Trials.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Would", "0.5797753"], [" the", "0.9999995"], [" characters", "0.9999988"], [" in", "1.0"], [" Harry", "1.0"], [" Potter", "1.0"]]}, "305": {"qid": "dfaf734cc6f1016cbb0c", "term": "Aretha Franklin", "description": "American singer, songwriter, and pianist", "question": "Has Aretha Franklin ever collaborated with a suicidal person?", "answer": true, "facts": ["Donny Hathaway was a singer and session musician that worked with Staple Singers, Jerry Butler, Aretha Franklin, the Impressions  and Curtis Mayfield.", "Donny Hathaway jumped from his 15th floor room and his death was ruled a suicide."], "decomposition": ["What music artists has Aretha Franklin done collaborations with?", "Did any of the artists listed in #1 commit suicide?"], "evidence": [[[["Donny Hathaway-4"]], [["Donny Hathaway-17"]]], [[["Donny Hathaway-1"], "no_evidence"], [["Donny Hathaway-16"], "operation"]], [[["Donny Hathaway-4"], "no_evidence"], [["Donny Hathaway-17"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Beatles ever have a song called \"I Want to Hold Your Hand\"?\nFacts: The Beatles did not write the song. The song was written by the British band The Kinks.\nAnswer (True or False): True\nQuestion: Did", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Has Aretha Franklin ever collaborated with a suicidal person?\nFacts: Donny Hathaway was a singer and session musician that worked with Staple Singers, Jerry Butler, Aretha Franklin, the Impressions and Curtis Mayfield. Donny Hathaway jumped from his 15th floor room and his death was ruled a suicide.\nAnswer (True or False):", "ct_subject": "Donny Hathaway was a singer and session musician that worked with Staple Singers, Jerry Butler, Aretha Franklin, the Impressions and Curtis Mayfield. Donny Hathaway jumped from his 15th floor room and his death was ruled a suicide.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.68078065"], [" the", "1.0"], [" Beatles", "0.99999726"], [" ever", "0.3287246"], [" have", "0.99999976"], [" a", "1.0"], [" song", "0.9999999"], [" called", "1.0"], [" \"", "1.0"], ["I", "1.0"], [" Want", "0.9999498"], [" to", "1.0"], [" Hold", "1.0"], [" Your", "1.0"], [" Hand", "1.0"], ["\"?", "0.99999964"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Beatles", "1.0"], [" did", "0.6907846"], [" not", "0.9994413"], [" write", "0.9999995"], [" the", "0.9802366"], [" song", "1.0"], [".", "0.99973625"], [" The", "0.3578129"], [" song", "1.0"], [" was", "1.0"], [" written", "1.0"], [" by", "1.0"], [" the", "0.9938803"], [" British", "0.99804866"], [" band", "0.9999994"], [" The", "0.9998678"], [" K", "1.0"], ["inks", "1.0"], [".", "1.0"], ["\n", "0.8953584"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999999"]]}, "306": {"qid": "06c7a2b41766c56b48f7", "term": "Nickel", "description": "Chemical element with atomic number 28", "question": "Would nickel boil in the outer core of the earth?", "answer": true, "facts": ["The boiling point of nickel is 3003 Kelvin", "The temperature of earth's outer core is 3,000\u20134,500 Kelvin"], "decomposition": ["What is the boiling point of nickel?", "What the temperature range of the earth's outer core?", "Is #1 within #2?"], "evidence": [[[["Nickel-5"], "no_evidence"], [["Nickel-4"], "no_evidence"], ["no_evidence"]], [["no_evidence"], [["Earth's outer core-3"]], ["operation"]], [["no_evidence"], [["Earth's outer core-3"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average height of a man in the United States?\nFacts: The average height of a man in the United States is 5'10\"\nAnswer (True or False): True\nQuestion: What is the average weight of a man in the United States?\nFacts: The average weight of a man in the United States is about 180 pounds\nAnswer (True or False", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would nickel boil in the outer core of the earth?\nFacts: The boiling point of nickel is 3003 Kelvin The temperature of earth's outer core is 3,000\u20134,500 Kelvin\nAnswer (True or False):", "ct_subject": "The boiling point of nickel is 3003 Kelvin The temperature of earth's outer core is 3,000\u20134,500 Kelvin", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999763"], [" is", "1.0"], [" the", "1.0"], [" average", "0.8985007"], [" height", "0.0013923399"], [" of", "1.0"], [" a", "0.99999404"], [" man", "0.99999774"], [" in", "0.99999225"], [" the", "1.0"], [" United", "0.9999995"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" height", "1.0"], [" of", "1.0"], [" a", "1.0"], [" man", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "1.0"], [" 5", "1.0"], ["'", "0.99999"], ["10", "0.8184126"], ["\"", "1.0"], ["\n", "0.99999833"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "1.0"], [" of", "1.0"], [" a", "1.0"], [" man", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" weight", "1.0"], [" of", "1.0"], [" a", "1.0"], [" man", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "1.0"], [" about", "0.99976975"], [" 180", "0.99921346"], [" pounds", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"]]}, "307": {"qid": "63830fb94ef200092420", "term": "Mental disorder", "description": "Distressing thought or behavior pattern", "question": "Are there mental disorders you can hide?", "answer": true, "facts": ["Many people do not notice depression in their friends or loved ones. ", "\"Masking\" is a phrase used to describe concealing the effects of one's personality, including mental disorder."], "decomposition": ["Do any mental disorders have symptoms/effects that can be hidden?"], "evidence": [[[["Mental disorder-39", "Mental disorder-4"], "no_evidence", "operation"]], [[["Major depressive disorder-2"]]], [[["Mental disorder-6"], "no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any diseases you can't get rid of?\nFacts: There are many diseases that can be cured. \u00a0Some of the most common are:\n- Cancer\n- Diabetes\n- Heart Disease\n- Stroke\n- Alzheimer's Disease\n- Parkinson's Disease\n- Multiple Sclerosis\n- Hepatitis C\n- HIV", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are there mental disorders you can hide?\nFacts: Many people do not notice depression in their friends or loved ones.  \"Masking\" is a phrase used to describe concealing the effects of one's personality, including mental disorder.\nAnswer (True or False):", "ct_subject": "Many people do not notice depression in their friends or loved ones.  \"Masking\" is a phrase used to describe concealing the effects of one's personality, including mental disorder.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.8718756"], [" there", "0.99999845"], [" any", "0.99991024"], [" diseases", "0.99997985"], [" you", "0.81113666"], [" can", "1.0"], ["'t", "0.99048716"], [" get", "1.0"], [" rid", "1.0"], [" of", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "0.9999993"], [" are", "1.0"], [" many", "1.0"], [" diseases", "1.0"], [" that", "1.0"], [" can", "0.99988604"], [" be", "1.0"], [" cured", "0.9999999"], [".", "0.99999774"], [" ", "1.0"], ["\u00a0", "0.9999957"], ["Some", "0.98767686"], [" of", "0.99993575"], [" the", "0.9987746"], [" most", "0.9945956"], [" common", "1.0"], [" are", "0.98012894"], [":", "0.99999976"], ["\n", "1.0"], ["-", "0.83127856"], [" Cancer", "1.0"], ["\n", "0.9999906"], ["-", "1.0"], [" Diabetes", "0.9990754"], ["\n", "1.0"], ["-", "1.0"], [" Heart", "1.0"], [" Disease", "0.8109894"], ["\n", "1.0"], ["-", "1.0"], [" Stro", "0.9610313"], ["ke", "1.0"], ["\n", "1.0"], ["-", "1.0"], [" Alzheimer", "0.99935824"], ["'s", "1.0"], [" Disease", "0.9999914"], ["\n", "1.0"], ["-", "1.0"], [" Parkinson", "0.85388106"], ["'s", "1.0"], [" Disease", "1.0"], ["\n", "1.0"], ["-", "1.0"], [" Multiple", "0.9060141"], [" S", "1.0"], ["clerosis", "1.0"], ["\n", "1.0"], ["-", "1.0"], [" Hep", "0.9999901"], ["at", "1.0"], ["itis", "1.0"], [" C", "0.9563354"], ["\n", "1.0"], ["-", "1.0"], [" HIV", "0.9516951"]]}, "308": {"qid": "018f6ea10facddae3b2e", "term": "Porsche", "description": "automotive brand manufacturing subsidiary of Volkswagen", "question": "Was Dorothea Wendling from same place Porsche originated?", "answer": true, "facts": ["Dorothea Wendling was a singer born in Stuttgart, Germany.", "Porsche was founded in 1931 in Stuttgart, Germany."], "decomposition": ["Where was Dorothea Wendling born?", "Where was Posche founded?", "Is #1 the same as #2?"], "evidence": [[[["Dorothea Wendling-1"]], [["Porsche-2"]], ["operation"]], [[["Dorothea Wendling-1"]], [["Porsche-1"]], ["operation"]], [[["Dorothea Wendling-1"], "operation"], ["no_evidence"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the first person to fly in a plane have a name that was the same as the plane?\nFacts: The first person to fly in a plane was named \"Pilot\" and the plane was named \"Pilotless\".\nAnswer (True or False): True\nQuestion: Did the first person to fly in a plane have a name that was the same as", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Dorothea Wendling from same place Porsche originated?\nFacts: Dorothea Wendling was a singer born in Stuttgart, Germany. Porsche was founded in 1931 in Stuttgart, Germany.\nAnswer (True or False):", "ct_subject": "Dorothea Wendling was a singer born in Stuttgart, Germany. Porsche was founded in 1931 in Stuttgart, Germany.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.021952756"], [" the", "1.0"], [" first", "0.99987257"], [" person", "0.52858084"], [" to", "1.0"], [" fly", "0.33785167"], [" in", "0.99012285"], [" a", "0.9999348"], [" plane", "1.0"], [" have", "0.8829011"], [" a", "0.99999976"], [" name", "1.0"], [" that", "0.99979097"], [" was", "0.9999994"], [" the", "0.9991774"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" plane", "0.99999964"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" fly", "1.0"], [" in", "1.0"], [" a", "1.0"], [" plane", "1.0"], [" was", "0.99999976"], [" named", "0.99233913"], [" \"", "1.0"], ["P", "0.99999714"], ["ilot", "1.0"], ["\"", "0.9989335"], [" and", "0.9979297"], [" the", "0.9999604"], [" plane", "1.0"], [" was", "1.0"], [" named", "1.0"], [" \"", "1.0"], ["P", "1.0"], ["ilot", "0.9999455"], ["less", "0.5228835"], ["\".", "0.9493241"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.97009563"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" fly", "1.0"], [" in", "1.0"], [" a", "1.0"], [" plane", "1.0"], [" have", "1.0"], [" a", "1.0"], [" name", "1.0"], [" that", "1.0"], [" was", "1.0"], [" the", "1.0"], [" same", "1.0"], [" as", "1.0"]]}, "309": {"qid": "f794258d1e33a50d1348", "term": "Surfing", "description": "sport that consists of riding a wave", "question": "Was Surfing popular when pogs came out?", "answer": true, "facts": ["Pogs came out in the 1990's.", "The 90's saw a rise in 'Big Wave Culture', a practice involving finding the largest possible waves to surf on."], "decomposition": ["When were Pogs released?", "Did surfing experience a growth in popularity in #1?"], "evidence": [[[["Milk caps (game)-8"]], [["Surf culture-68"]]], [[["Milk caps (game)-3"]], [["Surf culture-68"], "operation"]], [[["Milk caps (game)-1"]], [["Surf culture-28"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: Thomas Jefferson.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: Thomas Jefferson.\nAnswer (True or False): True\nQuestion: What is the name of", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Surfing popular when pogs came out?\nFacts: Pogs came out in the 1990's. The 90's saw a rise in 'Big Wave Culture', a practice involving finding the largest possible waves to surf on.\nAnswer (True or False):", "ct_subject": "Pogs came out in the 1990's. The 90's saw a rise in 'Big Wave Culture', a practice involving finding the largest possible waves to surf on.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999858"], [" is", "1.0"], [" the", "1.0"], [" name", "0.95417106"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9070774"], [" person", "0.9996076"], [" to", "1.0"], [" be", "0.63924193"], [" born", "0.99896836"], [" in", "1.0"], [" the", "1.0"], [" United", "0.60374063"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Thomas", "0.7035313"], [" Jefferson", "1.0"], [".", "0.9997055"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Thomas", "1.0"], [" Jefferson", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"]]}, "310": {"qid": "12ba3528a86c6ece9166", "term": "Upload", "description": "sending of data from a local system to a remote system", "question": "Can Centurylink max internet plan upload 1000GB in a fortnight?", "answer": true, "facts": ["A fortnight refers to a period of two weeks.", "Centurylink's max internet plan speed is 1,000MB per second.", "1000GB takes 2.5 hours to upload for every 1000MB speed."], "decomposition": ["How long is a fortnight?", "What is Centurylink's max internet plan speed?", "With #2, how long would it take to upload 1000GB?", "Is #3 less than #1?"], "evidence": [[[["Counting-9"]], [["CenturyLink-12"]], [["CenturyLink-12"]], ["operation"]], [["no_evidence"], [["CenturyLink-12"], "no_evidence"], ["operation"], ["operation"]], [[["Fortnight (disambiguation)-1"]], [["CenturyLink-12"]], [["Bit-15", "Megabyte-5"], "operation"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human?\nFacts: The average weight of a human is about 100 pounds.\nAnswer (True or False): True\nQuestion: What is the average height of a human?\nFacts: The average height of a human is about 5 feet, 8 inches.\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can Centurylink max internet plan upload 1000GB in a fortnight?\nFacts: A fortnight refers to a period of two weeks. Centurylink's max internet plan speed is 1,000MB per second. 1000GB takes 2.5 hours to upload for every 1000MB speed.\nAnswer (True or False):", "ct_subject": "A fortnight refers to a period of two weeks. Centurylink's max internet plan speed is 1,000MB per second. 1000GB takes 2.5 hours to upload for every 1000MB speed.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.5211787"], [" is", "1.0"], [" the", "1.0"], [" average", "0.9999914"], [" weight", "0.9999906"], [" of", "1.0"], [" a", "0.99999785"], [" human", "1.0"], ["?", "0.9923592"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" weight", "0.8998759"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" is", "1.0"], [" about", "0.9997625"], [" 100", "0.99984"], [" pounds", "0.9999912"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" height", "0.9998479"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" height", "1.0"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 5", "0.9999763"], [" feet", "0.9999999"], [",", "0.42046434"], [" 8", "0.95780635"], [" inches", "1.0"], [".", "0.99999857"], ["\n", "1.0"]]}, "311": {"qid": "6de9966e60e743c03611", "term": "Good", "description": "Term in religion, ethics, and philosophy", "question": "In star rating systems, is 5 stars considered good?", "answer": true, "facts": ["Most star rating systems are composed of 5 stars.", "In star rating, most people want to avoid a 1 star review."], "decomposition": ["What is the highest rating possible in most star rating systems?", "Is #1 equal to five stars?"], "evidence": [[[["Star (classification)-1"]], ["operation"]], [[["Nutritional rating systems-4"], "no_evidence"], ["operation"]], [[["Star (classification)-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"good\" and a \"bad\" review?\nFacts: A \"good\" review is one that is written by a person who is knowledgeable about the product. A \"bad\" review is one written by someone who is uninformed about the product.\nAnswer (True or False): True\nQuestion: What is the difference between a \"good\" and a \"", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: In star rating systems, is 5 stars considered good?\nFacts: Most star rating systems are composed of 5 stars. In star rating, most people want to avoid a 1 star review.\nAnswer (True or False):", "ct_subject": "Most star rating systems are composed of 5 stars. In star rating, most people want to avoid a 1 star review.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999964"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.99995816"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.99416167"], ["good", "0.9999893"], ["\"", "1.0"], [" and", "1.0"], [" a", "0.9505661"], [" \"", "1.0"], ["bad", "0.95382917"], ["\"", "1.0"], [" review", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" \"", "0.9999813"], ["good", "1.0"], ["\"", "1.0"], [" review", "1.0"], [" is", "1.0"], [" one", "0.9998485"], [" that", "1.0"], [" is", "1.0"], [" written", "0.9632104"], [" by", "0.9999956"], [" a", "0.6327652"], [" person", "1.0"], [" who", "1.0"], [" is", "0.99269086"], [" knowledgeable", "0.9937928"], [" about", "1.0"], [" the", "1.0"], [" product", "1.0"], [".", "0.9956255"], [" A", "1.0"], [" \"", "1.0"], ["bad", "1.0"], ["\"", "1.0"], [" review", "1.0"], [" is", "1.0"], [" one", "0.9995241"], [" written", "0.7390577"], [" by", "1.0"], [" someone", "0.7218964"], [" who", "1.0"], [" is", "1.0"], [" unin", "1.0"], ["formed", "1.0"], [" about", "1.0"], [" the", "1.0"], [" product", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["good", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"]]}, "312": {"qid": "0da3b98c8e6af0dd89d3", "term": "Geometry", "description": "Branch of mathematics that studies the shape, size and position of objects", "question": "Does Siri know geometry?", "answer": true, "facts": ["Geometry is the study of size, shape and distance of objects.", "Determining location requires geometry.", "Siri can determine your location."], "decomposition": ["What are some basic user information that Siri can determine?", "What mathematical concepts must be known in order to determine the location of a point?", "Is location included in #1 and geometry in #2?"], "evidence": [[[["Siri-3"]], [["Location-5"], "no_evidence"], ["operation"]], [[["Siri-3"]], [["Geometry-15"], "no_evidence"], ["operation"]], [[["Global Positioning System-1", "Siri-1", "Siri-11"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Does Siri know the difference between a \"c\" and a \"k\"?\nFacts: Siri can tell you the difference between a \"c\" and a \"k\".\nAnswer (True or False): True\nQuestion: Does Siri know the difference between a \"c\" and a \"k\"?\nFacts: Siri can tell you the difference between a \"c\" and a \"k\".\nAnswer (", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Siri know geometry?\nFacts: Geometry is the study of size, shape and distance of objects. Determining location requires geometry. Siri can determine your location.\nAnswer (True or False):", "ct_subject": "Geometry is the study of size, shape and distance of objects. Determining location requires geometry. Siri can determine your location.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Does", "0.9857978"], [" Siri", "1.0"], [" know", "1.0"], [" the", "0.9454939"], [" difference", "0.9993411"], [" between", "1.0"], [" a", "0.9999963"], [" \"", "0.8913643"], ["c", "0.94214094"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["k", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Siri", "0.9781146"], [" can", "0.99999917"], [" tell", "0.99999726"], [" you", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["k", "1.0"], ["\".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Does", "1.0"], [" Siri", "1.0"], [" know", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["k", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Siri", "1.0"], [" can", "1.0"], [" tell", "1.0"], [" you", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["k", "1.0"], ["\".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"]]}, "313": {"qid": "5f9ae2b7933da8c32505", "term": "Butter", "description": "dairy product", "question": "Does butter industry survive cow extinction?", "answer": true, "facts": ["Butter is a dairy product made from milk.", "Cows are the predominant source of milk in the US.", "Goats, sheep, buffalo, and other mammals produce milk.", "Goat butter, made from 100% Goat's Milk, is an excellent source of Vitamin A."], "decomposition": ["What animal product is butter made from?", "What common livestock animals produce #1?", "Are animals other than cows included in #2?"], "evidence": [[[["Butter-2"]], [["Butter-2"]], [["Butter-2"]]], [[["Butter-10"], "no_evidence"], [["Butter-2"], "no_evidence"], ["operation"]], [[["Butter-1"]], [["Milk-26"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the moon made of cheese?\nFacts: The moon is made of cheese. The moon is made of cheese. The moon is made of cheese. The moon is made of cheese. The moon is made of cheese. The moon is made of cheese. The moon is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does butter industry survive cow extinction?\nFacts: Butter is a dairy product made from milk. Cows are the predominant source of milk in the US. Goats, sheep, buffalo, and other mammals produce milk. Goat butter, made from 100% Goat's Milk, is an excellent source of Vitamin A.\nAnswer (True or False):", "ct_subject": "Butter is a dairy product made from milk. Cows are the predominant source of milk in the US. Goats, sheep, buffalo, and other mammals produce milk. Goat butter, made from 100% Goat's Milk, is an excellent source of Vitamin A.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.86877203"], [" the", "0.9364034"], [" moon", "0.15782589"], [" made", "0.99999845"], [" of", "1.0"], [" cheese", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" moon", "1.0"], [" is", "1.0"], [" made", "1.0"], [" of", "1.0"], [" cheese", "0.9997508"], [".", "1.0"], [" The", "0.9987255"], [" moon", "1.0"], [" is", "1.0"], [" made", "0.99998903"], [" of", "1.0"], [" cheese", "1.0"], [".", "1.0"], [" The", "0.9762833"], [" moon", "1.0"], [" is", "1.0"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], [".", "1.0"], [" The", "0.99998903"], [" moon", "1.0"], [" is", "1.0"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], [".", "1.0"], [" The", "0.9999994"], [" moon", "1.0"], [" is", "1.0"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], [".", "1.0"], [" The", "1.0"], [" moon", "1.0"], [" is", "1.0"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], [".", "1.0"], [" The", "1.0"], [" moon", "1.0"], [" is", "1.0"]]}, "314": {"qid": "3655a4efabd3358429d0", "term": "Chrome OS", "description": "Linux-based operating system developed by Google", "question": "Is an internet connection essential for someone using Chrome OS?", "answer": true, "facts": ["Most Chromebook apps require internet access to function properly.", "There are apps for the Chromebook that can function properly without internet access.", "To download any apps onto the Chromebook, including offline apps, one must connect to the internet."], "decomposition": ["What are the applications needed for essential functions on devices running Chrome OS?", "Is an internet connection needed for any of #1 to be downloaded or to work properly?"], "evidence": [[[["Chrome OS-1"]], [["Chrome OS-5"], "operation"]], [[["Chrome OS-2"]], ["operation"]], [[["Chrome OS-2", "Chrome OS-3"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a Chromebook and a Windows or Mac laptop?\nFacts: Chromebooks are designed to be used on the go. They are lightweight and portable. They are designed to be used in the classroom, at home, or in the office. They are designed to be used with a keyboard", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is an internet connection essential for someone using Chrome OS?\nFacts: Most Chromebook apps require internet access to function properly. There are apps for the Chromebook that can function properly without internet access. To download any apps onto the Chromebook, including offline apps, one must connect to the internet.\nAnswer (True or False):", "ct_subject": "Most Chromebook apps require internet access to function properly. There are apps for the Chromebook that can function properly without internet access. To download any apps onto the Chromebook, including offline apps, one must connect to the internet.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9750528"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9999999"], [" between", "1.0"], [" a", "0.9997663"], [" Chromebook", "1.0"], [" and", "1.0"], [" a", "1.0"], [" Windows", "0.8672396"], [" or", "0.9525797"], [" Mac", "1.0"], [" laptop", "0.9995554"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Chromebook", "1.0"], ["s", "1.0"], [" are", "1.0"], [" designed", "0.9999993"], [" to", "1.0"], [" be", "1.0"], [" used", "0.99981886"], [" on", "0.99999726"], [" the", "1.0"], [" go", "1.0"], [".", "1.0"], [" They", "1.0"], [" are", "1.0"], [" lightweight", "0.8473983"], [" and", "0.7351309"], [" portable", "0.99836963"], [".", "0.9999999"], [" They", "1.0"], [" are", "1.0"], [" designed", "1.0"], [" to", "1.0"], [" be", "1.0"], [" used", "1.0"], [" in", "0.90827715"], [" the", "0.5984899"], [" classroom", "1.0"], [",", "0.88243675"], [" at", "0.9992699"], [" home", "0.99999845"], [",", "1.0"], [" or", "1.0"], [" in", "0.9918439"], [" the", "1.0"], [" office", "1.0"], [".", "1.0"], [" They", "0.9999304"], [" are", "1.0"], [" designed", "1.0"], [" to", "1.0"], [" be", "1.0"], [" used", "1.0"], [" with", "0.99998415"], [" a", "1.0"], [" keyboard", "1.0"]]}, "315": {"qid": "b39a840f37ef3d97d498", "term": "Goofy", "description": "Disney cartoon character", "question": "Would Goofy hypothetically enjoy Nylabone?", "answer": true, "facts": ["Goofy is a popular Disney cartoon character that is a dog.", "Nylabone is a popular dog bone brand.", "Dogs chew bones for the taste, and to exercise the muscles of the jaw."], "decomposition": ["What kind of animal does Goofy portray?", "What kind of animal are Nylabones made for?", "Are #1 and #2 both dog?"], "evidence": [[[["Goofy-1"]], [["Dog toy-7"]], ["operation"]], [[["Goofy-1"]], [["Dog toy-7"]], ["operation"]], [[["Goofy-1"]], [["Dog toy-7"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the character in the movie \"The Princess and the Frog\"?\nFacts: The Princess and the Frog is a Disney animated film. The character is named Tiana.\nAnswer (True or False): True\nQuestion: What is the name of the character in the movie \"The Princess", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Goofy hypothetically enjoy Nylabone?\nFacts: Goofy is a popular Disney cartoon character that is a dog. Nylabone is a popular dog bone brand. Dogs chew bones for the taste, and to exercise the muscles of the jaw.\nAnswer (True or False):", "ct_subject": "Goofy is a popular Disney cartoon character that is a dog. Nylabone is a popular dog bone brand. Dogs chew bones for the taste, and to exercise the muscles of the jaw.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999464"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999914"], [" of", "1.0"], [" the", "1.0"], [" character", "0.22900176"], [" in", "1.0"], [" the", "1.0"], [" movie", "0.99999976"], [" \"", "1.0"], ["The", "1.0"], [" Princess", "0.93550545"], [" and", "0.5997269"], [" the", "1.0"], [" Frog", "1.0"], ["\"?", "0.9995303"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Princess", "0.9044054"], [" and", "1.0"], [" the", "1.0"], [" Frog", "1.0"], [" is", "1.0"], [" a", "1.0"], [" Disney", "1.0"], [" animated", "0.9900467"], [" film", "0.99905676"], [".", "0.708625"], [" The", "1.0"], [" character", "0.9999999"], [" is", "0.99999976"], [" named", "1.0"], [" T", "0.67902404"], ["iana", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" character", "1.0"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "1.0"], ["The", "1.0"], [" Princess", "0.99968946"]]}, "316": {"qid": "0f3ab70fd0a59071bb06", "term": "The Hobbit", "description": "Fantasy novel by J. R. R. Tolkien", "question": "Would a Drow tower over The Hobbit's hero?", "answer": true, "facts": ["The hero of the Hobbit is Bilbo Baggins.", "Bilbo Baggins is a hobbit, which is a race resembling very short humans with furry feet.", "Halfling is another term for hobbits, and halflings are described as being half the size of a human.", "The Drow are a race of dark elves described as being around five feet in height."], "decomposition": ["Who is the hero of The Hobbit", "What is a Drow?", "Is #2 taller than #1?"], "evidence": [[[["Hobbit-12", "Hobbit-2"]], [["Drow-28"]], ["operation"]], [[["The Hobbit-7"]], [["Drow-28"]], ["no_evidence", "operation"]], [[["The Hobbit-2"]], [["Drow-1"]], [["Drow-28", "Hobbit-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the movie The Lord of the Rings?\nFacts: The city of Mirkwood is the name of the forest", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a Drow tower over The Hobbit's hero?\nFacts: The hero of the Hobbit is Bilbo Baggins. Bilbo Baggins is a hobbit, which is a race resembling very short humans with furry feet. Halfling is another term for hobbits, and halflings are described as being half the size of a human. The Drow are a race of dark elves described as being around five feet in height.\nAnswer (True or False):", "ct_subject": "The hero of the Hobbit is Bilbo Baggins. Bilbo Baggins is a hobbit, which is a race resembling very short humans with furry feet. Halfling is another term for hobbits, and halflings are described as being half the size of a human. The Drow are a race of dark elves described as being around five feet in height.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9755652"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99996614"], [" of", "1.0"], [" the", "1.0"], [" city", "0.5412244"], [" in", "0.9128287"], [" the", "0.99999976"], [" movie", "0.9960355"], [" The", "0.9999976"], [" Lord", "0.99154156"], [" of", "1.0"], [" the", "1.0"], [" Rings", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" city", "0.99996674"], [" of", "0.947362"], [" Mir", "1.0"], ["k", "1.0"], ["wood", "1.0"], [" is", "1.0"], [" the", "0.92922217"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" forest", "1.0"]]}, "317": {"qid": "807244d53c3924958261", "term": "QWERTY", "description": "keyboard layout where the first line is \"QWERTYUIOP\"", "question": "Can first letter row of QWERTY keyboard spell a palindrome?", "answer": true, "facts": ["The first letter row of the QWERTY keyboard contains: QWERTYUIOP", "A palindrome is a word that can be spelled the same backwords and forwards such as racecar and rotor."], "decomposition": ["What letters are on the first line of a QWERTY keyboard?", "What vowels are listed in #!?", "What consonants are listed in #1?", "Could a word be formed by repeating the following process: Pick a consonant from #3 then select a vowel from #2. Construct a string of letters from those 2 elements such that the first and third letters are the selected consonant and the second is the vowel.", "If #4 is \"no\" then using the process in Step 4 insert a second occurrence of the same vowel adjacent to the first then choose a consonant from #3 and insert it between the repeated vowel. Is a word formed?"], "evidence": [[[["QWERTY-1"]], ["operation"], ["operation"], ["operation"], ["operation"]], [[["QWERTY-1"], "no_evidence"], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"], ["no_evidence", "operation"]], [[["QWERTY-1"], "no_evidence"], ["operation"], ["operation"], ["operation"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest word in the English language?\nFacts: The longest word in the English language is \"I love you\" which is 5,848 characters long.\nAnswer (True or False): True\nQuestion: What is the longest word in the English language?\nFacts", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can first letter row of QWERTY keyboard spell a palindrome?\nFacts: The first letter row of the QWERTY keyboard contains: QWERTYUIOP A palindrome is a word that can be spelled the same backwords and forwards such as racecar and rotor.\nAnswer (True or False):", "ct_subject": "The first letter row of the QWERTY keyboard contains: QWERTYUIOP A palindrome is a word that can be spelled the same backwords and forwards such as racecar and rotor.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.94150174"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.9970099"], [" word", "0.99999976"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], [" is", "1.0"], [" \"", "0.92816997"], ["I", "0.9267383"], [" love", "0.6515766"], [" you", "1.0"], ["\"", "0.9862211"], [" which", "1.0"], [" is", "1.0"], [" 5", "0.9811396"], [",", "1.0"], ["8", "0.66872656"], ["48", "0.4064642"], [" characters", "0.84339786"], [" long", "0.9999999"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.9999982"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"]]}, "318": {"qid": "f92ab04a1f144b593809", "term": "British royal family", "description": "Family consisting of close relatives of the monarch of the United Kingdom", "question": "Have any members of the 2020 British royal family allegedly committed a felony?", "answer": true, "facts": ["The 2020 British royal family includes Queen Elizabeth II and her children.", "Prince Andrew is the son of Queen Elizabeth II.", "Prince Andrew was accused of sexual abuse in 2019.", "Sexual assault is classified as a felony."], "decomposition": ["Which royal family does Prince Andrew belong to?", "What is Prince Andrew accused of?", "What type of crime is #2?", "Does #1 have a member accused of #3?"], "evidence": [[[["Prince Andrew, Duke of York-1"]], [["Prince Andrew, Duke of York-22"]], [["Adolescent sexuality in the United States-53"]], [["Prince Andrew, Duke of York-25"]]], [[["Prince Andrew, Duke of York-6"]], [["Prince Andrew, Duke of York-22", "Prince Andrew, Duke of York-25"]], [["Prince Andrew, Duke of York-22"]], [["Prince Andrew, Duke of York-25"], "operation"]], [[["Prince Andrew, Duke of York-1"]], [["Prince Andrew, Duke of York-3"]], [["Felony-6"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: The first person to be executed in the United States was John Brown. John Brown was executed in 1859.\nAnswer (True or False): True\nQuestion: What is the name of the first person", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Have any members of the 2020 British royal family allegedly committed a felony?\nFacts: The 2020 British royal family includes Queen Elizabeth II and her children. Prince Andrew is the son of Queen Elizabeth II. Prince Andrew was accused of sexual abuse in 2019. Sexual assault is classified as a felony.\nAnswer (True or False):", "ct_subject": "The 2020 British royal family includes Queen Elizabeth II and her children. Prince Andrew is the son of Queen Elizabeth II. Prince Andrew was accused of sexual abuse in 2019. Sexual assault is classified as a felony.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.999995"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99999166"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9990865"], [" person", "0.99985635"], [" to", "1.0"], [" be", "0.9999913"], [" executed", "0.99999964"], [" in", "0.9999932"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.999982"], [" first", "1.0"], [" person", "1.0"], [" to", "0.9999945"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" John", "0.99964976"], [" Brown", "0.9999882"], [".", "0.6561593"], [" John", "0.996585"], [" Brown", "1.0"], [" was", "1.0"], [" executed", "0.5125655"], [" in", "0.9999926"], [" 18", "1.0"], ["59", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"]]}, "319": {"qid": "8d3ddaee20ad48edc066", "term": "Brazilian jiu-jitsu", "description": "martial art focusing on grappling and ground fighting, originally based on Kodokan judo newaza taught by Japanese judoka, that developed independently in Brazil from experimentation and adaptation by Carlos and H\u00e9lio Gracie, Luiz Fran\u00e7a, et al.", "question": "Did Brazilian jiu-jitsu Gracie founders have at least a baker's dozen of kids between them?", "answer": true, "facts": ["A baker's dozen refers to 13 of anything.", "Brazilian jiu-jitsu was founded by Carlos and Helio Gracie.  ", "Helio Gracie had 9 children.", "Carlos Gracie had 11 children."], "decomposition": ["Who were the founders of Brazilian jiu-jitsu?", "How many children do #1 have altogether", "What is the number represented by the baker's dozen?", "Is #2 greater than or equal to #3?"], "evidence": [[[["Brazilian jiu-jitsu-2"]], [["Carlos Gracie-12", "H\u00e9lio Gracie-25"], "operation"], [["Dozen-7"]], ["operation"]], [[["Carlos Gracie-1", "H\u00e9lio Gracie-1"]], [["Carlos Gracie-12", "H\u00e9lio Gracie-25"]], [["Dozen-7"]], ["operation"]], [[["Gracie family-1"]], [["H\u00e9lio Gracie-25"], "no_evidence"], [["Dozen-7"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the first person to ever fly a plane have at least a baker's dozen of kids between them?\nFacts: The first person to ever fly a plane was a man named Charles Lindbergh. He had 9 children.\nAnswer (True or", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Brazilian jiu-jitsu Gracie founders have at least a baker's dozen of kids between them?\nFacts: A baker's dozen refers to 13 of anything. Brazilian jiu-jitsu was founded by Carlos and Helio Gracie.  Helio Gracie had 9 children. Carlos Gracie had 11 children.\nAnswer (True or False):", "ct_subject": "A baker's dozen refers to 13 of anything. Brazilian jiu-jitsu was founded by Carlos and Helio Gracie.  Helio Gracie had 9 children. Carlos Gracie had 11 children.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99999976"], [" the", "1.0"], [" first", "0.999944"], [" person", "0.9708685"], [" to", "1.0"], [" ever", "0.47625938"], [" fly", "0.9470055"], [" a", "0.21475634"], [" plane", "0.9681157"], [" have", "1.0"], [" at", "0.99926096"], [" least", "1.0"], [" a", "1.0"], [" baker", "1.0"], ["'s", "1.0"], [" dozen", "1.0"], [" of", "1.0"], [" kids", "1.0"], [" between", "1.0"], [" them", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" ever", "1.0"], [" fly", "1.0"], [" a", "1.0"], [" plane", "1.0"], [" was", "1.0"], [" a", "0.70098114"], [" man", "1.0"], [" named", "1.0"], [" Charles", "1.0"], [" Lind", "1.0"], ["ber", "1.0"], ["gh", "1.0"], [".", "1.0"], [" He", "1.0"], [" had", "1.0"], [" 9", "0.9433439"], [" children", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"]]}, "320": {"qid": "ec81e2442ee7421b632b", "term": "John Kerry", "description": "68th United States Secretary of State", "question": "Did any Golden Globe winners attend John Kerry's alma mater?", "answer": true, "facts": ["John Kerry graduated from Yale University.", "Jennifer Connelly attended Yale University in the late 1980s.", "Jennifer Connelly won a Golden Globe award for the film A Beautiful Mind,"], "decomposition": ["What is John Kerry's alma mater?", "Who has won a Golden Globe?", "Have any of #2 attended #1?"], "evidence": [[[["John Kerry-2"]], [["Meryl Streep-19"], "no_evidence"], [["Meryl Streep-9"], "no_evidence", "operation"]], [[["John Kerry-2"]], [["Claire Danes-2"]], [["Claire Danes-3"]]], [[["John Kerry-9"]], [["Meryl Streep-1"]], [["Meryl Streep-9"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did any Golden Globe winners attend the funeral of John Kerry?\nFacts: John Kerry attended the funeral of his father, Senator John Kerry, in the late 1980s.\nAnswer (True or False): True\nQuestion: Did any Golden Globe winners attend the funeral of Jennifer Connelly?\nFacts: Jennifer Connelly attended the funeral", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did any Golden Globe winners attend John Kerry's alma mater?\nFacts: John Kerry graduated from Yale University. Jennifer Connelly attended Yale University in the late 1980s. Jennifer Connelly won a Golden Globe award for the film A Beautiful Mind,\nAnswer (True or False):", "ct_subject": "John Kerry graduated from Yale University. Jennifer Connelly attended Yale University in the late 1980s. Jennifer Connelly won a Golden Globe award for the film A Beautiful Mind,", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999982"], [" any", "0.9796077"], [" Golden", "0.9999994"], [" Globe", "1.0"], [" winners", "1.0"], [" attend", "1.0"], [" the", "1.0"], [" funeral", "1.0"], [" of", "1.0"], [" John", "0.99935025"], [" Kerry", "1.0"], ["?", "0.9999999"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "1.0"], [" Kerry", "1.0"], [" attended", "0.99764556"], [" the", "1.0"], [" funeral", "1.0"], [" of", "1.0"], [" his", "1.0"], [" father", "0.99917954"], [",", "1.0"], [" Senator", "0.99312663"], [" John", "1.0"], [" Kerry", "1.0"], [",", "0.9978878"], [" in", "0.84789693"], [" the", "0.999824"], [" late", "0.9610781"], [" 1980", "0.9879792"], ["s", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" any", "1.0"], [" Golden", "1.0"], [" Globe", "1.0"], [" winners", "1.0"], [" attend", "1.0"], [" the", "1.0"], [" funeral", "1.0"], [" of", "1.0"], [" Jennifer", "1.0"], [" Conn", "1.0"], ["elly", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Jennifer", "1.0"], [" Conn", "1.0"], ["elly", "1.0"], [" attended", "1.0"], [" the", "1.0"], [" funeral", "1.0"]]}, "321": {"qid": "ce01c88f5d2c29a80cb3", "term": "Clown", "description": "A comic performer often for children's entertainment", "question": "Would Stephen King fans be likely to own an image of a clown?", "answer": true, "facts": ["Stephen King wrote a popular book called \"It\" about an evil clown.", "\"It\" has been made into two major films and has been merchandised. "], "decomposition": ["Who is the antagonist of popular Stephen King's book 'It'?", "Is #1 a clown?"], "evidence": [[[["It (novel)-1"]], ["operation"]], [[["It (character)-1"]], ["operation"]], [[["It (character)-1"]], [["It (character)-4"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: The first person to be executed in the United States was John Brown. He was hanged in 1859.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be executed in the United States?\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Stephen King fans be likely to own an image of a clown?\nFacts: Stephen King wrote a popular book called \"It\" about an evil clown. \"It\" has been made into two major films and has been merchandised. \nAnswer (True or False):", "ct_subject": "Stephen King wrote a popular book called \"It\" about an evil clown. \"It\" has been made into two major films and has been merchandised. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999968"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99996305"], [" of", "1.0"], [" the", "1.0"], [" first", "0.91720235"], [" person", "0.99821645"], [" to", "1.0"], [" be", "0.9996216"], [" executed", "0.9738443"], [" in", "0.99960595"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999578"], [" first", "1.0"], [" person", "1.0"], [" to", "0.9982401"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" John", "0.9998406"], [" Brown", "0.9999951"], [".", "0.9994159"], [" He", "0.98254377"], [" was", "1.0"], [" hanged", "1.0"], [" in", "0.9414097"], [" 18", "1.0"], ["59", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"]]}, "322": {"qid": "aa5d9115cc83aa120b7a", "term": "Burning Man", "description": "annual experimental festival based in Nevada, United States", "question": "Would it be impossible to get to Burning Man on the Mayflower?", "answer": true, "facts": ["The Mayflower was a seafaring vessel", "Burning Man is held in Black Rock Desert", "There are no bodies of water flowing into the Black Rock Desert"], "decomposition": ["What was the Mayflower?", "What environment does #1 work in?", "What kind of environment is The Burning Man?", "Is #2 different from #3?"], "evidence": [[[["Mayflower-1"]], [["Ship-1"]], [["Burning Man-1"]], ["operation"]], [[["Mayflower-1"]], [["Fluyt-1"]], [["Burning Man-1"]], ["operation"]], [[["Mayflower-1"]], [["Ship-1"]], [["Burning Man-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the man who invented the first computer?\nFacts: Alan Turing\nAnswer (True or False): True\nQuestion: What is the name of the man who invented the first computer?\nFacts: Alan Turing\nAnswer (True or False): True\nQuestion: What is the name of the man who invented the first computer?\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would it be impossible to get to Burning Man on the Mayflower?\nFacts: The Mayflower was a seafaring vessel Burning Man is held in Black Rock Desert There are no bodies of water flowing into the Black Rock Desert\nAnswer (True or False):", "ct_subject": "The Mayflower was a seafaring vessel Burning Man is held in Black Rock Desert There are no bodies of water flowing into the Black Rock Desert", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999988"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99996686"], [" of", "1.0"], [" the", "1.0"], [" man", "0.9051706"], [" who", "1.0"], [" invented", "1.0"], [" the", "1.0"], [" first", "0.9992951"], [" computer", "0.98006266"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Alan", "1.0"], [" Turing", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" man", "1.0"], [" who", "1.0"], [" invented", "1.0"], [" the", "1.0"], [" first", "1.0"], [" computer", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Alan", "1.0"], [" Turing", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" man", "1.0"], [" who", "1.0"], [" invented", "1.0"], [" the", "1.0"], [" first", "1.0"], [" computer", "1.0"], ["?", "1.0"], ["\n", "1.0"]]}, "323": {"qid": "b93a4174d1a7dca2ee0f", "term": "Hippopotamus", "description": "A large, mostly herbivorous, semiaquatic mammal native to sub-Saharan Africa", "question": "Are hippos dangerous to humans?", "answer": true, "facts": ["Hippos are large and have large teeth.", "If threatened, they aggressively defend themselves."], "decomposition": ["How do hippopotami respond to perceived threats?", "Considering #1 and their prominent physical features, could they hurt humans?"], "evidence": [[[["Hippopotamus-42"]], ["operation"]], [[["Hippopotamus-35", "Hippopotamus-36"]], [["Hippopotamus-42"], "operation"]], [[["Hippopotamus-42"]], [["Hippopotamus-42"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any animals that can't swim?\nFacts: There are no animals that can't swim.\nAnswer (True or False): True\nQuestion: Are there any animals that can't climb trees?\nFacts: There are no animals that can't climb trees.\nAnswer (True or False): True\nQuestion: Are there any animals that can't swim?\nFacts: There are no animals that can't swim.\nAnswer", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are hippos dangerous to humans?\nFacts: Hippos are large and have large teeth. If threatened, they aggressively defend themselves.\nAnswer (True or False):", "ct_subject": "Hippos are large and have large teeth. If threatened, they aggressively defend themselves.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.2566706"], [" there", "1.0"], [" any", "0.99988544"], [" animals", "0.8174379"], [" that", "1.0"], [" can", "0.041690357"], ["'t", "0.9783263"], [" swim", "0.99999905"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "0.9999467"], [" are", "1.0"], [" no", "0.30903"], [" animals", "1.0"], [" that", "1.0"], [" can", "1.0"], ["'t", "1.0"], [" swim", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.99999964"], [" there", "1.0"], [" any", "1.0"], [" animals", "1.0"], [" that", "1.0"], [" can", "1.0"], ["'t", "1.0"], [" climb", "0.99999976"], [" trees", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "1.0"], [" are", "1.0"], [" no", "1.0"], [" animals", "1.0"], [" that", "1.0"], [" can", "1.0"], ["'t", "1.0"], [" climb", "1.0"], [" trees", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "1.0"], [" any", "1.0"], [" animals", "1.0"], [" that", "1.0"], [" can", "1.0"], ["'t", "1.0"], [" swim", "0.9999856"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "1.0"], [" are", "1.0"], [" no", "1.0"], [" animals", "1.0"], [" that", "1.0"], [" can", "1.0"], ["'t", "1.0"], [" swim", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"]]}, "324": {"qid": "b6c4ef03511c60d02183", "term": "Holy Grail", "description": "Cup, dish or stone with miraculous powers, important motif in Arthurian literature", "question": "Has the Holy Grail been featured in at least five films?", "answer": true, "facts": ["1981's Excalibur film features King Arthur and his knights looking for the Holy Grail.", "Monty Python and the Holy Grail spoofs Arthurian legend.", "Indiana Jones and the Last Crusade features a search for the Holy Grail.", "Prince Killian and the Holy Grail focuses on retrieval of the grail.", "The Silver Chalice focuses on a man that has to sculpt the Holy Grail."], "decomposition": ["What movies have featured the Holy Grail?", "Are at least 5 movies listed in #1?"], "evidence": [[[["Indiana Jones and the Last Crusade-1", "Lancelot du Lac (film)-3", "Monty Python and the Holy Grail-2", "The Fisher King-4", "The Light in the Dark-4"]], ["operation"]], [[["Holy Grail-32"]], [["Holy Grail-32"], "operation"]], [[["Holy Grail-32"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: John Brown was executed in 1859.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Has the Holy Grail been featured in at least five films?\nFacts: 1981's Excalibur film features King Arthur and his knights looking for the Holy Grail. Monty Python and the Holy Grail spoofs Arthurian legend. Indiana Jones and the Last Crusade features a search for the Holy Grail. Prince Killian and the Holy Grail focuses on retrieval of the grail. The Silver Chalice focuses on a man that has to sculpt the Holy Grail.\nAnswer (True or False):", "ct_subject": "1981's Excalibur film features King Arthur and his knights looking for the Holy Grail. Monty Python and the Holy Grail spoofs Arthurian legend. Indiana Jones and the Last Crusade features a search for the Holy Grail. Prince Killian and the Holy Grail focuses on retrieval of the grail. The Silver Chalice focuses on a man that has to sculpt the Holy Grail.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99989533"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999937"], [" of", "1.0"], [" the", "1.0"], [" first", "0.98844403"], [" person", "0.96904236"], [" to", "1.0"], [" be", "0.9987896"], [" executed", "0.03912501"], [" in", "0.9920745"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "0.71300936"], [" Brown", "0.99999774"], [" was", "0.9989778"], [" executed", "0.9953898"], [" in", "0.99999535"], [" 18", "1.0"], ["59", "1.0"], [".", "0.96428674"]]}, "325": {"qid": "29c9d972bd56437185ee", "term": "Communist Party of China", "description": "Political party of the People's Republic of China", "question": "Did Karl Marx influence the communist party of China?", "answer": true, "facts": ["Communist ideology is the foundation of communist party of China.", "Marx produced a political pamphlet that has since come to be commonly known as the communist manifesto. "], "decomposition": ["What does the communist party of China stand to represent?", "What were the political activities of Karl Max?", "Does any of #1 have a source/origin in #2?"], "evidence": [[[["Communist Party of China-37"]], [["Das Kapital-3"]], [["Communist Party of China-37", "Das Kapital-3"]]], [[["Communist Party of China-2"]], [["Marxism-1"]], ["operation"]], [[["Communist Party of China-2"]], [["Karl Marx-3"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the United States of America have a constitution?\nFacts: The United States of America had a constitution in 1787. The constitution was written by Thomas Jefferson.\nAnswer (True or False): True\nQuestion: Did the United States of America have a constitution in 1787?\nFacts: The United States of America had a constitution in 1787. The constitution", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Karl Marx influence the communist party of China?\nFacts: Communist ideology is the foundation of communist party of China. Marx produced a political pamphlet that has since come to be commonly known as the communist manifesto. \nAnswer (True or False):", "ct_subject": "Communist ideology is the foundation of communist party of China. Marx produced a political pamphlet that has since come to be commonly known as the communist manifesto. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99875546"], [" the", "1.0"], [" United", "0.9852079"], [" States", "1.0"], [" of", "0.9999237"], [" America", "1.0"], [" have", "0.9998685"], [" a", "1.0"], [" constitution", "0.98735154"], ["?", "0.94189656"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "1.0"], [" States", "1.0"], [" of", "1.0"], [" America", "1.0"], [" had", "0.9547474"], [" a", "1.0"], [" constitution", "1.0"], [" in", "0.99751943"], [" 17", "1.0"], ["87", "1.0"], [".", "1.0"], [" The", "0.64160645"], [" constitution", "0.99999976"], [" was", "1.0"], [" written", "0.9999993"], [" by", "1.0"], [" Thomas", "0.93766415"], [" Jefferson", "1.0"], [".", "0.9607545"], ["\n", "0.9999999"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999975"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" of", "1.0"], [" America", "1.0"], [" have", "1.0"], [" a", "1.0"], [" constitution", "0.999119"], [" in", "0.9611379"], [" 17", "1.0"], ["87", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "1.0"], [" States", "1.0"], [" of", "1.0"], [" America", "1.0"], [" had", "1.0"], [" a", "1.0"], [" constitution", "1.0"], [" in", "1.0"], [" 17", "1.0"], ["87", "1.0"], [".", "1.0"], [" The", "1.0"], [" constitution", "1.0"]]}, "326": {"qid": "fa160aa61805e9d27398", "term": "Great Lakes", "description": "System of interconnected, large lakes in North America", "question": "Are the Great Lakes part of an international border?", "answer": true, "facts": ["The lakes are bordered on the north by Canada.", "The lakes are bordered on the south by United States.", "Canada and United States are two different countries."], "decomposition": ["What borders the great lakes to the north?", "What borders the great lakes to the south?", "Are #1 and #2 different countries? "], "evidence": [[[["Great Lakes-1", "Great Lakes-19"]], [["Great Lakes-5"]], ["operation"]], [[["Great Lakes-5"]], [["Great Lakes-5"]], ["no_evidence"]], [[["Great Lakes-1"]], [["Great Lakes-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city of Chicago?\nFacts: Chicago is the largest city in the United States. Chicago is the name of the city of Chicago.\nAnswer (True or False): True\nQuestion: What is the name of the city of New York?\nFacts: New York is the largest city in the United States. New York is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are the Great Lakes part of an international border?\nFacts: The lakes are bordered on the north by Canada. The lakes are bordered on the south by United States. Canada and United States are two different countries.\nAnswer (True or False):", "ct_subject": "The lakes are bordered on the north by Canada. The lakes are bordered on the south by United States. Canada and United States are two different countries.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.12101086"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "0.98290116"], [" of", "0.99999785"], [" Chicago", "0.93083215"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Chicago", "1.0"], [" is", "1.0"], [" the", "0.9999887"], [" largest", "0.45673838"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [".", "1.0"], [" Chicago", "0.96575814"], [" is", "1.0"], [" the", "0.9999999"], [" name", "0.8769608"], [" of", "1.0"], [" the", "0.988064"], [" city", "1.0"], [" of", "0.9960829"], [" Chicago", "0.9738763"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "0.9999124"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" New", "1.0"], [" York", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [".", "1.0"], [" New", "1.0"], [" York", "1.0"], [" is", "1.0"]]}, "327": {"qid": "98f8a80a3a83d0951176", "term": "Curiosity (rover)", "description": "American robotic rover exploring the crater Gale on Mars", "question": "Can Curiosity (rover) kill a cat?", "answer": true, "facts": ["Cats weigh on average between 7 to 10 pounds.", "Curiosity (rover), a space vehicle that explores Mars, weighs 1,982 pounds.", "As mass falls, it picks up acceleration and adds to the force of impact."], "decomposition": ["How much does a cat weigh?", "How much does Curiosity (rover) weigh?", "Is #2 more than #1?"], "evidence": [[[["Cat-28"], "no_evidence"], [["Curiosity (rover)-33"]], ["operation"]], [[["Cat-29"]], [["Curiosity (rover)-33"]], ["operation"]], [[["Meow (cat)-2"], "no_evidence"], [["Curiosity (rover)-6"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the largest number of people that can fit into a single room?\nFacts: The largest number of people that can fit into a single room is 2,000.\nAnswer (True or False): True\nQuestion: What is the largest number of people that can fit into a single room", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can Curiosity (rover) kill a cat?\nFacts: Cats weigh on average between 7 to 10 pounds. Curiosity (rover), a space vehicle that explores Mars, weighs 1,982 pounds. As mass falls, it picks up acceleration and adds to the force of impact.\nAnswer (True or False):", "ct_subject": "Cats weigh on average between 7 to 10 pounds. Curiosity (rover), a space vehicle that explores Mars, weighs 1,982 pounds. As mass falls, it picks up acceleration and adds to the force of impact.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99994457"], [" is", "1.0"], [" the", "1.0"], [" largest", "0.48472443"], [" number", "0.99998915"], [" of", "1.0"], [" people", "1.0"], [" that", "0.9999999"], [" can", "0.899442"], [" fit", "1.0"], [" into", "0.6898988"], [" a", "1.0"], [" single", "0.99999404"], [" room", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999994"], [" largest", "1.0"], [" number", "0.98324996"], [" of", "1.0"], [" people", "1.0"], [" that", "1.0"], [" can", "1.0"], [" fit", "1.0"], [" into", "1.0"], [" a", "1.0"], [" single", "1.0"], [" room", "1.0"], [" is", "1.0"], [" 2", "0.9982216"], [",", "1.0"], ["000", "0.99993074"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" that", "1.0"], [" can", "1.0"], [" fit", "1.0"], [" into", "1.0"], [" a", "1.0"], [" single", "1.0"], [" room", "1.0"]]}, "328": {"qid": "997096ba85eb878a4fd3", "term": "Eiffel Tower", "description": "Tower located on the Champ de Mars in Paris, France", "question": "Was King Kong climbing at a higher altitude than Eiffel Tower visitors?", "answer": true, "facts": ["The Eiffel Tower is 984 ft high, and the visitor platform is 906 ft high.", "King Kong climbed up to the top of the Empire State Building.", "The Empire State Building is 1230 ft high."], "decomposition": ["How high is the visitor platform at the Eiffel Tower?", "What is the height of the Empire State Building?", "Is #2 higher than #1?"], "evidence": [[[["Eiffel Tower-4"]], [["Empire State Building-1"]], ["operation"]], [[["Eiffel Tower-4"]], [["Empire State Building-1", "Empire State Building-28"]], ["operation"]], [[["Eiffel Tower-4"]], [["Empire State Building-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Titanic sink because of a mechanical failure?\nFacts: The Titanic sank because of a mechanical failure.\nAnswer (True or False): True\nQuestion: Did the Titanic sink because of a mechanical failure?\nFacts: The Titanic sank because of a mechanical failure.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was King Kong climbing at a higher altitude than Eiffel Tower visitors?\nFacts: The Eiffel Tower is 984 ft high, and the visitor platform is 906 ft high. King Kong climbed up to the top of the Empire State Building. The Empire State Building is 1230 ft high.\nAnswer (True or False):", "ct_subject": "The Eiffel Tower is 984 ft high, and the visitor platform is 906 ft high. King Kong climbed up to the top of the Empire State Building. The Empire State Building is 1230 ft high.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9729003"], [" the", "1.0"], [" Titanic", "0.9997002"], [" sink", "1.0"], [" because", "0.99999905"], [" of", "0.9999869"], [" a", "0.09694465"], [" mechanical", "0.9435028"], [" failure", "0.9999871"], ["?", "0.9523586"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Titanic", "1.0"], [" sank", "0.76243794"], [" because", "1.0"], [" of", "1.0"], [" a", "1.0"], [" mechanical", "1.0"], [" failure", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99997187"], [" the", "1.0"], [" Titanic", "1.0"], [" sink", "1.0"], [" because", "1.0"], [" of", "1.0"], [" a", "1.0"], [" mechanical", "1.0"], [" failure", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Titanic", "1.0"], [" sank", "1.0"], [" because", "1.0"], [" of", "1.0"], [" a", "1.0"], [" mechanical", "1.0"], [" failure", "1.0"], [".", "1.0"]]}, "329": {"qid": "24d689b8a99b3a83b60c", "term": "Constitution of the United States", "description": "Supreme law of the United States of America", "question": "Would Constitution of the United States paper offend PETA?", "answer": true, "facts": ["The Constitution of the United States is written on parchment.", "Parchment is  writing material made from specially prepared untanned skins of animals.", "PETA is an organization that advocates for the ethical treatment of animals."], "decomposition": ["What is the US Constitution written on?", "What is #1 made of?", "What does PETA hate?", "How is #2 acquired?", "Is there an overlap between #3 and #4?"], "evidence": [[[["Vellum-1"], "no_evidence"], [["Vellum-1"]], [["People for the Ethical Treatment of Animals-1"]], [["Vellum-7"]], ["operation"]], [[["Constitution of the United States-2"]], [["Parchment-1"]], [["Killing of animals-28"]], [["Parchment-1"]], ["operation"]], [[["U.S. Constitution hemp paper hoax-1"]], ["operation"], [["Lauren Anderson (model)-3"], "no_evidence"], [["Hot dog-12"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"pig\" and a \"pork\"?\nFacts: Pigs are mammals. Pigs are not birds. Pigs are not fish. Pigs are not reptiles. Pigs are not insects. Pigs are not insects. Pigs are not mammals. Pigs are not reptiles. Pigs are not fish. Pigs", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Constitution of the United States paper offend PETA?\nFacts: The Constitution of the United States is written on parchment. Parchment is writing material made from specially prepared untanned skins of animals. PETA is an organization that advocates for the ethical treatment of animals.\nAnswer (True or False):", "ct_subject": "The Constitution of the United States is written on parchment. Parchment is writing material made from specially prepared untanned skins of animals. PETA is an organization that advocates for the ethical treatment of animals.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999932"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.78999835"], [" between", "1.0"], [" a", "0.9999999"], [" \"", "0.9999982"], ["p", "0.5411398"], ["ig", "0.9997701"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["p", "1.0"], ["ork", "0.883453"], ["\"?", "0.9999995"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Pigs", "0.9999876"], [" are", "0.9999999"], [" mammals", "0.9996412"], [".", "0.99998164"], [" Pigs", "0.99944097"], [" are", "1.0"], [" not", "0.99997425"], [" birds", "0.9732947"], [".", "1.0"], [" Pigs", "1.0"], [" are", "1.0"], [" not", "1.0"], [" fish", "0.9334062"], [".", "1.0"], [" Pigs", "1.0"], [" are", "1.0"], [" not", "1.0"], [" reptiles", "1.0"], [".", "1.0"], [" Pigs", "1.0"], [" are", "1.0"], [" not", "1.0"], [" insects", "0.9999976"], [".", "1.0"], [" Pigs", "1.0"], [" are", "1.0"], [" not", "1.0"], [" insects", "0.97200495"], [".", "1.0"], [" Pigs", "1.0"], [" are", "1.0"], [" not", "1.0"], [" mammals", "0.9999976"], [".", "1.0"], [" Pigs", "1.0"], [" are", "1.0"], [" not", "1.0"], [" reptiles", "1.0"], [".", "1.0"], [" Pigs", "1.0"], [" are", "1.0"], [" not", "1.0"], [" fish", "0.99993455"], [".", "1.0"], [" Pigs", "1.0"]]}, "330": {"qid": "363cf9e4f1ab45e05a4b", "term": "Cream", "description": "Dairy product", "question": "If you bottle your own milk, would there be cream on top of it?", "answer": true, "facts": ["Milk that has been bottled straight from a cow has not been homogenized. ", "Homogenization causes the fats in milk to become emulsified.", "Non-homogenized milk will feature fats that separate and float to the top.", "The fats in non-homogenized milk are cream."], "decomposition": ["When milk is taken directly from a cow, what appearance and position do the fats assume?", "Is #1 cream and at the top?"], "evidence": [[[["Cream-1"]], ["operation"]], [[["Milk-59"]], ["operation"]], [[["Milk-53", "Milk-59"]], [["Milk-59"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: If you have a large family, how many children do you have?\nFacts: If you have a large family, you will have more children than you can support.  If you have a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: If you bottle your own milk, would there be cream on top of it?\nFacts: Milk that has been bottled straight from a cow has not been homogenized.  Homogenization causes the fats in milk to become emulsified. Non-homogenized milk will feature fats that separate and float to the top. The fats in non-homogenized milk are cream.\nAnswer (True or False):", "ct_subject": "Milk that has been bottled straight from a cow has not been homogenized.  Homogenization causes the fats in milk to become emulsified. Non-homogenized milk will feature fats that separate and float to the top. The fats in non-homogenized milk are cream.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" If", "0.90728396"], [" you", "1.0"], [" have", "0.24550368"], [" a", "1.0"], [" large", "0.9884759"], [" family", "0.48137128"], [",", "1.0"], [" how", "0.97936386"], [" many", "0.999915"], [" children", "1.0"], [" do", "0.9999598"], [" you", "1.0"], [" have", "0.999997"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" If", "0.9991443"], [" you", "1.0"], [" have", "1.0"], [" a", "1.0"], [" large", "1.0"], [" family", "1.0"], [",", "1.0"], [" you", "1.0"], [" will", "0.39272577"], [" have", "1.0"], [" more", "0.92420596"], [" children", "0.9999924"], [" than", "1.0"], [" you", "1.0"], [" can", "1.0"], [" support", "0.9999987"], [".", "1.0"], [" ", "0.99848855"], [" If", "0.99999917"], [" you", "1.0"], [" have", "1.0"], [" a", "1.0"]]}, "331": {"qid": "2b69557b4f4f87193f1c", "term": "Nine Inch Nails", "description": "American industrial rock band", "question": "Is Nine Inch Nails's lead singer associated with David Lynch?", "answer": true, "facts": ["David Lynch is a director that created the television show Twin Peaks.", "Trent Reznor is the lead singer of Nine Inch Nails.", "Trent Reznor appeared on Twin Peaks: The Return in 2017.", "David Lynch directed the music video for Nine Inch Nail's Came Back Haunted."], "decomposition": ["Who is the lead singer of Nine Inch Nails?", "What works has #1 appeared in?", "What are the works of David Lynch?", "Is there overlap between #2 and #3?"], "evidence": [[[["Trent Reznor-1"]], [["Trent Reznor-29"]], [["Trent Reznor-29"]], ["operation"]], [[["Trent Reznor-1"]], [["Trent Reznor-29"], "no_evidence"], [["David Lynch-1"], "no_evidence"], ["operation"]], [[["Nine Inch Nails-1"]], [["Part 8 (Twin Peaks)-13", "Trent Reznor-3"], "no_evidence"], [["David Lynch-1"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the band Nine Inch Nails a reference to the 9/11 attacks?\nFacts: The name of the band Nine Inch Nails is a reference to the 9/11 attacks. The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Nine Inch Nails's lead singer associated with David Lynch?\nFacts: David Lynch is a director that created the television show Twin Peaks. Trent Reznor is the lead singer of Nine Inch Nails. Trent Reznor appeared on Twin Peaks: The Return in 2017. David Lynch directed the music video for Nine Inch Nail's Came Back Haunted.\nAnswer (True or False):", "ct_subject": "David Lynch is a director that created the television show Twin Peaks. Trent Reznor is the lead singer of Nine Inch Nails. Trent Reznor appeared on Twin Peaks: The Return in 2017. David Lynch directed the music video for Nine Inch Nail's Came Back Haunted.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999993"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" band", "0.999977"], [" Nine", "0.99999785"], [" In", "1.0"], ["ch", "1.0"], [" N", "1.0"], ["ails", "1.0"], [" a", "1.0"], [" reference", "0.9999919"], [" to", "1.0"], [" the", "1.0"], [" 9", "0.97973734"], ["/", "0.9999939"], ["11", "1.0"], [" attacks", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.7289111"], [" name", "0.9999894"], [" of", "0.99983335"], [" the", "1.0"], [" band", "1.0"], [" Nine", "0.9997583"], [" In", "1.0"], ["ch", "1.0"], [" N", "1.0"], ["ails", "1.0"], [" is", "1.0"], [" a", "1.0"], [" reference", "1.0"], [" to", "1.0"], [" the", "1.0"], [" 9", "1.0"], ["/", "1.0"], ["11", "1.0"], [" attacks", "1.0"], [".", "1.0"], [" The", "0.9838373"]]}, "332": {"qid": "c5d50d6e2d1747880aed", "term": "Black Sea", "description": "Marginal sea of the Atlantic Ocean between Europe and Asia", "question": "Do people put creatures from the Black Sea on their pizza?", "answer": true, "facts": ["Pizza toppings include pepperoni, sausage, bacon, meatball, and anchovies.", "The Black Sea is home to many animals including dogfish, jellyfish, and anchovies."], "decomposition": ["What creatures are native to the Black Sea?", "What are common pizza toppings?", "Do any of #1 appear in #2?"], "evidence": [[[["Zebra mussel-14"]], [["Pizza-1"]], ["operation"]], [[["Black Sea-38", "Black Sea-41", "Black Sea-64"]], [["Pizza-1"]], ["operation"]], [[["Anchovy-2"], "no_evidence"], [["Anchovies as food-3"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the capital of the state of New York?\nFacts: The city of New York is the capital of the state of New York.\nAnswer (True or False): True\nQuestion: What is the name of the city in the United States that is the capital", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do people put creatures from the Black Sea on their pizza?\nFacts: Pizza toppings include pepperoni, sausage, bacon, meatball, and anchovies. The Black Sea is home to many animals including dogfish, jellyfish, and anchovies.\nAnswer (True or False):", "ct_subject": "Pizza toppings include pepperoni, sausage, bacon, meatball, and anchovies. The Black Sea is home to many animals including dogfish, jellyfish, and anchovies.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99500066"], [" is", "1.0"], [" the", "1.0"], [" name", "0.6627389"], [" of", "1.0"], [" the", "1.0"], [" city", "0.99811447"], [" in", "0.08727151"], [" the", "0.9999101"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "0.99147344"], [" the", "1.0"], [" capital", "0.80920535"], [" of", "1.0"], [" the", "1.0"], [" state", "0.99940264"], [" of", "1.0"], [" New", "0.8242696"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9341469"], [" city", "0.8638014"], [" of", "0.99998033"], [" New", "1.0"], [" York", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"]]}, "333": {"qid": "97f35d567ddd31b7da6c", "term": "Wool", "description": "Textile fibre from the hair of sheep or other mammals", "question": "Should wool be hand washed only?", "answer": true, "facts": ["Felting is a process through which wool is shaped and shrunken through agitation in soapy water. ", "Felting will often occur if you put a wool item in the washer."], "decomposition": ["What is felting?", "Will washing wool cause #1?"], "evidence": [[[["Felt-8"]], [["Felt-9"], "operation"]], [[["Felt-8", "Wool-5"], "no_evidence"], ["operation"]], [[["Sense-45"]], [["Wool-3"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"wool sweater\" and a \"wool scarf\"?\nFacts: A wool sweater is a garment made of wool. A wool scarf is a garment made of wool and other materials.\nAnswer (True or False): True\nQuestion: What is the difference between a \"wool sweater\" and a \"wool", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Should wool be hand washed only?\nFacts: Felting is a process through which wool is shaped and shrunken through agitation in soapy water.  Felting will often occur if you put a wool item in the washer.\nAnswer (True or False):", "ct_subject": "Felting is a process through which wool is shaped and shrunken through agitation in soapy water.  Felting will often occur if you put a wool item in the washer.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.47429785"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["w", "0.68875444"], ["ool", "0.9999958"], [" sweater", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["w", "1.0"], ["ool", "1.0"], [" scarf", "0.9969969"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999969"], [" wool", "1.0"], [" sweater", "1.0"], [" is", "1.0"], [" a", "1.0"], [" garment", "0.99862945"], [" made", "0.99893016"], [" of", "0.99999917"], [" wool", "1.0"], [".", "1.0"], [" A", "1.0"], [" wool", "1.0"], [" scarf", "1.0"], [" is", "1.0"], [" a", "1.0"], [" garment", "1.0"], [" made", "1.0"], [" of", "1.0"], [" wool", "1.0"], [" and", "0.99213094"], [" other", "0.69631624"], [" materials", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["w", "1.0"], ["ool", "1.0"], [" sweater", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["w", "1.0"], ["ool", "1.0"]]}, "334": {"qid": "5a20b692ab755caf4ae9", "term": "Paratrooper", "description": "Military parachutists functioning as part of an airborne force", "question": "Are paratroopers good at mountain rescue?", "answer": true, "facts": ["A paratrooper is a member of a military unit that deploys parachutes. ", "A PJ is the acronym name for a military parachute jumper.", "PJs are an elite mountain rescue unit. "], "decomposition": ["What military unit do paratroopers belong to?", "Do #1 use equipment that makes them suitable for mountain rescue?"], "evidence": [[[["Paratrooper-1"]], [["Paratrooper-2"], "no_evidence"]], [[["Paratrooper-1"]], ["no_evidence", "operation"]], [[["Paratrooper-74"]], [["Paratrooper-1"], "no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any other countries that have a population of over 1 billion?\nFacts: There are several countries that have a population of over 1 billion. \nAnswer (True or False): True\nQuestion: Are there any other countries that have a population of over 1 billion?\nFacts: There are several countries that have", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are paratroopers good at mountain rescue?\nFacts: A paratrooper is a member of a military unit that deploys parachutes.  A PJ is the acronym name for a military parachute jumper. PJs are an elite mountain rescue unit. \nAnswer (True or False):", "ct_subject": "A paratrooper is a member of a military unit that deploys parachutes.  A PJ is the acronym name for a military parachute jumper. PJs are an elite mountain rescue unit. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.9423132"], [" there", "1.0"], [" any", "0.9725528"], [" other", "0.99985015"], [" countries", "0.986155"], [" that", "1.0"], [" have", "0.9999999"], [" a", "1.0"], [" population", "0.9999862"], [" of", "0.8916179"], [" over", "0.9975713"], [" 1", "0.9995447"], [" billion", "1.0"], ["?", "0.99998033"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "0.9999069"], [" are", "1.0"], [" several", "0.9151735"], [" countries", "1.0"], [" that", "0.99832195"], [" have", "1.0"], [" a", "1.0"], [" population", "1.0"], [" of", "1.0"], [" over", "1.0"], [" 1", "1.0"], [" billion", "1.0"], [".", "1.0"], [" ", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.99997234"], [" there", "1.0"], [" any", "1.0"], [" other", "0.5279668"], [" countries", "1.0"], [" that", "1.0"], [" have", "1.0"], [" a", "1.0"], [" population", "1.0"], [" of", "1.0"], [" over", "1.0"], [" 1", "1.0"], [" billion", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "1.0"], [" are", "1.0"], [" several", "1.0"], [" countries", "1.0"], [" that", "1.0"], [" have", "1.0"]]}, "335": {"qid": "1517e4258ddaf94d85c7", "term": "Berlin University of the Arts", "description": "public art school in Berlin, Germany", "question": "Is the Berlin University of the Arts a Baroque period relic?", "answer": true, "facts": ["The Berlin University of the Arts is a German school created in 1696.", "The Baroque period lasted in Europe from 1600 to 1750."], "decomposition": ["In which year was the Berlin University of the Arts established?", "When the Baroque period start and end?", "Does #1 fall within #2?"], "evidence": [[[["Berlin University of the Arts-5", "Prussian Academy of Arts-1"]], [["17th century-1", "Baroque-1"]], ["operation"]], [[["Berlin University of the Arts-5"], "no_evidence"], [["Baroque music-1", "Baroque-1"]], ["no_evidence", "operation"]], [[["Berlin University of the Arts-5"]], [["Baroque-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the Berlin University of the Arts a Baroque period relic?\nFacts: The Berlin University of the Arts is a German school created in 1696. The Baroque period lasted in Europe from 1600 to 1750.\nAnswer (True or False): True\nQuestion: Is the Berlin University of the Arts a Baroque period relic?\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is the Berlin University of the Arts a Baroque period relic?\nFacts: The Berlin University of the Arts is a German school created in 1696. The Baroque period lasted in Europe from 1600 to 1750.\nAnswer (True or False):", "ct_subject": "The Berlin University of the Arts is a German school created in 1696. The Baroque period lasted in Europe from 1600 to 1750.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99990463"], [" the", "1.0"], [" Berlin", "1.0"], [" University", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Arts", "1.0"], [" a", "1.0"], [" Bar", "1.0"], ["o", "1.0"], ["que", "1.0"], [" period", "1.0"], [" relic", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Berlin", "1.0"], [" University", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Arts", "1.0"], [" is", "1.0"], [" a", "1.0"], [" German", "1.0"], [" school", "1.0"], [" created", "1.0"], [" in", "1.0"], [" 16", "1.0"], ["96", "1.0"], [".", "1.0"], [" The", "1.0"], [" Bar", "1.0"], ["o", "1.0"], ["que", "1.0"], [" period", "1.0"], [" lasted", "1.0"], [" in", "1.0"], [" Europe", "1.0"], [" from", "1.0"], [" 1600", "1.0"], [" to", "1.0"], [" 17", "1.0"], ["50", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" Berlin", "1.0"], [" University", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Arts", "1.0"], [" a", "1.0"], [" Bar", "1.0"], ["o", "1.0"], ["que", "1.0"], [" period", "1.0"], [" relic", "1.0"], ["?", "1.0"], ["\n", "1.0"]]}, "336": {"qid": "2847095b289d1f4b48fd", "term": "Rabbi", "description": "teacher of Torah in Judaism", "question": "Can a rabbi save the soul of a Christian?", "answer": true, "facts": ["Rabbis are spiritual leaders of the Jewish community.", "Baptism is a necessary Christian ritual for salvation.", "Any non-Christian can perform an emergency baptism if someone's life is in danger."], "decomposition": ["What are Rabbis's role in the Jewish community?", "What is a Christian ritual that is needed for salvation?", "In an emergency, can #1 perform #2?"], "evidence": [[[["Rabbi-1"]], [["Salvation in Christianity-5"]], [["Salvation in Christianity-2"]]], [[["Rabbi-1"]], [["Baptism-2", "Last rites-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Rabbi-1"]], [["Baptism-1"]], [["Baptism-59"], "no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a Jew and a Christian?\nFacts: Jews are a religious group that believes in the one true God. Christians are a religious group that believes in the Trinity.\nAnswer (True or False): True\nQuestion: What is the difference between a Jew and a Christian?\nFacts: Jews are a religious", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can a rabbi save the soul of a Christian?\nFacts: Rabbis are spiritual leaders of the Jewish community. Baptism is a necessary Christian ritual for salvation. Any non-Christian can perform an emergency baptism if someone's life is in danger.\nAnswer (True or False):", "ct_subject": "Rabbis are spiritual leaders of the Jewish community. Baptism is a necessary Christian ritual for salvation. Any non-Christian can perform an emergency baptism if someone's life is in danger.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9636769"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "0.99999976"], [" Jew", "0.82076627"], [" and", "1.0"], [" a", "1.0"], [" Christian", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Jews", "0.4842502"], [" are", "0.9999844"], [" a", "0.9740916"], [" religious", "0.99999976"], [" group", "0.9999999"], [" that", "1.0"], [" believes", "0.999736"], [" in", "1.0"], [" the", "0.99910647"], [" one", "0.9937232"], [" true", "0.9999726"], [" God", "1.0"], [".", "1.0"], [" Christians", "1.0"], [" are", "0.99970466"], [" a", "1.0"], [" religious", "1.0"], [" group", "1.0"], [" that", "1.0"], [" believes", "1.0"], [" in", "1.0"], [" the", "0.8846884"], [" Trinity", "0.061297525"], [".", "0.9999826"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" Jew", "1.0"], [" and", "1.0"], [" a", "1.0"], [" Christian", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Jews", "1.0"], [" are", "1.0"], [" a", "1.0"], [" religious", "1.0"]]}, "337": {"qid": "5f89881767cac98c56d6", "term": "Seven Years' War", "description": "Global conflict between 1756 and 1763", "question": "Could the Austrian casualties from Seven Years' War fit in Indianapolis Motor Speedway?", "answer": true, "facts": ["There were 373,588 Austrian casualties during the Seven Years' War.", "The infield seating at the Indianapolis Motor Speedway raises capacity to an approximate 400,000 people."], "decomposition": ["How many casualties did the Austrian's have in the Seven Year War?", "What is the seating capacity for the Indianapolis Motor Speedway?", "Is #1 less than #2?"], "evidence": [[[["Battle of Prague (1757)-1"], "no_evidence"], [["Indianapolis Motor Speedway-3"]], ["operation"]], [["no_evidence"], [["Indianapolis Motor Speedway-3"]], ["operation"]], [[["Seven Years' War-36"], "no_evidence"], [["Indianapolis Motor Speedway-3"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the largest number of people that can fit in the Grand Canyon?\nFacts: The Grand Canyon is the largest canyon in the world. The Grand Canyon is the largest canyon in the world.\nAnswer (True or False): True\nQuestion: What is the largest number of people that can fit in the Grand Canyon?\nF", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could the Austrian casualties from Seven Years' War fit in Indianapolis Motor Speedway?\nFacts: There were 373,588 Austrian casualties during the Seven Years' War. The infield seating at the Indianapolis Motor Speedway raises capacity to an approximate 400,000 people.\nAnswer (True or False):", "ct_subject": "There were 373,588 Austrian casualties during the Seven Years' War. The infield seating at the Indianapolis Motor Speedway raises capacity to an approximate 400,000 people.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9998274"], [" is", "1.0"], [" the", "1.0"], [" largest", "0.5760967"], [" number", "0.99999905"], [" of", "1.0"], [" people", "1.0"], [" that", "0.9999993"], [" can", "0.9999876"], [" fit", "1.0"], [" in", "0.99995375"], [" the", "0.9999908"], [" Grand", "0.99994385"], [" Canyon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999466"], [" Grand", "1.0"], [" Canyon", "1.0"], [" is", "1.0"], [" the", "0.99998915"], [" largest", "1.0"], [" canyon", "0.99999964"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [".", "1.0"], [" The", "0.776638"], [" Grand", "1.0"], [" Canyon", "1.0"], [" is", "1.0"], [" the", "0.8509383"], [" largest", "1.0"], [" canyon", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [".", "1.0"], ["\n", "0.99862015"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" that", "1.0"], [" can", "1.0"], [" fit", "1.0"], [" in", "1.0"], [" the", "1.0"], [" Grand", "1.0"], [" Canyon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"]]}, "338": {"qid": "3a810d24f086f07f71bf", "term": "Butler", "description": "male domestic worker in charge of all the male household staff", "question": "Did the butler Eugene Allen retire the same year a centuries-old war ended?", "answer": true, "facts": ["Eugene Allen was a butler at the White House for 34 years until 1986", "The United Kingdom and the Kingdom of the Netherlands ended the Three Hundred and Thirty Five Years' Warnin 1986"], "decomposition": ["In what year did Eugene Allen retire?", "Which war ended in #1?", "How many years did #2 last?", "Is #3 greater than 100?"], "evidence": [[[["Eugene Allen-6"]], [["Three Hundred and Thirty Five Years' War-1"]], [["Three Hundred and Thirty Five Years' War-1"]], ["operation"]], [[["Eugene Allen-1"]], [["Three Hundred and Thirty Five Years' War-1"]], [["Three Hundred and Thirty Five Years' War-1"]], ["operation"]], [[["Eugene Allen-1"]], [["Three Hundred and Thirty Five Years' War-1"]], [["Three Hundred and Thirty Five Years' War-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the butler Eugene Allen retire the same year a centuries-old war ended?\nFacts: Eugene Allen was a butler at the White House for 34 years until 1986 The United Kingdom and the Kingdom of the Netherlands ended the Three Hundred and Thirty Five Years' Warnin 1986\nAnswer (True or False", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did the butler Eugene Allen retire the same year a centuries-old war ended?\nFacts: Eugene Allen was a butler at the White House for 34 years until 1986 The United Kingdom and the Kingdom of the Netherlands ended the Three Hundred and Thirty Five Years' Warnin 1986\nAnswer (True or False):", "ct_subject": "Eugene Allen was a butler at the White House for 34 years until 1986 The United Kingdom and the Kingdom of the Netherlands ended the Three Hundred and Thirty Five Years' Warnin 1986", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.5560061"], [" the", "1.0"], [" but", "0.9943545"], ["ler", "1.0"], [" Eugene", "1.0"], [" Allen", "1.0"], [" retire", "1.0"], [" the", "1.0"], [" same", "1.0"], [" year", "1.0"], [" a", "1.0"], [" centuries", "1.0"], ["-", "1.0"], ["old", "1.0"], [" war", "1.0"], [" ended", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Eugene", "1.0"], [" Allen", "1.0"], [" was", "1.0"], [" a", "1.0"], [" but", "1.0"], ["ler", "1.0"], [" at", "1.0"], [" the", "1.0"], [" White", "1.0"], [" House", "1.0"], [" for", "1.0"], [" 34", "1.0"], [" years", "1.0"], [" until", "1.0"], [" 1986", "1.0"], [" The", "1.0"], [" United", "1.0"], [" Kingdom", "1.0"], [" and", "1.0"], [" the", "1.0"], [" Kingdom", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Netherlands", "1.0"], [" ended", "1.0"], [" the", "1.0"], [" Three", "1.0"], [" Hundred", "1.0"], [" and", "1.0"], [" Thirty", "1.0"], [" Five", "1.0"], [" Years", "1.0"], ["'", "1.0"], [" Warn", "0.9999962"], ["in", "1.0"], [" 1986", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"]]}, "339": {"qid": "e4945001c03fa74e1cc9", "term": "Hanuman", "description": "The divine monkey companion of Rama in Hindu mythology", "question": "Does Hanuman have some of the same duties as Athena?", "answer": true, "facts": ["Hanuman, the divine Hindu monkey, is a god of strength, knowledge, and victory.", "Athena was the Greek goddess of war, and wisdom."], "decomposition": [" Hanuman said to be the deity of what subjects?", "Athena said to be the deity of what subjects?", "Is at least one subject listed in both #1 and #2?"], "evidence": [[[["Hanuman-14"]], [["Athena-1"]], ["operation"]], [[["Hanuman-2"], "no_evidence"], [["Athena-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Hanuman-2"]], [["Athena-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first man to walk on the moon?\nFacts: Neil Armstrong was the first man to walk on the moon.\nAnswer (True or False): True\nQuestion: What is the name of the first man to walk on the moon?\nFacts: Neil Armstrong was the first man to walk on the moon.\nAnswer (", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Hanuman have some of the same duties as Athena?\nFacts: Hanuman, the divine Hindu monkey, is a god of strength, knowledge, and victory. Athena was the Greek goddess of war, and wisdom.\nAnswer (True or False):", "ct_subject": "Hanuman, the divine Hindu monkey, is a god of strength, knowledge, and victory. Athena was the Greek goddess of war, and wisdom.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9713926"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9997303"], [" of", "1.0"], [" the", "1.0"], [" first", "0.99987245"], [" man", "0.9996124"], [" to", "0.9731501"], [" walk", "0.99999917"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Neil", "0.9995221"], [" Armstrong", "1.0"], [" was", "0.99999774"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "0.9999999"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Neil", "1.0"], [" Armstrong", "1.0"], [" was", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"]]}, "340": {"qid": "d82193894aa1c12fbc40", "term": "Nissan", "description": "Japanese automobile manufacturer", "question": "Do workers at Nissan's headquarters eat with chopsticks?", "answer": true, "facts": ["Nissan's headquarters are located in Yokohama, Japan.", "It is customary to eat with chopsticks in East Asian countries.", "Japan is a country in East Asia."], "decomposition": ["Where is Nissan's headquarters located?", "Do people living in #1 usually eat with chopsticks?"], "evidence": [[[["Nissan-1", "Yokohama-1"]], [["Chopsticks-1"]]], [[["Nissan-1"]], [["Chopsticks-1", "Etiquette in Japan-21"]]], [[["Nissan-3"]], [["Chopsticks-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the capital of the state of New York?\nFacts: The city in the United States that is the capital of the state of New York is Albany.\nAnswer (True or False): True\nQuestion: What is the name of the city in the United States that is the capital of", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do workers at Nissan's headquarters eat with chopsticks?\nFacts: Nissan's headquarters are located in Yokohama, Japan. It is customary to eat with chopsticks in East Asian countries. Japan is a country in East Asia.\nAnswer (True or False):", "ct_subject": "Nissan's headquarters are located in Yokohama, Japan. It is customary to eat with chopsticks in East Asian countries. Japan is a country in East Asia.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99993217"], [" is", "1.0"], [" the", "1.0"], [" name", "0.999752"], [" of", "1.0"], [" the", "1.0"], [" city", "0.7754889"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "0.9826733"], [" the", "1.0"], [" capital", "0.9992878"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" New", "0.26800033"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.984637"], [" city", "0.9999993"], [" in", "0.16802546"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" is", "1.0"], [" Albany", "0.99370664"], [".", "0.9998975"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"]]}, "341": {"qid": "f606b2166555bf5e6bfd", "term": "Cream", "description": "Dairy product", "question": "If someone is lactose intolerant, do they have to avoid cream?", "answer": true, "facts": ["People with lactose intolerance are unable to fully digest the sugar (lactose) in milk.", "Cream is a dairy product composed of the higher-fat layer skimmed from the top of milk before homogenization", "Cream contains milk."], "decomposition": ["What do people who are lactose intolerant have to avoid?", "Does cream contain #1?"], "evidence": [[[["Lactose intolerance-1"]], [["Cream-1", "Milk-51"]]], [[["Lactose intolerance-1"]], [["Cream-1"], "operation"]], [[["Lactose intolerance-1"]], [["Cream-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: If you have a cold, do you have to take a cold shower?\nFacts: Cold showers are a natural way to relieve the symptoms of a cold. Cold showers are a natural way to relieve the symptoms of a cold.\nAnswer (True or False):", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: If someone is lactose intolerant, do they have to avoid cream?\nFacts: People with lactose intolerance are unable to fully digest the sugar (lactose) in milk. Cream is a dairy product composed of the higher-fat layer skimmed from the top of milk before homogenization Cream contains milk.\nAnswer (True or False):", "ct_subject": "People with lactose intolerance are unable to fully digest the sugar (lactose) in milk. Cream is a dairy product composed of the higher-fat layer skimmed from the top of milk before homogenization Cream contains milk.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" If", "0.9993748"], [" you", "0.9955865"], [" have", "0.9974124"], [" a", "1.0"], [" cold", "1.0"], [",", "1.0"], [" do", "0.99999917"], [" you", "1.0"], [" have", "0.9566544"], [" to", "1.0"], [" take", "0.873554"], [" a", "0.9528375"], [" cold", "1.0"], [" shower", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Cold", "1.0"], [" showers", "1.0"], [" are", "1.0"], [" a", "1.0"], [" natural", "0.9999701"], [" way", "0.5338083"], [" to", "1.0"], [" relieve", "0.82907754"], [" the", "0.99999964"], [" symptoms", "1.0"], [" of", "1.0"], [" a", "1.0"], [" cold", "1.0"], [".", "1.0"], [" Cold", "0.99983406"], [" showers", "1.0"], [" are", "1.0"], [" a", "0.05848128"], [" natural", "0.9332941"], [" way", "1.0"], [" to", "1.0"], [" relieve", "1.0"], [" the", "1.0"], [" symptoms", "1.0"], [" of", "1.0"], [" a", "1.0"], [" cold", "1.0"], [".", "1.0"], ["\n", "0.76973"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"]]}, "342": {"qid": "f3584715f2b8d7894a3a", "term": "Edward Snowden", "description": "American whistleblower and former National Security Agency contractor", "question": "Could Edward Snowden join MENSA?", "answer": true, "facts": ["Snowden scored above 145 on two separate IQ tests.", "The minimum accepted IQ score for MENSA on the Stanford\u2013Binet is 132, while for the Cattell it is 148."], "decomposition": ["What is the minimum accepted IQ score to be admitted to MENSA?", "What is Edward Snowden's IQ?", "Is #2 greater than or equal to #1?"], "evidence": [[[["Mensa International-5"]], [["Edward Snowden-5"]], ["operation"]], [[["Mensa International-5"]], [["Edward Snowden-5"]], [["Edward Snowden-5", "IQ Award-1"], "operation"]], [[["Mensa International-5"]], [["Edward Snowden-5"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the Earth round?\nFacts: The Earth is not round. It is a sphere.\nAnswer (True or False): True\nQuestion: Is the Earth flat?\nFacts: The Earth is not flat. It is a sphere.\nAnswer (True or False): True\nQuestion: Is the Earth round?\nFacts: The Earth", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could Edward Snowden join MENSA?\nFacts: Snowden scored above 145 on two separate IQ tests. The minimum accepted IQ score for MENSA on the Stanford\u2013Binet is 132, while for the Cattell it is 148.\nAnswer (True or False):", "ct_subject": "Snowden scored above 145 on two separate IQ tests. The minimum accepted IQ score for MENSA on the Stanford\u2013Binet is 132, while for the Cattell it is 148.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.12875728"], [" the", "0.9971426"], [" Earth", "0.96745545"], [" round", "0.9037624"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" not", "0.9999999"], [" round", "1.0"], [".", "1.0"], [" It", "0.7619967"], [" is", "1.0"], [" a", "1.0"], [" sphere", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99995446"], [" the", "0.99999976"], [" Earth", "1.0"], [" flat", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" not", "0.29636776"], [" flat", "1.0"], [".", "1.0"], [" It", "1.0"], [" is", "1.0"], [" a", "1.0"], [" sphere", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" Earth", "1.0"], [" round", "0.99854493"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Earth", "1.0"]]}, "343": {"qid": "b1dc0601af214a930eb2", "term": "French people", "description": "People from France", "question": "Were French people involved in the American Civil War?", "answer": true, "facts": ["The French General Lafayette allied with the American Revolutionaries.", "Lafayette's army scored several key victories for the rebels."], "decomposition": ["Which allies did the American Revolutionaries have during the war?", "Which of #1 scored many important victories for them?", "Are any of #2 French?"], "evidence": [[[["France in the American Revolutionary War-16"]], [["Major General Comte Jean de Rochambeau-1"]], [["Jean-Baptiste Donatien de Vimeur, comte de Rochambeau-1"], "operation"]], [[["George Washington in the American Revolution-10"], "no_evidence"], [["France in the American Revolutionary War-5"], "no_evidence"], ["operation"]], [[["American Revolutionary War-42"], "no_evidence"], [["Battle of the Chesapeake-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the French Revolutionaries have a secret society?\nFacts: The French Revolutionaries were not a secret society. The French Revolutionaries were a group of revolutionaries who were fighting for the rights of the people.\nAnswer (True or False): True\nQuestion: Did the French Revolutionaries have a secret society?\nFacts: The French Revolutionaries were not a secret society. The French Revolutionaries were a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Were French people involved in the American Civil War?\nFacts: The French General Lafayette allied with the American Revolutionaries. Lafayette's army scored several key victories for the rebels.\nAnswer (True or False):", "ct_subject": "The French General Lafayette allied with the American Revolutionaries. Lafayette's army scored several key victories for the rebels.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9640737"], [" the", "1.0"], [" French", "0.9360226"], [" Revolution", "1.0"], ["aries", "1.0"], [" have", "0.9997439"], [" a", "1.0"], [" secret", "1.0"], [" society", "1.0"], ["?", "0.99999917"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" French", "1.0"], [" Revolution", "1.0"], ["aries", "1.0"], [" were", "0.9999193"], [" not", "0.94075614"], [" a", "1.0"], [" secret", "1.0"], [" society", "1.0"], [".", "1.0"], [" The", "0.19528136"], [" French", "1.0"], [" Revolution", "1.0"], ["aries", "1.0"], [" were", "1.0"], [" a", "0.99992585"], [" group", "0.43979824"], [" of", "1.0"], [" revolutionaries", "0.96564543"], [" who", "1.0"], [" were", "0.99974436"], [" fighting", "0.9847888"], [" for", "1.0"], [" the", "0.9999987"], [" rights", "0.9994722"], [" of", "1.0"], [" the", "1.0"], [" people", "0.47203696"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999548"], [" the", "1.0"], [" French", "1.0"], [" Revolution", "1.0"], ["aries", "1.0"], [" have", "1.0"], [" a", "1.0"], [" secret", "1.0"], [" society", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" French", "1.0"], [" Revolution", "1.0"], ["aries", "1.0"], [" were", "1.0"], [" not", "1.0"], [" a", "1.0"], [" secret", "1.0"], [" society", "1.0"], [".", "1.0"], [" The", "1.0"], [" French", "1.0"], [" Revolution", "1.0"], ["aries", "1.0"], [" were", "1.0"], [" a", "1.0"]]}, "344": {"qid": "515231a39af241acbf14", "term": "Bulk carrier", "description": "merchant ship specially designed to transport unpackaged bulk cargo", "question": "Does Southwest Airlines use bulk carriers?", "answer": true, "facts": ["Southwest Airlines requires jet fuel, which is brought in by bulk carrier transport.", "Southwest Airlines requires glycol for de-icing their planes, which is brought in by bulk transport."], "decomposition": ["Which vehicles does Southwest Airlines use to provide their services?", "What are some common products needed for the running and maintenance of #1?", "Would Southwest Airlines require #2 in quantities large enough for bulk carriers?"], "evidence": [[[["Southwest Airlines fleet-1"]], [["Aircraft maintenance-16", "Aviation fuel-20"]], [["Boeing 737 Next Generation-41"], "operation"]], [[["Southwest Airlines fleet-1", "Southwest Airlines-2"], "no_evidence"], ["no_evidence"], [["Bulk carrier-1"], "no_evidence", "operation"]], [[["Southwest Airlines fleet-1"]], ["no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"bulk carrier\" and a \"jet fuel carrier\"?\nFacts: A bulk carrier is a vessel that transports fuel from one place to another. A jet fuel carrier is a vessel that transports jet fuel from one place to another.\nAnswer (True or False): True\nQuestion: What is the difference between a \"jet fuel", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Southwest Airlines use bulk carriers?\nFacts: Southwest Airlines requires jet fuel, which is brought in by bulk carrier transport. Southwest Airlines requires glycol for de-icing their planes, which is brought in by bulk transport.\nAnswer (True or False):", "ct_subject": "Southwest Airlines requires jet fuel, which is brought in by bulk carrier transport. Southwest Airlines requires glycol for de-icing their planes, which is brought in by bulk transport.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9997913"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9031453"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["b", "0.8825168"], ["ulk", "0.99999905"], [" carrier", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["jet", "1.0"], [" fuel", "1.0"], [" carrier", "0.13304156"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9998572"], [" bulk", "1.0"], [" carrier", "1.0"], [" is", "1.0"], [" a", "1.0"], [" vessel", "0.9999231"], [" that", "1.0"], [" transports", "0.44265908"], [" fuel", "0.9999995"], [" from", "0.26099727"], [" one", "0.9984565"], [" place", "0.67164195"], [" to", "1.0"], [" another", "1.0"], [".", "1.0"], [" A", "1.0"], [" jet", "1.0"], [" fuel", "1.0"], [" carrier", "1.0"], [" is", "1.0"], [" a", "1.0"], [" vessel", "1.0"], [" that", "1.0"], [" transports", "1.0"], [" jet", "0.9996909"], [" fuel", "1.0"], [" from", "1.0"], [" one", "1.0"], [" place", "1.0"], [" to", "1.0"], [" another", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["jet", "1.0"], [" fuel", "1.0"]]}, "345": {"qid": "c16870454604d281f0f1", "term": "Pelvis", "description": "lower part of the trunk of the human body between the abdomen and the thighs (sometimes also called pelvic region of the trunk", "question": "Is dysphoria around one's pelvis treatable without surgery?", "answer": true, "facts": ["For individuals experiencing dysphoria around having a vagina, packers can relieve symptoms.", "For people experiencing dysphoria about having a penis, there are tucking underwear and comfort gaffs available."], "decomposition": ["Which dysphoria could be experienced around the pelvis/genitals?", "What are the ways of treating #1?", "Are there others apart from surgery included in #2?"], "evidence": [[[["Gender dysphoria-7"]], [["Gender dysphoria-16"]], [["Gender dysphoria-16", "Gender dysphoria-18", "Gender dysphoria-20"]]], [[["Dysphoria-1"], "no_evidence"], ["no_evidence"], ["operation"]], [[["Dysphoria-5"]], [["Gender dysphoria-16"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it possible to have a baby with a penis and a vagina?\nFacts: It is possible to have a baby with a penis and a vagina. The baby will be born with a penis and a vagina.\nAnswer (True or False): True\nQuestion: Is it possible to have a baby with a penis", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is dysphoria around one's pelvis treatable without surgery?\nFacts: For individuals experiencing dysphoria around having a vagina, packers can relieve symptoms. For people experiencing dysphoria about having a penis, there are tucking underwear and comfort gaffs available.\nAnswer (True or False):", "ct_subject": "For individuals experiencing dysphoria around having a vagina, packers can relieve symptoms. For people experiencing dysphoria about having a penis, there are tucking underwear and comfort gaffs available.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999906"], [" it", "0.99984026"], [" possible", "0.99999845"], [" to", "1.0"], [" have", "0.9998287"], [" a", "1.0"], [" baby", "0.9997423"], [" with", "0.9999999"], [" a", "1.0"], [" penis", "0.9998758"], [" and", "0.9999802"], [" a", "0.99999547"], [" vagina", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" It", "0.027797269"], [" is", "1.0"], [" possible", "1.0"], [" to", "0.99999964"], [" have", "1.0"], [" a", "1.0"], [" baby", "1.0"], [" with", "1.0"], [" a", "1.0"], [" penis", "1.0"], [" and", "1.0"], [" a", "1.0"], [" vagina", "1.0"], [".", "1.0"], [" The", "0.78628474"], [" baby", "1.0"], [" will", "0.99825436"], [" be", "0.9999411"], [" born", "1.0"], [" with", "1.0"], [" a", "0.9999356"], [" penis", "1.0"], [" and", "1.0"], [" a", "1.0"], [" vagina", "1.0"], [".", "1.0"], ["\n", "0.9961965"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" possible", "1.0"], [" to", "1.0"], [" have", "1.0"], [" a", "1.0"], [" baby", "1.0"], [" with", "1.0"], [" a", "1.0"], [" penis", "1.0"]]}, "346": {"qid": "5b78898b7ef289f3cd26", "term": "Keyboard layout", "description": "any specific mechanical, visual, or functional arrangement of the keys of a keyboard or typewriter", "question": "Is the QWERTY keyboard layout meant to be slow?", "answer": true, "facts": ["The QWERTY keyboard layout was made for use on typewriters.", "Typewriters could not be used too quickly, or they would jam. "], "decomposition": ["What machine was QWERTY keyboard layout created for?", "Why was QWERTY keyboard layout created for #1?", "Will typing slow solve the problem of #2?"], "evidence": [[[["QWERTY-1"]], [["QWERTY-17"]], ["operation"]], [[["QWERTY-1"]], [["QWERTY-4"], "no_evidence"], [["QWERTY-6", "QWERTY-7"], "operation"]], [[["QWERTY-1"]], [["QWERTY-5"]], [["QWERTY-17"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the word \"God\" a Christian term?\nFacts: The word \"God\" is not a Christian term. It is a term that was used by the ancient Greeks and Romans.\nAnswer (True or False): True\nQuestion: Is the word \"God\" a Muslim term?\nFacts: The word \"God\" is not a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is the QWERTY keyboard layout meant to be slow?\nFacts: The QWERTY keyboard layout was made for use on typewriters. Typewriters could not be used too quickly, or they would jam. \nAnswer (True or False):", "ct_subject": "The QWERTY keyboard layout was made for use on typewriters. Typewriters could not be used too quickly, or they would jam. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.7097481"], [" the", "1.0"], [" word", "0.6279024"], [" \"", "1.0"], ["God", "0.40875533"], ["\"", "1.0"], [" a", "0.49274546"], [" Christian", "0.9956091"], [" term", "0.97882336"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["God", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" not", "0.3125667"], [" a", "1.0"], [" Christian", "1.0"], [" term", "1.0"], [".", "1.0"], [" It", "0.3848471"], [" is", "1.0"], [" a", "1.0"], [" term", "0.99999726"], [" that", "0.6359964"], [" was", "0.9251538"], [" used", "0.9999293"], [" by", "1.0"], [" the", "0.99999964"], [" ancient", "1.0"], [" Greeks", "1.0"], [" and", "0.99997044"], [" Romans", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["God", "0.9999864"], ["\"", "1.0"], [" a", "1.0"], [" Muslim", "1.0"], [" term", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["God", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" not", "1.0"], [" a", "1.0"]]}, "347": {"qid": "3d18b12522dc4b2404ea", "term": "Longitude", "description": "A geographic coordinate that specifies the east-west position of a point on the Earth's surface", "question": "Can I find my home with latitude and longitude?", "answer": true, "facts": ["My home is a location on earth where I live. ", "Latitude and Longitude are geographic coordinate systems that identify east/west and north/south locations.", "Specific Latitude and Longitude coordinates can be used to pinpoint specific locations. ", "Every point on the earth has a corresponding latitude and longitude coordinate. "], "decomposition": ["What are the uses of latitude and longitude?", "What is the range of latitude and longitude?", "Does #1 and #2 make it possible to locate most people's homes?"], "evidence": [[[["Geographic coordinate system-4"], "no_evidence"], [["Geographic coordinate system-16"], "no_evidence"], ["no_evidence", "operation"]], [[["Geographic coordinate system-15"]], [["Geographic coordinate system-16"], "no_evidence"], ["operation"]], [[["Geographic coordinate system-15"]], [["Geographic coordinate system-15"], "no_evidence"], [["Geographic coordinate system-15"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Can I find my home with latitude and longitude?\nFacts: My home is a location on earth where I live.  Latitude and Longitude are geographic coordinate systems that identify east/west and", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can I find my home with latitude and longitude?\nFacts: My home is a location on earth where I live.  Latitude and Longitude are geographic coordinate systems that identify east/west and north/south locations. Specific Latitude and Longitude coordinates can be used to pinpoint specific locations.  Every point on the earth has a corresponding latitude and longitude coordinate. \nAnswer (True or False):", "ct_subject": "My home is a location on earth where I live.  Latitude and Longitude are geographic coordinate systems that identify east/west and north/south locations. Specific Latitude and Longitude coordinates can be used to pinpoint specific locations.  Every point on the earth has a corresponding latitude and longitude coordinate. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Can", "0.3024393"], [" I", "1.0"], [" find", "1.0"], [" my", "1.0"], [" home", "1.0"], [" with", "1.0"], [" latitude", "1.0"], [" and", "1.0"], [" long", "1.0"], ["itude", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" My", "1.0"], [" home", "1.0"], [" is", "1.0"], [" a", "1.0"], [" location", "1.0"], [" on", "1.0"], [" earth", "1.0"], [" where", "1.0"], [" I", "1.0"], [" live", "1.0"], [".", "1.0"], [" ", "0.99999535"], [" Lat", "1.0"], ["itude", "1.0"], [" and", "1.0"], [" Long", "1.0"], ["itude", "1.0"], [" are", "1.0"], [" geographic", "1.0"], [" coordinate", "1.0"], [" systems", "1.0"], [" that", "1.0"], [" identify", "1.0"], [" east", "1.0"], ["/", "1.0"], ["west", "1.0"], [" and", "1.0"]]}, "348": {"qid": "798252398fa282fa38e6", "term": "Messiah (Handel)", "description": "Oratorio by Handel", "question": "Would Bruce Gandy be an odd choice for Messiah (Handel)?", "answer": true, "facts": ["Messiah (Handel) is a 1741 Oratorio by George Frideric Handel.", "Messiah (Handel) requires the following instruments: 2 trumpets; timpani; 2 oboes; 2 violins; and a viola.", "Bruce Gandy is a world renowned bagpipe player."], "decomposition": ["What instruments are used in Messiah (Handel)?", "What instrument is played by Bruce Gandy?", "Is #2 listed in #1?"], "evidence": [[[["Messiah (Handel)-3", "Orchestra-1"], "no_evidence"], [["Bruce Gandy-1"]], ["operation"]], [[["Structure of Handel's Messiah-9"], "no_evidence"], [["Bruce Gandy-1"]], ["operation"]], [[["Structure of Handel's Messiah-7"]], [["Bruce Gandy-5"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the song \"The Star Spangled Banner\"?\nFacts: The Star Spangled Banner is a song written by Francis Scott Key in 1814. The song was first performed", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Bruce Gandy be an odd choice for Messiah (Handel)?\nFacts: Messiah (Handel) is a 1741 Oratorio by George Frideric Handel. Messiah (Handel) requires the following instruments: 2 trumpets; timpani; 2 oboes; 2 violins; and a viola. Bruce Gandy is a world renowned bagpipe player.\nAnswer (True or False):", "ct_subject": "Messiah (Handel) is a 1741 Oratorio by George Frideric Handel. Messiah (Handel) requires the following instruments: 2 trumpets; timpani; 2 oboes; 2 violins; and a viola. Bruce Gandy is a world renowned bagpipe player.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.973538"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9994875"], [" of", "1.0"], [" the", "1.0"], [" song", "0.99999535"], [" \"", "0.7572675"], ["The", "1.0"], [" Star", "1.0"], [" Sp", "1.0"], ["angled", "1.0"], [" Banner", "1.0"], ["\"?", "0.79561836"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999865"], [" Star", "1.0"], [" Sp", "1.0"], ["angled", "1.0"], [" Banner", "1.0"], [" is", "1.0"], [" a", "0.99259233"], [" song", "1.0"], [" written", "0.9999676"], [" by", "1.0"], [" Francis", "1.0"], [" Scott", "1.0"], [" Key", "1.0"], [" in", "0.99895096"], [" 18", "1.0"], ["14", "1.0"], [".", "1.0"], [" The", "0.99999297"], [" song", "0.999997"], [" was", "0.9995735"], [" first", "0.99989355"], [" performed", "0.9996605"]]}, "349": {"qid": "505424c2715e4c11506b", "term": "Accountant", "description": "practitioner of accountancy or accounting", "question": "Is accountant a difficult profession for a person suffering from Dyscalculia?", "answer": true, "facts": ["Accounting is a math intensive profession in which a person keeps or inspects financial accounts.", "Dyscalculia is a math learning disability that impairs an individual's ability to represent and process numerical magnitude in a typical way. ", "Common symptoms of Dyscalculia include: difficulty with number sense. difficulty with fact and calculation", "Dyscalculia is sometimes called \u201cnumber dyslexia\u201d or \u201cmath dyslexia.\u201d"], "decomposition": ["What skills does dyscalculia impair?", "What skills are necessary to be an accountant?", "Are some parts of #2 also in #1?"], "evidence": [[[["Dyscalculia-1"]], ["no_evidence"], ["no_evidence"]], [[["Dyscalculia-1"]], [["Accountant-2"]], ["operation"]], [[["Dyscalculia-4"]], [["Accounting-12"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it true that the first person to be executed in the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is accountant a difficult profession for a person suffering from Dyscalculia?\nFacts: Accounting is a math intensive profession in which a person keeps or inspects financial accounts. Dyscalculia is a math learning disability that impairs an individual's ability to represent and process numerical magnitude in a typical way.  Common symptoms of Dyscalculia include: difficulty with number sense. difficulty with fact and calculation Dyscalculia is sometimes called \u201cnumber dyslexia\u201d or \u201cmath dyslexia.\u201d\nAnswer (True or False):", "ct_subject": "Accounting is a math intensive profession in which a person keeps or inspects financial accounts. Dyscalculia is a math learning disability that impairs an individual's ability to represent and process numerical magnitude in a typical way.  Common symptoms of Dyscalculia include: difficulty with number sense. difficulty with fact and calculation Dyscalculia is sometimes called \u201cnumber dyslexia\u201d or \u201cmath dyslexia.\u201d", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99999166"], [" it", "0.9990402"], [" true", "0.99730814"], [" that", "1.0"], [" the", "1.0"], [" first", "0.99997437"], [" person", "0.9999596"], [" to", "1.0"], [" be", "0.99337864"], [" executed", "0.9999802"], [" in", "0.70985574"], [" the", "1.0"]]}, "350": {"qid": "3210c34ec2b8fcda4cae", "term": "German Shepherd", "description": "Dog breed", "question": "Would Robert Wadlow tower over a German Shepherd?", "answer": true, "facts": ["German Shepherds have a height between 22 and 26 inches.", "Robert Wadlow was the tallest man ever, reaching a height of 8 ft 11.1 inches at his death."], "decomposition": ["What is the typical height range of German Shepherds?", "How tall was Robert Wadlow?", "Is #2 greater than #1?"], "evidence": [[[["German Shepherd-3"]], [["Robert Wadlow-2"]], ["operation"]], [[["German Shepherd-3"]], [["Robert Wadlow-2"]], ["operation"]], [[["German Shepherd-3"], "no_evidence"], [["Robert Wadlow-2"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the tallest building in the world?\nFacts: The tallest building in the world is the Burj Khalifa in Dubai, United Arab Emirates. It is a skyscraper that is over 1,000 feet tall.\nAnswer (True or False): True\nQuestion: What is the tallest building in the world?\nFacts:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Robert Wadlow tower over a German Shepherd?\nFacts: German Shepherds have a height between 22 and 26 inches. Robert Wadlow was the tallest man ever, reaching a height of 8 ft 11.1 inches at his death.\nAnswer (True or False):", "ct_subject": "German Shepherds have a height between 22 and 26 inches. Robert Wadlow was the tallest man ever, reaching a height of 8 ft 11.1 inches at his death.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999666"], [" is", "1.0"], [" the", "1.0"], [" tallest", "0.67426866"], [" building", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" tallest", "1.0"], [" building", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" is", "1.0"], [" the", "1.0"], [" Bur", "1.0"], ["j", "1.0"], [" Khal", "1.0"], ["ifa", "1.0"], [" in", "1.0"], [" Dubai", "1.0"], [",", "0.9981287"], [" United", "0.99999964"], [" Arab", "1.0"], [" Emirates", "1.0"], [".", "1.0"], [" It", "0.9997893"], [" is", "1.0"], [" a", "0.99994445"], [" skysc", "0.9999999"], ["raper", "1.0"], [" that", "1.0"], [" is", "0.9995801"], [" over", "0.99325585"], [" 1", "1.0"], [",", "1.0"], ["000", "1.0"], [" feet", "1.0"], [" tall", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" tallest", "0.9993087"], [" building", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "0.9999993"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"]]}, "351": {"qid": "64f18d020f552f7dd4ec", "term": "Judge", "description": "official who presides over court proceedings", "question": "Would an Orthodox Presbyterian object to 1700s judge's attire?", "answer": true, "facts": ["Judges in the 1700s wore powdered wigs and large robes during court proceedings.", "Many Orthodox Presbyterians argue that the Bible prohibits adornment such as wigs and jewelry.", "The 1 Timothy 2:8-9 Bible verse warns against adorning oneself with objects."], "decomposition": ["What attire did judges in the 1700's wear?", "What things are prohibited by Orthodox Presbyterians?", "Are some elements of #1 also found in #2?"], "evidence": [[[["Wig-16"]], ["no_evidence"], ["operation"]], [[["Court dress-110"]], ["no_evidence"], ["operation"]], [[["Wig-16"], "no_evidence"], [["Orthodox Presbyterian Church-1"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"sister\" and a \"brother\"?\nFacts: A sister is a female relative who is related to you by blood. A brother is a male relative who is related to you by blood.\nAnswer (", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would an Orthodox Presbyterian object to 1700s judge's attire?\nFacts: Judges in the 1700s wore powdered wigs and large robes during court proceedings. Many Orthodox Presbyterians argue that the Bible prohibits adornment such as wigs and jewelry. The 1 Timothy 2:8-9 Bible verse warns against adorning oneself with objects.\nAnswer (True or False):", "ct_subject": "Judges in the 1700s wore powdered wigs and large robes during court proceedings. Many Orthodox Presbyterians argue that the Bible prohibits adornment such as wigs and jewelry. The 1 Timothy 2:8-9 Bible verse warns against adorning oneself with objects.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.52158266"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "0.98057544"], [" \"", "0.9999988"], ["s", "0.9956388"], ["ister", "0.9949728"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["brother", "0.95688415"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.99139965"], [" sister", "0.99995923"], [" is", "1.0"], [" a", "1.0"], [" female", "0.93739015"], [" relative", "1.0"], [" who", "0.99557096"], [" is", "1.0"], [" related", "0.95091975"], [" to", "0.9999999"], [" you", "1.0"], [" by", "0.99999917"], [" blood", "1.0"], [".", "0.9974649"], [" A", "1.0"], [" brother", "1.0"], [" is", "1.0"], [" a", "1.0"], [" male", "1.0"], [" relative", "1.0"], [" who", "1.0"], [" is", "1.0"], [" related", "0.99992263"], [" to", "1.0"], [" you", "1.0"], [" by", "1.0"], [" blood", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"]]}, "352": {"qid": "5c62f57b2fd2423b5f59", "term": "Cherokee", "description": "Native American people indigenous to the Southeastern United States", "question": "Did the Cherokee people send a delegation to oppose allotment?", "answer": true, "facts": ["The Four Mothers Society or Four Mothers Nation is a religious, political, and traditionalist organization of Muscogee Creek, Cherokee, Choctaw and Chickasaw people.", "The Four Mothers Society sent a delegation to Congress in 1906 to oppose the Curtis Act and the Dawes Act.", "With the passage of the Curtis Act in 1898 and Dawes Act, allotment became US policy and the various tribal governments were forced to allot land."], "decomposition": ["Which acts cause allotment to become US policy?", "Who sent as a delegate to congress to oppose #1?", "Is #2 related to the Cherokee people?"], "evidence": [[[["Dawes Act-3", "Dawes Act-4"]], [["Redbird Smith-3"]], ["operation"]], [[["Dawes Act-1"]], [["Dawes Act-3"]], ["operation"]], [[["Dawes Act-1"]], [["Dawes Act-3"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Cherokee people send a delegation to oppose allotment?\nFacts: The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did the Cherokee people send a delegation to oppose allotment?\nFacts: The Four Mothers Society or Four Mothers Nation is a religious, political, and traditionalist organization of Muscogee Creek, Cherokee, Choctaw and Chickasaw people. The Four Mothers Society sent a delegation to Congress in 1906 to oppose the Curtis Act and the Dawes Act. With the passage of the Curtis Act in 1898 and Dawes Act, allotment became US policy and the various tribal governments were forced to allot land.\nAnswer (True or False):", "ct_subject": "The Four Mothers Society or Four Mothers Nation is a religious, political, and traditionalist organization of Muscogee Creek, Cherokee, Choctaw and Chickasaw people. The Four Mothers Society sent a delegation to Congress in 1906 to oppose the Curtis Act and the Dawes Act. With the passage of the Curtis Act in 1898 and Dawes Act, allotment became US policy and the various tribal governments were forced to allot land.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99999654"], [" the", "1.0"], [" Cherokee", "1.0"], [" people", "1.0"], [" send", "0.76021147"], [" a", "1.0"], [" delegation", "1.0"], [" to", "1.0"], [" oppose", "1.0"], [" allot", "1.0"], ["ment", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"]]}, "353": {"qid": "088e22a7b1b40fd2d95b", "term": "Moscow Kremlin", "description": "fortified complex in Moscow, Russia", "question": "Can the Moscow Kremlin fit inside Disney Land?", "answer": true, "facts": ["The Moscow Kremlin is a fortified complex in the middle of Moscow Russia.", "The Kremlin takes up sixty eight acres.", "Disney Land is an amusement park in California. ", "Disney Land occupies eighty five acres."], "decomposition": ["What is the area of Moscow Kremlin?", "What is the size of Disney Land?", "Is #1 smaller than #2?"], "evidence": [[["no_evidence"], [["Disneyland-22"]], ["no_evidence", "operation"]], [[["Moscow Kremlin-1"], "no_evidence"], [["Disneyland-2"], "no_evidence"], ["operation"]], [[["Moscow Kremlin-18"]], [["Disneyland-22"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the largest city in the world?\nFacts: The largest city in the world is New York City. The largest city in the world is Tokyo.\nAnswer (True or False): True\nQuestion: What is the largest city in the world?\nFacts: The largest city in the world is New York City.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can the Moscow Kremlin fit inside Disney Land?\nFacts: The Moscow Kremlin is a fortified complex in the middle of Moscow Russia. The Kremlin takes up sixty eight acres. Disney Land is an amusement park in California.  Disney Land occupies eighty five acres.\nAnswer (True or False):", "ct_subject": "The Moscow Kremlin is a fortified complex in the middle of Moscow Russia. The Kremlin takes up sixty eight acres. Disney Land is an amusement park in California.  Disney Land occupies eighty five acres.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999949"], [" is", "1.0"], [" the", "1.0"], [" largest", "0.9993255"], [" city", "0.99991906"], [" in", "1.0"], [" the", "1.0"], [" world", "0.99999714"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" largest", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" is", "1.0"], [" New", "0.99540186"], [" York", "1.0"], [" City", "1.0"], [".", "1.0"], [" The", "0.99214935"], [" largest", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" is", "1.0"], [" Tokyo", "0.9999727"], [".", "0.99996734"], ["\n", "0.999845"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "0.9999989"], ["?", "0.9999974"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" largest", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" is", "1.0"], [" New", "0.96189564"], [" York", "1.0"], [" City", "1.0"], [".", "1.0"]]}, "354": {"qid": "1857d2f86cc49c8eb728", "term": "Euphoria", "description": "mental and emotional condition in which a person experiences intense feelings of well-being, elation, happiness and excitement", "question": "Is euphoria associated with drug addiction?", "answer": true, "facts": ["Euphoria is a state of unusually extreme happiness.", "Several drugs are known to artificially induce this reaction including cannabis and opium."], "decomposition": ["What is euphoria?", "Do some drugs create the feeling of #1?"], "evidence": [[[["Euphoria-1"]], [["Euphoria-15"]]], [[["Euphoria-19"]], [["Euphoria-19"]]], [[["Euphoria-1"]], [["Euphoria-15", "Euphoria-16"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the word \"candy\" derived from the word \"candy\"?\nFacts: The word \"candy\" is a corruption of the word \"candy\" which is a corruption of the word \"candy\".\nAnswer (True or False): True\nQuestion: Is the word \"candy\" derived from the word \"candy\"?\nFacts: The word \"candy\" is a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is euphoria associated with drug addiction?\nFacts: Euphoria is a state of unusually extreme happiness. Several drugs are known to artificially induce this reaction including cannabis and opium.\nAnswer (True or False):", "ct_subject": "Euphoria is a state of unusually extreme happiness. Several drugs are known to artificially induce this reaction including cannabis and opium.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999906"], [" the", "0.9821729"], [" word", "0.8909159"], [" \"", "1.0"], ["c", "0.9643944"], ["andy", "0.99386287"], ["\"", "1.0"], [" derived", "0.9943726"], [" from", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"?", "0.9942872"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999988"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "0.8465429"], [" corruption", "0.9998481"], [" of", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"", "1.0"], [" which", "0.99999917"], [" is", "0.9999274"], [" a", "0.9852798"], [" corruption", "1.0"], [" of", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "0.9965898"], ["\".", "0.8625128"], ["\n", "0.99999976"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"", "1.0"], [" derived", "0.9999999"], [" from", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"]]}, "355": {"qid": "07c218fd4846cf6b77a8", "term": "Milky Way", "description": "Spiral galaxy containing our Solar System", "question": "Is Ganymede in the Milky Way galaxy?", "answer": true, "facts": ["Ganymede is a moon of Jupiter.", "Jupiter is the largest planet in our solar system.", "The solar system is part of the Milky Way galaxy."], "decomposition": ["What does Ganymede orbit?", "What larger astronomical system is #1 part of?", "Is #2 located in the Milky Way?"], "evidence": [[[["Ganymede (moon)-1"]], [["Ganymede (moon)-1"]], [["Solar System-6"]]], [[["Ganymede (moon)-1"]], [["Milky Way-1"]], ["operation"]], [[["Ganymede (moon)-1"]], [["Jupiter-1"]], [["Milky Way-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the Earth a planet?\nFacts: The Earth is a planet.\nAnswer (True or False): True\nQuestion: Is the Earth a planet?\nFacts: The Earth is a planet.\nAnswer (True or False): True\nQuestion: Is the Earth a planet?\nFacts: The Earth is a planet.\nAnswer (True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Ganymede in the Milky Way galaxy?\nFacts: Ganymede is a moon of Jupiter. Jupiter is the largest planet in our solar system. The solar system is part of the Milky Way galaxy.\nAnswer (True or False):", "ct_subject": "Ganymede is a moon of Jupiter. Jupiter is the largest planet in our solar system. The solar system is part of the Milky Way galaxy.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999994"], [" the", "0.99999917"], [" Earth", "0.99428684"], [" a", "0.9886231"], [" planet", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" a", "1.0"], [" planet", "1.0"], [".", "0.999984"], ["\n", "0.9892517"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" Earth", "1.0"], [" a", "1.0"], [" planet", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" a", "1.0"], [" planet", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" Earth", "1.0"], [" a", "1.0"], [" planet", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" a", "1.0"], [" planet", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"]]}, "356": {"qid": "5a5cdf519ba94a287202", "term": "Goofy", "description": "Disney cartoon character", "question": "Could Goofy have counted nine planets in his first year?", "answer": true, "facts": ["Goofy was created in 1932", "Pluto (the celestial object) was discovered in 1930 and labeled a planet"], "decomposition": ["When was Goofy first created?", "What year was the ninth planet found?", "Was #1 after #2?"], "evidence": [[[["Goofy-1"]], [["Pluto-2"]], [["Goofy-1", "Pluto-2"]]], [[["Goofy-1"]], [["Pluto-2"]], ["operation"]], [[["Goofy-1"]], [["Pluto-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the planet that is farthest from the sun?\nFacts: The planet that is farthest from the sun is called Proxima Centauri.\nAnswer (True or False): True\nQuestion: What is the name of the planet that is farthest from the sun?\nFacts: The planet that is farthest from the sun is called Proxima Centauri.\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could Goofy have counted nine planets in his first year?\nFacts: Goofy was created in 1932 Pluto (the celestial object) was discovered in 1930 and labeled a planet\nAnswer (True or False):", "ct_subject": "Goofy was created in 1932 Pluto (the celestial object) was discovered in 1930 and labeled a planet", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.91123897"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" planet", "0.99545467"], [" that", "1.0"], [" is", "0.9525324"], [" fart", "0.9725556"], ["hest", "1.0"], [" from", "1.0"], [" the", "1.0"], [" sun", "0.99999976"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" planet", "1.0"], [" that", "0.2882224"], [" is", "1.0"], [" fart", "1.0"], ["hest", "1.0"], [" from", "1.0"], [" the", "1.0"], [" sun", "1.0"], [" is", "1.0"], [" called", "1.0"], [" Pro", "1.0"], ["x", "1.0"], ["ima", "1.0"], [" Centauri", "1.0"], [".", "0.9984005"], ["\n", "0.99999976"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" planet", "1.0"], [" that", "1.0"], [" is", "1.0"], [" fart", "1.0"], ["hest", "1.0"], [" from", "1.0"], [" the", "1.0"], [" sun", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" planet", "1.0"], [" that", "1.0"], [" is", "1.0"], [" fart", "1.0"], ["hest", "1.0"], [" from", "1.0"], [" the", "1.0"], [" sun", "1.0"], [" is", "1.0"], [" called", "1.0"], [" Pro", "1.0"], ["x", "1.0"], ["ima", "1.0"], [" Centauri", "1.0"], [".", "1.0"], ["\n", "1.0"]]}, "357": {"qid": "315575d5cbdd1d154744", "term": "Kurt Cobain", "description": "American singer, composer, and musician", "question": "Would Kurt Cobain have benefited from Project Semicolon?", "answer": true, "facts": ["Project Semicolon is an American nonprofit organization known for its advocacy of mental health wellness and its focus as an anti-suicide initiative.", "During the last years of his life, Cobain struggled with heroin addiction and chronic health problems such as depression.", "Cobain died at the age of 27 from apparent suicide by gunshot."], "decomposition": ["What problems does Project Semicolon work to solve?", "Did Kurt Cobain have any of the problems listed in #1?"], "evidence": [[[["Project Semicolon-1"]], [["Suicide of Kurt Cobain-2"]]], [[["Project Semicolon-1"]], [["Kurt Cobain-3"]]], [[["Project Semicolon-5"]], [["Kurt Cobain-47", "Kurt Cobain-49", "Project Semicolon-5"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the year 2000?\nFacts: The year 2000 was the first year that the year 2000 was the first year that the year 2000 was", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Kurt Cobain have benefited from Project Semicolon?\nFacts: Project Semicolon is an American nonprofit organization known for its advocacy of mental health wellness and its focus as an anti-suicide initiative. During the last years of his life, Cobain struggled with heroin addiction and chronic health problems such as depression. Cobain died at the age of 27 from apparent suicide by gunshot.\nAnswer (True or False):", "ct_subject": "Project Semicolon is an American nonprofit organization known for its advocacy of mental health wellness and its focus as an anti-suicide initiative. During the last years of his life, Cobain struggled with heroin addiction and chronic health problems such as depression. Cobain died at the age of 27 from apparent suicide by gunshot.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999918"], [" is", "1.0"], [" the", "1.0"], [" name", "0.7648057"], [" of", "1.0"], [" the", "1.0"], [" first", "0.99997807"], [" person", "0.9943521"], [" to", "1.0"], [" be", "0.99909294"], [" born", "0.99211836"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" year", "0.71970695"], [" 2000", "1.0"], [" was", "0.99999845"], [" the", "1.0"], [" first", "0.99999595"], [" year", "1.0"], [" that", "0.99908733"], [" the", "0.9989555"], [" year", "1.0"], [" 2000", "0.9984629"], [" was", "1.0"], [" the", "0.9999374"], [" first", "1.0"], [" year", "1.0"], [" that", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "1.0"]]}, "358": {"qid": "43da475d5c2db929c458", "term": "Arctic Ocean", "description": "The smallest and shallowest of the world's five major oceans, located in the north polar regions", "question": "Could the Eiffel Tower be completely submerged at the Arctic Ocean's deepest point?", "answer": true, "facts": ["The deepest point in the Arctic Ocean is 18,210 feet below the surface.", "The Eiffel Tower is 1,063 feet tall."], "decomposition": ["How deep is the deepest point in the Arctic Ocean?", "How tall is the Eiffel Tower?", "Is #2 smaller than #1?"], "evidence": [[[["Molloy Deep-2"]], [["Eiffel Tower-3"]], ["operation"]], [[["Molloy Deep-2"]], [["Eiffel Tower-3"]], [["Eiffel Tower-3", "Molloy Deep-2"], "operation"]], [[["Fram Strait-4"]], [["Eiffel Tower-3"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance between two points on Earth?\nFacts: The longest distance between two points on Earth is about 1,000 miles.\nAnswer (True or False): True\nQuestion: What is the longest distance between two points on the Moon?\nFacts: The longest distance between two points on the Moon is about 1,", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could the Eiffel Tower be completely submerged at the Arctic Ocean's deepest point?\nFacts: The deepest point in the Arctic Ocean is 18,210 feet below the surface. The Eiffel Tower is 1,063 feet tall.\nAnswer (True or False):", "ct_subject": "The deepest point in the Arctic Ocean is 18,210 feet below the surface. The Eiffel Tower is 1,063 feet tall.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99992657"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.9380993"], [" distance", "0.99999976"], [" between", "0.9999968"], [" two", "1.0"], [" points", "1.0"], [" on", "0.99989426"], [" Earth", "0.9970913"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "0.9172248"], [" distance", "1.0"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"], [" on", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" about", "0.35572162"], [" 1", "0.99942386"], [",", "1.0"], ["000", "1.0"], [" miles", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.9999995"], [" distance", "0.9999999"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"], [" on", "1.0"], [" the", "1.0"], [" Moon", "0.9782497"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"], [" on", "1.0"], [" the", "1.0"], [" Moon", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 1", "1.0"], [",", "1.0"]]}, "359": {"qid": "d7886984475ee7c616a2", "term": "Haiku", "description": "very short form of Japanese poetry", "question": "Is Lines on the Antiquity of Microbes briefer than any haiku?", "answer": true, "facts": ["A haiku is a short Japanese poem that follows a 5, 7, 5 syllable structure.", "Lines on the Antiquity of Microbes, also known simply as Fleas is said to be the shortest poem written.", "Lines on the Antiquity of Microbes is made of one brief phrase: Adam. Had 'em."], "decomposition": ["How long is a haiku?", "How long is Lines on the Antiquity of Microbes?", "Is #2 shorter than #1?"], "evidence": [[[["Haiku-2"]], [["Lines on the Antiquity of Microbes-1", "Lines on the Antiquity of Microbes-3"]], ["operation"]], [[["Haiku-2"], "no_evidence"], [["Lines on the Antiquity of Microbes-1"], "operation"], ["no_evidence"]], [[["Haiku-2"]], [["Lines on the Antiquity of Microbes-3"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the first man on the moon named after the first man on the moon?\nFacts: The first man on the moon was named after the first man", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Lines on the Antiquity of Microbes briefer than any haiku?\nFacts: A haiku is a short Japanese poem that follows a 5, 7, 5 syllable structure. Lines on the Antiquity of Microbes, also known simply as Fleas is said to be the shortest poem written. Lines on the Antiquity of Microbes is made of one brief phrase: Adam. Had 'em.\nAnswer (True or False):", "ct_subject": "A haiku is a short Japanese poem that follows a 5, 7, 5 syllable structure. Lines on the Antiquity of Microbes, also known simply as Fleas is said to be the shortest poem written. Lines on the Antiquity of Microbes is made of one brief phrase: Adam. Had 'em.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999368"], [" the", "0.83440393"], [" name", "0.99858713"], [" of", "1.0"], [" the", "1.0"], [" first", "0.3556852"], [" man", "0.9997367"], [" on", "0.9999999"], [" the", "1.0"], [" moon", "1.0"], [" named", "0.9300841"], [" after", "1.0"], [" the", "0.9970169"], [" first", "1.0"], [" man", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "0.8751915"], [" man", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" was", "1.0"], [" named", "1.0"], [" after", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"]]}, "360": {"qid": "2ed50522610a3683933f", "term": "Jalape\u00f1o", "description": "Hot pepper", "question": "Is jalapeno heat outclassed by Bhut jolokia?", "answer": true, "facts": ["The Scoville scale measures how hot peppers are.", "The jalapeno pepper has a Scoville scale rating of between 3,500 and 3,600 SHU (Scoville Heat Units).", "The Bhut jolokia (ghost pepper) has a Scoville scale rating of 1 million SHU (Scoville Heat Units)."], "decomposition": ["How many Scoville units does a Jalapeno have?", "How many Scoville units does a  Bhut jolokia have?", "Is #2 greater than #1?"], "evidence": [[[["Jalape\u00f1o-16"]], [["Bhut jolokia-4"]], ["operation"]], [[["Jalape\u00f1o-1"]], [["Bhut jolokia-2"]], ["operation"]], [[["Jalape\u00f1o-16"]], [["Race to grow the hottest pepper-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the word \"jalapeno\" a misspelling of \"jalapeno pepper\"?\nFacts: The word \"jal", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is jalapeno heat outclassed by Bhut jolokia?\nFacts: The Scoville scale measures how hot peppers are. The jalapeno pepper has a Scoville scale rating of between 3,500 and 3,600 SHU (Scoville Heat Units). The Bhut jolokia (ghost pepper) has a Scoville scale rating of 1 million SHU (Scoville Heat Units).\nAnswer (True or False):", "ct_subject": "The Scoville scale measures how hot peppers are. The jalapeno pepper has a Scoville scale rating of between 3,500 and 3,600 SHU (Scoville Heat Units). The Bhut jolokia (ghost pepper) has a Scoville scale rating of 1 million SHU (Scoville Heat Units).", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9799208"], [" the", "0.9999145"], [" word", "0.99977106"], [" \"", "1.0"], ["j", "0.62635803"], ["al", "0.9999974"], ["ap", "1.0"], ["eno", "1.0"], ["\"", "1.0"], [" a", "0.9999571"], [" miss", "0.9982504"], ["pelling", "1.0"], [" of", "1.0"], [" \"", "0.99428093"], ["j", "0.999253"], ["al", "1.0"], ["ap", "1.0"], ["eno", "1.0"], [" pepper", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["j", "1.0"], ["al", "1.0"]]}, "361": {"qid": "a4de29904c45a2d17511", "term": "Linux", "description": "Family of free and open-source software operating systems based on the Linux kernel", "question": "If you're running focal fossa, are you using linux?", "answer": true, "facts": ["Focal Fossa is the most recent Ubuntu release.", "Ubuntu is a Linux distribution. "], "decomposition": ["Which operating system was codenamed focal fossa?", "Is #1 a Linux distribution?"], "evidence": [[[["Ubuntu version history-146"]], [["Ubuntu-1"], "operation"]], [[["Ubuntu version history-146"]], [["Ubuntu-1"]]], [[["Ubuntu-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: Thomas Jefferson.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: Thomas Jefferson.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the United States?\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: If you're running focal fossa, are you using linux?\nFacts: Focal Fossa is the most recent Ubuntu release. Ubuntu is a Linux distribution. \nAnswer (True or False):", "ct_subject": "Focal Fossa is the most recent Ubuntu release. Ubuntu is a Linux distribution. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.69198126"], [" is", "1.0"], [" the", "1.0"], [" name", "0.32814118"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9963337"], [" person", "0.9999989"], [" to", "1.0"], [" be", "0.72153825"], [" born", "0.9999968"], [" in", "1.0"], [" the", "1.0"], [" United", "0.36676338"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Thomas", "0.41130048"], [" Jefferson", "1.0"], [".", "0.9984798"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Thomas", "1.0"], [" Jefferson", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"]]}, "362": {"qid": "0c30c162923eb59ae816", "term": "Badminton", "description": "racquet sport", "question": "Are birds important to badminton?", "answer": true, "facts": ["Badminton is played with racquets and a shuttlecock.", "A shuttlecock is a projectile made of feathers attached to a cork base.", "Birds have feathers covering their body."], "decomposition": ["What are the equipment needed to play badminton?", "Is any of #1 made with a bird product?"], "evidence": [[[["Badminton-1"]], [["Badminton-3"], "operation"]], [[["Badminton-35"], "no_evidence"], [["Shuttlecock-4"]]], [[["Badminton-28"], "no_evidence"], [["Badminton-28"], "no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any animals that are not related to humans?\nFacts: There are many animals that are not related to humans.\nAnswer (True or False): True\nQuestion: Are there any animals that are not related to humans?\nFacts: There are many animals that are not related to humans.\nAnswer (True or False): True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are birds important to badminton?\nFacts: Badminton is played with racquets and a shuttlecock. A shuttlecock is a projectile made of feathers attached to a cork base. Birds have feathers covering their body.\nAnswer (True or False):", "ct_subject": "Badminton is played with racquets and a shuttlecock. A shuttlecock is a projectile made of feathers attached to a cork base. Birds have feathers covering their body.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.9916951"], [" there", "1.0"], [" any", "0.9998771"], [" animals", "0.9998814"], [" that", "1.0"], [" are", "0.9800448"], [" not", "1.0"], [" related", "0.13782957"], [" to", "1.0"], [" humans", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "0.9999926"], [" are", "1.0"], [" many", "1.0"], [" animals", "1.0"], [" that", "1.0"], [" are", "1.0"], [" not", "1.0"], [" related", "1.0"], [" to", "1.0"], [" humans", "1.0"], [".", "1.0"], ["\n", "0.9672116"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "1.0"], [" any", "1.0"], [" animals", "1.0"], [" that", "1.0"], [" are", "1.0"], [" not", "1.0"], [" related", "1.0"], [" to", "1.0"], [" humans", "1.0"], ["?", "0.99999344"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "1.0"], [" are", "1.0"], [" many", "1.0"], [" animals", "1.0"], [" that", "1.0"], [" are", "1.0"], [" not", "1.0"], [" related", "1.0"], [" to", "1.0"], [" humans", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"]]}, "363": {"qid": "1479ad9ab9d5dbc5c140", "term": "The Matrix", "description": "1999 science fiction action film directed by the Wachowskis", "question": "Do the directors of The Matrix advocate for transgender rights?", "answer": true, "facts": ["Lilly Wachowski is a trans woman who was a director of The Matrix.", "Lena Wachowski is a trans woman who was a director of The Matrix.", "The Wachowski sisters speak actively about viewing their films through a \"lens of transness\""], "decomposition": ["Who directed The Matrix?", "Are #1 transgender rights advocates?"], "evidence": [[[["The Matrix-1"]], [["The Wachowskis-1", "The Wachowskis-57"]]], [[["The Matrix-1"]], [["The Wachowskis-57"], "operation"]], [[["The Wachowskis-2"]], [["The Wachowskis-53"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Do you believe that the world is flat?\nFacts: The Earth is not flat. The Earth is round.\nAnswer (True or False): True\nQuestion: Do you believe that the world is flat?\nFacts: The Earth is not flat", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do the directors of The Matrix advocate for transgender rights?\nFacts: Lilly Wachowski is a trans woman who was a director of The Matrix. Lena Wachowski is a trans woman who was a director of The Matrix. The Wachowski sisters speak actively about viewing their films through a \"lens of transness\"\nAnswer (True or False):", "ct_subject": "Lilly Wachowski is a trans woman who was a director of The Matrix. Lena Wachowski is a trans woman who was a director of The Matrix. The Wachowski sisters speak actively about viewing their films through a \"lens of transness\"", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Do", "0.98121864"], [" you", "0.97747815"], [" believe", "0.9989177"], [" that", "0.9999989"], [" the", "1.0"], [" world", "0.68830097"], [" is", "1.0"], [" flat", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99610907"], [" Earth", "0.99899405"], [" is", "1.0"], [" not", "0.7532246"], [" flat", "1.0"], [".", "0.9999994"], [" The", "0.9999838"], [" Earth", "1.0"], [" is", "1.0"], [" round", "0.9999995"], [".", "1.0"], ["\n", "0.9684391"], ["Answer", "0.99999976"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Do", "1.0"], [" you", "1.0"], [" believe", "1.0"], [" that", "1.0"], [" the", "1.0"], [" world", "0.5342898"], [" is", "1.0"], [" flat", "0.52705026"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" not", "1.0"], [" flat", "1.0"]]}, "364": {"qid": "a874e8bbb2230cdacf19", "term": "Chickpea", "description": "species of plant", "question": "Would vegans consider chickpeas for a tuna substitute?", "answer": true, "facts": ["Vegans do not eat any animal products, including milk, eggs, meat, and cheese. ", "Vegan alternatives for things like mayo exist. ", "There are vegan recipes for \"tuna\" online that include chickpeas as the main component."], "decomposition": ["What kinds of foods do vegans avoid?", "What are replacements for #1 in vegan diets called?", "What are some #2 for tuna?", "Are chickpeas included in the recipes of any of #3?"], "evidence": [[[["Veganism-1"]], [["Meat analogue-16"]], ["no_evidence", "operation"], ["operation"]], [[["Veganism-1"]], [["Veganism-34"]], [["Chickpea-14"], "no_evidence"], ["operation"]], [[["Veganism-1"]], [["Chickpea-1", "Meat analogue-1"]], [["Chickpea-17"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"pig\" and a \"piglet\"?\nFacts: A piglet is a young pig that has not yet reached sexual maturity. A pig is a member of the family Cervidae, which includes the pig, the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would vegans consider chickpeas for a tuna substitute?\nFacts: Vegans do not eat any animal products, including milk, eggs, meat, and cheese.  Vegan alternatives for things like mayo exist.  There are vegan recipes for \"tuna\" online that include chickpeas as the main component.\nAnswer (True or False):", "ct_subject": "Vegans do not eat any animal products, including milk, eggs, meat, and cheese.  Vegan alternatives for things like mayo exist.  There are vegan recipes for \"tuna\" online that include chickpeas as the main component.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.98868877"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.99998665"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.9999999"], ["p", "0.025225528"], ["ig", "0.9990632"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["p", "0.9443568"], ["ig", "0.9678196"], ["let", "0.99036866"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999665"], [" pig", "1.0"], ["let", "1.0"], [" is", "1.0"], [" a", "1.0"], [" young", "0.12440117"], [" pig", "1.0"], [" that", "0.18753833"], [" has", "0.9999639"], [" not", "1.0"], [" yet", "1.0"], [" reached", "0.99999404"], [" sexual", "0.7972154"], [" maturity", "1.0"], [".", "1.0"], [" A", "1.0"], [" pig", "1.0"], [" is", "1.0"], [" a", "1.0"], [" member", "0.9857259"], [" of", "1.0"], [" the", "1.0"], [" family", "1.0"], [" C", "1.0"], ["erv", "1.0"], ["idae", "1.0"], [",", "0.5399651"], [" which", "0.99982506"], [" includes", "1.0"], [" the", "0.14638396"], [" pig", "0.9873226"], [",", "1.0"], [" the", "0.2294643"]]}, "365": {"qid": "47cb228abaa8a8106df9", "term": "Hundred Years' War", "description": "Series of conflicts and wars between England and France during the 14th and 15th-century", "question": "Was Hundred Years' War a misnomer?", "answer": true, "facts": ["A misnomer is a wrong or inaccurate name.", "The Hundred Years' War lasted for longer than one hundred years.", "The Hundred Years' War lasted from 1337-1453."], "decomposition": ["How many years the the Hundred Years' War actually last?", "Is #1 greater or less than 100?"], "evidence": [[[["Hundred Years' War-1"]], ["operation"]], [[["Hundred Years' War-1"]], ["operation"]], [[["Hundred Years' War-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake's eye\"?\nFacts: A snowflake is a small, round, or irregularly shaped crystal. A snowflake's eye is a small, round, or irregularly shaped crystal.\nAnswer (True or False): True\nQuestion: What is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Hundred Years' War a misnomer?\nFacts: A misnomer is a wrong or inaccurate name. The Hundred Years' War lasted for longer than one hundred years. The Hundred Years' War lasted from 1337-1453.\nAnswer (True or False):", "ct_subject": "A misnomer is a wrong or inaccurate name. The Hundred Years' War lasted for longer than one hundred years. The Hundred Years' War lasted from 1337-1453.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.7079674"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.4565109"], [" between", "1.0"], [" a", "0.9999987"], [" \"", "0.99999726"], ["s", "0.8726147"], ["now", "0.9988961"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["'s", "0.99877566"], [" eye", "0.46410033"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.99994206"], [" snow", "0.9999999"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" small", "0.9998166"], [",", "1.0"], [" round", "0.9992101"], [",", "1.0"], [" or", "0.2520417"], [" irregular", "0.99873394"], ["ly", "1.0"], [" shaped", "1.0"], [" crystal", "0.9999968"], [".", "1.0"], [" A", "0.68684286"], [" snow", "1.0"], ["flake", "1.0"], ["'s", "1.0"], [" eye", "1.0"], [" is", "1.0"], [" a", "1.0"], [" small", "1.0"], [",", "1.0"], [" round", "1.0"], [",", "1.0"], [" or", "1.0"], [" irregular", "1.0"], ["ly", "1.0"], [" shaped", "1.0"], [" crystal", "1.0"], [".", "0.99999964"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"]]}, "366": {"qid": "4f08d9a0fba58ad9ca73", "term": "Taco Bell", "description": "American fast-food chain", "question": "Will more people go in and out of Taco Bell than a Roy Rogers each year?", "answer": true, "facts": ["Taco Bell has over 7,072 restaurants as of 2018.", "Roy Rogers had over 600 restaurants at its peak.", "Roy Rogers has 48 locations as of 2019."], "decomposition": ["How many restaurants does Taco Bell have?", "How many restaurants does Roy Rogers have?", "Is #1 significantly greater than #2?"], "evidence": [[[["Taco Bell-1"]], [["Roy Rogers Restaurants-1"]], ["operation"]], [["no_evidence"], [["Roy Rogers Restaurants-1"]], ["operation"]], [["no_evidence"], [["Roy Rogers Restaurants-1"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human?\nFacts: The average human weight is about 100 pounds.\nAnswer (True or False): True\nQuestion: What is the average height of a human?\nFacts: The average human height is about 5 feet, 9 inches.\nAnswer (True or False): True\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Will more people go in and out of Taco Bell than a Roy Rogers each year?\nFacts: Taco Bell has over 7,072 restaurants as of 2018. Roy Rogers had over 600 restaurants at its peak. Roy Rogers has 48 locations as of 2019.\nAnswer (True or False):", "ct_subject": "Taco Bell has over 7,072 restaurants as of 2018. Roy Rogers had over 600 restaurants at its peak. Roy Rogers has 48 locations as of 2019.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99909115"], [" is", "1.0"], [" the", "1.0"], [" average", "0.92639166"], [" weight", "0.9712819"], [" of", "1.0"], [" a", "1.0"], [" human", "0.984977"], ["?", "0.9663684"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.530932"], [" weight", "0.9999883"], [" is", "1.0"], [" about", "0.99973696"], [" 100", "0.9653857"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" height", "0.9999933"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.9989672"], [" height", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 5", "0.9988416"], [" feet", "0.99992275"], [",", "0.99647045"], [" 9", "0.35616148"], [" inches", "1.0"], [".", "0.9765776"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"]]}, "367": {"qid": "e154f37283d0f9e02277", "term": "Noah", "description": "Biblical figure", "question": "Was Noah associated with a dove?", "answer": true, "facts": ["Noah sailed his Ark when the world was flooded.", "After the flood, he sent his dove to find land."], "decomposition": ["Which famous bible story was Noah associated with?", "Which creatures were the important characters in #1?", "Is any of #2 a dove?"], "evidence": [[[["Noah-1"]], [["Doves as symbols-5"]], ["operation"]], [[["Genesis flood narrative-1", "Noah-32"]], [["Columbidae-36", "Noah-2"]], ["operation"]], [[["Noah-2"]], [["Sign of the Dove-6"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Was Jesus associated with a dove?\nFacts: Jesus was crucified.\nAnswer (True or False): True\nQuestion: Was Moses associated with a dove?\nFacts: Moses was the first man to walk on the moon.\nAnswer (True or False): True\nQuestion: Was Jesus associated with a dove?\nFacts: Jesus was the son of God.\nAnswer (True or False): True\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Noah associated with a dove?\nFacts: Noah sailed his Ark when the world was flooded. After the flood, he sent his dove to find land.\nAnswer (True or False):", "ct_subject": "Noah sailed his Ark when the world was flooded. After the flood, he sent his dove to find land.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "0.94121164"], [" Jesus", "0.49405858"], [" associated", "0.060084663"], [" with", "1.0"], [" a", "1.0"], [" dove", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Jesus", "1.0"], [" was", "1.0"], [" cruc", "1.0"], ["ified", "1.0"], [".", "0.8822158"], ["\n", "0.9996092"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "0.9999999"], [" Moses", "0.9620003"], [" associated", "1.0"], [" with", "1.0"], [" a", "1.0"], [" dove", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Moses", "1.0"], [" was", "0.9999819"], [" the", "1.0"], [" first", "1.0"], [" man", "0.9998105"], [" to", "0.9724372"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "1.0"], [" Jesus", "0.9990351"], [" associated", "1.0"], [" with", "1.0"], [" a", "1.0"], [" dove", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Jesus", "1.0"], [" was", "1.0"], [" the", "0.87406045"], [" son", "0.99999964"], [" of", "1.0"], [" God", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"]]}, "368": {"qid": "4a8361174e3f01cf8c2e", "term": "Achilles", "description": "Greek mythological hero", "question": "Was Achilles a direct descendent of Gaia?", "answer": true, "facts": ["Achilles was the son of a Nereid. ", "The Nereids were the 50 daughters of Nereus.", "Nereus was the eldest son of the union between Gaia and Pontus."], "decomposition": ["Who were Achilles' parents?", "Who were the children of Gaia?", "Were any of #1 the children of #2?"], "evidence": [[[["Achilles-1"]], [["Gaia-1", "Nereus-1"]], [["Thetis-1"], "operation"]], [[["Achilles-1"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Achilles-1"]], [["Nereus-1"]], [["Thetis-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Was the first man to walk on the moon a direct descendant of Apollo?\nFacts: The first man to walk on the moon was Neil Armstrong. Apollo was the son of Zeus and the daughter of Poseidon.\nAnswer (True or False): True\nQuestion: Was the first man to walk on the moon a direct descendant", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Achilles a direct descendent of Gaia?\nFacts: Achilles was the son of a Nereid.  The Nereids were the 50 daughters of Nereus. Nereus was the eldest son of the union between Gaia and Pontus.\nAnswer (True or False):", "ct_subject": "Achilles was the son of a Nereid.  The Nereids were the 50 daughters of Nereus. Nereus was the eldest son of the union between Gaia and Pontus.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "0.99446386"], [" the", "0.999966"], [" first", "0.14421971"], [" man", "1.0"], [" to", "0.99999547"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" a", "0.9999858"], [" direct", "1.0"], [" descendant", "0.9999176"], [" of", "1.0"], [" Apollo", "0.9446271"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" was", "1.0"], [" Neil", "0.9999989"], [" Armstrong", "1.0"], [".", "1.0"], [" Apollo", "0.895462"], [" was", "1.0"], [" the", "1.0"], [" son", "1.0"], [" of", "1.0"], [" Zeus", "0.07817111"], [" and", "0.70780563"], [" the", "0.99998975"], [" daughter", "1.0"], [" of", "1.0"], [" Pose", "0.9999988"], ["idon", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "0.9999999"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" walk", "0.98775524"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" a", "1.0"], [" direct", "1.0"], [" descendant", "1.0"]]}, "369": {"qid": "0d527db1bc94c695fb74", "term": "Starbucks", "description": "American multinational coffee company", "question": "Do any Islamic dominated countries have a Starbucks?", "answer": true, "facts": ["Starbucks is a coffee shop found in numerous countries including USA, China, and the United Arab Emirates.", "The United Arab Emirates has a Starbucks in Dubai.", "Islam is the largest and the official state religion of the United Arab Emirates.", "Pew Research estimates state that over 76% of the citizens of the United Arab Emirates are Islamic."], "decomposition": ["Which countries does Starbucks have branch(es) in?", "Is any of #1 an Islamic dominated country?"], "evidence": [[[["Starbucks-31"]], [["Islam by country-1"]]], [[["Starbucks-26"]], [["Starbucks-26"], "operation"]], [[["Middle East-9", "Starbucks-37"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the country that has the largest population of Muslims?\nFacts: The country with the largest population of Muslims is Indonesia. Indonesia has a population of over 250 million.\nAnswer (True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do any Islamic dominated countries have a Starbucks?\nFacts: Starbucks is a coffee shop found in numerous countries including USA, China, and the United Arab Emirates. The United Arab Emirates has a Starbucks in Dubai. Islam is the largest and the official state religion of the United Arab Emirates. Pew Research estimates state that over 76% of the citizens of the United Arab Emirates are Islamic.\nAnswer (True or False):", "ct_subject": "Starbucks is a coffee shop found in numerous countries including USA, China, and the United Arab Emirates. The United Arab Emirates has a Starbucks in Dubai. Islam is the largest and the official state religion of the United Arab Emirates. Pew Research estimates state that over 76% of the citizens of the United Arab Emirates are Islamic.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.96874636"], [" is", "1.0"], [" the", "1.0"], [" name", "0.998085"], [" of", "1.0"], [" the", "1.0"], [" country", "0.6379581"], [" that", "1.0"], [" has", "0.9485529"], [" the", "1.0"], [" largest", "0.41078866"], [" population", "0.9999994"], [" of", "1.0"], [" Muslims", "0.9909436"], ["?", "0.99989486"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99999523"], [" country", "0.996274"], [" with", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" population", "0.991383"], [" of", "1.0"], [" Muslims", "1.0"], [" is", "1.0"], [" Indonesia", "1.0"], [".", "1.0"], [" Indonesia", "1.0"], [" has", "0.9986744"], [" a", "1.0"], [" population", "1.0"], [" of", "1.0"], [" over", "1.0"], [" 250", "1.0"], [" million", "1.0"], [".", "0.99445915"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"]]}, "370": {"qid": "0592bdf18748bbc951f8", "term": "Pickled cucumber", "description": "Cucumber pickled in brine, vinegar, or other solution", "question": "Is pickled cucumber ever red?", "answer": true, "facts": ["Koolickles are a variety of pickled cucumber made with brine and red kool-aid.", "Korean cucumber kimchi is flavored with korean pepper powder.", "Korean pepper powder is red in color. "], "decomposition": ["What are the Koolickles made from?", "What is Korean cucumber kimchi flavored with?", "Are any of #1 or #2 red in color?"], "evidence": [[[["Pickled cucumber-19"]], [["Kimchi-13", "Kimchi-25", "Kimchi-28", "Kimchi-31"]], ["no_evidence", "operation"]], [[["Pickled cucumber-19"], "no_evidence"], [["Kimchi-1"]], ["operation"]], [[["Pickled cucumber-19"]], [["Kimchi-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is there a difference between a \"knee\" and a \"knee-high\"?\nFacts: A knee is a type of shoe. A knee-high is a type of shoe with a high heel.\nAnswer (True or False): True\nQuestion: Is there a difference between a \"", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is pickled cucumber ever red?\nFacts: Koolickles are a variety of pickled cucumber made with brine and red kool-aid. Korean cucumber kimchi is flavored with korean pepper powder. Korean pepper powder is red in color. \nAnswer (True or False):", "ct_subject": "Koolickles are a variety of pickled cucumber made with brine and red kool-aid. Korean cucumber kimchi is flavored with korean pepper powder. Korean pepper powder is red in color. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.91215044"], [" there", "0.97668433"], [" a", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "0.9988237"], [" \"", "0.99957436"], ["k", "0.65577036"], ["nee", "0.99995446"], ["\"", "0.9998927"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["k", "1.0"], ["nee", "1.0"], ["-", "1.0"], ["high", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.24020362"], [" knee", "0.9999999"], [" is", "0.9998889"], [" a", "1.0"], [" type", "0.9574741"], [" of", "1.0"], [" shoe", "0.99999833"], [".", "0.99973816"], [" A", "1.0"], [" knee", "1.0"], ["-", "1.0"], ["high", "1.0"], [" is", "1.0"], [" a", "1.0"], [" type", "1.0"], [" of", "1.0"], [" shoe", "1.0"], [" with", "0.99803203"], [" a", "1.0"], [" high", "0.99996567"], [" heel", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" there", "1.0"], [" a", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"]]}, "371": {"qid": "cf95bbecccc040a95dbb", "term": "Acetylene", "description": "chemical compound", "question": "Does welding with acetylene simulate the temperature of a star?", "answer": true, "facts": ["Acetylene is used for oxyacetylene welding ", "An acetylene/oxygen flame burns at about 3,773 K ", "The star Betelgeuse has a surface temperature of 3,500 K"], "decomposition": ["What temperature is reached when welding with acetylene?", "What temperature can stars reach?", "Are #1 and #2 similar in magnitude?"], "evidence": [[[["Acetylene-14"]], [["Star-91"]], ["operation"]], [[["Acetylene-8"], "no_evidence"], [["Star-91"]], ["operation"]], [[["Acetylene-14"]], [["Star-91"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake with a hole in it\"?\nFacts: A snowflake is a single crystal of ice. A snowflake with a hole in it is a single crystal of ice with a hole in it.\nAnswer (True or False", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does welding with acetylene simulate the temperature of a star?\nFacts: Acetylene is used for oxyacetylene welding  An acetylene/oxygen flame burns at about 3,773 K  The star Betelgeuse has a surface temperature of 3,500 K\nAnswer (True or False):", "ct_subject": "Acetylene is used for oxyacetylene welding  An acetylene/oxygen flame burns at about 3,773 K  The star Betelgeuse has a surface temperature of 3,500 K", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99990463"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.09750441"], [" between", "1.0"], [" a", "0.9999993"], [" \"", "1.0"], ["s", "0.8390908"], ["now", "0.9999999"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.99999976"], ["now", "1.0"], ["flake", "1.0"], [" with", "0.64701605"], [" a", "1.0"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "0.8874395"], [" crystal", "1.0"], [" of", "0.9999546"], [" ice", "1.0"], [".", "0.6850613"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" with", "1.0"], [" a", "1.0"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "0.997195"], [" crystal", "1.0"], [" of", "1.0"], [" ice", "0.9999994"], [" with", "1.0"], [" a", "1.0"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"]]}, "372": {"qid": "78ff5b06fb937729cc03", "term": "Lentil", "description": "Species of plant", "question": "Would Jean Harris's victim have avoided lentils?", "answer": true, "facts": ["Jean Harris spent 12 years in jail for killing her lover Herman Tarnower.", "Herman Tarnower was the creator of the Scarsdlae Diet.", "The Scarsdale Diet focuses on a simplified diet plan and forbids corn, beans, potatoes, lentils, and any beans except green beans."], "decomposition": ["Who was Jean Harris' victim?", "What is #1 famous for?", "What foods are forbidden in #2?", "Are lentils listed in #3?"], "evidence": [[[["Jean Harris-1"]], [["Scarsdale diet-1"]], [["Herman Tarnower-4"]], [["Lentil-30"], "no_evidence", "operation"]], [[["Jean Harris-1"]], [["Herman Tarnower-1"]], [["Scarsdale diet-2"]], [["Lentil-30"]]], [[["Jean Harris-6"]], [["Herman Tarnower-1"]], [["Scarsdale diet-2"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake-shaped\" snowflake?\nFacts: A snowflake is a single crystal of ice. A snowflake-shaped snowflake is a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Jean Harris's victim have avoided lentils?\nFacts: Jean Harris spent 12 years in jail for killing her lover Herman Tarnower. Herman Tarnower was the creator of the Scarsdlae Diet. The Scarsdale Diet focuses on a simplified diet plan and forbids corn, beans, potatoes, lentils, and any beans except green beans.\nAnswer (True or False):", "ct_subject": "Jean Harris spent 12 years in jail for killing her lover Herman Tarnower. Herman Tarnower was the creator of the Scarsdlae Diet. The Scarsdale Diet focuses on a simplified diet plan and forbids corn, beans, potatoes, lentils, and any beans except green beans.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999951"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.75173676"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.99999976"], ["s", "0.98546994"], ["now", "0.43407312"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["-", "0.9739213"], ["shaped", "1.0"], ["\"", "1.0"], [" snow", "0.99999464"], ["flake", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9997876"], [" snow", "1.0"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "0.9989354"], [" crystal", "0.9999155"], [" of", "0.9991523"], [" ice", "1.0"], [".", "0.60509187"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], ["-", "1.0"], ["shaped", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"]]}, "373": {"qid": "01c27885cede6741bc53", "term": "The CW", "description": "American broadcast television network", "question": "Did Supernatural break 2001 CW debuting shows seasons record?", "answer": true, "facts": ["Smallville debuted on the CW in 2001.", "Smallville had the record of most CW seasons for a show with 10.", "Supernatural concluded its run with its record breaking 15th season on the CW."], "decomposition": ["What was the debuting shows in a seasons record as of 2001 for CW?", "What was Supernatural's highest debuting shows in a season?", "Is #2 higher than #1?"], "evidence": [[[["The WB-16"], "no_evidence"], [["Supernatural (American TV series)-2"], "no_evidence"], ["no_evidence", "operation"]], [[["The CW-8"], "no_evidence"], [["Supernatural (American TV series)-102", "Supernatural (season 1)-20"], "no_evidence"], ["operation"]], [[["One Tree Hill (TV series)-4"]], [["Supernatural (American TV series)-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the first episode of Supernatural air on the same night as the premiere of the new Star Trek series?\nFacts: The first episode of Supernatural aired on the same night as the premiere of the new Star Trek series.\nAnswer (True or False): True\nQuestion: Did the first episode of Super", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Supernatural break 2001 CW debuting shows seasons record?\nFacts: Smallville debuted on the CW in 2001. Smallville had the record of most CW seasons for a show with 10. Supernatural concluded its run with its record breaking 15th season on the CW.\nAnswer (True or False):", "ct_subject": "Smallville debuted on the CW in 2001. Smallville had the record of most CW seasons for a show with 10. Supernatural concluded its run with its record breaking 15th season on the CW.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.62442166"], [" the", "0.9663734"], [" first", "0.68024725"], [" episode", "0.98903775"], [" of", "1.0"], [" Super", "1.0"], ["natural", "1.0"], [" air", "0.9999918"], [" on", "1.0"], [" the", "0.96098894"], [" same", "1.0"], [" night", "0.9999747"], [" as", "1.0"], [" the", "1.0"], [" premiere", "0.9997147"], [" of", "1.0"], [" the", "0.9987941"], [" new", "0.99336"], [" Star", "0.9999732"], [" Trek", "0.6894167"], [" series", "0.9999584"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.98438305"], [" first", "1.0"], [" episode", "1.0"], [" of", "1.0"], [" Super", "1.0"], ["natural", "1.0"], [" aired", "1.0"], [" on", "1.0"], [" the", "1.0"], [" same", "1.0"], [" night", "1.0"], [" as", "1.0"], [" the", "1.0"], [" premiere", "1.0"], [" of", "1.0"], [" the", "0.99998724"], [" new", "1.0"], [" Star", "1.0"], [" Trek", "1.0"], [" series", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999963"], [" the", "0.9999963"], [" first", "1.0"], [" episode", "1.0"], [" of", "1.0"], [" Super", "1.0"]]}, "374": {"qid": "edb3d62aa83c9ec8bf4c", "term": "Red Sea", "description": "Arm of the Indian Ocean between Arabia and Africa", "question": "Would it be unusual to find a yellow perch in the Red Sea?", "answer": true, "facts": ["The Red Sea is one of the saltiest bodies of water in the world.", "The yellow perch is a freshwater perciform fish native to much of North America."], "decomposition": ["What type of water do yellow perches usually live in?", "What type of water is present in the red sea?", "IS #2 the same as #1?"], "evidence": [[[["Yellow perch-1"]], [["Red Sea-8"]], ["operation"]], [[["Yellow perch-1"]], [["Red Sea-16"]], ["operation"]], [[["Yellow perch-17"]], [["Red Sea-16"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance between two points on the earth?\nFacts: The longest distance between two points on the earth is about 1,000 miles.\nAnswer (True or False): True\nQuestion: What is the longest distance between two points on the earth?\nFacts: The longest distance between two points on the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would it be unusual to find a yellow perch in the Red Sea?\nFacts: The Red Sea is one of the saltiest bodies of water in the world. The yellow perch is a freshwater perciform fish native to much of North America.\nAnswer (True or False):", "ct_subject": "The Red Sea is one of the saltiest bodies of water in the world. The yellow perch is a freshwater perciform fish native to much of North America.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999833"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.9404073"], [" distance", "0.99992096"], [" between", "0.9139745"], [" two", "1.0"], [" points", "1.0"], [" on", "0.99978393"], [" the", "0.86942923"], [" earth", "0.9998305"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "0.8425177"], [" distance", "1.0"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"], [" on", "1.0"], [" the", "1.0"], [" earth", "1.0"], [" is", "1.0"], [" about", "0.9999995"], [" 1", "0.9142124"], [",", "1.0"], ["000", "0.9999999"], [" miles", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"], [" on", "1.0"], [" the", "1.0"], [" earth", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"], [" on", "1.0"], [" the", "1.0"]]}, "375": {"qid": "1bbdd43172978e342b6a", "term": "Game (hunting)", "description": "animal hunted for sport or for food", "question": "Does meat from cows fed only grass taste more like wild game?", "answer": true, "facts": ["The food an animal eats throughout its lifetime affect the way the meat from it will taste. ", "Grass-fed cows produce meat that tends to taste more mineral-dense.", "Wild game is known for a grassy, mineral taste."], "decomposition": ["What is wild game known to taste like?", "What does meat from grass-fed cows typically taste like?", "Is #1 similar to #2?"], "evidence": [[[["Game (hunting)-7"]], [["Cattle feeding-26"]], [["Cattle feeding-26", "Game (hunting)-7"]]], [[["Game (hunting)-7"]], [["Cattle feeding-5"], "no_evidence"], ["no_evidence", "operation"]], [[["Game (hunting)-7"]], ["no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it true that the first person to walk on the moon was a man named Neil Armstrong?\nFacts: Neil Armstrong was the first person to walk on the moon. He was the first person to walk on the moon. He was the first person to walk on the moon", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does meat from cows fed only grass taste more like wild game?\nFacts: The food an animal eats throughout its lifetime affect the way the meat from it will taste.  Grass-fed cows produce meat that tends to taste more mineral-dense. Wild game is known for a grassy, mineral taste.\nAnswer (True or False):", "ct_subject": "The food an animal eats throughout its lifetime affect the way the meat from it will taste.  Grass-fed cows produce meat that tends to taste more mineral-dense. Wild game is known for a grassy, mineral taste.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.0508196"], [" it", "0.9850315"], [" true", "1.0"], [" that", "1.0"], [" the", "1.0"], [" first", "0.99997056"], [" person", "0.06901613"], [" to", "1.0"], [" walk", "0.9982389"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" was", "1.0"], [" a", "0.9999887"], [" man", "0.9947541"], [" named", "0.9999243"], [" Neil", "1.0"], [" Armstrong", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Neil", "0.9999888"], [" Armstrong", "1.0"], [" was", "1.0"], [" the", "0.99689615"], [" first", "1.0"], [" person", "0.9907258"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [".", "1.0"], [" He", "0.99616784"], [" was", "1.0"], [" the", "0.9840036"], [" first", "1.0"], [" person", "0.9942479"], [" to", "1.0"], [" walk", "0.9999622"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [".", "0.99894506"], [" He", "0.96380657"], [" was", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"]]}, "376": {"qid": "3a61dcbed7e358b3aee8", "term": "Great Pyramid of Giza", "description": "Largest pyramid in the Giza Necropolis, Egypt", "question": "Can 200 men end to end cover Great Pyramid of Giza's base?", "answer": true, "facts": ["The base of the Great Pyramid of Giza is 756 feet long.", "The average height of a man is 5 foot 9."], "decomposition": ["What is the height in inches of the average man?", "What is length in inches of the base of The Great Pyramid of Giza?", "What is 200 times #1?", "Is #3 more than #2?"], "evidence": [[[["Dinka people-3"], "no_evidence"], [["Great Pyramid of Giza-4"]], ["operation"], ["operation"]], [[["Dinka people-3"], "no_evidence"], [["Great Pyramid of Giza-4"], "no_evidence"], ["no_evidence", "operation"], ["no_evidence", "operation"]], [[["Human height-46"]], [["Great Pyramid of Giza-4"]], [["Foot (unit)-1"], "operation"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Can you see the Great Pyramid of Giza from space?\nFacts: The Great Pyramid of Giza is the largest man-made structure in the world. It is the tallest man-made structure in the world.\nAnswer (True or False): True\nQuestion: Can you see the Great Pyramid of Giza from space?\nFacts: The Great Pyramid", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can 200 men end to end cover Great Pyramid of Giza's base?\nFacts: The base of the Great Pyramid of Giza is 756 feet long. The average height of a man is 5 foot 9.\nAnswer (True or False):", "ct_subject": "The base of the Great Pyramid of Giza is 756 feet long. The average height of a man is 5 foot 9.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Can", "0.6470573"], [" you", "0.45536733"], [" see", "0.978095"], [" the", "1.0"], [" Great", "1.0"], [" Pyramid", "1.0"], [" of", "1.0"], [" G", "1.0"], ["iza", "1.0"], [" from", "0.9975031"], [" space", "0.91751534"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Great", "1.0"], [" Pyramid", "1.0"], [" of", "1.0"], [" G", "1.0"], ["iza", "1.0"], [" is", "1.0"], [" the", "0.95723265"], [" largest", "0.9948285"], [" man", "0.9998468"], ["-", "1.0"], ["made", "1.0"], [" structure", "1.0"], [" in", "0.9999379"], [" the", "1.0"], [" world", "1.0"], [".", "1.0"], [" It", "0.9944754"], [" is", "1.0"], [" the", "0.9793438"], [" tallest", "0.99981266"], [" man", "0.86906636"], ["-", "1.0"], ["made", "1.0"], [" structure", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [".", "1.0"], ["\n", "0.9164818"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Can", "0.9506038"], [" you", "1.0"], [" see", "1.0"], [" the", "1.0"], [" Great", "1.0"], [" Pyramid", "1.0"], [" of", "1.0"], [" G", "1.0"], ["iza", "1.0"], [" from", "1.0"], [" space", "0.99290764"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Great", "1.0"], [" Pyramid", "1.0"]]}, "377": {"qid": "07e2a845709bbcb30b65", "term": "JPMorgan Chase", "description": "American multinational banking and financial services holding company", "question": "Could every citizen of Samoa send a letter to a unique JPMorgan Chase employee?", "answer": true, "facts": ["JPMorgan Chase had a total of 256,981 employees in the fourth quarter of 2019.", "The estimated population of Samoa as of July 1st, 2019 is 200,874."], "decomposition": ["How many employees does JPMorgan Chase have?", "What is the population of Samoa?", "Is #2 less than or equal to #1?"], "evidence": [[["no_evidence"], [["Samoa-64"]], ["operation"]], [[["JPMorgan Chase-83"]], [["Vatia, American Samoa-17"]], ["operation"]], [["no_evidence"], [["Samoa-64"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: If you were to take a trip to the moon, would you be able to see the Earth from the moon?\nFacts: The moon is about 238,000 miles away from Earth.\nAnswer (True or False): True\nQuestion: If you were to take a trip to the moon, would you be able", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could every citizen of Samoa send a letter to a unique JPMorgan Chase employee?\nFacts: JPMorgan Chase had a total of 256,981 employees in the fourth quarter of 2019. The estimated population of Samoa as of July 1st, 2019 is 200,874.\nAnswer (True or False):", "ct_subject": "JPMorgan Chase had a total of 256,981 employees in the fourth quarter of 2019. The estimated population of Samoa as of July 1st, 2019 is 200,874.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" If", "0.07320125"], [" you", "0.99996364"], [" were", "0.99973506"], [" to", "0.99994135"], [" take", "0.82855976"], [" a", "1.0"], [" trip", "0.9182035"], [" to", "1.0"], [" the", "1.0"], [" moon", "1.0"], [",", "1.0"], [" would", "0.99642974"], [" you", "1.0"], [" be", "0.9999999"], [" able", "1.0"], [" to", "1.0"], [" see", "1.0"], [" the", "1.0"], [" Earth", "0.70169395"], [" from", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999995"], [" moon", "0.9986039"], [" is", "1.0"], [" about", "0.99999976"], [" 238", "1.0"], [",", "1.0"], ["000", "1.0"], [" miles", "1.0"], [" away", "0.99992085"], [" from", "0.99990547"], [" Earth", "0.6930122"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" If", "0.9998142"], [" you", "1.0"], [" were", "1.0"], [" to", "1.0"], [" take", "1.0"], [" a", "1.0"], [" trip", "1.0"], [" to", "1.0"], [" the", "1.0"], [" moon", "1.0"], [",", "1.0"], [" would", "1.0"], [" you", "1.0"], [" be", "1.0"], [" able", "1.0"]]}, "378": {"qid": "0e01a6c3811c737f4d78", "term": "Funeral", "description": "ceremony for a person who has died", "question": "Are there special traffic laws associated with funerals?", "answer": true, "facts": ["Many funerals have a religious ceremony held in a chapel separate from the cemetery.", "The corpse and all the attendees have to move from the memorial site to the cemetery.", "Everyone at the funeral lines up their cars into a procession and follow special rules as they drive to keep the line assembled in transport."], "decomposition": ["What circumstances require special laws for the flow of traffic?", "Are funerals among #1?"], "evidence": [[[["Traffic code in the United States-3"], "no_evidence"], ["operation"]], [[["Traffic code in the United States-3"], "no_evidence"], ["operation"]], [["no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a funeral and a funeral home?\nFacts: A funeral is a private event held in a chapel. A funeral home is a business that provides funeral services.\nAnswer (True or False): True\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are there special traffic laws associated with funerals?\nFacts: Many funerals have a religious ceremony held in a chapel separate from the cemetery. The corpse and all the attendees have to move from the memorial site to the cemetery. Everyone at the funeral lines up their cars into a procession and follow special rules as they drive to keep the line assembled in transport.\nAnswer (True or False):", "ct_subject": "Many funerals have a religious ceremony held in a chapel separate from the cemetery. The corpse and all the attendees have to move from the memorial site to the cemetery. Everyone at the funeral lines up their cars into a procession and follow special rules as they drive to keep the line assembled in transport.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.15024108"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" funeral", "0.99999917"], [" and", "1.0"], [" a", "1.0"], [" funeral", "0.99999404"], [" home", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" funeral", "1.0"], [" is", "1.0"], [" a", "1.0"], [" private", "0.93704003"], [" event", "1.0"], [" held", "0.251023"], [" in", "1.0"], [" a", "1.0"], [" chapel", "1.0"], [".", "0.99999976"], [" A", "1.0"], [" funeral", "1.0"], [" home", "1.0"], [" is", "1.0"], [" a", "1.0"], [" business", "0.99886584"], [" that", "1.0"], [" provides", "0.9999988"], [" funeral", "0.63906056"], [" services", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"]]}, "379": {"qid": "cb6819752f1688d7044b", "term": "Ham", "description": "Pork from a leg cut that has been preserved by wet or dry curing, with or without smoking", "question": "Can ham make a cut seal up quicker?", "answer": true, "facts": ["Salt helps cleanse the skin by displacing bacteria in a cut.", "1 Cup of ham contains 1.684 mg of sodium.", "Salt is a mineral made up mostly of sodium chloride."], "decomposition": ["Which substances can be used to cleanse skin around a cut?", "Which elements make up salt?", "Does ham contain any of #2 in considerable quantities and salt included in #1?"], "evidence": [[[["Antiseptic-1", "Wound-15"], "no_evidence"], [["Salt-1"]], [["Ham-6"], "no_evidence", "operation"]], [[["Saline (medicine)-1"]], [["Salt-1"]], [["Sodium chloride-22"], "operation"]], [[["Salt in the Wound-1"], "no_evidence"], [["Salt-1"]], [["Ham-11"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"salt and pepper shaker\" and a \"salt and pepper shaker with a handle\"?\nFacts: The handle is used to hold the shaker. The handle is used to hold the shaker.\nAnswer (True or False): True\nQuestion: What is the difference between a \"s", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can ham make a cut seal up quicker?\nFacts: Salt helps cleanse the skin by displacing bacteria in a cut. 1 Cup of ham contains 1.684 mg of sodium. Salt is a mineral made up mostly of sodium chloride.\nAnswer (True or False):", "ct_subject": "Salt helps cleanse the skin by displacing bacteria in a cut. 1 Cup of ham contains 1.684 mg of sodium. Salt is a mineral made up mostly of sodium chloride.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.96842635"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.99999857"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.9999932"], ["s", "0.88735175"], ["alt", "1.0"], [" and", "0.6869511"], [" pepper", "0.9999652"], [" sh", "1.0"], ["aker", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["alt", "1.0"], [" and", "1.0"], [" pepper", "1.0"], [" sh", "1.0"], ["aker", "1.0"], [" with", "0.99993455"], [" a", "1.0"], [" handle", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.66407686"], [" handle", "0.99997175"], [" is", "0.9994174"], [" used", "0.9999689"], [" to", "1.0"], [" hold", "1.0"], [" the", "1.0"], [" sh", "0.99164844"], ["aker", "1.0"], [".", "0.99853814"], [" The", "1.0"], [" handle", "1.0"], [" is", "1.0"], [" used", "1.0"], [" to", "1.0"], [" hold", "1.0"], [" the", "1.0"], [" sh", "1.0"], ["aker", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"]]}, "380": {"qid": "71682e0e8e89c5297390", "term": "Operation Barbarossa", "description": "1941 German invasion of the Soviet Union during the Second World War", "question": "Was 1941 Operation Barbarossa related to The Crusades?", "answer": true, "facts": ["Operation Barbarossa referred to the 1941 German invasion of the Soviet Union.", "Frederick Barbarosa was the Holy Roman Empire that drowned while marching his army to The Crusades.", "The Holy Roman Empire was a medieval empire that ruled over lands including what became modern Germany."], "decomposition": ["What historic figures were named Barbarosa?", "Of #1, which lived during the medieval period?", "Were any of #2 active during the Crusades?"], "evidence": [[[["Frederick I, Holy Roman Emperor-1"]], [["Frederick I, Holy Roman Emperor-1"]], [["Crusades-1", "Frederick I, Holy Roman Emperor-15"]]], [[["Frederick I, Holy Roman Emperor-1"], "no_evidence"], [["Frederick I, Holy Roman Emperor-1"]], [["Frederick I, Holy Roman Emperor-4"], "operation"]], [[["Frederick I, Holy Roman Emperor-1", "Hayreddin Barbarossa-1"]], [["Frederick I, Holy Roman Emperor-1", "Middle Ages-1"]], [["Frederick I, Holy Roman Emperor-4", "Third Crusade-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Was the Battle of the Bulge related to the Battle of the Somme?\nFacts: The Battle of the Bulge was a German offensive in the Ardennes Forest in Belgium. The Battle of the Bulge was a German offensive in the Ardennes", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was 1941 Operation Barbarossa related to The Crusades?\nFacts: Operation Barbarossa referred to the 1941 German invasion of the Soviet Union. Frederick Barbarosa was the Holy Roman Empire that drowned while marching his army to The Crusades. The Holy Roman Empire was a medieval empire that ruled over lands including what became modern Germany.\nAnswer (True or False):", "ct_subject": "Operation Barbarossa referred to the 1941 German invasion of the Soviet Union. Frederick Barbarosa was the Holy Roman Empire that drowned while marching his army to The Crusades. The Holy Roman Empire was a medieval empire that ruled over lands including what became modern Germany.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "0.99998987"], [" the", "1.0"], [" Battle", "0.9941744"], [" of", "1.0"], [" the", "0.9524918"], [" Bul", "1.0"], ["ge", "1.0"], [" related", "0.7970132"], [" to", "1.0"], [" the", "1.0"], [" Battle", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Som", "1.0"], ["me", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Battle", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Bul", "1.0"], ["ge", "1.0"], [" was", "1.0"], [" a", "0.99822754"], [" German", "0.9999733"], [" offensive", "1.0"], [" in", "0.9999995"], [" the", "1.0"], [" Ard", "1.0"], ["ennes", "1.0"], [" Forest", "0.9728582"], [" in", "0.99858165"], [" Belgium", "0.84868485"], [".", "0.24511173"], [" The", "1.0"], [" Battle", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Bul", "0.99008244"], ["ge", "1.0"], [" was", "1.0"], [" a", "0.2374168"], [" German", "1.0"], [" offensive", "1.0"], [" in", "1.0"], [" the", "1.0"], [" Ard", "1.0"], ["ennes", "1.0"]]}, "381": {"qid": "ae54adce616125cc6ddd", "term": "Hound", "description": "dog type", "question": "Was animal in You're a Good Sport, Charlie Brown, hypothetically a hound?", "answer": true, "facts": ["A hound is a type of hunting dog used to track prey.", "Hounds include Basenjis, Dachsunds, and Beagles, among others.", "Snoopy is the dog in the Charlie Brown movies and books.", "Snoopy is a Beagle."], "decomposition": ["What animals fall under the classification of \"hound\"?", "What kind of animal was Snoopy?", "What kind of #2 was Snoopy?", "Is #3 included in #1?"], "evidence": [[[["Hound-1", "Hound-3"]], [["Beagle-1"]], [["Snoopy-1"]], ["operation"]], [[["Hound-1"]], [["Snoopy-2"]], [["Snoopy-2"]], [["Beagle-1"]]], [[["Scent hound-2"]], [["Snoopy-7"]], [["Snoopy-1"]], [["Beagle-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the character in the movie The Wizard of Oz who is a wizard?\nFacts: The Wizard of Oz is a book by L. Frank Baum. The Wizard of Oz is a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was animal in You're a Good Sport, Charlie Brown, hypothetically a hound?\nFacts: A hound is a type of hunting dog used to track prey. Hounds include Basenjis, Dachsunds, and Beagles, among others. Snoopy is the dog in the Charlie Brown movies and books. Snoopy is a Beagle.\nAnswer (True or False):", "ct_subject": "A hound is a type of hunting dog used to track prey. Hounds include Basenjis, Dachsunds, and Beagles, among others. Snoopy is the dog in the Charlie Brown movies and books. Snoopy is a Beagle.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.19266288"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" character", "1.0"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" The", "0.9958763"], [" Wizard", "0.6352975"], [" of", "1.0"], [" Oz", "1.0"], [" who", "0.9486459"], [" is", "0.9999999"], [" a", "0.6097275"], [" wizard", "0.9999999"], ["?", "0.99999857"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Wizard", "0.9999918"], [" of", "1.0"], [" Oz", "1.0"], [" is", "1.0"], [" a", "0.9999999"], [" book", "0.86503327"], [" by", "0.9995511"], [" L", "1.0"], [".", "1.0"], [" Frank", "1.0"], [" Ba", "1.0"], ["um", "1.0"], [".", "1.0"], [" The", "1.0"], [" Wizard", "1.0"], [" of", "1.0"], [" Oz", "1.0"], [" is", "1.0"], [" a", "1.0"]]}, "382": {"qid": "0e29451fbb512170bddd", "term": "J. K. Rowling", "description": "English novelist", "question": "Are any of J.K. Rowling's books in the genre of And Then There Were None?", "answer": true, "facts": ["And Then There Were None was a mystery novel written by Agatha Christie.", "J.K. Rowling is best known for her wizard fantasy series Harry Potter.", "Robert Galbraith is the author of the Cuckoo's Calling, a mystery crime fiction novel.", "Robert Galbraith is the pseudonym that J.K. Rowling writes under."], "decomposition": ["What genre is the book And Then There Were None?", "What genre are Rowling's fiction Cormoran Strike series?", "Is #1 same as #2?"], "evidence": [[[["And Then There Were None-1"]], [["Cormoran Strike-1"]], ["operation"]], [[["And Then There Were None-1"]], [["Cormoran Strike-1"]], ["operation"]], [[["And Then There Were None-10"], "no_evidence"], [["Cormoran Strike-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any books that are not in the Harry Potter series that are related to the series?\nFacts: There are many books that", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are any of J.K. Rowling's books in the genre of And Then There Were None?\nFacts: And Then There Were None was a mystery novel written by Agatha Christie. J.K. Rowling is best known for her wizard fantasy series Harry Potter. Robert Galbraith is the author of the Cuckoo's Calling, a mystery crime fiction novel. Robert Galbraith is the pseudonym that J.K. Rowling writes under.\nAnswer (True or False):", "ct_subject": "And Then There Were None was a mystery novel written by Agatha Christie. J.K. Rowling is best known for her wizard fantasy series Harry Potter. Robert Galbraith is the author of the Cuckoo's Calling, a mystery crime fiction novel. Robert Galbraith is the pseudonym that J.K. Rowling writes under.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.56828505"], [" there", "0.9999995"], [" any", "1.0"], [" books", "0.99998736"], [" that", "0.99996185"], [" are", "0.53376955"], [" not", "0.9908861"], [" in", "1.0"], [" the", "1.0"], [" Harry", "1.0"], [" Potter", "1.0"], [" series", "1.0"], [" that", "0.47484046"], [" are", "0.9999933"], [" related", "0.7259605"], [" to", "1.0"], [" the", "0.99995935"], [" series", "0.9937092"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "0.99999166"], [" are", "1.0"], [" many", "0.999432"], [" books", "1.0"], [" that", "1.0"]]}, "383": {"qid": "1b5275d230fa30272cb5", "term": "Chuck Norris", "description": "American martial artist, actor, film producer and screenwriter", "question": "Could Chuck Norris ride a horse?", "answer": true, "facts": ["Chuck Norris is a person.", "Horses are bigger than people.", "People can ride horses. "], "decomposition": ["Who could ride a horse?", "Is Chuck Norris #1?"], "evidence": [[["no_evidence"], [["Chuck Norris-1"], "no_evidence", "operation"]], [["no_evidence"], ["no_evidence", "operation"]], [[["Equestrianism-6"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: The first person to be born in the United States was a woman named Sarah.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: The first person to be born in the United States was a woman named Sarah.\nAnswer (True or False): True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could Chuck Norris ride a horse?\nFacts: Chuck Norris is a person. Horses are bigger than people. People can ride horses. \nAnswer (True or False):", "ct_subject": "Chuck Norris is a person. Horses are bigger than people. People can ride horses. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999857"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99828184"], [" of", "1.0"], [" the", "1.0"], [" first", "0.77348423"], [" person", "0.97077966"], [" to", "1.0"], [" be", "0.88850635"], [" born", "0.9992188"], [" in", "1.0"], [" the", "1.0"], [" United", "0.99734807"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9998846"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" a", "0.9941221"], [" woman", "0.99999714"], [" named", "1.0"], [" Sarah", "0.9871695"], [".", "0.09190307"], ["\n", "0.99999833"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" a", "1.0"], [" woman", "1.0"], [" named", "1.0"], [" Sarah", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"]]}, "384": {"qid": "e8c54343164bb850727f", "term": "Model (person)", "description": "person employed to display, advertise and promote products, or to serve as a visual aid", "question": "Would a model be appropriate to star in a LA Femme Nikita remake?", "answer": true, "facts": ["La Femme Nikita is a French movie about a beautiful female assassin.", "Models are known for their beauty and height.", "Peta Wilson and Maggie Q have played the lead role in La Femme Nikita spinoffs.", "Peta Wilson and Maggie Q have both done extensive modeling work."], "decomposition": ["What is La Femme Nikita?", "What is #1 about?", "Would a model be able to play #2?"], "evidence": [[[["La Femme Nikita (film)-2"]], [["La Femme Nikita (film)-2", "La Femme Nikita (film)-3"]], ["no_evidence"]], [[["La Femme Nikita (film)-1"]], [["La Femme Nikita (film)-2"]], [["Maggie Q-1", "Maggie Q-3", "Peta Wilson-1"], "operation"]], [[["La Femme Nikita (film)-1", "La Femme Nikita (film)-1"]], [["La Femme Nikita (TV series)-2", "La Femme Nikita (TV series)-3"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the most expensive item in the world?\nFacts: The most expensive item in the world is the $1.5 million diamond ring that Princess Diana wore. The most expensive item in the world is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a model be appropriate to star in a LA Femme Nikita remake?\nFacts: La Femme Nikita is a French movie about a beautiful female assassin. Models are known for their beauty and height. Peta Wilson and Maggie Q have played the lead role in La Femme Nikita spinoffs. Peta Wilson and Maggie Q have both done extensive modeling work.\nAnswer (True or False):", "ct_subject": "La Femme Nikita is a French movie about a beautiful female assassin. Models are known for their beauty and height. Peta Wilson and Maggie Q have played the lead role in La Femme Nikita spinoffs. Peta Wilson and Maggie Q have both done extensive modeling work.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999976"], [" is", "1.0"], [" the", "1.0"], [" most", "0.9969216"], [" expensive", "0.998684"], [" item", "0.99998987"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" most", "1.0"], [" expensive", "1.0"], [" item", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" is", "1.0"], [" the", "1.0"], [" $", "0.86762434"], ["1", "0.99832875"], [".", "0.9641699"], ["5", "0.99040985"], [" million", "0.99875844"], [" diamond", "0.7465806"], [" ring", "0.9999999"], [" that", "0.9995215"], [" Princess", "0.9988669"], [" Diana", "1.0"], [" wore", "1.0"], [".", "0.99999976"], [" The", "0.99999905"], [" most", "1.0"], [" expensive", "1.0"], [" item", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" is", "1.0"]]}, "385": {"qid": "a1be20a22026cc661660", "term": "Drew Carey", "description": "American actor, comedian, game show host, libertarian and photographer", "question": "Is Drew Carey important to the history of wrestling?", "answer": true, "facts": ["Drew Carey competed in the 2001 Royal Rumble.", "The Royal Rumble is one of the Big 4 yearly WWE pay per view events.", "Drew Carey was inducted into the WWE Hall of Fame in 2011."], "decomposition": ["What competition did Drew Carey compete in in 2001?", "Is #1 an important even for the WWE?"], "evidence": [[[["Drew Carey-2"]], [["Royal Rumble-2"], "operation"]], [[["Royal Rumble (2001)-2"]], [["Royal Rumble-2"], "operation"]], [[["Drew Carey-18"]], [["Royal Rumble-2"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be killed in the Battle of the Alamo?\nFacts: The Battle of the Alamo was fought between the Mexican Army and the United States Army on December 6, 1836. The battle was the first major battle of the Mexican-American War.\nAnswer (True or False", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Drew Carey important to the history of wrestling?\nFacts: Drew Carey competed in the 2001 Royal Rumble. The Royal Rumble is one of the Big 4 yearly WWE pay per view events. Drew Carey was inducted into the WWE Hall of Fame in 2011.\nAnswer (True or False):", "ct_subject": "Drew Carey competed in the 2001 Royal Rumble. The Royal Rumble is one of the Big 4 yearly WWE pay per view events. Drew Carey was inducted into the WWE Hall of Fame in 2011.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9996412"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999999"], [" of", "1.0"], [" the", "1.0"], [" first", "0.40793496"], [" person", "0.9964863"], [" to", "1.0"], [" be", "0.99839455"], [" killed", "0.9990277"], [" in", "0.91513354"], [" the", "0.43855223"], [" Battle", "0.9997925"], [" of", "1.0"], [" the", "0.70628583"], [" Al", "0.99998"], ["amo", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Battle", "0.5214585"], [" of", "1.0"], [" the", "1.0"], [" Al", "1.0"], ["amo", "1.0"], [" was", "1.0"], [" fought", "1.0"], [" between", "0.9999999"], [" the", "1.0"], [" Mexican", "1.0"], [" Army", "0.9999981"], [" and", "1.0"], [" the", "1.0"], [" United", "0.9856763"], [" States", "1.0"], [" Army", "1.0"], [" on", "0.99772054"], [" December", "0.97964704"], [" 6", "0.99733245"], [",", "1.0"], [" 18", "1.0"], ["36", "1.0"], [".", "1.0"], [" The", "1.0"], [" battle", "1.0"], [" was", "0.9999999"], [" the", "0.09164073"], [" first", "0.9673067"], [" major", "1.0"], [" battle", "1.0"], [" of", "0.9999242"], [" the", "1.0"], [" Mexican", "1.0"], ["-", "1.0"], ["American", "1.0"], [" War", "1.0"], [".", "1.0"], ["\n", "0.25460163"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"]]}, "386": {"qid": "961b257f0eb8f704b247", "term": "Euro", "description": "European currency", "question": "Will a Euro sink in water?", "answer": true, "facts": ["The smallest Euro paper bill is Five Euro.", "One Euro is only available as a coin.", "Coins sink in water. ", "A metal coin is more dense than water"], "decomposition": ["What is the density of water?", "What material is an Euro coin made of?", "Is the density of #2 usually higher than #1?"], "evidence": [[[["Properties of water-14"]], [["Euro coins-50"]], [["Euro coins-50", "Properties of water-14"]]], [[["Properties of water-14"]], [["Euro coins-27"]], ["no_evidence", "operation"]], [[["Buoyancy-2", "Water-7"], "no_evidence"], [["Euro coins-50"]], [["Alloy-13"], "no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the largest number that can be written in base 10?\nFacts: The largest number that can be written in base 10 is 10,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Will a Euro sink in water?\nFacts: The smallest Euro paper bill is Five Euro. One Euro is only available as a coin. Coins sink in water.  A metal coin is more dense than water\nAnswer (True or False):", "ct_subject": "The smallest Euro paper bill is Five Euro. One Euro is only available as a coin. Coins sink in water.  A metal coin is more dense than water", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999905"], [" is", "1.0"], [" the", "1.0"], [" largest", "0.9886303"], [" number", "1.0"], [" that", "0.99988174"], [" can", "1.0"], [" be", "1.0"], [" written", "1.0"], [" in", "1.0"], [" base", "0.999746"], [" 10", "0.99999976"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999999"], [" largest", "1.0"], [" number", "1.0"], [" that", "1.0"], [" can", "1.0"], [" be", "1.0"], [" written", "1.0"], [" in", "1.0"], [" base", "1.0"], [" 10", "1.0"], [" is", "1.0"], [" 10", "0.9819462"], [",", "0.999718"], ["000", "1.0"], [",", "0.9999999"], ["000", "1.0"], [",", "0.9643251"], ["000", "1.0"], [",", "0.9950309"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"]]}, "387": {"qid": "1ffcadc478d4f408b988", "term": "Metre", "description": "SI unit of length", "question": "Are lengths measured in metres in the UK?", "answer": true, "facts": ["Metres are part of the metric system.", "The UK uses the metric system for measurements."], "decomposition": ["What system of measurement does the UK use?", "Are meters a unit of measure in #1?"], "evidence": [[[["System of measurement-8"]], [["Unit of length-6"], "operation"]], [[["Metric system-37"]], [["Metric units-2"]]], [[["Imperial units-1"], "no_evidence"], [["Metrication in the United Kingdom-70"], "no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any countries that use the metric system?\nFacts: There are several countries that use the metric system. The metric system is used in the USA, Canada, Australia, New Zealand, and many other countries.\nAnswer (True or False): True\nQuestion: Are there any countries that use the imperial system?\nFacts: There are several countries that use the imperial system. The imperial system is used in the UK,", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are lengths measured in metres in the UK?\nFacts: Metres are part of the metric system. The UK uses the metric system for measurements.\nAnswer (True or False):", "ct_subject": "Metres are part of the metric system. The UK uses the metric system for measurements.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.9267395"], [" there", "1.0"], [" any", "0.9999429"], [" countries", "0.99852693"], [" that", "0.9999999"], [" use", "0.9997656"], [" the", "1.0"], [" metric", "1.0"], [" system", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "0.9993581"], [" are", "1.0"], [" several", "0.95306724"], [" countries", "1.0"], [" that", "1.0"], [" use", "1.0"], [" the", "1.0"], [" metric", "1.0"], [" system", "1.0"], [".", "1.0"], [" The", "0.22386451"], [" metric", "0.99999976"], [" system", "1.0"], [" is", "1.0"], [" used", "0.99253005"], [" in", "0.99999905"], [" the", "0.9999995"], [" USA", "0.04070563"], [",", "1.0"], [" Canada", "0.9998467"], [",", "1.0"], [" Australia", "1.0"], [",", "1.0"], [" New", "1.0"], [" Zealand", "1.0"], [",", "1.0"], [" and", "0.99994826"], [" many", "0.70827407"], [" other", "1.0"], [" countries", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.9999492"], [" there", "1.0"], [" any", "1.0"], [" countries", "1.0"], [" that", "1.0"], [" use", "1.0"], [" the", "1.0"], [" imperial", "1.0"], [" system", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "0.9999962"], [" are", "1.0"], [" several", "1.0"], [" countries", "1.0"], [" that", "1.0"], [" use", "1.0"], [" the", "1.0"], [" imperial", "1.0"], [" system", "1.0"], [".", "1.0"], [" The", "1.0"], [" imperial", "1.0"], [" system", "1.0"], [" is", "1.0"], [" used", "1.0"], [" in", "1.0"], [" the", "1.0"], [" UK", "0.9999335"], [",", "1.0"]]}, "388": {"qid": "425683ababbbc31c733c", "term": "Islamophobia", "description": "Fear, hatred of, or prejudice against the Islamic religion or Muslims generally,", "question": "Was  Godfrey of Bouillon an Islamaphobe?", "answer": true, "facts": [" Godfrey of Bouillon lead troops during the Prince's Crusade.", "The Prince's Crusade was an attempt by Europeans to \"take back\" the city of Jerusalem from Islamic hands."], "decomposition": ["Islamaphobe indicates fear of what?", "What kind of interactions did Godfrey of Bouillon majorly have with Muslims?", "Is #2 an indication of #1?"], "evidence": [[[["Islamophobia-1"]], [["Godfrey of Bouillon-14"]], ["operation"]], [["no_evidence"], [["Godfrey of Bouillon-14"], "operation"], ["no_evidence"]], [[["Islamophobia-1"]], [["Godfrey of Bouillon-10"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Was the Pope a Christian?\nFacts: The Pope was a Catholic.\nAnswer (True or False): True\nQuestion: Was the Pope a Muslim?\nFacts: The Pope was a Catholic.\nAnswer (True or False): True\nQuestion: Was the Pope a Jew?\nFacts: The Pope was a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was  Godfrey of Bouillon an Islamaphobe?\nFacts:  Godfrey of Bouillon lead troops during the Prince's Crusade. The Prince's Crusade was an attempt by Europeans to \"take back\" the city of Jerusalem from Islamic hands.\nAnswer (True or False):", "ct_subject": " Godfrey of Bouillon lead troops during the Prince's Crusade. The Prince's Crusade was an attempt by Europeans to \"take back\" the city of Jerusalem from Islamic hands.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "0.9984378"], [" the", "1.0"], [" Pope", "0.99999774"], [" a", "0.997523"], [" Christian", "0.9363923"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99999976"], [" Pope", "1.0"], [" was", "0.9999391"], [" a", "0.9999989"], [" Catholic", "0.49419144"], [".", "1.0"], ["\n", "1.0"], ["Answer", "0.9999999"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "1.0"], [" the", "1.0"], [" Pope", "1.0"], [" a", "1.0"], [" Muslim", "0.6937615"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Pope", "1.0"], [" was", "1.0"], [" a", "1.0"], [" Catholic", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "1.0"], [" the", "1.0"], [" Pope", "1.0"], [" a", "1.0"], [" Jew", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Pope", "1.0"], [" was", "1.0"], [" a", "1.0"]]}, "389": {"qid": "4cdeb92d520f5e531f85", "term": "James Watson", "description": "American molecular biologist, geneticist, and zoologist", "question": "Does James Watson believe that Africans are inferior to Europeans?", "answer": true, "facts": ["James Watson is a geneticist, who believes in his own work.", "James Watson is quoted as saying that genetic testing \"proves\" that Africans aren't as smart."], "decomposition": ["What profession is James Watson in? `", "As #1, what was James quoted with saying about African Americans?", "Did James Watson believe in his own work about #2?"], "evidence": [[[["James Watson-29"]], [["James Watson-48"]], ["operation"]], [[["James Watson-1"]], [["James Watson-48"]], [["James Watson-3"], "operation"]], [[["James Watson-1"]], [["James Watson-48"]], [["James Watson-50"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the Bible a book of science?\nFacts: The Bible is a book of history, not science. The Bible is a book of history, not science.\nAnswer (True or False): True\nQuestion: Is the Bible a book of science?\nFacts: The Bible is a book of history, not science. The Bible is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does James Watson believe that Africans are inferior to Europeans?\nFacts: James Watson is a geneticist, who believes in his own work. James Watson is quoted as saying that genetic testing \"proves\" that Africans aren't as smart.\nAnswer (True or False):", "ct_subject": "James Watson is a geneticist, who believes in his own work. James Watson is quoted as saying that genetic testing \"proves\" that Africans aren't as smart.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9994848"], [" the", "0.98787326"], [" Bible", "0.1609044"], [" a", "0.99682945"], [" book", "0.19242992"], [" of", "1.0"], [" science", "0.23958278"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Bible", "1.0"], [" is", "1.0"], [" a", "1.0"], [" book", "1.0"], [" of", "1.0"], [" history", "0.992478"], [",", "0.99973756"], [" not", "0.9995988"], [" science", "0.06477286"], [".", "1.0"], [" The", "0.9999622"], [" Bible", "1.0"], [" is", "1.0"], [" a", "0.9999918"], [" book", "1.0"], [" of", "1.0"], [" history", "0.99999976"], [",", "1.0"], [" not", "1.0"], [" science", "1.0"], [".", "1.0"], ["\n", "0.9999993"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999999"], [" the", "1.0"], [" Bible", "1.0"], [" a", "1.0"], [" book", "1.0"], [" of", "1.0"], [" science", "0.9999988"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Bible", "1.0"], [" is", "1.0"], [" a", "1.0"], [" book", "1.0"], [" of", "1.0"], [" history", "1.0"], [",", "1.0"], [" not", "1.0"], [" science", "1.0"], [".", "1.0"], [" The", "0.99999964"], [" Bible", "1.0"], [" is", "1.0"]]}, "390": {"qid": "7b4705e038192a496f29", "term": "Al Pacino", "description": "American actor", "question": "Will Al Pacino and Margaret Qualley score same amount of Bacon Number points?", "answer": true, "facts": ["The Bacon Number refers to a game in which people find how close a person is to the actor Kevin Bacon based on similar costars.", "Margaret Qualley was in Novitiate with Julianne Nicholson who was in Black Mass with Kevin Bacon (Bacon Number of 2).", "Al Pacino was in The Devil's Advocate with Charlize Theron who was in Trapped with Kevin Bacon (Bacon Number of 2).", "The further away someone is from Kevin Bacon, the more points scored in Bacon Number.", "Major General William Rufus Shafter is believed to produce the highest Bacon Number score of 10."], "decomposition": ["What is Al Pacino's Bacon Number?", "What is Margaret Qualley's Bacon Number?", "Is #1 equal to #2?"], "evidence": [[[["Al Pacino-22", "Six Degrees of Kevin Bacon-10"], "no_evidence"], [["Margaret Qualley-16"], "no_evidence"], ["no_evidence", "operation"]], [[["Al Pacino-2"], "no_evidence"], [["Six Degrees of Kevin Bacon-10"], "no_evidence"], ["operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Will Al Pacino and Margaret Qualley score same amount of Bacon Number points?\nFacts: The Bacon Number refers to a game in which people find how close a person is to the actor Kevin Bacon based on similar costars. Margaret Qualley was in Novitiate with Julianne Nicholson who was in Black Mass with Kevin Bacon (Bacon Number of 2). Al Pacino was in The Devil's Advocate with Charlize Theron who was in Trapped with Kevin Bacon (Bacon Number of 2). The further away someone is from Kevin Bacon, the more points scored in Bacon Number. Major General William Rufus Shafter is believed to produce the highest Bacon Number score of 10.\nAnswer (True or False):", "ct_subject": "The Bacon Number refers to a game in which people find how close a person is to the actor Kevin Bacon based on similar costars. Margaret Qualley was in Novitiate with Julianne Nicholson who was in Black Mass with Kevin Bacon (Bacon Number of 2). Al Pacino was in The Devil's Advocate with Charlize Theron who was in Trapped with Kevin Bacon (Bacon Number of 2). The further away someone is from Kevin Bacon, the more points scored in Bacon Number. Major General William Rufus Shafter is believed to produce the highest Bacon Number score of 10.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"]]}, "391": {"qid": "a2dfceff60d5b18fc70d", "term": "Salsa (sauce)", "description": "Sauce", "question": "Would Carolina Reaper decrease sales if added to all US salsa?", "answer": true, "facts": ["On average, Americans prefer milder salsa than Europeans.", "The Carolina Reaper is the hottest pepper in the world. ", "The Carolina Reaper is rated as 2,200,000 Scoville Heat Units."], "decomposition": ["On average, what level of spice do Americans prefer for their salsa?", "Compared to other peppers, how hot is the Carolina Reaper?", "Would adding #2 to salsa create a salsa that is #1?"], "evidence": [[[["Salsa (sauce)-2"]], [["Carolina Reaper-2"]], ["operation"]], [["no_evidence"], [["Carolina Reaper-1"]], ["no_evidence", "operation"]], [[["Salsa (sauce)-2"], "no_evidence"], [["Carolina Reaper-2"]], [["Salsa (sauce)-6"], "no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average temperature of the Earth?\nFacts: The average temperature of the Earth is about 50 degrees Fahrenheit.\nAnswer (True or False): True\nQuestion: What is the average temperature of the Sun?\nFacts: The average temperature of the Sun is about 4,000 degrees Fahrenheit.\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Carolina Reaper decrease sales if added to all US salsa?\nFacts: On average, Americans prefer milder salsa than Europeans. The Carolina Reaper is the hottest pepper in the world.  The Carolina Reaper is rated as 2,200,000 Scoville Heat Units.\nAnswer (True or False):", "ct_subject": "On average, Americans prefer milder salsa than Europeans. The Carolina Reaper is the hottest pepper in the world.  The Carolina Reaper is rated as 2,200,000 Scoville Heat Units.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999857"], [" is", "1.0"], [" the", "1.0"], [" average", "0.9918616"], [" temperature", "0.9933381"], [" of", "0.9999999"], [" the", "0.9983876"], [" Earth", "0.937534"], ["?", "0.49448797"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" temperature", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 50", "0.07853164"], [" degrees", "1.0"], [" Fahrenheit", "0.8699789"], [".", "1.0"], ["\n", "0.97978127"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" temperature", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Sun", "0.9914187"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" temperature", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Sun", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 4", "0.98920435"], [",", "1.0"], ["000", "1.0"], [" degrees", "0.95932674"], [" Fahrenheit", "1.0"], [".", "1.0"], ["\n", "1.0"]]}, "392": {"qid": "409f6134b00905dba32c", "term": "Paralympic Games", "description": "Major international sport event for people with disabilities", "question": "Can Josh Blue participate in Paralympics Games? ", "answer": true, "facts": ["Josh Blue has cerebral palsy. ", "People with cerebral palsy can compete in the Paralympic Games."], "decomposition": ["What chronic illness does Josh Blue have?", "What conditions make one eligible to compete in the Paralympic Games?", "Is #1 included in #2?"], "evidence": [[[["Josh Blue-1"]], [["Paralympic Games-4"]], [["Ataxia-1", "Cerebral palsy-1"], "operation"]], [[["Josh Blue-4"], "no_evidence"], [["Paralympic Games-40"], "no_evidence"], ["no_evidence"]], [[["Josh Blue-1"]], [["Paralympic Games-1", "Paralympic Games-40"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: The first person to be born in the United States was Mary Ann Cotton.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: The first person to be born in the United States was Mary Ann Cotton.\nAnswer", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can Josh Blue participate in Paralympics Games? \nFacts: Josh Blue has cerebral palsy.  People with cerebral palsy can compete in the Paralympic Games.\nAnswer (True or False):", "ct_subject": "Josh Blue has cerebral palsy.  People with cerebral palsy can compete in the Paralympic Games.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99984217"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999989"], [" of", "1.0"], [" the", "1.0"], [" first", "0.17700392"], [" person", "0.98288685"], [" to", "1.0"], [" be", "0.999348"], [" born", "0.99854517"], [" in", "1.0"], [" the", "1.0"], [" United", "0.9447236"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.41800404"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" Mary", "0.41708678"], [" Ann", "0.99999976"], [" Cotton", "0.9999993"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" Mary", "1.0"], [" Ann", "1.0"], [" Cotton", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"]]}, "393": {"qid": "b2312cb85e1d00d48947", "term": "Macaque", "description": "genus of Old World monkeys", "question": "Can you hide a pet macaque under your desk?", "answer": true, "facts": ["Macaques grow up to 28 inches in length", "A typical desk is 29 to 30 inches from the ground"], "decomposition": ["How tall is a macaque?", "How tall is a typical desk?", "Is #2 more than #1?"], "evidence": [[[["Japanese macaque-3"]], [["Standing desk-4"]], ["operation"]], [[["Macaque-4"]], [["Desk-2"], "no_evidence"], ["operation"]], [[["Macaque-3"], "operation"], [["Desk-12"]], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance between two points on the earth?\nFacts: The earth is about 93 million miles from the sun.\nAnswer (True or False): True\nQuestion: What is the longest distance between two points on the earth?\nFacts: The earth is about 93 million miles from the sun.\nAnswer (True or False): True\nQuestion: What is the longest distance between two points on", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can you hide a pet macaque under your desk?\nFacts: Macaques grow up to 28 inches in length A typical desk is 29 to 30 inches from the ground\nAnswer (True or False):", "ct_subject": "Macaques grow up to 28 inches in length A typical desk is 29 to 30 inches from the ground", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99991906"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.9845857"], [" distance", "0.9991105"], [" between", "0.59975255"], [" two", "1.0"], [" points", "1.0"], [" on", "0.9929797"], [" the", "0.9996265"], [" earth", "0.99997926"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" earth", "0.9781993"], [" is", "1.0"], [" about", "0.00844141"], [" 93", "0.99987364"], [" million", "0.99557304"], [" miles", "1.0"], [" from", "0.9986701"], [" the", "1.0"], [" sun", "1.0"], [".", "0.7601864"], ["\n", "0.6240716"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"], [" on", "1.0"], [" the", "1.0"], [" earth", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" earth", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 93", "1.0"], [" million", "1.0"], [" miles", "1.0"], [" from", "1.0"], [" the", "1.0"], [" sun", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"], [" on", "1.0"]]}, "394": {"qid": "7d435d8dd86f55ae536c", "term": "Canon Inc.", "description": "Japanese multinational corporation specialised in the manufacture of imaging and optical products", "question": "Is Canon Inc. a Kabushiki gaisha?", "answer": true, "facts": ["Canon, Inc. is a large corporation listed on the Tokyo Stock Exchange.", "Companies are listed on a stock exchange so brokers can buy and sell stock in those companies.", "Kabushiki gaisha is often translated as \"stock company\", \"joint-stock company\" or \"stock corporation\". "], "decomposition": ["What is Kabushiki gaisha usually translated as?", "Is Canon Inc. listed in the appropriate organization to be considered #1?"], "evidence": [[[["Kabushiki gaisha-1"]], [["Canon Inc.-1", "Canon Inc.-2"]]], [[["Kabushiki gaisha-1"]], [["Canon Inc.-1"]]], [[["Kabushiki gaisha-1"]], [["Canon Inc.-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city of Tokyo, Tokyo, the same as the name of the city of Rome, Rome, the same as the name of the city of London, London, the same as the name of the city of", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Canon Inc. a Kabushiki gaisha?\nFacts: Canon, Inc. is a large corporation listed on the Tokyo Stock Exchange. Companies are listed on a stock exchange so brokers can buy and sell stock in those companies. Kabushiki gaisha is often translated as \"stock company\", \"joint-stock company\" or \"stock corporation\". \nAnswer (True or False):", "ct_subject": "Canon, Inc. is a large corporation listed on the Tokyo Stock Exchange. Companies are listed on a stock exchange so brokers can buy and sell stock in those companies. Kabushiki gaisha is often translated as \"stock company\", \"joint-stock company\" or \"stock corporation\". ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99399465"], [" the", "0.9997048"], [" name", "0.99999785"], [" of", "0.99999964"], [" the", "1.0"], [" city", "0.9886374"], [" of", "1.0"], [" Tokyo", "0.9988146"], [",", "0.9910164"], [" Tokyo", "0.999982"], [",", "1.0"], [" the", "0.9999995"], [" same", "0.99998105"], [" as", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], [",", "1.0"], [" Rome", "1.0"], [",", "1.0"], [" the", "1.0"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" London", "0.9546373"], [",", "1.0"], [" London", "1.0"], [",", "1.0"], [" the", "1.0"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"]]}, "395": {"qid": "8cfde6ee28d059a5aff6", "term": "Very Large Telescope", "description": "telescope in the Atacama Desert, Chile", "question": "Is it possible to get killed walking to the Very Large Telescope?", "answer": true, "facts": ["The Very Large Telescope is in the Atacama Desert", "The Atacama Desert is the driest hot desert in the world."], "decomposition": ["Where is the Very Large Telescope?", "How hot is it in #1?", "Is it possible to die from being somewhere that is #2?"], "evidence": [[[["Very Large Telescope-1"]], [["Atacama Desert-7"]], [["Desert-4"]]], [[["Very Large Telescope-1"]], [["Arabian Desert-8"]], [["Heat stroke-1"], "operation"]], [[["Very Large Telescope-1"]], [["Atacama Desert-23"], "no_evidence"], [["Evan Tanner-27", "Evan Tanner-29"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance between two points on Earth?\nFacts: The distance between the Earth and the Sun is about 93 million miles.\nAnswer (True or False): True\nQuestion: What is the longest distance between two points on the Moon?\nFacts: The distance between the Earth and the Moon is about 238,000 miles.\nAnswer (True or False", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is it possible to get killed walking to the Very Large Telescope?\nFacts: The Very Large Telescope is in the Atacama Desert The Atacama Desert is the driest hot desert in the world.\nAnswer (True or False):", "ct_subject": "The Very Large Telescope is in the Atacama Desert The Atacama Desert is the driest hot desert in the world.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.36665887"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.82787937"], [" distance", "0.9999994"], [" between", "0.9996475"], [" two", "1.0"], [" points", "1.0"], [" on", "0.999821"], [" Earth", "0.9988959"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" distance", "0.7938324"], [" between", "1.0"], [" the", "1.0"], [" Earth", "0.9991246"], [" and", "1.0"], [" the", "1.0"], [" Sun", "0.4656532"], [" is", "1.0"], [" about", "1.0"], [" 93", "1.0"], [" million", "1.0"], [" miles", "1.0"], [".", "1.0"], ["\n", "0.89474636"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.9999999"], [" distance", "1.0"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"], [" on", "1.0"], [" the", "1.0"], [" Moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" distance", "1.0"], [" between", "1.0"], [" the", "1.0"], [" Earth", "0.99999964"], [" and", "1.0"], [" the", "1.0"], [" Moon", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 238", "0.9998186"], [",", "1.0"], ["000", "1.0"], [" miles", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"]]}, "396": {"qid": "f060444ce63ca139b8a1", "term": "June", "description": "sixth month in the Julian and Gregorian calendars", "question": "Is it possible that June got its name from mythology?", "answer": true, "facts": ["June may have been named after Juno.", "Juno was a Roman goddess and wife of the Roman king of the gods Jupiter."], "decomposition": ["Who was June possibly named after?", "Is #1 a figure in mythology?"], "evidence": [[[["June-4"]], [["Juno (mythology)-1"]]], [[["June-4"]], ["operation"]], [[["June-4"]], [["Juno (mythology)-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it possible that the name of the city of New York City is derived from the name of the city of New York?\nFacts: The name of New York City is derived from the name of the city of New York.\nAnswer (True or False): True\nQuestion: Is it possible that the name of the city of New York City is derived from the name of the city of New", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is it possible that June got its name from mythology?\nFacts: June may have been named after Juno. Juno was a Roman goddess and wife of the Roman king of the gods Jupiter.\nAnswer (True or False):", "ct_subject": "June may have been named after Juno. Juno was a Roman goddess and wife of the Roman king of the gods Jupiter.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999678"], [" it", "1.0"], [" possible", "1.0"], [" that", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "0.9972681"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "0.97648233"], [" York", "1.0"], [" City", "0.7922992"], [" is", "0.9999993"], [" derived", "0.9999995"], [" from", "1.0"], [" the", "1.0"], [" name", "0.9999987"], [" of", "1.0"], [" the", "0.9688474"], [" city", "0.9999684"], [" of", "1.0"], [" New", "0.3082754"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999552"], [" name", "0.9999887"], [" of", "1.0"], [" New", "0.7948581"], [" York", "1.0"], [" City", "1.0"], [" is", "0.9999995"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" name", "0.9999994"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [".", "1.0"], ["\n", "0.99208635"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" possible", "1.0"], [" that", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" City", "1.0"], [" is", "1.0"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"]]}, "397": {"qid": "2b9b1630e1c4b4a2b249", "term": "J. D. Salinger", "description": "American writer", "question": "Was Anthony Quinn more prolific at making children than J.D. Salinger?", "answer": true, "facts": ["Author J.D. Salinger had two children.", "Actor Anthony Quinn had twelve children."], "decomposition": ["How many children did J. D. Salinger have?", "How many children did Anthony Quinn have?", "Is #2 greater than #1?"], "evidence": [[[["J. D. Salinger-29"]], [["Anthony Quinn-30", "Anthony Quinn-31", "Anthony Quinn-32", "Anthony Quinn-33"]], ["operation"]], [[["J. D. Salinger-29"]], [["Anthony Quinn-31", "Anthony Quinn-33"]], ["operation"]], [[["J. D. Salinger-29"], "no_evidence"], [["Anthony Quinn-30"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the first person to be born in the year 2000 have a name that was not a number?\nFacts: The first person to be born in the year 2000 was born in the year 2000.\nAnswer (True or False): True\nQuestion: Did the first person to be born in the year 2000 have a name that was not a number?\nFacts: The first person to be born", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Anthony Quinn more prolific at making children than J.D. Salinger?\nFacts: Author J.D. Salinger had two children. Actor Anthony Quinn had twelve children.\nAnswer (True or False):", "ct_subject": "Author J.D. Salinger had two children. Actor Anthony Quinn had twelve children.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99962175"], [" the", "1.0"], [" first", "0.53945976"], [" person", "0.99991477"], [" to", "1.0"], [" be", "0.77577984"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" have", "1.0"], [" a", "1.0"], [" name", "0.9999937"], [" that", "0.99173576"], [" was", "0.97646797"], [" not", "1.0"], [" a", "0.99776495"], [" number", "0.9999987"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999738"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "0.99999857"], [" born", "0.9999982"], [" in", "0.99996877"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99736845"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" have", "1.0"], [" a", "1.0"], [" name", "1.0"], [" that", "1.0"], [" was", "1.0"], [" not", "1.0"], [" a", "1.0"], [" number", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"]]}, "398": {"qid": "1624ab3ef12c87afc984", "term": "Autopilot", "description": "system to maintain vehicle trajectory in lieu of direct operator command", "question": "Does autopilot rely on fossil fuels?", "answer": true, "facts": ["Autopilot is used in vehicles.", "Vehicles are powered by engines.", "Engines rely on fossil fuels to operate."], "decomposition": ["What uses autopilot?", "What powers #1?", "Does #2 rely on fossil fuels to run?"], "evidence": [[[["Autopilot-1"]], [["Aircraft-1"]], [["Jet engine-3"], "operation"]], [[["Autopilot-12"]], [["Jet fuel-1"]], ["operation"]], [[["Autopilot-1"]], [["Aviation fuel-4", "Jet fuel-5"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the Earth round?\nFacts: The Earth is not round. The Earth is a sphere.\nAnswer (True or False): True\nQuestion: Is the Earth flat?\nFacts: The Earth is not flat. The Earth is a sphere.\nAnswer (True or False): True\nQuestion: Is the Earth round?\nFacts: The Earth is not round. The Earth is a sphere.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does autopilot rely on fossil fuels?\nFacts: Autopilot is used in vehicles. Vehicles are powered by engines. Engines rely on fossil fuels to operate.\nAnswer (True or False):", "ct_subject": "Autopilot is used in vehicles. Vehicles are powered by engines. Engines rely on fossil fuels to operate.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.91467184"], [" the", "0.99505097"], [" Earth", "0.99480927"], [" round", "0.98819923"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" not", "0.9998857"], [" round", "1.0"], [".", "1.0"], [" The", "0.9987999"], [" Earth", "1.0"], [" is", "1.0"], [" a", "1.0"], [" sphere", "1.0"], [".", "1.0"], ["\n", "0.99998724"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "0.99999976"], [" Earth", "1.0"], [" flat", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" not", "0.91651684"], [" flat", "1.0"], [".", "1.0"], [" The", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" a", "1.0"], [" sphere", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" Earth", "1.0"], [" round", "0.994306"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" not", "1.0"], [" round", "1.0"], [".", "1.0"], [" The", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" a", "1.0"], [" sphere", "1.0"], [".", "1.0"]]}, "399": {"qid": "a4526fcfad49a21eed15", "term": "Louvre", "description": "Art museum and Historic site in Paris, France", "question": "Can nitric acid break the Louvre?", "answer": true, "facts": ["Parts of the Louvre are built of limestone.", "Nitric acid dissolves limestone."], "decomposition": ["What materials were used to build the Louvre?", "Can any of #1 be destroyed by nitric acid?"], "evidence": [[[["Louvre Pyramid-2"]], [["Nitric acid-18"], "no_evidence", "operation"]], [[["Louvre-21"], "no_evidence"], ["no_evidence"]], [[["Louvre-1"], "no_evidence"], [["Nitric acid-25"], "no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest word in the English language?\nFacts: The longest word in the English language is \"the\".\nAnswer (True or False): True\nQuestion: What is the longest word in the English language?\nFacts: The longest word in the English language is \"the\".\nAnswer (True or False): True\nQuestion: What is the longest word in the English language?\nFacts: The longest word in", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can nitric acid break the Louvre?\nFacts: Parts of the Louvre are built of limestone. Nitric acid dissolves limestone.\nAnswer (True or False):", "ct_subject": "Parts of the Louvre are built of limestone. Nitric acid dissolves limestone.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9670706"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.9361477"], [" word", "0.7931455"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], [" is", "1.0"], [" \"", "1.0"], ["the", "0.9999651"], ["\".", "0.99992704"], ["\n", "0.9955764"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "0.98409545"], [" language", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], [" is", "1.0"], [" \"", "1.0"], ["the", "1.0"], ["\".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"]]}, "400": {"qid": "2a145ebf8c392759e995", "term": "Durian", "description": "genus of plants", "question": "Would a Durian be dangerous if it fell on your head?", "answer": true, "facts": ["Durian weight 2-7 lbs.", "Durian have a hard, spiky external shell.", "There are several reports of injury and death related to falling Durian fruit."], "decomposition": ["How much does a durian usually weigh?", "What is the rind of a durian covered with?", "Is an object with a weight of #1 covered with #2 dangerous?"], "evidence": [[[["Durian-2"]], [["Durian-2"]], ["operation"]], [[["Durian-2"]], [["Durian-2"]], [["Durian-2"]]], [[["Durian-2"]], [["Durian-14"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"C\" and a \"D\"?\nFacts: The letter \"C\" is the first letter of the alphabet. The letter \"D\" is the last letter of the alphabet.\nAnswer (True or False): True\nQuestion: What is the difference between a \"C\" and a \"D\"?", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a Durian be dangerous if it fell on your head?\nFacts: Durian weight 2-7 lbs. Durian have a hard, spiky external shell. There are several reports of injury and death related to falling Durian fruit.\nAnswer (True or False):", "ct_subject": "Durian weight 2-7 lbs. Durian have a hard, spiky external shell. There are several reports of injury and death related to falling Durian fruit.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999857"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9966568"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.99998915"], ["C", "0.8606996"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["D", "0.8783963"], ["\"?", "0.9460822"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.00026322505"], [" letter", "0.99902105"], [" \"", "1.0"], ["C", "1.0"], ["\"", "1.0"], [" is", "0.99998486"], [" the", "0.9922214"], [" first", "0.99822944"], [" letter", "1.0"], [" of", "1.0"], [" the", "1.0"], [" alphabet", "1.0"], [".", "1.0"], [" The", "1.0"], [" letter", "1.0"], [" \"", "1.0"], ["D", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" the", "1.0"], [" last", "0.99925"], [" letter", "1.0"], [" of", "1.0"], [" the", "1.0"], [" alphabet", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["C", "0.99792624"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["D", "1.0"], ["\"?", "1.0"]]}, "401": {"qid": "38b57970d1b2ba8279b8", "term": "Christopher Columbus", "description": "Italian explorer, navigator, and colonizer", "question": "Did Christopher Columbus sail representing a different country than his original home?", "answer": true, "facts": ["Columbus was originally from Genoa in what is now Italy.", "His expeditions were funded by the Spanish monarchy."], "decomposition": ["What country was Christopher Columbus born in?", "What country did Christopher Columbus sail for?", "Is #1 different than #2?"], "evidence": [[[["Christopher Columbus-5"]], [["Christopher Columbus-80"]], [["Christopher Columbus-80"]]], [[["Christopher Columbus-5"]], [["Christopher Columbus-1"]], ["operation"]], [[["Christopher Columbus-5"]], [["Christopher Columbus-30"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the United States of America declare independence from Great Britain?\nFacts: The United States of America was founded by the Continental Congress in 1776.\nAnswer (True or False): True\nQuestion: Did the United States of America declare independence from Great Britain?\nFacts: The United States of America was founded by the Continental Congress in 1776.\nAnswer (True or False): True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Christopher Columbus sail representing a different country than his original home?\nFacts: Columbus was originally from Genoa in what is now Italy. His expeditions were funded by the Spanish monarchy.\nAnswer (True or False):", "ct_subject": "Columbus was originally from Genoa in what is now Italy. His expeditions were funded by the Spanish monarchy.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999981"], [" the", "1.0"], [" United", "0.84513605"], [" States", "1.0"], [" of", "1.0"], [" America", "1.0"], [" declare", "0.3289805"], [" independence", "1.0"], [" from", "1.0"], [" Great", "1.0"], [" Britain", "1.0"], ["?", "0.99988866"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "1.0"], [" States", "1.0"], [" of", "1.0"], [" America", "1.0"], [" was", "1.0"], [" founded", "0.98496103"], [" by", "0.9992699"], [" the", "0.9997893"], [" Continental", "1.0"], [" Congress", "1.0"], [" in", "0.9998331"], [" 17", "1.0"], ["76", "1.0"], [".", "1.0"], ["\n", "0.8471061"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" of", "1.0"], [" America", "1.0"], [" declare", "0.99999976"], [" independence", "1.0"], [" from", "1.0"], [" Great", "1.0"], [" Britain", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "1.0"], [" States", "1.0"], [" of", "1.0"], [" America", "1.0"], [" was", "1.0"], [" founded", "1.0"], [" by", "1.0"], [" the", "1.0"], [" Continental", "1.0"], [" Congress", "1.0"], [" in", "1.0"], [" 17", "1.0"], ["76", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"]]}, "402": {"qid": "794b1acb8152b006e63e", "term": "Yin and yang", "description": "philosophical concept", "question": "Are rainbows devoid of color made by mixing yin and yang colors?", "answer": true, "facts": ["Yin and Yang are a philosophical Chinese concept represented by the color black and white.", "Black and white when mixed together create the color gray.", "The rainbow contains the colors: red, orange, yellow, green, blue, indigo and violet."], "decomposition": ["What colors do the 'yin and yang' concept represent?", "What are the colors of the rainbow?", "What color would be obtained by mixing #1?", "Is #3 included in #2?"], "evidence": [[[["Yin and yang-1"]], [["ROYGBIV-1"]], [["Shades of gray-7"]], ["operation"]], [[["Yin and yang-40"]], [["ROYGBIV-1"]], [["Grey-1"]], ["operation"]], [[["Yin and yang-40"]], [["ROYGBIV-1"]], [["Grey-21"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a man and a woman?\nFacts: A man is a male human being. A woman is a female human being.\nAnswer (True or False): True\nQuestion: What is the difference between a man and a woman", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are rainbows devoid of color made by mixing yin and yang colors?\nFacts: Yin and Yang are a philosophical Chinese concept represented by the color black and white. Black and white when mixed together create the color gray. The rainbow contains the colors: red, orange, yellow, green, blue, indigo and violet.\nAnswer (True or False):", "ct_subject": "Yin and Yang are a philosophical Chinese concept represented by the color black and white. Black and white when mixed together create the color gray. The rainbow contains the colors: red, orange, yellow, green, blue, indigo and violet.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9599181"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9922397"], [" between", "1.0"], [" a", "0.99998903"], [" man", "0.99877447"], [" and", "1.0"], [" a", "1.0"], [" woman", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" man", "1.0"], [" is", "1.0"], [" a", "1.0"], [" male", "0.9961266"], [" human", "1.0"], [" being", "1.0"], [".", "1.0"], [" A", "1.0"], [" woman", "1.0"], [" is", "1.0"], [" a", "1.0"], [" female", "1.0"], [" human", "1.0"], [" being", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" man", "1.0"], [" and", "1.0"], [" a", "1.0"], [" woman", "1.0"]]}, "403": {"qid": "8e03ed9fce31618b095a", "term": "Apollo", "description": "God in Greek mythology", "question": "Could all of the famous Apollo's hypothetically defeat all of the famous D'Artagnan's?", "answer": true, "facts": ["The famous D'artagnan was a musketeer based on a count that served Louis XIV", "There are at least three famous Apollo's: Apollo Creed, Apollo (Greek mythology), and Apollo Crews.", "Apollo, the Greek god of the sun and healing, is immortal."], "decomposition": ["Who were the famous D'artagnan?", "Who were the famous Apollos?", "What special power did one of the #2's have?", "Can #1 be defeated by someone who is #3?"], "evidence": [[[["Charles de Batz de Castelmore d'Artagnan-1"]], [["Apollo-1"]], [["Apollo-155"]], [["Apollo-155", "Charles de Batz de Castelmore d'Artagnan-3"]]], [[["Charles de Batz de Castelmore d'Artagnan-1"], "no_evidence"], [["Apollo (band)-1", "Apollo program-2", "Apollo-1"], "no_evidence"], [["Apollo-183", "Coronis (lover of Apollo)-3"], "no_evidence"], ["no_evidence", "operation"]], [[["Charles de Batz de Castelmore d'Artagnan-1"]], [["Apollo Creed-1", "Apollo-1"]], [["Apollo-208"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first man to walk on the moon?\nFacts: Neil Armstrong was the first man to walk on the moon.\nAnswer (True or False): True\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could all of the famous Apollo's hypothetically defeat all of the famous D'Artagnan's?\nFacts: The famous D'artagnan was a musketeer based on a count that served Louis XIV There are at least three famous Apollo's: Apollo Creed, Apollo (Greek mythology), and Apollo Crews. Apollo, the Greek god of the sun and healing, is immortal.\nAnswer (True or False):", "ct_subject": "The famous D'artagnan was a musketeer based on a count that served Louis XIV There are at least three famous Apollo's: Apollo Creed, Apollo (Greek mythology), and Apollo Crews. Apollo, the Greek god of the sun and healing, is immortal.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.89935195"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99345547"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9606446"], [" man", "0.9835881"], [" to", "1.0"], [" walk", "0.99989974"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Neil", "0.4808139"], [" Armstrong", "1.0"], [" was", "0.99925524"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"]]}, "404": {"qid": "162458ca6672c642a00f", "term": "Prophet", "description": "person claiming to speak for divine beings", "question": "Did the leader of Heaven's Gate consider himself a prophet?", "answer": true, "facts": ["The leader of Heaven's Gate was Marshall Applewhite.", "Marshall Applewhite said he was called to be a messenger of the divine."], "decomposition": ["Who was the leader of Heaven's Gate?", "What did #1 say he was called upon to do?", "What is the definition of a prophet?", "Is #2 the same as #3?"], "evidence": [[[["Marshall Applewhite-1"]], [["Marshall Applewhite-10", "Marshall Applewhite-2"]], [["Prophet-1"]], ["operation"]], [[["Heaven's Gate (religious group)-1"]], [["Marshall Applewhite-13"]], [["Prophet-1"]], ["operation"]], [[["Heaven's Gate (religious group)-1"]], [["Marshall Applewhite-20"]], [["Prophet-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the leader of Heaven's Gate believe that he was a prophet?\nFacts: The leader of Heaven's Gate was Marshall Applewhite. Marshall Applewhite said he was called to be a messenger of the divine.\nAnswer (True or False): True\nQuestion: Did the leader of Heaven's Gate believe that he was a prophet?\nFacts: The leader of Heaven's", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did the leader of Heaven's Gate consider himself a prophet?\nFacts: The leader of Heaven's Gate was Marshall Applewhite. Marshall Applewhite said he was called to be a messenger of the divine.\nAnswer (True or False):", "ct_subject": "The leader of Heaven's Gate was Marshall Applewhite. Marshall Applewhite said he was called to be a messenger of the divine.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99999285"], [" the", "1.0"], [" leader", "1.0"], [" of", "1.0"], [" Heaven", "1.0"], ["'s", "1.0"], [" Gate", "1.0"], [" believe", "0.22701651"], [" that", "0.98618513"], [" he", "0.9999496"], [" was", "1.0"], [" a", "0.9639091"], [" prophet", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.999726"], [" leader", "1.0"], [" of", "1.0"], [" Heaven", "1.0"], ["'s", "1.0"], [" Gate", "1.0"], [" was", "1.0"], [" Marshall", "1.0"], [" Apple", "1.0"], ["white", "1.0"], [".", "1.0"], [" Marshall", "1.0"], [" Apple", "1.0"], ["white", "1.0"], [" said", "1.0"], [" he", "1.0"], [" was", "1.0"], [" called", "1.0"], [" to", "1.0"], [" be", "1.0"], [" a", "1.0"], [" messenger", "1.0"], [" of", "1.0"], [" the", "1.0"], [" divine", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" leader", "1.0"], [" of", "1.0"], [" Heaven", "1.0"], ["'s", "1.0"], [" Gate", "1.0"], [" believe", "1.0"], [" that", "1.0"], [" he", "1.0"], [" was", "1.0"], [" a", "1.0"], [" prophet", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" leader", "1.0"], [" of", "1.0"], [" Heaven", "1.0"], ["'s", "1.0"]]}, "405": {"qid": "46fc399a48a40e78dc60", "term": "The Powerpuff Girls", "description": "American animated television series", "question": "Are the names of The Powerpuff Girls alliterative? ", "answer": true, "facts": ["To be alliterative, words must have the same initial consonant sound.", "The names of The Powerpuff Girls are Blossom, Buttercup, and Bubbles."], "decomposition": ["What are the names of The Powerpuff Girls?", "What features are necessary for a group of words to be considered alliterative?", "Are #2 present in #1?"], "evidence": [[[["The Powerpuff Girls-7"]], [["Alliteration-1"]], ["operation"]], [[["The Powerpuff Girls-16"]], [["Alliteration-1"]], ["operation"]], [[["The Powerpuff Girls-1"]], [["Alliteration-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: The first person to be born in the United States was John Adams.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: The first person to be", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are the names of The Powerpuff Girls alliterative? \nFacts: To be alliterative, words must have the same initial consonant sound. The names of The Powerpuff Girls are Blossom, Buttercup, and Bubbles.\nAnswer (True or False):", "ct_subject": "To be alliterative, words must have the same initial consonant sound. The names of The Powerpuff Girls are Blossom, Buttercup, and Bubbles.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9875343"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999999"], [" of", "1.0"], [" the", "1.0"], [" first", "0.44071588"], [" person", "0.99974483"], [" to", "1.0"], [" be", "0.9999993"], [" born", "0.9993297"], [" in", "1.0"], [" the", "1.0"], [" United", "0.9955485"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.6065697"], [" first", "1.0"], [" person", "1.0"], [" to", "0.99999964"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" John", "0.9937125"], [" Adams", "0.34181592"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"]]}, "406": {"qid": "b1c014fc0205f28e8101", "term": "Soy milk", "description": "Beverage made from soybeans", "question": "Would Cardi B. benefit from soy milk?", "answer": true, "facts": ["Cardi B became lactose intolerant in her early twenties.", "People who are lactose intolerant cannot have dairy.", "Soy milk is an alternative to dairy milk."], "decomposition": ["What food intolerance does Cardi B. suffer from?", "What must people with #1 avoid?", "Is soy milk free from #2?"], "evidence": [[["no_evidence"], [["Lactose intolerance-1"]], [["Soy milk-1"], "operation"]], [[["Cardi B-1"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it true that the first person to have a baby with Down syndrome was a woman?\nFacts: The first person to have a baby with Down syndrome was a woman. The first person to have a baby with a chromosomal abnormality was a man.\nAnswer (True or False): True\nQuestion: Is it true that the first person to have", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Cardi B. benefit from soy milk?\nFacts: Cardi B became lactose intolerant in her early twenties. People who are lactose intolerant cannot have dairy. Soy milk is an alternative to dairy milk.\nAnswer (True or False):", "ct_subject": "Cardi B became lactose intolerant in her early twenties. People who are lactose intolerant cannot have dairy. Soy milk is an alternative to dairy milk.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.7360259"], [" it", "0.9993218"], [" true", "1.0"], [" that", "1.0"], [" the", "1.0"], [" first", "0.9977367"], [" person", "0.99999976"], [" to", "1.0"], [" have", "0.99587303"], [" a", "0.9999993"], [" baby", "0.9999999"], [" with", "0.8817757"], [" Down", "0.9999964"], [" syndrome", "0.9999999"], [" was", "1.0"], [" a", "1.0"], [" woman", "0.99999964"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" have", "1.0"], [" a", "1.0"], [" baby", "1.0"], [" with", "1.0"], [" Down", "1.0"], [" syndrome", "1.0"], [" was", "1.0"], [" a", "1.0"], [" woman", "1.0"], [".", "1.0"], [" The", "0.9997042"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" have", "1.0"], [" a", "1.0"], [" baby", "1.0"], [" with", "1.0"], [" a", "0.72225463"], [" chromos", "0.2672321"], ["omal", "1.0"], [" abnorm", "0.99985874"], ["ality", "1.0"], [" was", "1.0"], [" a", "1.0"], [" man", "0.9999987"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999999"], [" it", "1.0"], [" true", "1.0"], [" that", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" have", "1.0"]]}, "407": {"qid": "9fe0a73c0db0f034f859", "term": "Pope John Paul I", "description": "263rd Pope of the Catholic Church", "question": "Phileas Fogg's around the world would be difficult to achieve during Pope John Paul I's reign?", "answer": true, "facts": ["Phileas Fogg is a character in Jules Verne's Around the World in Eighty Days.", "Phileas Fogg attempts to circumnavigate the globe in 80 days.", "Pope John Paul I reigned for only 33 days."], "decomposition": ["How long did it take Phileas Fogg to go around the world?", "How long did Pope John Paul I reign?", "Is #1 longer than #2?"], "evidence": [[[["Phileas Fogg-1"]], [["Pope John Paul I-1"]], ["operation"]], [[["Around the World in Eighty Days-1"]], [["Pope John Paul I-1"]], ["operation"]], [[["Phileas Fogg-2"]], [["Pope John Paul I-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: The first person to be born in the United States was a woman named Mary Ann Cotton.\nAnswer (True or False", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Phileas Fogg's around the world would be difficult to achieve during Pope John Paul I's reign?\nFacts: Phileas Fogg is a character in Jules Verne's Around the World in Eighty Days. Phileas Fogg attempts to circumnavigate the globe in 80 days. Pope John Paul I reigned for only 33 days.\nAnswer (True or False):", "ct_subject": "Phileas Fogg is a character in Jules Verne's Around the World in Eighty Days. Phileas Fogg attempts to circumnavigate the globe in 80 days. Pope John Paul I reigned for only 33 days.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999917"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99964654"], [" of", "1.0"], [" the", "1.0"], [" first", "0.91325265"], [" person", "0.8407233"], [" to", "1.0"], [" be", "0.9626459"], [" born", "0.9988949"], [" in", "1.0"], [" the", "1.0"], [" United", "0.99893445"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99980444"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" a", "0.99970704"], [" woman", "0.9998758"], [" named", "1.0"], [" Mary", "0.92848384"], [" Ann", "0.98537666"], [" Cotton", "1.0"], [".", "1.0"], ["\n", "0.9999919"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"]]}, "408": {"qid": "3413c919d24c59eadde3", "term": "Potato", "description": "plant species producing the tuber used as a staple food", "question": "Can someone with celiac disease have potato vodka?", "answer": true, "facts": ["Celiac disease makes it unsafe for someone to eat gluten.", "Potato vodka is a gluten free product."], "decomposition": ["For people with celiac disease, what must they avoid?", "Does Potato Vodka contain #1?"], "evidence": [[[["Gluten-15"]], [["Grey Goose (vodka)-7"]]], [[["Coeliac disease-2"]], [["Vodka-24"], "no_evidence", "operation"]], [[["Coeliac disease-2"]], [["Potato-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"sugar\" and a \"sugar substitute\"?\nFacts: Sugar is a carbohydrate. Sugar substitutes are a carbohydrate that is made from sugar.\nAnswer (True or False): True\nQuestion: What is the difference between a \"sugar\" and a \"sugar substitute\"?\nFacts: Sugar is a carbohydrate. Sugar substitutes are a carbohydrate that is made from sugar.\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can someone with celiac disease have potato vodka?\nFacts: Celiac disease makes it unsafe for someone to eat gluten. Potato vodka is a gluten free product.\nAnswer (True or False):", "ct_subject": "Celiac disease makes it unsafe for someone to eat gluten. Potato vodka is a gluten free product.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9601687"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9999181"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.75052094"], ["ugar", "0.7557411"], ["\"", "0.99978167"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["ugar", "1.0"], [" substitute", "0.9998299"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Sugar", "1.0"], [" is", "1.0"], [" a", "1.0"], [" carbohydrate", "0.6270691"], [".", "0.9999988"], [" Sugar", "1.0"], [" substitutes", "1.0"], [" are", "1.0"], [" a", "0.9990877"], [" carbohydrate", "0.019264793"], [" that", "0.99392045"], [" is", "0.9958436"], [" made", "0.9872651"], [" from", "1.0"], [" sugar", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["ugar", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["ugar", "1.0"], [" substitute", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Sugar", "1.0"], [" is", "1.0"], [" a", "1.0"], [" carbohydrate", "1.0"], [".", "1.0"], [" Sugar", "1.0"], [" substitutes", "1.0"], [" are", "1.0"], [" a", "1.0"], [" carbohydrate", "1.0"], [" that", "1.0"], [" is", "1.0"], [" made", "1.0"], [" from", "1.0"], [" sugar", "1.0"], [".", "1.0"], ["\n", "1.0"]]}, "409": {"qid": "68e7ad683f1090b19a61", "term": "Tonsure", "description": "hairstyle related to religious devotion", "question": "Does ancient Olympics crown fail to hide tonsure?", "answer": true, "facts": ["Tonsure refers to the religious hairstyle in which hair from the top of the head is shaved off.", "Winners of the ancient Olympics were given a laurel crown to wear.", "A laurel crown involves leaves arranged in a circle that cover only the edges of the head."], "decomposition": ["Which part of the head is affected by tonsure?", "What kind of crown was given to winners in ancient Olympics?", "Does #2 cover #1 when worn?"], "evidence": [[[["Tonsure-1"]], [["Ancient Olympic Games-2"]], [["Olive wreath-1"], "operation"]], [[["Tonsure-1"]], [["Olive wreath-1"]], ["operation"]], [[["Tonsure-9"]], [["Olive wreath-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the ancient Greeks and Romans have a word for \"candy\"?\nFacts: The word \"candy\" is derived from the Greek word \"kakon\" which means \"to eat\". The word \"candy\" is also used to", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does ancient Olympics crown fail to hide tonsure?\nFacts: Tonsure refers to the religious hairstyle in which hair from the top of the head is shaved off. Winners of the ancient Olympics were given a laurel crown to wear. A laurel crown involves leaves arranged in a circle that cover only the edges of the head.\nAnswer (True or False):", "ct_subject": "Tonsure refers to the religious hairstyle in which hair from the top of the head is shaved off. Winners of the ancient Olympics were given a laurel crown to wear. A laurel crown involves leaves arranged in a circle that cover only the edges of the head.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9449114"], [" the", "1.0"], [" ancient", "0.99999344"], [" Greeks", "0.9647173"], [" and", "0.99743634"], [" Romans", "1.0"], [" have", "0.99999976"], [" a", "1.0"], [" word", "0.99313074"], [" for", "1.0"], [" \"", "0.99999964"], ["c", "0.78337175"], ["andy", "0.879269"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"", "1.0"], [" is", "0.99999857"], [" derived", "0.8577997"], [" from", "1.0"], [" the", "1.0"], [" Greek", "1.0"], [" word", "1.0"], [" \"", "0.9999938"], ["k", "1.0"], ["ak", "1.0"], ["on", "0.99998105"], ["\"", "1.0"], [" which", "0.9998945"], [" means", "1.0"], [" \"", "1.0"], ["to", "0.9999999"], [" eat", "0.99999726"], ["\".", "0.999708"], [" The", "0.9999975"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"", "1.0"], [" is", "0.99975914"], [" also", "0.9979532"], [" used", "0.9999305"], [" to", "0.999268"]]}, "410": {"qid": "7065ff7b7641acd0330b", "term": "Dessert", "description": "A course that concludes a meal; usually sweet", "question": "Can dessert be made with vegetables?", "answer": true, "facts": ["A popular desert is carrot cake.", "Carrot cake is made with carrots.", "Carrots are a kind of vegetable."], "decomposition": ["What are some popularly known desserts?", "Do any of #1 contain vegetables?"], "evidence": [[[["Cake-14"]], [["Carrot cake-1"]]], [[["Dessert-1"]], ["no_evidence", "operation"]], [[["Carrot cake-1"]], [["Carrot-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"sugar\" and a \"sugar-free\" candy?\nFacts: Sugar is a sweetener. Sugar-free is a term used to describe a candy that is made without sugar.\nAnswer (True or False): True\nQuestion: What is the difference between a \"sugar\" and a \"sugar-free\" candy?\nFacts: Sugar is a sweet", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can dessert be made with vegetables?\nFacts: A popular desert is carrot cake. Carrot cake is made with carrots. Carrots are a kind of vegetable.\nAnswer (True or False):", "ct_subject": "A popular desert is carrot cake. Carrot cake is made with carrots. Carrots are a kind of vegetable.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9995753"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.66611916"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.87634844"], ["s", "0.64022696"], ["ugar", "0.65388894"], ["\"", "0.9969139"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["ugar", "1.0"], ["-", "0.87779355"], ["free", "1.0"], ["\"", "1.0"], [" candy", "0.99825126"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Sugar", "1.0"], [" is", "0.9449145"], [" a", "1.0"], [" sweet", "0.8653495"], ["ener", "1.0"], [".", "1.0"], [" Sugar", "1.0"], ["-", "1.0"], ["free", "1.0"], [" is", "1.0"], [" a", "1.0"], [" term", "0.99997854"], [" used", "0.99999"], [" to", "1.0"], [" describe", "1.0"], [" a", "0.99999523"], [" candy", "1.0"], [" that", "1.0"], [" is", "0.9771085"], [" made", "0.6073682"], [" without", "1.0"], [" sugar", "0.5602038"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["ugar", "1.0"], ["\"", "0.9984983"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["ugar", "1.0"], ["-", "1.0"], ["free", "1.0"], ["\"", "1.0"], [" candy", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Sugar", "1.0"], [" is", "1.0"], [" a", "1.0"], [" sweet", "1.0"]]}, "411": {"qid": "8d06743969021bb22a3d", "term": "Lieutenant", "description": "junior commissioned officer in many nations' armed forces", "question": "Can children become lieutenants?", "answer": true, "facts": ["Lieutenant the second junior-most or in some cases the junior-most commissioned officer in the armed forces, fire services, police, and other organizations of many nations.", "Many gangs use military rankings to describe their internal heirarchy.", "Many young children are inducted into gangs with heirarchies.", "Children can grow up to be whatever they want to be. "], "decomposition": ["What position in a hierarchical system does lieutenant imply?", "Would a gang use the system and assign children to #1 positions?"], "evidence": [[[["Lieutenant-11"]], [["Ghetto Boys-7"]]], [[["Lieutenant-1"]], [["Gang-24", "Gang-47"], "no_evidence", "operation"]], [[["Lieutenant-1"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"sister\" and a \"sister-in-law\"?\nFacts: A sister is a woman who is married to a man. A sister", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can children become lieutenants?\nFacts: Lieutenant the second junior-most or in some cases the junior-most commissioned officer in the armed forces, fire services, police, and other organizations of many nations. Many gangs use military rankings to describe their internal heirarchy. Many young children are inducted into gangs with heirarchies. Children can grow up to be whatever they want to be. \nAnswer (True or False):", "ct_subject": "Lieutenant the second junior-most or in some cases the junior-most commissioned officer in the armed forces, fire services, police, and other organizations of many nations. Many gangs use military rankings to describe their internal heirarchy. Many young children are inducted into gangs with heirarchies. Children can grow up to be whatever they want to be. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.7347871"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.9999964"], ["s", "0.9575036"], ["ister", "0.9997315"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.08982816"], ["ister", "1.0"], ["-", "1.0"], ["in", "1.0"], ["-", "1.0"], ["law", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999999"], [" sister", "0.977103"], [" is", "1.0"], [" a", "1.0"], [" woman", "0.7016971"], [" who", "1.0"], [" is", "0.9999988"], [" married", "0.6121372"], [" to", "1.0"], [" a", "0.99869746"], [" man", "1.0"], [".", "0.9998337"], [" A", "1.0"], [" sister", "1.0"]]}, "412": {"qid": "15586c73003c301ef18f", "term": "Cousin", "description": "any descendant of an ancestor's sibling", "question": "Could SNL be why Jenny McCarthy does not get along with her cousin?", "answer": true, "facts": ["Jenny McCarthy is cousin's with Melissa McCarthy.", "Melissa McCarthy and Jenny McCarthy are not close and Melissa did not even attend Jenny's wedding.", "Jenny McCarthy was spoofed in n episode of SNL (Saturday Night Live) for a rant she did on The View.", "Melissa McCarthy has been a frequent guest on SNL (Saturday Night Live) from 2011-2017.", "Melissa McCarthy was nominated five times for a Primetime Emmy Award for Outstanding Guest Actress in a Comedy Series for her appearances on SNL."], "decomposition": ["Who is Jenny McCarthy's cousin?", "What show has #1 been a frequent guest of?", "Was Jenny McCarthy made fun of on #2?"], "evidence": [[[["Jenny McCarthy-3"]], [["Melissa McCarthy-2"]], ["no_evidence"]], [[["Melissa McCarthy-1"]], [["Melissa McCarthy-2"]], ["no_evidence", "operation"]], [[["Melissa McCarthy-5"]], [["Melissa McCarthy-8"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could SNL be why Jenny McCarthy does not get along with her cousin?\nFacts: Jenny McCarthy is cousin's with Melissa McCarthy. Melissa McCarthy and Jenny McCarthy are not close and Melissa did not even attend Jenny's wedding. Jenny McCarthy was spoofed in n episode of SNL (Saturday Night Live) for a rant she did on The View. Melissa McCarthy has been a frequent guest on SNL (Saturday Night Live) from 2011-2017. Melissa McCarthy was nominated five times for a Primetime Emmy Award for Outstanding Guest Actress in a Comedy Series for her appearances on SNL.\nAnswer (True or False):", "ct_subject": "Jenny McCarthy is cousin's with Melissa McCarthy. Melissa McCarthy and Jenny McCarthy are not close and Melissa did not even attend Jenny's wedding. Jenny McCarthy was spoofed in n episode of SNL (Saturday Night Live) for a rant she did on The View. Melissa McCarthy has been a frequent guest on SNL (Saturday Night Live) from 2011-2017. Melissa McCarthy was nominated five times for a Primetime Emmy Award for Outstanding Guest Actress in a Comedy Series for her appearances on SNL.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9985959"]]}, "413": {"qid": "e684937aefb2df3eebcf", "term": "H", "description": "letter in the Latin alphabet", "question": "Is H's most common two letter pair partner a freebie in Wheel of Fortune bonus round?", "answer": true, "facts": ["H forms the most common two letter pair in the English language along with the letter T.", "The Wheel of Fortune bonus round gives the player six free letters: R, S, T, L, N, E."], "decomposition": ["What letter forms the most common two letter pair in English along with the letter H?", "What free letters does the Wheel of Fortune bonus round give players?", "Is #1 included in #2?"], "evidence": [[[["Th (digraph)-1"]], [["Wheel of Fortune (American game show)-13"]], ["operation"]], [[["Letter frequency-11"]], [["Wheel of Fortune (Australian game show)-33"]], ["operation"]], [[["Most common words in English-5"], "no_evidence"], [["Wheel of Fortune (Australian game show)-33"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the year 2000?\nFacts: The year 2000 was the first year that the year zero was used.\nAnswer (True or False): True\nQuestion: What is the name of the first person to", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is H's most common two letter pair partner a freebie in Wheel of Fortune bonus round?\nFacts: H forms the most common two letter pair in the English language along with the letter T. The Wheel of Fortune bonus round gives the player six free letters: R, S, T, L, N, E.\nAnswer (True or False):", "ct_subject": "H forms the most common two letter pair in the English language along with the letter T. The Wheel of Fortune bonus round gives the player six free letters: R, S, T, L, N, E.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.832845"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9997528"], [" of", "1.0"], [" the", "1.0"], [" first", "0.99950624"], [" person", "0.9998927"], [" to", "1.0"], [" be", "0.99965966"], [" born", "0.9999995"], [" in", "1.0"], [" the", "1.0"], [" year", "0.93706334"], [" 2000", "0.99802977"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "0.9944735"], [" the", "1.0"], [" first", "0.9998579"], [" year", "1.0"], [" that", "0.99998665"], [" the", "0.99999833"], [" year", "1.0"], [" zero", "0.9905709"], [" was", "1.0"], [" used", "0.9481403"], [".", "0.9999994"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"]]}, "414": {"qid": "0b577826d5e7b6a50aad", "term": "Alice's Adventures in Wonderland", "description": "book by Lewis Carroll", "question": "Is tobacco use made to seem enjoyable in Alice's Adventures in Wonderland?", "answer": true, "facts": ["In Alice's Adventures in Wonderland, one of the characters is a caterpillar that smokes hookah.", "Hookah is a water pipe used to smoke tobacco products.", "The caterpillar speaks to Alice while making letters out of the smoke he blows."], "decomposition": ["In Alice's Adventures in Wonderland, what is a caterpillar seen smoking?", "What do you use #1 to do?", "Does it seem like the caterpillar enjoys #2?"], "evidence": [[[["Alice's Adventures in Wonderland-13"]], [["Hookah-1"]], ["no_evidence"]], [[["Alice's Adventures in Wonderland-13"]], [["Hookah-1"]], ["no_evidence", "operation"]], [[["Alice's Adventures in Wonderland-13"]], [["Hookah-1"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the word \"candy\" a corruption of the word \"candy\"?\nFacts: The word \"candy\" is a corruption of the word \"candy\" which is a corruption of the word \"candy\".\nAnswer (True or False):", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is tobacco use made to seem enjoyable in Alice's Adventures in Wonderland?\nFacts: In Alice's Adventures in Wonderland, one of the characters is a caterpillar that smokes hookah. Hookah is a water pipe used to smoke tobacco products. The caterpillar speaks to Alice while making letters out of the smoke he blows.\nAnswer (True or False):", "ct_subject": "In Alice's Adventures in Wonderland, one of the characters is a caterpillar that smokes hookah. Hookah is a water pipe used to smoke tobacco products. The caterpillar speaks to Alice while making letters out of the smoke he blows.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "0.9999764"], [" word", "0.99155265"], [" \"", "1.0"], ["c", "0.9969097"], ["andy", "0.9316028"], ["\"", "1.0"], [" a", "0.18658549"], [" corruption", "0.52082795"], [" of", "1.0"], [" the", "0.9999999"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"?", "0.9949604"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "0.99999034"], [" corruption", "1.0"], [" of", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"", "0.99999404"], [" which", "0.9999988"], [" is", "0.9999987"], [" a", "1.0"], [" corruption", "1.0"], [" of", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\".", "0.9706308"], ["\n", "0.9999993"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"]]}, "415": {"qid": "e0fb32d50fba9fe0aa32", "term": "Cell (biology)", "description": "The basic structural and functional unit of all organisms; the smallest unit of life.", "question": "Can a cell fit inside of a shoebox?", "answer": true, "facts": ["The average shoebox is around 14 inches by 10 inches by 5 inches", "The average eukaryotic cell is between 1 and 100 micrometers in diameter"], "decomposition": ["How big is a cell?", "How big is a shoebox?", "Is #1 smaller than #2?"], "evidence": [[[["Cell (biology)-2"]], ["no_evidence"], ["operation"]], [[["Cell (biology)-2"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Cell (biology)-1", "Electrochemical cell-1", "Fuel cell-1", "Monastic cell-1", "Prison cell-1", "Solar cell-1", "Storm cell-1"], "no_evidence"], [["Shoe-1"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human?\nFacts: The average human weighs about 100 pounds\nAnswer (True or False): True\nQuestion: What is the average height of a human?\nFacts: The average human height is about 5 feet, 8 inches\nAnswer (True or False): True\nQuestion: What is the average weight of a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can a cell fit inside of a shoebox?\nFacts: The average shoebox is around 14 inches by 10 inches by 5 inches The average eukaryotic cell is between 1 and 100 micrometers in diameter\nAnswer (True or False):", "ct_subject": "The average shoebox is around 14 inches by 10 inches by 5 inches The average eukaryotic cell is between 1 and 100 micrometers in diameter", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999628"], [" is", "1.0"], [" the", "1.0"], [" average", "0.9097366"], [" weight", "0.9999951"], [" of", "1.0"], [" a", "0.9999827"], [" human", "1.0"], ["?", "0.99999917"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "1.0"], [" weighs", "0.9809957"], [" about", "0.9945146"], [" 100", "0.83809865"], [" pounds", "1.0"], ["\n", "0.9999887"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" height", "0.98960364"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "1.0"], [" height", "0.9999999"], [" is", "1.0"], [" about", "0.9999852"], [" 5", "0.99997854"], [" feet", "1.0"], [",", "0.85391283"], [" 8", "0.99513584"], [" inches", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "1.0"], [" of", "1.0"], [" a", "1.0"]]}, "416": {"qid": "a0d0c2ac289c7a59911d", "term": "Lactobacillus", "description": "genus of bacteria", "question": "Is overfeeding Lactobacillus unwise for people without dental insurance?", "answer": true, "facts": ["Lactobacillus species convert sugars they digest to lactic acid ", "The lactic acid of some Lactobacillus species is associated with tooth decay", "Dental procedures can be expensive without insurance"], "decomposition": ["What are the products of Lactobacillus?", "What conditions are caused by #1?", "What medical procedures would be required to fix #2?", "Would #3 be more affordable with dental insurance?"], "evidence": [[[["Lactobacillus-1"]], [["Lactic acid-5"]], [["Tooth decay-77"]], [["Dental insurance-1"]]], [[["Lactobacillus-1", "Lactobacillus-2"]], [["Lactic acid bacteria-14"]], [["Tooth decay-77"]], [["Dental insurance-1"]]], [[["Lactobacillus-10"]], [["Lactobacillus-10"]], [["Tooth decay-76"]], [["Dental insurance-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it true that the average person has more than one heart?\nFacts: The average person has one heart. The heart is a muscle. The heart is a muscle. The heart is a muscle. The heart is a muscle. The heart is a muscle. The heart is a muscle.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is overfeeding Lactobacillus unwise for people without dental insurance?\nFacts: Lactobacillus species convert sugars they digest to lactic acid  The lactic acid of some Lactobacillus species is associated with tooth decay Dental procedures can be expensive without insurance\nAnswer (True or False):", "ct_subject": "Lactobacillus species convert sugars they digest to lactic acid  The lactic acid of some Lactobacillus species is associated with tooth decay Dental procedures can be expensive without insurance", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" true", "0.9941737"], [" that", "1.0"], [" the", "1.0"], [" average", "0.84879696"], [" person", "0.9999641"], [" has", "0.98526186"], [" more", "0.98392004"], [" than", "1.0"], [" one", "0.9881549"], [" heart", "0.99999964"], ["?", "0.99999964"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99999857"], [" average", "0.99998"], [" person", "1.0"], [" has", "1.0"], [" one", "0.8767838"], [" heart", "1.0"], [".", "1.0"], [" The", "0.99999654"], [" heart", "1.0"], [" is", "1.0"], [" a", "0.9999999"], [" muscle", "1.0"], [".", "0.51599336"], [" The", "0.99999976"], [" heart", "1.0"], [" is", "0.9999999"], [" a", "0.9603702"], [" muscle", "1.0"], [".", "0.99983"], [" The", "0.9996836"], [" heart", "1.0"], [" is", "1.0"], [" a", "1.0"], [" muscle", "1.0"], [".", "1.0"], [" The", "1.0"], [" heart", "1.0"], [" is", "1.0"], [" a", "1.0"], [" muscle", "1.0"], [".", "1.0"], [" The", "1.0"], [" heart", "1.0"], [" is", "1.0"], [" a", "1.0"], [" muscle", "1.0"], [".", "1.0"], [" The", "1.0"], [" heart", "1.0"], [" is", "1.0"], [" a", "1.0"], [" muscle", "1.0"], [".", "1.0"]]}, "417": {"qid": "71e9bed5efda19d9c4ce", "term": "Pikachu", "description": "Pok\u00e9mon species and the mascot of the Pok\u00e9mon franchise", "question": "Does Pikachu like Charles Darwin?", "answer": true, "facts": ["Charles Darwin developed the theory of evolution", "Pikachu is a Pokemon", "Pokemon rely on evolution in order to transform into stronger forms"], "decomposition": ["How does a Pokemon improve upon itself? ", "What theory is Charles Darwin best known for?", "Is #1 and #2 related?"], "evidence": [[[["Pok\u00e9mon-6"]], [["Charles Darwin-1"]], ["operation"]], [[["Pok\u00e9mon-6"]], [["Charles Darwin-2"]], ["operation"]], [[["Gameplay of Pok\u00e9mon-57"]], [["Charles Darwin-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first man to be born in the year 2000?\nFacts: The first man to be born in the year 2000 was born in the year 2000.\nAnswer (True or False): True\nQuestion: What is the name of the first man to be born in the year 2000?\nFacts: The first man to be born in the year 2000 was born in the year 2000.\nAnswer (", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Pikachu like Charles Darwin?\nFacts: Charles Darwin developed the theory of evolution Pikachu is a Pokemon Pokemon rely on evolution in order to transform into stronger forms\nAnswer (True or False):", "ct_subject": "Charles Darwin developed the theory of evolution Pikachu is a Pokemon Pokemon rely on evolution in order to transform into stronger forms", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999019"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99999917"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9999684"], [" man", "0.5885418"], [" to", "0.99993074"], [" be", "0.9946683"], [" born", "0.99996006"], [" in", "1.0"], [" the", "1.0"], [" year", "0.94905543"], [" 2000", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "0.56907076"], [" man", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "1.0"], [" born", "0.9999927"], [" in", "0.99999917"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "0.99999964"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"]]}, "418": {"qid": "e39e7996cb59f6c932e0", "term": "Mitsubishi", "description": "group of autonomous, Japanese multinational companies", "question": "Can Aerosmith fit in a 2020 Mitsubishi Outlander?", "answer": true, "facts": ["Aerosmith is an American rock band that has five active members.", "The 2020 Mitsubishi Outlander has flexible seating that allows for seven seat capacity."], "decomposition": ["How many people are members of the band Aerosmith?", "What is the searing capacity of the 2020 Mitsubishi Outlander?", "Is #2 greater than or equal to #1?"], "evidence": [[[["Aerosmith-1"]], [["Mitsubishi Outlander-10"]], [["Aerosmith-1", "Mitsubishi Outlander-10"], "operation"]], [[["Aerosmith-1"]], [["Mitsubishi Outlander-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Aerosmith-1"]], ["no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the year 2000?\nFacts: The first person to be born in the year 2000 was a woman named Mary.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the year 2000?\nFacts: The first person to", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can Aerosmith fit in a 2020 Mitsubishi Outlander?\nFacts: Aerosmith is an American rock band that has five active members. The 2020 Mitsubishi Outlander has flexible seating that allows for seven seat capacity.\nAnswer (True or False):", "ct_subject": "Aerosmith is an American rock band that has five active members. The 2020 Mitsubishi Outlander has flexible seating that allows for seven seat capacity.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99995005"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9988292"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9883813"], [" person", "0.73706007"], [" to", "1.0"], [" be", "0.99964106"], [" born", "0.56457424"], [" in", "1.0"], [" the", "1.0"], [" year", "0.9997353"], [" 2000", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "0.998601"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "1.0"], [" a", "0.79354376"], [" woman", "0.99997044"], [" named", "1.0"], [" Mary", "0.9998627"], [".", "0.99999905"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"]]}, "419": {"qid": "afa32406c205674efb7f", "term": "Amazons", "description": "warrior women from Greek mythology", "question": "Did any of the amazons on Xena: Warrior Princess star on later shows?", "answer": true, "facts": ["Xena\" Warrior Princess was a fantasy TV series based on Greek mythology.", "Amazons on Xena: Warrior Princess were played by numerous actresses including: Danielle Cormack and Melinda Clarke.", "Melinda Clarke starred in numerous TV shows after Xena: Warrior Princess including The O.C. and Nikita."], "decomposition": ["Who played the roles of Amazons on Xena: Warrior Princess?", "Did Melinda Clarke do any other shows after Xena\" Warrior Princess?", "Is #2 listed in #1?"], "evidence": [[[["Melinda Clarke-3"]], [["Melinda Clarke-4"]], ["operation"]], [[["Xena: Warrior Princess-14"], "no_evidence"], [["Melinda Clarke-3"]], ["operation"]], [[["Melinda Clarke-3"]], [["Melinda Clarke-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the original Xena: Warrior Princess movie have a sequel?\nFacts: No sequel was ever made.\nAnswer (True or False): True\nQuestion: Did the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did any of the amazons on Xena: Warrior Princess star on later shows?\nFacts: Xena\" Warrior Princess was a fantasy TV series based on Greek mythology. Amazons on Xena: Warrior Princess were played by numerous actresses including: Danielle Cormack and Melinda Clarke. Melinda Clarke starred in numerous TV shows after Xena: Warrior Princess including The O.C. and Nikita.\nAnswer (True or False):", "ct_subject": "Xena\" Warrior Princess was a fantasy TV series based on Greek mythology. Amazons on Xena: Warrior Princess were played by numerous actresses including: Danielle Cormack and Melinda Clarke. Melinda Clarke starred in numerous TV shows after Xena: Warrior Princess including The O.C. and Nikita.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999926"], [" the", "0.99998045"], [" original", "0.9992754"], [" X", "0.98028266"], ["ena", "0.9531253"], [":", "1.0"], [" Warrior", "1.0"], [" Princess", "1.0"], [" movie", "0.99563533"], [" have", "1.0"], [" a", "1.0"], [" sequel", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" No", "0.61920536"], [" sequel", "0.9939061"], [" was", "0.99999523"], [" ever", "0.9999999"], [" made", "1.0"], [".", "0.95354074"], ["\n", "0.99905175"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999999"], [" the", "1.0"]]}, "420": {"qid": "a68cfdfc49fdc73dbd2e", "term": "Lapidary", "description": "gemstone cutter", "question": "Was Dioskourides a lapidary?", "answer": true, "facts": ["A lapidary is a person or machine who cuts gemstones; classically, it refers to a person who engraves gemstones.", "Dioskourides put his signature on a Roman amethyst ringstone with a portrait of Demosthenes circa late 1st century BC.", "Artists sign their work."], "decomposition": ["What is a lapidary?", "What do #1's do once they finish their work?", "Did Dioskourides do #1 and #2?"], "evidence": [[[["Lapidary-1"]], ["no_evidence"], [["Pedanius Dioscorides-1"], "operation"]], [[["Lapidary-1"]], [["Lapidary-12"], "no_evidence"], [["Pedanius Dioscorides-1"], "no_evidence", "operation"]], [[["Lapidary-1"]], [["Lapidary-11"]], [["Pedanius Dioscorides-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Was the Roman Empire founded by Julius Caesar?\nFacts: Julius Caesar was born in 44 BC. The Roman Empire was founded by Augustus in 30 BC.\nAnswer (True or False): True\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Dioskourides a lapidary?\nFacts: A lapidary is a person or machine who cuts gemstones; classically, it refers to a person who engraves gemstones. Dioskourides put his signature on a Roman amethyst ringstone with a portrait of Demosthenes circa late 1st century BC. Artists sign their work.\nAnswer (True or False):", "ct_subject": "A lapidary is a person or machine who cuts gemstones; classically, it refers to a person who engraves gemstones. Dioskourides put his signature on a Roman amethyst ringstone with a portrait of Demosthenes circa late 1st century BC. Artists sign their work.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "0.9594064"], [" the", "0.99999976"], [" Roman", "0.9999999"], [" Empire", "0.9990339"], [" founded", "0.9999782"], [" by", "1.0"], [" Julius", "0.9999125"], [" Caesar", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Julius", "0.9999286"], [" Caesar", "1.0"], [" was", "1.0"], [" born", "0.9997911"], [" in", "1.0"], [" 44", "0.9896988"], [" BC", "1.0"], [".", "0.9999764"], [" The", "0.9999995"], [" Roman", "1.0"], [" Empire", "1.0"], [" was", "1.0"], [" founded", "1.0"], [" by", "0.31503093"], [" Augustus", "0.9999999"], [" in", "0.9999938"], [" 30", "0.2836788"], [" BC", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"]]}, "421": {"qid": "ff8cbfd0e19d8c30b1c1", "term": "Arab\u2013Israeli conflict", "description": "geopolitical conflict in the Middle East and North Africa", "question": "Was England directly involved in the Arab-Israeli conflict?", "answer": true, "facts": ["The Arab-Israeli conflict began hundreds of years ago.", "England occupied what is now Israel until 1945 when Israel became independent.", "During Israel's war for independence England allied with their Arab neighbors to fight the Israeli rebels."], "decomposition": ["Which nation did Israel gain independence from?", "Which other nation did #1 ally with to resist Israeli rebels during the war for independence?", "Is #1 England and #2 Arab?"], "evidence": [[[["Israel-5"]], [["Arab\u2013Israeli conflict-3"]], ["no_evidence"]], [[["United Nations Partition Plan for Palestine-1", "United Nations Partition Plan for Palestine-2"], "no_evidence"], [["1948 Arab\u2013Israeli War-14", "1948 Arab\u2013Israeli War-9", "Israel-30"], "no_evidence"], ["no_evidence", "operation"]], [[["Israel-32"]], ["no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Was the United States directly involved in the Arab-Israeli conflict?\nFacts: The United States was not directly involved in the Arab-Israeli conflict. The United States was a neutral party during the Arab-Israeli conflict.\nAnswer (True or False): True\nQuestion: Was the United States directly involved in", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was England directly involved in the Arab-Israeli conflict?\nFacts: The Arab-Israeli conflict began hundreds of years ago. England occupied what is now Israel until 1945 when Israel became independent. During Israel's war for independence England allied with their Arab neighbors to fight the Israeli rebels.\nAnswer (True or False):", "ct_subject": "The Arab-Israeli conflict began hundreds of years ago. England occupied what is now Israel until 1945 when Israel became independent. During Israel's war for independence England allied with their Arab neighbors to fight the Israeli rebels.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "0.8054295"], [" the", "1.0"], [" United", "0.9999298"], [" States", "1.0"], [" directly", "0.9981894"], [" involved", "1.0"], [" in", "1.0"], [" the", "1.0"], [" Arab", "1.0"], ["-", "1.0"], ["Israeli", "1.0"], [" conflict", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "0.9999187"], [" not", "1.0"], [" directly", "1.0"], [" involved", "1.0"], [" in", "1.0"], [" the", "1.0"], [" Arab", "1.0"], ["-", "1.0"], ["Israeli", "1.0"], [" conflict", "1.0"], [".", "1.0"], [" The", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" a", "0.99447"], [" neutral", "1.0"], [" party", "0.99999785"], [" during", "0.9470854"], [" the", "1.0"], [" Arab", "0.9897793"], ["-", "1.0"], ["Israeli", "1.0"], [" conflict", "0.7657083"], [".", "1.0"], ["\n", "0.999522"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "0.999941"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" directly", "1.0"], [" involved", "1.0"], [" in", "1.0"]]}, "422": {"qid": "2e508efbf0b72f1af2c2", "term": "Quartz", "description": "mineral composed of silicon and oxygen atoms in a continuous framework of SiO\u2084 silicon\u2013oxygen tetrahedra, with each oxygen being shared between two tetrahedra, giving an overall chemical formula of SiO\u2082", "question": "Could Quartz be useful to humans if plants died off and there was no oxygen?", "answer": true, "facts": ["Plants produce oxygen which is needed by humans to survive.", "Quartz is a hard mineral substance made of several elements.", "Quartz is composed of silicon and oxygen.", "Quartz can be melted at high temperatures."], "decomposition": ["What are the constituents elements of quartz?", "Is oxygen included in #1?"], "evidence": [[[["Quartz-1"]], ["operation"]], [[["Quartz-1"]], ["operation"]], [[["Quartz-1"]], [["Oxygen-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake\"?\nFacts: A snowflake is a solid crystal that has been broken into many pieces. A snowflake is a solid crystal that has been broken into many pieces.\nAnswer (True or False): True\nQuestion", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could Quartz be useful to humans if plants died off and there was no oxygen?\nFacts: Plants produce oxygen which is needed by humans to survive. Quartz is a hard mineral substance made of several elements. Quartz is composed of silicon and oxygen. Quartz can be melted at high temperatures.\nAnswer (True or False):", "ct_subject": "Plants produce oxygen which is needed by humans to survive. Quartz is a hard mineral substance made of several elements. Quartz is composed of silicon and oxygen. Quartz can be melted at high temperatures.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.7012538"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.2917758"], [" between", "1.0"], [" a", "0.99999964"], [" \"", "0.22049011"], ["s", "0.99631315"], ["now", "1.0"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.0043895165"], ["now", "1.0"], ["flake", "1.0"], ["\"?", "0.8263774"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" snow", "0.99966764"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" solid", "0.8043532"], [" crystal", "0.9999676"], [" that", "0.9963863"], [" has", "0.5135351"], [" been", "0.9969934"], [" broken", "0.36535555"], [" into", "1.0"], [" many", "1.0"], [" pieces", "0.99998343"], [".", "1.0"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" is", "1.0"], [" a", "0.9999355"], [" solid", "0.99999905"], [" crystal", "1.0"], [" that", "1.0"], [" has", "1.0"], [" been", "1.0"], [" broken", "1.0"], [" into", "1.0"], [" many", "1.0"], [" pieces", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"]]}, "423": {"qid": "9d279701351e38e0400f", "term": "United States Secretary of State", "description": "U.S. cabinet member and head of the U.S. State Department", "question": "Can United States Secretary of State do crimes in U.K. without being arrested?", "answer": true, "facts": ["Diplomatic Immunity allows for diplomats in other countries to not be tried for their transgressions.", "Countries that signed the Vienna Convention on Diplomatic Relations allow for Diplomatic Immunity.", "All UN member states besides Palau, The Solomon Islands, and South Sudan have signed the Vienna Convention on Diplomatic Relations treaty.", "The U.K. is one of the original UN member nations."], "decomposition": ["Under which agreement is modern diplomatic immunity applicable?", "Which countries have signed #1?", "Is the U.K. included in #2?"], "evidence": [[[["Diplomatic immunity-14"]], [["Vienna Convention on Consular Relations-7"], "no_evidence"], ["no_evidence", "operation"]], [[["Diplomatic immunity-11", "Diplomatic immunity-14", "Diplomatic immunity-17"], "no_evidence"], [["Vienna Convention on Diplomatic Relations-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Diplomatic immunity-14"]], [["Vienna Convention on Diplomatic Relations-8"], "no_evidence"], [["Diplomatic immunity-10", "Diplomatic immunity-53"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"C\" and a \"D\" grade?\nFacts: A \"C\" grade", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can United States Secretary of State do crimes in U.K. without being arrested?\nFacts: Diplomatic Immunity allows for diplomats in other countries to not be tried for their transgressions. Countries that signed the Vienna Convention on Diplomatic Relations allow for Diplomatic Immunity. All UN member states besides Palau, The Solomon Islands, and South Sudan have signed the Vienna Convention on Diplomatic Relations treaty. The U.K. is one of the original UN member nations.\nAnswer (True or False):", "ct_subject": "Diplomatic Immunity allows for diplomats in other countries to not be tried for their transgressions. Countries that signed the Vienna Convention on Diplomatic Relations allow for Diplomatic Immunity. All UN member states besides Palau, The Solomon Islands, and South Sudan have signed the Vienna Convention on Diplomatic Relations treaty. The U.K. is one of the original UN member nations.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999424"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9989786"], [" between", "1.0"], [" a", "0.99998546"], [" \"", "1.0"], ["C", "0.9978295"], ["\"", "0.99999785"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["D", "0.9072431"], ["\"", "0.999463"], [" grade", "0.9833576"], ["?", "0.9999982"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.99999475"], [" \"", "0.8875279"], ["C", "1.0"], ["\"", "1.0"], [" grade", "1.0"]]}, "424": {"qid": "a593481c1ddb91e9f96b", "term": "Asian black bear", "description": "species of mammal", "question": "Is the Asian black bear multicolored?", "answer": true, "facts": ["The Asian black bear is an animal that lives in habitats with trees.", "Multicolored refers to anything that is composed of more than one color.", "The Sian black bear has a black coat with a white V-shaped patch."], "decomposition": ["How many colors of fur does the asian black bear have?", "Is #1 greater than 1?"], "evidence": [[[["Asian black bear-2"]], ["operation"]], [[["Asian black bear-2"]], ["operation"]], [[["Asian black bear-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the American bald eagle a member of the family of the eagle?\nFacts: The American bald eagle is a member of the family of the eagle. The American bald eagle is a member of the family of the eagle. The American bald eagle is a member of the family of the eagle. The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is the Asian black bear multicolored?\nFacts: The Asian black bear is an animal that lives in habitats with trees. Multicolored refers to anything that is composed of more than one color. The Sian black bear has a black coat with a white V-shaped patch.\nAnswer (True or False):", "ct_subject": "The Asian black bear is an animal that lives in habitats with trees. Multicolored refers to anything that is composed of more than one color. The Sian black bear has a black coat with a white V-shaped patch.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99986804"], [" the", "1.0"], [" American", "1.0"], [" bald", "0.93152094"], [" eagle", "1.0"], [" a", "1.0"], [" member", "0.43582648"], [" of", "1.0"], [" the", "1.0"], [" family", "0.39970344"], [" of", "1.0"], [" the", "1.0"], [" eagle", "0.9976547"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" American", "1.0"], [" bald", "1.0"], [" eagle", "1.0"], [" is", "1.0"], [" a", "1.0"], [" member", "1.0"], [" of", "1.0"], [" the", "1.0"], [" family", "1.0"], [" of", "1.0"], [" the", "1.0"], [" eagle", "1.0"], [".", "1.0"], [" The", "0.68187904"], [" American", "0.9999999"], [" bald", "1.0"], [" eagle", "1.0"], [" is", "1.0"], [" a", "0.9999989"], [" member", "1.0"], [" of", "1.0"], [" the", "1.0"], [" family", "1.0"], [" of", "1.0"], [" the", "1.0"], [" eagle", "1.0"], [".", "1.0"], [" The", "0.30851892"], [" American", "1.0"], [" bald", "1.0"], [" eagle", "1.0"], [" is", "1.0"], [" a", "1.0"], [" member", "1.0"], [" of", "1.0"], [" the", "1.0"], [" family", "1.0"], [" of", "1.0"], [" the", "1.0"], [" eagle", "1.0"], [".", "1.0"], [" The", "0.9999993"]]}, "425": {"qid": "97f9a0a30c9dc2e77d42", "term": "5", "description": "Natural number", "question": "Does Homer Simpson need two hands worth of fingers to count to 5?", "answer": true, "facts": ["Homer Simpson is a character of the long running comedy animated series \"The Simpsons\".", "All characters in \"The Simpsons\" have 4 fingers on each hand."], "decomposition": ["How many fingers does Homer Simpson have on each hand?", "Is #1 less than 5?"], "evidence": [[[["Trilogy of Error-1"], "no_evidence"], ["operation"]], [["no_evidence"], ["no_evidence", "operation"]], [[["Homer Simpson-1"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the year 2000?\nFacts: The year 2000 was the year of the first human birth.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the year 2000?\nFacts: The year 2000 was the year of the first", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Homer Simpson need two hands worth of fingers to count to 5?\nFacts: Homer Simpson is a character of the long running comedy animated series \"The Simpsons\". All characters in \"The Simpsons\" have 4 fingers on each hand.\nAnswer (True or False):", "ct_subject": "Homer Simpson is a character of the long running comedy animated series \"The Simpsons\". All characters in \"The Simpsons\" have 4 fingers on each hand.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999987"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99963975"], [" of", "1.0"], [" the", "1.0"], [" first", "0.6412098"], [" person", "0.95006615"], [" to", "1.0"], [" be", "0.9500443"], [" born", "0.9999784"], [" in", "1.0"], [" the", "1.0"], [" year", "0.9998677"], [" 2000", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" year", "0.99998415"], [" 2000", "1.0"], [" was", "0.99703467"], [" the", "1.0"], [" year", "0.8982768"], [" of", "0.99746275"], [" the", "1.0"], [" first", "0.9805238"], [" human", "0.99999285"], [" birth", "1.0"], [".", "0.99999464"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "1.0"], [" the", "1.0"], [" year", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"]]}, "426": {"qid": "80e1855030cd5c8f136a", "term": "Lieutenant", "description": "junior commissioned officer in many nations' armed forces", "question": "Are pirate lieutenants like navy lieutenants?", "answer": true, "facts": ["Lieutenant Richards was a pirate that sailed alongside Blackbeard.", "Lieutenant RIchards had many duties and was placed in charge of the ship, Bonnet's Revenge.", "In the Royal Navy and Commonwealth navies, the second-in-command of a vessel is known as the first lieutenant.", "Royal Navy lieutenants are in charge of other crew members and have many duties."], "decomposition": ["What are the duties of a pirate lieutenant?", "What are the duties of a navy lieutenant?", "Is there significant overlap between #1 and #2?"], "evidence": [[["no_evidence"], [["Lieutenant (navy)-10", "Lieutenant (navy)-11"]], ["operation"]], [[["Piracy-87"], "no_evidence"], [["Lieutenant (navy)-1", "Lieutenant (navy)-11"], "no_evidence"], ["no_evidence", "operation"]], [[["Governance in 18th-century piracy-4"], "no_evidence"], [["Lieutenant (navy)-12"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there more people in the world today than there were in the year 1000?\nFacts: There are more people in the world today than there were", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are pirate lieutenants like navy lieutenants?\nFacts: Lieutenant Richards was a pirate that sailed alongside Blackbeard. Lieutenant RIchards had many duties and was placed in charge of the ship, Bonnet's Revenge. In the Royal Navy and Commonwealth navies, the second-in-command of a vessel is known as the first lieutenant. Royal Navy lieutenants are in charge of other crew members and have many duties.\nAnswer (True or False):", "ct_subject": "Lieutenant Richards was a pirate that sailed alongside Blackbeard. Lieutenant RIchards had many duties and was placed in charge of the ship, Bonnet's Revenge. In the Royal Navy and Commonwealth navies, the second-in-command of a vessel is known as the first lieutenant. Royal Navy lieutenants are in charge of other crew members and have many duties.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.99012333"], [" there", "0.99936205"], [" more", "0.7579372"], [" people", "0.9995772"], [" in", "0.99916697"], [" the", "1.0"], [" world", "1.0"], [" today", "1.0"], [" than", "0.9999876"], [" there", "1.0"], [" were", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 1000", "0.9799933"], ["?", "0.99999726"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "0.9999989"], [" are", "1.0"], [" more", "1.0"], [" people", "1.0"], [" in", "0.9997466"], [" the", "1.0"], [" world", "1.0"], [" today", "1.0"], [" than", "1.0"], [" there", "1.0"], [" were", "1.0"]]}, "427": {"qid": "249673d73deb8d6c28f9", "term": "Rand Paul", "description": "American politician, ophthalmologist, and United States Senator from Kentucky", "question": "Is Rand Paul guilty of catch-phrase used to attack John Kerry in 2004?", "answer": true, "facts": ["John Kerry was attacked by his opponents in the 2004 Presidential Campaign by being called a Flip-Flopper.", "A Flip-Flopper is someone that makes a complete change in policy from one thing to another.", "In May 2010 Rand Paul called for an electronic fence to keep out immigrants and deplored amnesty in any form.", "In 2013 Rand Paul said he was in favor of undocumented immigrants being granted  legal status."], "decomposition": ["What catch-phrase was used against John Kerry in 2004?", "What policy changes has Rand Paul expressed?", "Would #2 be considered #1?"], "evidence": [[[["Flip-flop (politics)-5"]], [["Political positions of Rand Paul-63"]], ["operation"]], [["no_evidence"], [["Rand Paul-85"], "no_evidence"], ["no_evidence", "operation"]], [[["Flip-flop (politics)-5"]], [["Rand Paul-67"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the United States the only country in the world that has a national debt?\nFacts:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Rand Paul guilty of catch-phrase used to attack John Kerry in 2004?\nFacts: John Kerry was attacked by his opponents in the 2004 Presidential Campaign by being called a Flip-Flopper. A Flip-Flopper is someone that makes a complete change in policy from one thing to another. In May 2010 Rand Paul called for an electronic fence to keep out immigrants and deplored amnesty in any form. In 2013 Rand Paul said he was in favor of undocumented immigrants being granted legal status.\nAnswer (True or False):", "ct_subject": "John Kerry was attacked by his opponents in the 2004 Presidential Campaign by being called a Flip-Flopper. A Flip-Flopper is someone that makes a complete change in policy from one thing to another. In May 2010 Rand Paul called for an electronic fence to keep out immigrants and deplored amnesty in any form. In 2013 Rand Paul said he was in favor of undocumented immigrants being granted legal status.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "0.9999839"], [" United", "0.9998167"], [" States", "1.0"], [" the", "0.41338968"], [" only", "1.0"], [" country", "1.0"], [" in", "0.99999774"], [" the", "1.0"], [" world", "1.0"], [" that", "1.0"], [" has", "1.0"], [" a", "1.0"], [" national", "0.8444696"], [" debt", "0.9631701"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"]]}, "428": {"qid": "3d7050c0e9bfbeef27c1", "term": "Melania Trump", "description": "First Lady of the United States", "question": "Did Melania Trump have same profession as Olga Kurylenko?", "answer": true, "facts": ["Melania Trump is the first lady of the United States and was previously a model.", "Olga Kurylenko is a professional actress that also works as a model."], "decomposition": ["What professions has Melania Trump had?", "What professions has Olga Kurylenko had?", "Is at least one profession listed in #1 also found in #2?"], "evidence": [[[["Melania Trump-1"]], [["Olga Storozhenko-1"]], ["operation"]], [[["Melania Trump-1"]], [["Olga Kurylenko-2"]], ["operation"]], [[["Melania Trump-1"]], [["Olga Kurylenko-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Titanic sink because of a faulty steering wheel?\nFacts: The Titanic was a luxury liner that was built in Belfast, Ireland. The Titanic was the largest ship ever built at the time. The Titanic was built to carry 1,500 passengers and crew. The Titanic was built to be the largest ship ever built. The Titanic was", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Melania Trump have same profession as Olga Kurylenko?\nFacts: Melania Trump is the first lady of the United States and was previously a model. Olga Kurylenko is a professional actress that also works as a model.\nAnswer (True or False):", "ct_subject": "Melania Trump is the first lady of the United States and was previously a model. Olga Kurylenko is a professional actress that also works as a model.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99952495"], [" the", "1.0"], [" Titanic", "0.7054565"], [" sink", "1.0"], [" because", "0.9999995"], [" of", "1.0"], [" a", "0.7588877"], [" faulty", "0.9977324"], [" steering", "0.84210575"], [" wheel", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Titanic", "1.0"], [" was", "0.8514324"], [" a", "0.9988619"], [" luxury", "0.99951303"], [" liner", "1.0"], [" that", "1.0"], [" was", "1.0"], [" built", "1.0"], [" in", "1.0"], [" Belfast", "0.6923528"], [",", "1.0"], [" Ireland", "1.0"], [".", "1.0"], [" The", "0.9999676"], [" Titanic", "0.9986885"], [" was", "1.0"], [" the", "0.6518967"], [" largest", "0.9999286"], [" ship", "0.9703611"], [" ever", "0.999258"], [" built", "1.0"], [" at", "0.9999958"], [" the", "1.0"], [" time", "1.0"], [".", "0.99999845"], [" The", "0.99993384"], [" Titanic", "1.0"], [" was", "1.0"], [" built", "0.9804417"], [" to", "0.9026963"], [" carry", "1.0"], [" 1", "0.9999999"], [",", "1.0"], ["500", "0.979684"], [" passengers", "0.99996626"], [" and", "0.99590707"], [" crew", "0.9965823"], [".", "1.0"], [" The", "1.0"], [" Titanic", "1.0"], [" was", "1.0"], [" built", "0.2862833"], [" to", "0.9989538"], [" be", "0.99999404"], [" the", "0.99999917"], [" largest", "0.99999917"], [" ship", "1.0"], [" ever", "1.0"], [" built", "1.0"], [".", "0.99999976"], [" The", "1.0"], [" Titanic", "1.0"], [" was", "1.0"]]}, "429": {"qid": "964da699d1ada747b266", "term": "BBC World Service", "description": "The BBC's international Chor radio station", "question": "Is the BBC World Service hosted in Europe?", "answer": true, "facts": ["The BBC World Service is part of the BBC network.", "The BBC operates in England.", "England is part of Europe."], "decomposition": ["Where is the BBC World Service located?", "Is #1 located in Europe?"], "evidence": [[[["BBC World Service-15"]], [["London-1", "Outline of the United Kingdom-1"]]], [[["BBC World Service-15"]], ["operation"]], [[["BBC World Service-2"]], [["United Kingdom-25"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the BBC World Service hosted in the United States?\nFacts: The BBC World Service is part of the BBC network. The BBC operates in the United States.\nAnswer (True or False): True\nQuestion: Is the BBC World Service hosted in the United Kingdom?\nFacts: The BBC World Service is part of the BBC network. The BBC operates in the United Kingdom.\nAnswer (True or", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is the BBC World Service hosted in Europe?\nFacts: The BBC World Service is part of the BBC network. The BBC operates in England. England is part of Europe.\nAnswer (True or False):", "ct_subject": "The BBC World Service is part of the BBC network. The BBC operates in England. England is part of Europe.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999752"], [" the", "1.0"], [" BBC", "1.0"], [" World", "1.0"], [" Service", "1.0"], [" hosted", "1.0"], [" in", "1.0"], [" the", "0.99999976"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" BBC", "1.0"], [" World", "1.0"], [" Service", "1.0"], [" is", "1.0"], [" part", "1.0"], [" of", "1.0"], [" the", "1.0"], [" BBC", "1.0"], [" network", "1.0"], [".", "1.0"], [" The", "1.0"], [" BBC", "1.0"], [" operates", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [".", "1.0"], ["\n", "0.9919429"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" BBC", "1.0"], [" World", "1.0"], [" Service", "1.0"], [" hosted", "1.0"], [" in", "1.0"], [" the", "0.9925149"], [" United", "0.99999976"], [" Kingdom", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" BBC", "1.0"], [" World", "1.0"], [" Service", "1.0"], [" is", "1.0"], [" part", "1.0"], [" of", "1.0"], [" the", "1.0"], [" BBC", "1.0"], [" network", "1.0"], [".", "1.0"], [" The", "1.0"], [" BBC", "1.0"], [" operates", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" Kingdom", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"]]}, "430": {"qid": "3b0c98640caf02aa66e8", "term": "Beaver", "description": "Genus of mammals", "question": "Does the land in close proximity to beaver dams suffer?", "answer": true, "facts": ["Beaver dams often lead to flooding in the areas around them.", "Flooding can lead to loosening of the soil.", "Loosened soil can cause trees to fall over. ", "Flooding can lead to soil erosion."], "decomposition": ["What are the effects of beaver dams on surrounding lands?", "Are any of #1 significantly negative?"], "evidence": [[[["Beaver eradication in Tierra del Fuego-4", "North American beaver-9"]], ["operation"]], [[["Beaver dam-11"]], [["Beaver dam-26"]]], [[["Beaver dam-26"]], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the earth flat?\nFacts: The earth is round.\nAnswer (True or False): True\nQuestion: Is the earth flat?\nFacts: The earth is round.\nAnswer (True or False): True\nQuestion: Is the earth flat?\nFacts: The earth", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does the land in close proximity to beaver dams suffer?\nFacts: Beaver dams often lead to flooding in the areas around them. Flooding can lead to loosening of the soil. Loosened soil can cause trees to fall over.  Flooding can lead to soil erosion.\nAnswer (True or False):", "ct_subject": "Beaver dams often lead to flooding in the areas around them. Flooding can lead to loosening of the soil. Loosened soil can cause trees to fall over.  Flooding can lead to soil erosion.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.49212205"], [" the", "0.9840226"], [" earth", "0.8265947"], [" flat", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999999"], [" earth", "1.0"], [" is", "1.0"], [" round", "0.979224"], [".", "1.0"], ["\n", "0.99990165"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99999976"], [" the", "0.99999976"], [" earth", "1.0"], [" flat", "0.9890725"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" earth", "1.0"], [" is", "1.0"], [" round", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" earth", "1.0"], [" flat", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" earth", "1.0"]]}, "431": {"qid": "05046d2981f68118f047", "term": "Mona Lisa", "description": "Painting by Leonardo da Vinci", "question": "Is the Mona Lisa in the same museum as the Venus de Milo?", "answer": true, "facts": ["The Mona Lisa is in the Louvre.", "The Venus de Milo is in the Louvre."], "decomposition": ["What museum stores the Mona Lisa?", "What museum stores the Venus de Milo?", "Is #1 the same as #2?"], "evidence": [[[["Mona Lisa-54"]], [["Venus de Milo-2"]], ["operation"]], [[["Mona Lisa-18"]], [["Venus de Milo-17"]], ["operation"]], [[["Mona Lisa-2"]], [["Venus de Milo-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the Mona Lisa in the same museum as the Venus de Milo?\nFacts: The Mona Lisa is in the Louvre. The Venus de Milo is in the Louvre.\nAnswer (True or False): True\nQuestion: Is the Mona Lisa in the same museum as the Venus de Milo?\nFacts: The Mona Lisa is in the Louvre. The Venus", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is the Mona Lisa in the same museum as the Venus de Milo?\nFacts: The Mona Lisa is in the Louvre. The Venus de Milo is in the Louvre.\nAnswer (True or False):", "ct_subject": "The Mona Lisa is in the Louvre. The Venus de Milo is in the Louvre.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.98692083"], [" the", "1.0"], [" Mon", "1.0"], ["a", "1.0"], [" Lisa", "1.0"], [" in", "1.0"], [" the", "1.0"], [" same", "1.0"], [" museum", "1.0"], [" as", "1.0"], [" the", "1.0"], [" Venus", "1.0"], [" de", "1.0"], [" Milo", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Mon", "1.0"], ["a", "1.0"], [" Lisa", "1.0"], [" is", "1.0"], [" in", "1.0"], [" the", "1.0"], [" Lou", "1.0"], ["vre", "1.0"], [".", "1.0"], [" The", "1.0"], [" Venus", "1.0"], [" de", "1.0"], [" Milo", "1.0"], [" is", "1.0"], [" in", "1.0"], [" the", "1.0"], [" Lou", "1.0"], ["vre", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" Mon", "1.0"], ["a", "1.0"], [" Lisa", "1.0"], [" in", "1.0"], [" the", "1.0"], [" same", "1.0"], [" museum", "1.0"], [" as", "1.0"], [" the", "1.0"], [" Venus", "1.0"], [" de", "1.0"], [" Milo", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Mon", "1.0"], ["a", "1.0"], [" Lisa", "1.0"], [" is", "1.0"], [" in", "1.0"], [" the", "1.0"], [" Lou", "1.0"], ["vre", "1.0"], [".", "1.0"], [" The", "1.0"], [" Venus", "1.0"]]}, "432": {"qid": "ef8cd9d65ecb85d74a19", "term": "Will Ferrell", "description": "American actor, comedian, producer, writer and businessman", "question": "Would it be difficult for Will Ferrell to win Empire Award for Best Newcomer?", "answer": true, "facts": ["The Empire Award for Best Newcomer was awarded for an actor in their debut role.", "Will Ferrell debuted in 1995."], "decomposition": ["When do actors get to win the Empire Award for Best Newcomer?", "When did Will Ferrell participate in #1?", "Is #2 a long time ago?"], "evidence": [[[["Empire Award for Best Newcomer-1"]], [["Will Ferrell-1"]], ["operation"]], [[["23rd Empire Awards-1", "Empire Award for Best Male Newcomer-1"]], [["Will Ferrell-1"]], ["operation"]], [[["Empire Award for Best Newcomer-1"]], [["On Our Own (1994 TV series)-2"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the year 2000?\nFacts: The year 2000 was the first year that the year zero was used.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the year 2000?\nFacts: The year 2000 was the first year that", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would it be difficult for Will Ferrell to win Empire Award for Best Newcomer?\nFacts: The Empire Award for Best Newcomer was awarded for an actor in their debut role. Will Ferrell debuted in 1995.\nAnswer (True or False):", "ct_subject": "The Empire Award for Best Newcomer was awarded for an actor in their debut role. Will Ferrell debuted in 1995.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999999"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9992787"], [" of", "1.0"], [" the", "1.0"], [" first", "0.4146049"], [" person", "0.9790012"], [" to", "1.0"], [" be", "0.9999895"], [" born", "0.9898705"], [" in", "1.0"], [" the", "1.0"], [" year", "0.9997737"], [" 2000", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" year", "0.7328369"], [" 2000", "1.0"], [" was", "0.99999976"], [" the", "1.0"], [" first", "0.9996117"], [" year", "1.0"], [" that", "1.0"], [" the", "0.9999112"], [" year", "1.0"], [" zero", "0.7137539"], [" was", "1.0"], [" used", "0.99991095"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "1.0"], [" the", "1.0"], [" first", "1.0"], [" year", "1.0"], [" that", "1.0"]]}, "433": {"qid": "15ea72669f16beecac5a", "term": "Confederate States Army", "description": "Southern army in American Civil War", "question": "Did Confederate States Army influence West Point fashion?", "answer": true, "facts": ["The Confederate States Army was clad in cadet gray uniforms.", "West Point uniforms are cadet gray and white.", "Confederate States Army uniforms contained Generally, the uniform jacket of the Confederate soldier was single breasted, made of gray or brown fabric, with a six to nine button front and hat.", " West Point uniforms contain a standing collar, white trousers, and black shakos (known as a \"tarbucket hat\" in U.S. Army nomenclature)."], "decomposition": ["What were the main features of the Confederate States Army uniforms?", "What are the most notable features of West Point uniforms?", "Is there a significant overlap between #1 and #2?"], "evidence": [[[["Uniforms of the Confederate States Armed Forces-17"]], [["United States Military Academy-62"], "no_evidence"], ["operation"]], [[["Confederate States Army-49"], "no_evidence"], [["United States Military Academy-74"], "no_evidence"], ["no_evidence", "operation"]], [[["Uniforms of the Confederate States Armed Forces-12"]], ["no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the U.S. Army use the term \"", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Confederate States Army influence West Point fashion?\nFacts: The Confederate States Army was clad in cadet gray uniforms. West Point uniforms are cadet gray and white. Confederate States Army uniforms contained Generally, the uniform jacket of the Confederate soldier was single breasted, made of gray or brown fabric, with a six to nine button front and hat.  West Point uniforms contain a standing collar, white trousers, and black shakos (known as a \"tarbucket hat\" in U.S. Army nomenclature).\nAnswer (True or False):", "ct_subject": "The Confederate States Army was clad in cadet gray uniforms. West Point uniforms are cadet gray and white. Confederate States Army uniforms contained Generally, the uniform jacket of the Confederate soldier was single breasted, made of gray or brown fabric, with a six to nine button front and hat.  West Point uniforms contain a standing collar, white trousers, and black shakos (known as a \"tarbucket hat\" in U.S. Army nomenclature).", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9990277"], [" the", "1.0"], [" U", "0.9458829"], [".", "1.0"], ["S", "1.0"], [".", "1.0"], [" Army", "0.9999994"], [" use", "0.99994385"], [" the", "1.0"], [" term", "0.9998765"], [" \"", "1.0"]]}, "434": {"qid": "ac3de52d3bc2e3b5d8a4", "term": "Reproduction", "description": "Biological process by which new organisms are generated from one or more parent organisms", "question": "Would an environmentalist advocate for preventing domestic canine reproduction?", "answer": true, "facts": ["Domestic dogs are a large contributor to species depopulation and displacement.", "Domestic dogs have a diet that largely contributes to a harmful environmental impact. "], "decomposition": ["What do environmentalists try to protect?", "Do domestic dogs harm #1?"], "evidence": [[[["Environmentalism-1"]], [["Dog-61"], "no_evidence", "operation"]], [[["Environmentalist-1"]], [["Dog-42"], "no_evidence", "operation"]], [[["Environmentalist-1"]], [["Dog-30", "Overpopulation in domestic pets-4"], "no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Would an environmentalist advocate for preventing domestic human reproduction?\nFacts: Human reproduction is a large contributor to species depopulation and displacement. Human reproduction is also a large contributor to the destruction of the environment. \nAnswer (True or False): True\nQuestion: Would an environmentalist advocate for preventing human reproduction?\nFacts: Human reproduction is a large contributor to", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would an environmentalist advocate for preventing domestic canine reproduction?\nFacts: Domestic dogs are a large contributor to species depopulation and displacement. Domestic dogs have a diet that largely contributes to a harmful environmental impact. \nAnswer (True or False):", "ct_subject": "Domestic dogs are a large contributor to species depopulation and displacement. Domestic dogs have a diet that largely contributes to a harmful environmental impact. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Would", "0.9998846"], [" an", "1.0"], [" environmental", "1.0"], ["ist", "1.0"], [" advocate", "1.0"], [" for", "1.0"], [" preventing", "1.0"], [" domestic", "0.99910563"], [" human", "0.9983985"], [" reproduction", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Human", "0.8694655"], [" reproduction", "1.0"], [" is", "1.0"], [" a", "1.0"], [" large", "1.0"], [" contributor", "1.0"], [" to", "1.0"], [" species", "1.0"], [" dep", "1.0"], ["op", "1.0"], ["ulation", "1.0"], [" and", "1.0"], [" displacement", "1.0"], [".", "1.0"], [" Human", "1.0"], [" reproduction", "1.0"], [" is", "0.9592361"], [" also", "0.5052641"], [" a", "1.0"], [" large", "1.0"], [" contributor", "1.0"], [" to", "1.0"], [" the", "0.99536765"], [" destruction", "0.5196459"], [" of", "1.0"], [" the", "1.0"], [" environment", "1.0"], [".", "1.0"], [" ", "0.95388234"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Would", "1.0"], [" an", "1.0"], [" environmental", "1.0"], ["ist", "1.0"], [" advocate", "1.0"], [" for", "1.0"], [" preventing", "1.0"], [" human", "0.99759775"], [" reproduction", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Human", "1.0"], [" reproduction", "1.0"], [" is", "1.0"], [" a", "1.0"], [" large", "1.0"], [" contributor", "1.0"], [" to", "1.0"]]}, "435": {"qid": "b679b7210ebf7fec17b8", "term": "Othello", "description": "play by Shakespeare", "question": "Are there options for students who struggle to understand the writing style of Othello?", "answer": true, "facts": ["\"No Fear Shakespeare\" is a line of books that translate the language of original Shakespeare plays into modern English.", "Scripts can be understood more easily when read alongside a production of the play itself.", "\"No Fear Shakespeare\" features Othello in their book lineup."], "decomposition": ["What is the name of a line of books that translate the language of Shakespeare plays into modern English?", "Does #1 feature Othello in their book lineup?"], "evidence": [[[["Hamlet-2"]], ["no_evidence", "operation"]], [[["SparkNotes-5"]], ["no_evidence", "operation"]], [[["SparkNotes-5"]], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"C\" and a \"K\"?\nFacts: The letter \"C\" is the letter that is used to represent the letter \"K\".\nAnswer (True or False): True\nQuestion", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are there options for students who struggle to understand the writing style of Othello?\nFacts: \"No Fear Shakespeare\" is a line of books that translate the language of original Shakespeare plays into modern English. Scripts can be understood more easily when read alongside a production of the play itself. \"No Fear Shakespeare\" features Othello in their book lineup.\nAnswer (True or False):", "ct_subject": "\"No Fear Shakespeare\" is a line of books that translate the language of original Shakespeare plays into modern English. Scripts can be understood more easily when read alongside a production of the play itself. \"No Fear Shakespeare\" features Othello in their book lineup.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99874496"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.7967916"], [" between", "1.0"], [" a", "0.9955428"], [" \"", "1.0"], ["C", "0.7178856"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["K", "0.8442385"], ["\"?", "0.9999993"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99971884"], [" letter", "0.7325559"], [" \"", "1.0"], ["C", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" the", "0.9999752"], [" letter", "0.2909968"], [" that", "0.994619"], [" is", "0.99999917"], [" used", "1.0"], [" to", "0.9961514"], [" represent", "0.4511376"], [" the", "1.0"], [" letter", "0.99998724"], [" \"", "1.0"], ["K", "1.0"], ["\".", "0.75960445"], ["\n", "0.99993956"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"]]}, "436": {"qid": "7162ff9d73e524fef983", "term": "French Defence", "description": "Chess opening", "question": "Would most grand masters know what the French Defense is?", "answer": true, "facts": ["Grand master is the highest title a chess player can get.", "The French Defense is a well known chess opening that is in many books."], "decomposition": ["What is the French Defence?", "What is a Grandmaster?", "Would #2 likely know what #1 is?"], "evidence": [[[["French Defence-1"]], [["Grandmaster (chess)-1"]], ["operation"]], [[["French Defence-46"]], [["Grandmaster (chess)-1"]], ["operation"]], [[["French Defence-2"]], [["Grandmaster (chess)-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first chess player to win a tournament?\nFacts: The first chess player to win a tournament was Alexander Alekhine in 1887.\nAnswer (True or False): True\nQuestion: What is the name of the first chess player to win a tournament?\nFacts: The first chess player to win a tournament was Alexander Alekhine in", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would most grand masters know what the French Defense is?\nFacts: Grand master is the highest title a chess player can get. The French Defense is a well known chess opening that is in many books.\nAnswer (True or False):", "ct_subject": "Grand master is the highest title a chess player can get. The French Defense is a well known chess opening that is in many books.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "0.4227076"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9999808"], [" chess", "1.0"], [" player", "0.9999969"], [" to", "0.70761305"], [" win", "0.9999999"], [" a", "1.0"], [" tournament", "0.99375844"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" chess", "1.0"], [" player", "1.0"], [" to", "1.0"], [" win", "1.0"], [" a", "1.0"], [" tournament", "1.0"], [" was", "1.0"], [" Alexander", "0.23405874"], [" Ale", "1.0"], ["kh", "1.0"], ["ine", "1.0"], [" in", "0.15840611"], [" 18", "1.0"], ["87", "0.9972007"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" chess", "1.0"], [" player", "1.0"], [" to", "1.0"], [" win", "1.0"], [" a", "1.0"], [" tournament", "1.0"], ["?", "0.9999994"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" chess", "1.0"], [" player", "1.0"], [" to", "1.0"], [" win", "1.0"], [" a", "1.0"], [" tournament", "1.0"], [" was", "1.0"], [" Alexander", "1.0"], [" Ale", "1.0"], ["kh", "1.0"], ["ine", "1.0"], [" in", "1.0"]]}, "437": {"qid": "2780b837af5373ff2cb4", "term": "The Matrix", "description": "1999 science fiction action film directed by the Wachowskis", "question": "Will Gremlins sequels tie number of Matrix sequels?", "answer": true, "facts": ["The Matrix films had two sequels.", "Gremlins has one sequel, Gremlins 2: The New Batch.", "The script for a Gremlins 3 is being written by Carl Ellsworth."], "decomposition": ["How many sequels did The Matrix have?", "How many sequels did Gremlins have?", "How many Gremlins movies are currently being worked on?", "What is the sum of #2 and #3?", "Is #4 equal to #1?"], "evidence": [[[["The Matrix (franchise)-1"]], [["Gremlins-48"]], [["Gremlins-51"]], [["The Matrix (franchise)-1"]], [["The Matrix (franchise)-1"]]], [[["The Matrix (franchise)-1"]], [["Gremlins-2"]], [["Gremlins: Secrets of the Mogwai-2"], "operation"], ["operation"], ["operation"]], [[["The Matrix (franchise)-1"]], [["Gremlins-2"]], [["Gremlins 2: The New Batch-46"]], ["operation"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Will Gremlins 3: The New Batch be released in theaters?\nFacts: Gremlins 3 is being written by Carl Ellsworth.\nAnswer (True or False): True\nQuestion: Will Gremlins 3: The New Batch be released on DVD?\nFacts: Gremlins 3", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Will Gremlins sequels tie number of Matrix sequels?\nFacts: The Matrix films had two sequels. Gremlins has one sequel, Gremlins 2: The New Batch. The script for a Gremlins 3 is being written by Carl Ellsworth.\nAnswer (True or False):", "ct_subject": "The Matrix films had two sequels. Gremlins has one sequel, Gremlins 2: The New Batch. The script for a Gremlins 3 is being written by Carl Ellsworth.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Will", "0.89189816"], [" G", "0.92140615"], ["rem", "1.0"], ["lins", "1.0"], [" 3", "0.9746805"], [":", "0.98942167"], [" The", "1.0"], [" New", "1.0"], [" B", "1.0"], ["atch", "1.0"], [" be", "1.0"], [" released", "0.9999995"], [" in", "1.0"], [" theaters", "0.9898329"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" G", "0.9890646"], ["rem", "1.0"], ["lins", "1.0"], [" 3", "1.0"], [" is", "0.9881803"], [" being", "0.99996364"], [" written", "1.0"], [" by", "1.0"], [" Carl", "1.0"], [" Ell", "1.0"], ["sworth", "1.0"], [".", "1.0"], ["\n", "0.9999517"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Will", "1.0"], [" G", "1.0"], ["rem", "1.0"], ["lins", "1.0"], [" 3", "1.0"], [":", "1.0"], [" The", "1.0"], [" New", "1.0"], [" B", "1.0"], ["atch", "1.0"], [" be", "1.0"], [" released", "1.0"], [" on", "0.99999344"], [" DVD", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" G", "1.0"], ["rem", "1.0"], ["lins", "1.0"], [" 3", "1.0"]]}, "438": {"qid": "e87b63e92165b417d37f", "term": "People for the Ethical Treatment of Animals", "description": "American animal rights organization", "question": "Is Michael Vick on People for the Ethical Treatment of Animals's hypothetical blacklist?", "answer": true, "facts": ["People for the Ethical Treatment of Animals (PETA) is an animal rights organization that fights for the welfare of animals.", "A blacklist is a list of people that are unacceptable and should be avoided or excluded.", "Michael Vick is an ex-football player that spent 21 months in prison for his vicious dog fighting enterprise."], "decomposition": ["What do the People for the Ethical Treatment of Animals advocate for?", "What crime has Michael Vick done time for?", "Does #2 strongly violate #1?"], "evidence": [[[["People for the Ethical Treatment of Animals-1"]], [["Michael Vick-2"]], [["Dog fighting in the United States-1"]]], [[["People for the Ethical Treatment of Animals-11"]], [["Michael Vick-2"]], ["operation"]], [[["People for the Ethical Treatment of Animals-1"]], [["Michael Vick-2"]], [["Dog fighting-2"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city in the United States of America, New York, derived from the name of the city in the Mongolian Empire, Mongolia?\nFacts: The name", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Michael Vick on People for the Ethical Treatment of Animals's hypothetical blacklist?\nFacts: People for the Ethical Treatment of Animals (PETA) is an animal rights organization that fights for the welfare of animals. A blacklist is a list of people that are unacceptable and should be avoided or excluded. Michael Vick is an ex-football player that spent 21 months in prison for his vicious dog fighting enterprise.\nAnswer (True or False):", "ct_subject": "People for the Ethical Treatment of Animals (PETA) is an animal rights organization that fights for the welfare of animals. A blacklist is a list of people that are unacceptable and should be avoided or excluded. Michael Vick is an ex-football player that spent 21 months in prison for his vicious dog fighting enterprise.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99870944"], [" the", "0.9999987"], [" name", "0.75029"], [" of", "1.0"], [" the", "1.0"], [" city", "0.99991703"], [" in", "0.868051"], [" the", "0.96438074"], [" United", "0.99930036"], [" States", "1.0"], [" of", "0.99998045"], [" America", "1.0"], [",", "0.99999964"], [" New", "1.0"], [" York", "1.0"], [",", "0.60462147"], [" derived", "0.9731137"], [" from", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "0.99999964"], [" city", "1.0"], [" in", "1.0"], [" the", "0.5562565"], [" Mongol", "1.0"], ["ian", "1.0"], [" Empire", "1.0"], [",", "1.0"], [" Mongolia", "0.51282924"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" name", "1.0"]]}, "439": {"qid": "d57b4438cf31f3ca05fe", "term": "Roman numerals", "description": "Numbers in the Roman numeral system", "question": "Is MIX a word and a roman numeral?", "answer": true, "facts": ["\"Mix\" means to combine in english.", "M equals one thousand in roman numerals", "I equals one in roman numerals ", "I before X in roman numerals equals nine.", "MIX equals one thousand nine in roman numerals. "], "decomposition": ["What does Mix mean in english language?", "Is Mix a number in Roman numerals?", "Based on #1 and #2, is mix both a word and a roman numeral?"], "evidence": [[[["Audio mixing (recorded music)-1"]], [["Roman numerals-5"]], [["Audio mixing (recorded music)-1", "Roman numerals-5"]]], [[["Mix (magazine)-1", "Mixing (process engineering)-38"], "no_evidence"], [["1009-1", "Roman numerals-1"]], ["operation"]], [[["DJ mix-1"]], [["Roman numerals-5"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"M\" and a \"MIX\"?\nFacts: A \"M\" is a \"Mixed\" number. A \"MIX\" is a \"Mixed\" number with a \"M\" in front of it.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is MIX a word and a roman numeral?\nFacts: \"Mix\" means to combine in english. M equals one thousand in roman numerals I equals one in roman numerals  I before X in roman numerals equals nine. MIX equals one thousand nine in roman numerals. \nAnswer (True or False):", "ct_subject": "\"Mix\" means to combine in english. M equals one thousand in roman numerals I equals one in roman numerals  I before X in roman numerals equals nine. MIX equals one thousand nine in roman numerals. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.6154491"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9974589"], [" between", "1.0"], [" a", "0.99999154"], [" \"", "1.0"], ["M", "0.99981445"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["M", "0.9945738"], ["IX", "0.30125928"], ["\"?", "0.99999547"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.99374384"], [" \"", "1.0"], ["M", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" \"", "0.4285882"], ["M", "1.0"], ["ixed", "1.0"], ["\"", "1.0"], [" number", "0.9999622"], [".", "1.0"], [" A", "1.0"], [" \"", "1.0"], ["M", "1.0"], ["IX", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["M", "1.0"], ["ixed", "1.0"], ["\"", "1.0"], [" number", "1.0"], [" with", "0.99998295"], [" a", "1.0"], [" \"", "1.0"], ["M", "1.0"], ["\"", "0.99999964"], [" in", "1.0"], [" front", "0.98530847"], [" of", "1.0"], [" it", "1.0"], [".", "1.0"]]}, "440": {"qid": "6047d085ff9246481d12", "term": "Little Women", "description": "1860s novel by Louisa May Alcott", "question": "Could Little Women have been a book read by veterans of the civil war?", "answer": true, "facts": ["Little Women was published in 1868.", "The civil war ended in 1865."], "decomposition": ["When was the book Little Women written?", "When did the civil war take place?", "Could adults as of #2 still be alive by #1?"], "evidence": [[[["Little Women-1"]], [["American Civil War-1"]], ["operation"]], [[["Little Women-1"]], [["American Civil War-1"]], ["operation"]], [[["Little Women-1"]], [["American Civil War-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Civil War have a \"War of Northern Aggression\"?\nFacts: The Civil War was fought between the states of the United States. The war was not a war between the states.\nAnswer (True or False): True\nQuestion: Did the Civil War have a \"War of Northern Aggression\"?\nFacts: The Civil War was fought between the states of the United States. The war was not a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could Little Women have been a book read by veterans of the civil war?\nFacts: Little Women was published in 1868. The civil war ended in 1865.\nAnswer (True or False):", "ct_subject": "Little Women was published in 1868. The civil war ended in 1865.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.94256353"], [" the", "1.0"], [" Civil", "0.99999654"], [" War", "1.0"], [" have", "0.8714304"], [" a", "1.0"], [" \"", "0.9999993"], ["War", "0.9998846"], [" of", "1.0"], [" Northern", "0.9841463"], [" Agg", "1.0"], ["ression", "1.0"], ["\"?", "0.9966737"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Civil", "1.0"], [" War", "1.0"], [" was", "1.0"], [" fought", "0.9988979"], [" between", "1.0"], [" the", "1.0"], [" states", "0.9999944"], [" of", "1.0"], [" the", "1.0"], [" United", "0.9999994"], [" States", "1.0"], [".", "0.9565024"], [" The", "0.9999721"], [" war", "0.99999785"], [" was", "1.0"], [" not", "0.9999999"], [" a", "0.9999999"], [" war", "1.0"], [" between", "1.0"], [" the", "1.0"], [" states", "0.96015507"], [".", "0.99990296"], ["\n", "0.9999969"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99318117"], [" the", "1.0"], [" Civil", "1.0"], [" War", "1.0"], [" have", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["War", "1.0"], [" of", "1.0"], [" Northern", "0.6908068"], [" Agg", "1.0"], ["ression", "1.0"], ["\"?", "0.26427233"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Civil", "1.0"], [" War", "1.0"], [" was", "1.0"], [" fought", "1.0"], [" between", "1.0"], [" the", "1.0"], [" states", "1.0"], [" of", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [".", "1.0"], [" The", "1.0"], [" war", "1.0"], [" was", "1.0"], [" not", "1.0"], [" a", "1.0"]]}, "441": {"qid": "9642838ebb5c1c382ade", "term": "Holy Saturday", "description": "Saturday before Easter Sunday", "question": "Did Holy Saturday 2019 have special significance to pot smokers?", "answer": true, "facts": ["Holy Saturday 2019 took place on April 20th.", "April 20th, known as 4/20 day, National Pot Smokers Day, Weed Day or National Weed Day, is a holiday for pot smokers."], "decomposition": ["What date was Holy Saturday in 2019?", "What date is an unofficial holiday for pop smokers?", "Is #1 the same as #2?"], "evidence": [[[["Holy Saturday-3"], "no_evidence"], [["420 (cannabis culture)-1"]], ["operation"]], [["no_evidence"], [["420 (cannabis culture)-1"]], ["no_evidence", "operation"]], [[["2019 Australian federal election-38"], "no_evidence"], [["420 (cannabis culture)-15"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the first person to use the word \"cannabis\" in English come from the United States?\nFacts: The first person to use the word \"cannabis\" in English came from the United States. The word \"cannabis\" was first used in the United States in the early 1800s", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Holy Saturday 2019 have special significance to pot smokers?\nFacts: Holy Saturday 2019 took place on April 20th. April 20th, known as 4/20 day, National Pot Smokers Day, Weed Day or National Weed Day, is a holiday for pot smokers.\nAnswer (True or False):", "ct_subject": "Holy Saturday 2019 took place on April 20th. April 20th, known as 4/20 day, National Pot Smokers Day, Weed Day or National Weed Day, is a holiday for pot smokers.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99953973"], [" the", "1.0"], [" first", "0.99995637"], [" person", "0.999941"], [" to", "1.0"], [" use", "0.9574371"], [" the", "1.0"], [" word", "0.99999976"], [" \"", "1.0"], ["c", "0.9967236"], ["ann", "0.9999938"], ["abis", "1.0"], ["\"", "1.0"], [" in", "0.99995327"], [" English", "0.989078"], [" come", "0.99699557"], [" from", "1.0"], [" the", "0.9999813"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "0.98585343"], [" person", "0.9999914"], [" to", "1.0"], [" use", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["ann", "1.0"], ["abis", "1.0"], ["\"", "1.0"], [" in", "1.0"], [" English", "1.0"], [" came", "1.0"], [" from", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [".", "1.0"], [" The", "0.21949169"], [" word", "0.9999999"], [" \"", "1.0"], ["c", "1.0"], ["ann", "1.0"], ["abis", "1.0"], ["\"", "1.0"], [" was", "0.9999987"], [" first", "0.9999987"], [" used", "1.0"], [" in", "1.0"], [" the", "0.9879084"], [" United", "1.0"], [" States", "1.0"], [" in", "1.0"], [" the", "0.9999999"], [" early", "0.99354506"], [" 1800", "1.0"], ["s", "0.8465975"]]}, "442": {"qid": "ec71dcb7d6ace3e73ef9", "term": "Soup", "description": "primarily liquid food", "question": "Is shoe soup innocuous?", "answer": true, "facts": ["Soup is a primarily liquid food containing various meats and beans.", "Director Werner Herzog lost a bet and cooked his shoe into a soup and ate it in 1980.", "Werner Herzog turned 77 in 2019 and had a role in the hit TV series the Mandalorian."], "decomposition": ["What film director ate shoe soup in the year 1980?", "Is #1 still alive?"], "evidence": [[[["Werner Herzog Eats His Shoe-1"]], [["Werner Herzog-1"], "operation"]], [[["Werner Herzog-12"]], [["Werner Herzog-30"], "operation"]], [[["Werner Herzog Eats His Shoe-1"]], [["Werner Herzog-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"soup\" and a \"soup\" and a \"soup\"?\nFacts: A \"soup\" is a liquid food containing various meats and beans. A \"soup\" is a soup. A \"soup\" is a soup.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is shoe soup innocuous?\nFacts: Soup is a primarily liquid food containing various meats and beans. Director Werner Herzog lost a bet and cooked his shoe into a soup and ate it in 1980. Werner Herzog turned 77 in 2019 and had a role in the hit TV series the Mandalorian.\nAnswer (True or False):", "ct_subject": "Soup is a primarily liquid food containing various meats and beans. Director Werner Herzog lost a bet and cooked his shoe into a soup and ate it in 1980. Werner Herzog turned 77 in 2019 and had a role in the hit TV series the Mandalorian.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.3942737"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.989427"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.9984267"], ["s", "0.9972097"], ["oup", "0.9999795"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["oup", "1.0"], ["\"", "0.9999807"], [" and", "0.99998534"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["oup", "1.0"], ["\"?", "0.9999995"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999628"], [" \"", "0.67814386"], ["s", "1.0"], ["oup", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" liquid", "1.0"], [" food", "1.0"], [" containing", "0.99974173"], [" various", "1.0"], [" meats", "1.0"], [" and", "1.0"], [" beans", "1.0"], [".", "1.0"], [" A", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["oup", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" soup", "0.99998844"], [".", "1.0"], [" A", "0.24157046"], [" \"", "1.0"], ["s", "1.0"], ["oup", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" soup", "1.0"], [".", "1.0"]]}, "443": {"qid": "6497da19a2bb3f0dfcbb", "term": "Swallow", "description": "family of birds", "question": "In a hypothetical race between a Swallow and an American Woodcock, would the Swallow win?", "answer": true, "facts": ["Swallow can fly about 30-40mph. ", "The American woodcock can fly approximately 5mph. "], "decomposition": ["How quickly can a swallow fly?", "How quickly can an American woodcock fly?", "Is #1 greater than #2?"], "evidence": [[[["Barn swallow-18"]], [["American woodcock-14"]], [["Barn swallow-18"]]], [["no_evidence"], [["American woodcock-14"]], ["no_evidence", "operation"]], [[["Swallow-16"], "no_evidence"], [["American woodcock-14"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance a human being has ever traveled?\nFacts: The longest distance a human being has ever traveled is about 1,000 miles. \nAnswer (True or False): True\nQuestion: What is the longest distance a human being has ever traveled?\nFacts: The longest distance a human being has ever traveled is about 1,000", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: In a hypothetical race between a Swallow and an American Woodcock, would the Swallow win?\nFacts: Swallow can fly about 30-40mph.  The American woodcock can fly approximately 5mph. \nAnswer (True or False):", "ct_subject": "Swallow can fly about 30-40mph.  The American woodcock can fly approximately 5mph. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.98045236"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.691827"], [" distance", "1.0"], [" a", "0.9999622"], [" human", "0.9958527"], [" being", "0.99993813"], [" has", "0.9999995"], [" ever", "1.0"], [" traveled", "0.99963593"], ["?", "0.99961793"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "1.0"], [" has", "1.0"], [" ever", "1.0"], [" traveled", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 1", "0.9993401"], [",", "0.9999976"], ["000", "0.9999999"], [" miles", "1.0"], [".", "1.0"], [" ", "0.999998"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "1.0"], [" has", "1.0"], [" ever", "1.0"], [" traveled", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "1.0"], [" has", "1.0"], [" ever", "1.0"], [" traveled", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 1", "1.0"], [",", "1.0"], ["000", "1.0"]]}, "444": {"qid": "2da278a3d315ec89e21c", "term": "Marxism", "description": "Economic and sociopolitical worldview based on the works of Karl Marx", "question": "Are right wing Amreicans opposed to marxism?", "answer": true, "facts": ["Right Wing Americans view socialism as an enemy to civil liberties and the economy.", "Socialism is a tenant of Marxism, giving workers the means of the production."], "decomposition": ["What stance do most right-wing Americans take towards socialism?", "Is #1 against that which Marxists proposes on the subject?"], "evidence": [[[["Right-wing politics-7"]], [["Marxism-21"], "operation"]], [[["Conservatism-1", "Conservatism-7"]], [["Means of production-5"], "operation"]], [[["Right-wing politics-7"]], [["Timeline of Karl Marx-2"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are right wing Amreicans opposed to communism?\nFacts: Right Wing Americans view communism as an enemy to civil liberties and the economy. Communism is a tenant of Marxism, giving workers the means of the production.\nAnswer (True or False): True\nQuestion: Are right wing Amreicans opposed to capitalism?\nFacts: Right Wing Americans view capitalism", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are right wing Amreicans opposed to marxism?\nFacts: Right Wing Americans view socialism as an enemy to civil liberties and the economy. Socialism is a tenant of Marxism, giving workers the means of the production.\nAnswer (True or False):", "ct_subject": "Right Wing Americans view socialism as an enemy to civil liberties and the economy. Socialism is a tenant of Marxism, giving workers the means of the production.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" right", "0.94855064"], [" wing", "1.0"], [" Am", "1.0"], ["re", "1.0"], ["icans", "1.0"], [" opposed", "1.0"], [" to", "1.0"], [" communism", "0.34133276"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Right", "1.0"], [" Wing", "1.0"], [" Americans", "1.0"], [" view", "1.0"], [" communism", "0.9999999"], [" as", "1.0"], [" an", "1.0"], [" enemy", "1.0"], [" to", "1.0"], [" civil", "1.0"], [" liberties", "1.0"], [" and", "1.0"], [" the", "1.0"], [" economy", "1.0"], [".", "1.0"], [" Communism", "1.0"], [" is", "1.0"], [" a", "1.0"], [" tenant", "1.0"], [" of", "1.0"], [" Marxism", "1.0"], [",", "1.0"], [" giving", "1.0"], [" workers", "1.0"], [" the", "1.0"], [" means", "1.0"], [" of", "1.0"], [" the", "1.0"], [" production", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" right", "1.0"], [" wing", "1.0"], [" Am", "1.0"], ["re", "1.0"], ["icans", "1.0"], [" opposed", "1.0"], [" to", "1.0"], [" capitalism", "0.8454117"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Right", "1.0"], [" Wing", "1.0"], [" Americans", "1.0"], [" view", "1.0"], [" capitalism", "1.0"]]}, "445": {"qid": "e83400194f9cf2a3b1ef", "term": "Stone Cold Steve Austin", "description": "American professional wrestler", "question": "Can Stone Cold Steve Austin apply his finisher to a mule deer?", "answer": true, "facts": ["Steve Austin's finisher, The Stone Cold Stunner, involves lifting the opponent and then pulling their head down.", "The largest male mule deer's are around 330 pounds.", "Steve Austin has applied his finisher to the wrestler The Big Show.", "The Big Show weighs 383 pounds."], "decomposition": ["What activity is involved in Stone Cold Steve Austin's finisher?", "How much does a mule deer weigh?", "What was the weight of Stone Cold Steve Austin's largest opponent that he used #1 on?", "Is #2 less than or equal to #3?"], "evidence": [[[["Stone Cold Steve Austin-70", "Stunner (professional wrestling)-13"]], [["Mule deer-6"]], [["The Undertaker-72"], "no_evidence"], ["operation"]], [[["Stone Cold Steve Austin-70"]], [["Mule deer-6"]], ["no_evidence"], ["operation"]], [[["Stone Cold Steve Austin-15", "Stunner (professional wrestling)-1"]], [["Mule deer-6"]], [["Stone Cold Steve Austin-21"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first man to be killed by a shark?\nFacts: The first man to be killed by a shark was a fisherman named John H. Smith. Smith was killed by a shark while fishing", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can Stone Cold Steve Austin apply his finisher to a mule deer?\nFacts: Steve Austin's finisher, The Stone Cold Stunner, involves lifting the opponent and then pulling their head down. The largest male mule deer's are around 330 pounds. Steve Austin has applied his finisher to the wrestler The Big Show. The Big Show weighs 383 pounds.\nAnswer (True or False):", "ct_subject": "Steve Austin's finisher, The Stone Cold Stunner, involves lifting the opponent and then pulling their head down. The largest male mule deer's are around 330 pounds. Steve Austin has applied his finisher to the wrestler The Big Show. The Big Show weighs 383 pounds.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999654"], [" is", "1.0"], [" the", "1.0"], [" name", "0.017263316"], [" of", "1.0"], [" the", "1.0"], [" first", "0.8070867"], [" man", "0.52599174"], [" to", "1.0"], [" be", "0.99999213"], [" killed", "0.9976942"], [" by", "1.0"], [" a", "1.0"], [" shark", "0.99999297"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" be", "1.0"], [" killed", "1.0"], [" by", "1.0"], [" a", "1.0"], [" shark", "1.0"], [" was", "1.0"], [" a", "1.0"], [" fisherman", "0.77375096"], [" named", "1.0"], [" John", "1.0"], [" H", "0.698296"], [".", "1.0"], [" Smith", "0.6852078"], [".", "1.0"], [" Smith", "0.9999993"], [" was", "1.0"], [" killed", "0.9923618"], [" by", "1.0"], [" a", "1.0"], [" shark", "0.99999094"], [" while", "1.0"], [" fishing", "1.0"]]}, "446": {"qid": "d1c8f0835896d18c99c2", "term": "Martin Luther", "description": "Saxon priest, monk and theologian, seminal figure in Protestant Reformation", "question": "If Martin Luther did one theses a day would he run out in half a year?", "answer": true, "facts": ["Martin Luther published a list of 95 theses as his critique of the church.", "There are approximately 182 days in 6 months."], "decomposition": ["How many theses did Martin Luther publish in a list?", "How many days are in a year?", "What is #2 divided by 2?", "Is #1 less than #3?"], "evidence": [[[["Martin Luther-1"]], [["Year-4"]], ["operation"], ["operation"]], [[["Ninety-five Theses-1"]], [["Calendar year-2"]], ["operation"], ["operation"]], [[["Martin Luther-1"]], [["Year-3"]], ["operation"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: If you were to take a piece of paper and write on it, \"I am a man,\" would you be able to read it?\nFacts: Yes.\nAnswer (True or False): True\nQuestion: If you were to take a piece of paper and write on it, \"I am a man,\" would you be able to read it?", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: If Martin Luther did one theses a day would he run out in half a year?\nFacts: Martin Luther published a list of 95 theses as his critique of the church. There are approximately 182 days in 6 months.\nAnswer (True or False):", "ct_subject": "Martin Luther published a list of 95 theses as his critique of the church. There are approximately 182 days in 6 months.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" If", "0.9941918"], [" you", "0.9900519"], [" were", "0.782228"], [" to", "0.9999989"], [" take", "0.9984174"], [" a", "1.0"], [" piece", "1.0"], [" of", "1.0"], [" paper", "1.0"], [" and", "1.0"], [" write", "0.9999999"], [" on", "0.96801364"], [" it", "1.0"], [",", "0.76294607"], [" \"", "1.0"], ["I", "1.0"], [" am", "1.0"], [" a", "0.99999166"], [" man", "0.99996054"], [",\"", "0.87726223"], [" would", "1.0"], [" you", "1.0"], [" be", "0.9998369"], [" able", "1.0"], [" to", "1.0"], [" read", "0.9918252"], [" it", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Yes", "0.9999403"], [".", "0.7566768"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" If", "1.0"], [" you", "1.0"], [" were", "1.0"], [" to", "1.0"], [" take", "1.0"], [" a", "1.0"], [" piece", "1.0"], [" of", "1.0"], [" paper", "1.0"], [" and", "1.0"], [" write", "1.0"], [" on", "1.0"], [" it", "1.0"], [",", "1.0"], [" \"", "1.0"], ["I", "1.0"], [" am", "1.0"], [" a", "1.0"], [" man", "0.9999999"], [",\"", "1.0"], [" would", "1.0"], [" you", "1.0"], [" be", "1.0"], [" able", "1.0"], [" to", "1.0"], [" read", "1.0"], [" it", "1.0"], ["?", "1.0"]]}, "447": {"qid": "3d726babfd864d2fd6a3", "term": "Antarctic Peninsula", "description": "peninsula", "question": "Would a 75 degree Fahrenheit day be unusual on the Antarctic Peninsula? ", "answer": true, "facts": ["The warmest days on record in the Antarctic Peninsula are in the high 50's. ", "On average, the Antarctic Peninsula is between -4 and 36 degrees Fahrenheit. "], "decomposition": ["What is the average temperature in the Antarctic Peninsula?", "Does 75 degrees Fahrenheit fall outside the range of #1?"], "evidence": [[[["Antarctic Peninsula-21"]], ["operation"]], [[["Antarctic Peninsula-21"]], ["operation"]], [[["Antarctic Peninsula-21"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average temperature in the United States?\nFacts: The average temperature in the United States is about 60 degrees Fahrenheit. \nAnswer (True or False): True\nQuestion: What is the average temperature in the United States?\nFacts: The average temperature in the United States is about 60 degrees Fahrenheit. \n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a 75 degree Fahrenheit day be unusual on the Antarctic Peninsula? \nFacts: The warmest days on record in the Antarctic Peninsula are in the high 50's.  On average, the Antarctic Peninsula is between -4 and 36 degrees Fahrenheit. \nAnswer (True or False):", "ct_subject": "The warmest days on record in the Antarctic Peninsula are in the high 50's.  On average, the Antarctic Peninsula is between -4 and 36 degrees Fahrenheit. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99998474"], [" is", "1.0"], [" the", "1.0"], [" average", "0.99583215"], [" temperature", "1.0"], [" in", "0.99768317"], [" the", "1.0"], [" United", "0.9837801"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" temperature", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 60", "0.9580523"], [" degrees", "1.0"], [" Fahrenheit", "1.0"], [".", "1.0"], [" ", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" temperature", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "0.9902568"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" temperature", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 60", "1.0"], [" degrees", "1.0"], [" Fahrenheit", "1.0"], [".", "1.0"], [" ", "1.0"], ["\n", "1.0"]]}, "448": {"qid": "fa6c3c6b8471d6489f43", "term": "Napoleonic Wars", "description": "Series of early 19th century European wars", "question": "Did earth complete at least one orbit around the sun during the Napoleonic Wars?", "answer": true, "facts": ["Earth orbits around the Sun in 365 days.", "Napoleonic Wars lasted 12 years, 5 months and 4 weeks."], "decomposition": ["How long is the orbit of the earth around the sun?", "How long were the Napoleonic Wars?", "Is #2 greater than #1?"], "evidence": [[[["Year-1"]], [["Napoleonic Wars-1"]], ["operation"]], [[["Earth's orbit-1"]], [["Napoleonic Wars-1"]], ["operation"]], [[["Earth's orbit-1"]], [["Napoleonic Wars-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: John Adams was born on July 4, 1735.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: John Adams was born on July 4, 1735.\nAnswer (", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did earth complete at least one orbit around the sun during the Napoleonic Wars?\nFacts: Earth orbits around the Sun in 365 days. Napoleonic Wars lasted 12 years, 5 months and 4 weeks.\nAnswer (True or False):", "ct_subject": "Earth orbits around the Sun in 365 days. Napoleonic Wars lasted 12 years, 5 months and 4 weeks.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.5091042"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99495894"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9542681"], [" person", "0.92538077"], [" to", "1.0"], [" be", "0.99991095"], [" born", "0.99954945"], [" in", "1.0"], [" the", "1.0"], [" United", "0.9999964"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "0.6929136"], [" Adams", "0.99999833"], [" was", "1.0"], [" born", "0.99999976"], [" on", "0.9985044"], [" July", "0.9984902"], [" 4", "1.0"], [",", "1.0"], [" 17", "1.0"], ["35", "0.99999356"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "0.999689"], [" Adams", "1.0"], [" was", "1.0"], [" born", "1.0"], [" on", "1.0"], [" July", "1.0"], [" 4", "1.0"], [",", "1.0"], [" 17", "1.0"], ["35", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"]]}, "449": {"qid": "9d277acbb432c3ddab16", "term": "Morris County, New Jersey", "description": "County in New Jersey", "question": "Was Morris County named after a chief justice?", "answer": true, "facts": ["The Morris County was named after Colonel Lewis Morris.", "Colonel Lewis Morris was the chief justice of New York."], "decomposition": ["Who was Morris County, New Jersey named after?", "Did #1 serve as a chief justice?"], "evidence": [[[["Morris County, New Jersey-4"]], [["Lewis Morris (governor)-1"]]], [[["Morris County, New Jersey-4"]], [["Lewis Morris (governor)-1"]]], [[["Morris County, New Jersey-4"]], [["Lewis Morris (governor)-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the county in the state of New York that is named after the first president of the United States?\nFacts: The county in the state of New York that is named after the first president of the United States is the county of Washington.\nAnswer (True or False): True\nQuestion: What is the name of the county in the state of New York that is named after the first president of", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Morris County named after a chief justice?\nFacts: The Morris County was named after Colonel Lewis Morris. Colonel Lewis Morris was the chief justice of New York.\nAnswer (True or False):", "ct_subject": "The Morris County was named after Colonel Lewis Morris. Colonel Lewis Morris was the chief justice of New York.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.92723304"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" county", "0.99587184"], [" in", "0.9999976"], [" the", "0.6473812"], [" state", "0.99999845"], [" of", "1.0"], [" New", "0.9998325"], [" York", "0.9999999"], [" that", "0.99999857"], [" is", "0.9999989"], [" named", "0.9999912"], [" after", "1.0"], [" the", "0.69776016"], [" first", "0.9989411"], [" president", "0.99999726"], [" of", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" county", "1.0"], [" in", "0.8885413"], [" the", "0.9998586"], [" state", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" that", "1.0"], [" is", "1.0"], [" named", "1.0"], [" after", "1.0"], [" the", "1.0"], [" first", "1.0"], [" president", "1.0"], [" of", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "1.0"], [" the", "0.99992144"], [" county", "0.9990276"], [" of", "1.0"], [" Washington", "0.9915656"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" county", "1.0"], [" in", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" that", "1.0"], [" is", "1.0"], [" named", "1.0"], [" after", "1.0"], [" the", "1.0"], [" first", "1.0"], [" president", "1.0"], [" of", "1.0"]]}, "450": {"qid": "c096042cd5db3c7ed663", "term": "Dodo", "description": "Extinct large flightless pigeon from Mauritius", "question": "Would a Dodo hypothetically tower over Ma Petite?", "answer": true, "facts": ["A Dodo was an extinct bird that was over 3 feet tall.", "Ma Petite was a character on American Horror Story played by Jyoti Amge.", "Jyoti Amge is around 2 feet tall."], "decomposition": ["How tall were dodos?", "Who played the role of Ma Petite?", "How tall is #2?", "Is #1 greater than #3?"], "evidence": [[[["Dodo-2"]], [["Jyoti Amge-1"]], [["Jyoti Amge-2"]], ["operation"]], [[["Dodo-2"]], [["Jyoti Amge-3"]], [["Jyoti Amge-2"]], ["operation"]], [[["Dodo-2"]], [["Jyoti Amge-3"]], [["Jyoti Amge-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the tallest building in the world?\nFacts: The tallest building in the world is the Burj Khalifa in Dubai, United Arab Emirates. It is a skyscraper that is over 1,000 feet tall.\nAnswer (True or False): True\nQuestion: What is the tallest", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a Dodo hypothetically tower over Ma Petite?\nFacts: A Dodo was an extinct bird that was over 3 feet tall. Ma Petite was a character on American Horror Story played by Jyoti Amge. Jyoti Amge is around 2 feet tall.\nAnswer (True or False):", "ct_subject": "A Dodo was an extinct bird that was over 3 feet tall. Ma Petite was a character on American Horror Story played by Jyoti Amge. Jyoti Amge is around 2 feet tall.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99997437"], [" is", "1.0"], [" the", "1.0"], [" tallest", "0.9988003"], [" building", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" tallest", "1.0"], [" building", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" is", "1.0"], [" the", "1.0"], [" Bur", "1.0"], ["j", "1.0"], [" Khal", "1.0"], ["ifa", "1.0"], [" in", "1.0"], [" Dubai", "1.0"], [",", "0.98923045"], [" United", "0.99999905"], [" Arab", "1.0"], [" Emirates", "1.0"], [".", "1.0"], [" It", "0.9997788"], [" is", "1.0"], [" a", "0.9973131"], [" skysc", "1.0"], ["raper", "1.0"], [" that", "1.0"], [" is", "1.0"], [" over", "1.0"], [" 1", "1.0"], [",", "1.0"], ["000", "1.0"], [" feet", "1.0"], [" tall", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" tallest", "0.9999205"]]}, "451": {"qid": "0c79bdba76e80f70f6f4", "term": "World of Warcraft", "description": "video game by Blizzard Entertainment", "question": "Can you find Depala's race in World of Warcraft?", "answer": true, "facts": ["World of Warcraft has several races including humans, night elves, and dwarves.", "Depala is a character in the Magic the Gathering card game that is a dwarf."], "decomposition": ["What are the different races found in World of Warcraft?", "What race is Depala?", "Is #2 listed in #1?"], "evidence": [[[["World of Warcraft-8"]], [["Magic: The Gathering-2"], "no_evidence"], ["operation"]], [[["Gameplay of World of Warcraft-5"], "no_evidence"], [["Depala Vas-2"], "no_evidence"], ["no_evidence", "operation"]], [[["Gameplay of World of Warcraft-5"]], ["no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is there a way to find out if a person is a vampire?\nFacts: There are several ways to determine if a person is a vampire.\nAnswer (True or False): True\nQuestion: Is there a way to find out if a person is a vampire?\nFacts: There are several ways to determine if a person is a vampire", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can you find Depala's race in World of Warcraft?\nFacts: World of Warcraft has several races including humans, night elves, and dwarves. Depala is a character in the Magic the Gathering card game that is a dwarf.\nAnswer (True or False):", "ct_subject": "World of Warcraft has several races including humans, night elves, and dwarves. Depala is a character in the Magic the Gathering card game that is a dwarf.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9401231"], [" there", "0.90722245"], [" a", "1.0"], [" way", "0.86374074"], [" to", "1.0"], [" find", "0.2608716"], [" out", "1.0"], [" if", "0.79713625"], [" a", "1.0"], [" person", "1.0"], [" is", "1.0"], [" a", "0.9999963"], [" vampire", "0.6305961"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "0.9999999"], [" are", "0.45088825"], [" several", "1.0"], [" ways", "1.0"], [" to", "1.0"], [" determine", "0.5130662"], [" if", "1.0"], [" a", "0.9970499"], [" person", "1.0"], [" is", "1.0"], [" a", "1.0"], [" vampire", "1.0"], [".", "1.0"], ["\n", "0.098349124"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99999964"], [" there", "1.0"], [" a", "1.0"], [" way", "1.0"], [" to", "1.0"], [" find", "1.0"], [" out", "1.0"], [" if", "1.0"], [" a", "1.0"], [" person", "1.0"], [" is", "1.0"], [" a", "1.0"], [" vampire", "0.9999727"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "1.0"], [" are", "1.0"], [" several", "1.0"], [" ways", "1.0"], [" to", "1.0"], [" determine", "1.0"], [" if", "1.0"], [" a", "1.0"], [" person", "1.0"], [" is", "1.0"], [" a", "1.0"], [" vampire", "1.0"]]}, "452": {"qid": "ff93998995b7b2a8f125", "term": "Apollo 15", "description": "Fourth crewed mission to land on the Moon", "question": "Would a triples tandem bike support Apollo 15 crew?", "answer": true, "facts": ["A tandem bike has seats for more than one rider.", "A triples tandem bike has three seats and can support three riders.", "The crew of Apollo 15 consisted of three people."], "decomposition": ["How many people were on the Apollo 15 mission?", "How many people can ride a triple tandem bike?", "Is #2 at least #1?"], "evidence": [[[["Alfred Worden-1", "Apollo 15-2"]], [["Tandem bicycle-11"]], ["operation"]], [[["Apollo 15-8"]], [["Tandem bicycle-2"], "no_evidence"], ["operation"]], [[["Apollo 15-6"]], [["Tandem bicycle-11"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first man to walk on the moon?\nFacts: Neil Armstrong was the first man to walk on the moon.\nAnswer (True or False): True\nQuestion: What is the name of the first man to walk on the moon?\nFacts: Neil Armstrong was the first man to walk on the moon", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a triples tandem bike support Apollo 15 crew?\nFacts: A tandem bike has seats for more than one rider. A triples tandem bike has three seats and can support three riders. The crew of Apollo 15 consisted of three people.\nAnswer (True or False):", "ct_subject": "A tandem bike has seats for more than one rider. A triples tandem bike has three seats and can support three riders. The crew of Apollo 15 consisted of three people.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99992466"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9457167"], [" of", "1.0"], [" the", "1.0"], [" first", "0.8657136"], [" man", "0.80771434"], [" to", "1.0"], [" walk", "0.999689"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Neil", "0.99993074"], [" Armstrong", "1.0"], [" was", "0.3482329"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "0.99999845"], [" to", "1.0"], [" walk", "0.9762083"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Neil", "1.0"], [" Armstrong", "1.0"], [" was", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"]]}, "453": {"qid": "3d63f800400fd9505ea5", "term": "Library of Alexandria", "description": "one of the largest libraries in the ancient world, located in Alexandria, Egypt", "question": "Would Library of Alexandria need less shelf space than Library of Congress?", "answer": true, "facts": ["The Library of Alexandria was an ancient library that was destroyed.", "The Library of Alexandria is estimated to have had around 100,000 books.", "The Library of Congress has over 170 million items."], "decomposition": ["What is the number of books (and other materials) housed in the Library of Congress?", "What was the number of books (and other materials) housed in the Library of Alexandria?", "Is #2 less than #1?"], "evidence": [[[["Library of Congress-33"]], [["Library of Alexandria-45"]], [["Library of Alexandria-45", "Library of Congress-33"]]], [[["Library of Congress-33"]], [["Library of Alexandria-1"]], ["operation"]], [[["Library of Congress-33"]], [["Library of Alexandria-45"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the largest city in the world?\nFacts: The largest city in the world is Beijing, China. The largest city in the world is New York City, USA.\nAnswer (True or False): True\nQuestion: What is the largest city in the world?\nFacts: The largest city in the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Library of Alexandria need less shelf space than Library of Congress?\nFacts: The Library of Alexandria was an ancient library that was destroyed. The Library of Alexandria is estimated to have had around 100,000 books. The Library of Congress has over 170 million items.\nAnswer (True or False):", "ct_subject": "The Library of Alexandria was an ancient library that was destroyed. The Library of Alexandria is estimated to have had around 100,000 books. The Library of Congress has over 170 million items.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99995947"], [" is", "1.0"], [" the", "1.0"], [" largest", "0.9968297"], [" city", "0.9499363"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" largest", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" is", "1.0"], [" Beijing", "0.9980888"], [",", "0.35421664"], [" China", "1.0"], [".", "1.0"], [" The", "0.999087"], [" largest", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" is", "1.0"], [" New", "0.99701035"], [" York", "1.0"], [" City", "0.9971443"], [",", "0.9999963"], [" USA", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], ["?", "0.99941015"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" largest", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"]]}, "454": {"qid": "cc6e607ec6b68023be26", "term": "Auburn, New York", "description": "City in New York, United States", "question": "Can you fit every resident of Auburn, New York, in Tropicana Field?", "answer": true, "facts": ["The capacity of Tropicana Field is 36,973", "The population of Auburn, NY is 27,687"], "decomposition": ["What is the capacity of Tropicana Field?", "What is the population of Auburn, NY?", "Is #1 greater than #2?"], "evidence": [[[["Tropicana Field-31"]], [["Auburn, New York-1"]], [["Tropicana Field-31"], "operation"]], [[["Tropicana Field-31"]], [["Auburn, New York-1"]], ["operation"]], [[["Tropicana Field-31"]], [["Auburn, New York-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the largest number of people that can fit into a single room?\nFacts: The largest number of people that can fit into a single room is 2,000.\nAnswer (True or False): True\nQuestion: What is the largest number of people that can fit into a single room?\nFacts: The largest number of people that can fit into a single", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can you fit every resident of Auburn, New York, in Tropicana Field?\nFacts: The capacity of Tropicana Field is 36,973 The population of Auburn, NY is 27,687\nAnswer (True or False):", "ct_subject": "The capacity of Tropicana Field is 36,973 The population of Auburn, NY is 27,687", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.6315552"], [" is", "1.0"], [" the", "1.0"], [" largest", "0.99957055"], [" number", "0.9998847"], [" of", "1.0"], [" people", "1.0"], [" that", "0.9999976"], [" can", "1.0"], [" fit", "1.0"], [" into", "0.72091806"], [" a", "0.99989486"], [" single", "0.99984205"], [" room", "0.99999845"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" largest", "1.0"], [" number", "0.95736945"], [" of", "1.0"], [" people", "1.0"], [" that", "1.0"], [" can", "1.0"], [" fit", "1.0"], [" into", "1.0"], [" a", "1.0"], [" single", "1.0"], [" room", "1.0"], [" is", "1.0"], [" 2", "0.9803987"], [",", "1.0"], ["000", "0.9999989"], [".", "0.5246972"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" that", "1.0"], [" can", "1.0"], [" fit", "1.0"], [" into", "1.0"], [" a", "1.0"], [" single", "1.0"], [" room", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" largest", "1.0"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" that", "1.0"], [" can", "1.0"], [" fit", "1.0"], [" into", "1.0"], [" a", "1.0"], [" single", "1.0"]]}, "455": {"qid": "b09deccfa0bca4973aff", "term": "Albanian Declaration of Independence", "description": "declaration of independence", "question": "Can an Arvanite Greek understand some of the Albanian Declaration of Independence?", "answer": true, "facts": ["The Albanian Declaration of Independence is written in Albanian, Gheg, Tosk, and Ottoman Turkish.", "The Arvanite Greek's are a major Tosk speaking group of southern Albania."], "decomposition": ["What languages is the Albanian Declaration of Independence written in?", "What language do Arvanite Greeks speak?", "Is #2 in #1?"], "evidence": [[["no_evidence"], [["Arvanites-1"]], ["operation"]], [[["Albanian Declaration of Independence-3"]], [["Arvanites-1"]], ["operation"]], [[["Albanian Declaration of Independence-1", "Albanian Declaration of Independence-7", "Albanian language-1"], "no_evidence"], [["Arvanites-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city of Rome?\nFacts: Rome is the capital of the Roman Empire.\nAnswer (True or False): True\nQuestion: What is the name of the city of Rome?\nFacts: Rome is the capital of the Roman Empire.\nAnswer (", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can an Arvanite Greek understand some of the Albanian Declaration of Independence?\nFacts: The Albanian Declaration of Independence is written in Albanian, Gheg, Tosk, and Ottoman Turkish. The Arvanite Greek's are a major Tosk speaking group of southern Albania.\nAnswer (True or False):", "ct_subject": "The Albanian Declaration of Independence is written in Albanian, Gheg, Tosk, and Ottoman Turkish. The Arvanite Greek's are a major Tosk speaking group of southern Albania.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.8401494"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999918"], [" of", "1.0"], [" the", "1.0"], [" city", "0.7242276"], [" of", "0.99999845"], [" Rome", "0.9993074"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "0.9998925"], [" is", "0.9999987"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"]]}, "456": {"qid": "73ca2ef1da65b2a2ebe6", "term": "Darth Vader", "description": "fictional character in the Star Wars franchise", "question": "Was Darth Vader monogamous?", "answer": true, "facts": ["Monogamy refers to when a person is married or committed to only one person.", "Darth Vader was only married to Padme Amidala and had two children with her."], "decomposition": ["If someone is monogamous, how many people are they committed to?", "How many people was Darth Vader committed to?", "Does #2 equal #1?"], "evidence": [[[["Monogamy-1"]], [["Padm\u00e9 Amidala-1"]], ["operation"]], [[["Monogamy-1"]], [["Padm\u00e9 Amidala-1"]], ["operation"]], [[["Monogamy-1"]], [["Darth Vader-2"]], [["Darth Vader-2"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Nazis use the gas chambers to kill Jews?\nFacts: The Nazis did use the gas chambers to kill Jews.\nAnswer (True or False): True\nQuestion: Did the Nazis use the gas chambers to kill Jews?\nFacts: The Nazis did use the gas chambers to kill Jews.\nAnswer (True or False): True\nQuestion: Did the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Darth Vader monogamous?\nFacts: Monogamy refers to when a person is married or committed to only one person. Darth Vader was only married to Padme Amidala and had two children with her.\nAnswer (True or False):", "ct_subject": "Monogamy refers to when a person is married or committed to only one person. Darth Vader was only married to Padme Amidala and had two children with her.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99471176"], [" the", "1.0"], [" Nazis", "0.37880653"], [" use", "0.9999964"], [" the", "1.0"], [" gas", "0.9999925"], [" chambers", "1.0"], [" to", "0.97706884"], [" kill", "1.0"], [" Jews", "0.999982"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Nazis", "0.99999595"], [" did", "0.24356653"], [" use", "0.48213956"], [" the", "0.7892794"], [" gas", "1.0"], [" chambers", "1.0"], [" to", "1.0"], [" kill", "1.0"], [" Jews", "1.0"], [".", "1.0"], ["\n", "0.9948502"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.999995"], [" the", "1.0"], [" Nazis", "1.0"], [" use", "1.0"], [" the", "1.0"], [" gas", "1.0"], [" chambers", "1.0"], [" to", "1.0"], [" kill", "1.0"], [" Jews", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Nazis", "1.0"], [" did", "1.0"], [" use", "1.0"], [" the", "1.0"], [" gas", "1.0"], [" chambers", "1.0"], [" to", "1.0"], [" kill", "1.0"], [" Jews", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999999"], [" the", "1.0"]]}, "457": {"qid": "5633fa480c01d39119ee", "term": "B", "description": "letter in the Latin alphabet", "question": "Is B's place in alphabet same as Prince Harry's birth order?", "answer": true, "facts": ["B is the second letter of the alphabet.", "Prince Harry was the second son of Charles, Prince of Wales and Diana, Princess of Wales."], "decomposition": ["What position in the alphabet does \"B\" hold?", "What is the nominal number associated with #1?", "Does Prince Harry have exactly #2 minus 1 older siblings?"], "evidence": [[[["B-1"]], [["Nominal number-1", "Nominal number-2"], "operation"], [["Prince Harry, Duke of Sussex-1", "Prince Harry, Duke of Sussex-2"], "operation"]], [[["B-1"]], [["Ordinal numeral-9"]], [["Prince Harry, Duke of Sussex-4"]]], [[["B-1"]], ["operation"], [["Prince Harry, Duke of Sussex-4"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the first president of the United States, George Washington, spelled with an \"e\" or an \"i\"?\nFacts: The name of the first president of the United States, George Washington, was spelled with an \"e\" or an \"i\".\nAnswer (True or False): True\nQuestion: Is the name of the first president", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is B's place in alphabet same as Prince Harry's birth order?\nFacts: B is the second letter of the alphabet. Prince Harry was the second son of Charles, Prince of Wales and Diana, Princess of Wales.\nAnswer (True or False):", "ct_subject": "B is the second letter of the alphabet. Prince Harry was the second son of Charles, Prince of Wales and Diana, Princess of Wales.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9896052"], [" the", "0.9536575"], [" name", "0.99987566"], [" of", "1.0"], [" the", "1.0"], [" first", "0.8897156"], [" president", "0.0069583138"], [" of", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [",", "0.9999945"], [" George", "1.0"], [" Washington", "1.0"], [",", "1.0"], [" spelled", "0.90539277"], [" with", "0.9999999"], [" an", "0.9986319"], [" \"", "1.0"], ["e", "0.99973494"], ["\"", "0.770934"], [" or", "0.99999964"], [" an", "0.99011785"], [" \"", "1.0"], ["i", "0.9999993"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" name", "0.38399246"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" president", "1.0"], [" of", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [",", "0.99998975"], [" George", "1.0"], [" Washington", "1.0"], [",", "1.0"], [" was", "0.03695637"], [" spelled", "1.0"], [" with", "1.0"], [" an", "1.0"], [" \"", "1.0"], ["e", "1.0"], ["\"", "0.9488595"], [" or", "0.999997"], [" an", "1.0"], [" \"", "1.0"], ["i", "1.0"], ["\".", "0.98834187"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99999845"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" president", "1.0"]]}, "458": {"qid": "71711173efbb350885b3", "term": "New Brunswick", "description": "province in Canada", "question": "Can Burundi's communicate with citizens of New Brunswick?", "answer": true, "facts": ["French and English are the official languages of New Brunswick.", "French is one of the official languages of Burundi."], "decomposition": ["What are the official languages of New Brunswick, Canada?", "What are the official languages of Burundi?", "Are some elements of #2 also in #1?"], "evidence": [[[["New Brunswick-36"]], [["Burundi-87"]], ["operation"]], [[["Official language-14"]], [["Burundi-6"]], ["operation"]], [[["Languages of Canada-58"], "operation"], [["Languages of Burundi-1"], "operation"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Can a person be born blind?\nFacts: A person can be born blind, but it is not possible to be born blind in the same way that a person can be born blind in the United States.\nAnswer (True or False): True\nQuestion: Can a person be born deaf?\nFacts: A person can be born deaf, but it is not possible to be born deaf", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can Burundi's communicate with citizens of New Brunswick?\nFacts: French and English are the official languages of New Brunswick. French is one of the official languages of Burundi.\nAnswer (True or False):", "ct_subject": "French and English are the official languages of New Brunswick. French is one of the official languages of Burundi.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Can", "0.85790783"], [" a", "0.86806047"], [" person", "0.9996132"], [" be", "0.9853601"], [" born", "0.99935"], [" blind", "0.9954085"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999008"], [" person", "1.0"], [" can", "1.0"], [" be", "1.0"], [" born", "1.0"], [" blind", "1.0"], [",", "0.9257742"], [" but", "1.0"], [" it", "0.17680658"], [" is", "1.0"], [" not", "1.0"], [" possible", "0.99110234"], [" to", "0.99995315"], [" be", "0.9998982"], [" born", "1.0"], [" blind", "1.0"], [" in", "0.5855235"], [" the", "1.0"], [" same", "0.8991279"], [" way", "1.0"], [" that", "0.9936898"], [" a", "1.0"], [" person", "1.0"], [" can", "1.0"], [" be", "1.0"], [" born", "1.0"], [" blind", "0.9999889"], [" in", "1.0"], [" the", "0.9999989"], [" United", "0.98517066"], [" States", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Can", "1.0"], [" a", "1.0"], [" person", "1.0"], [" be", "1.0"], [" born", "1.0"], [" deaf", "0.99987566"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" person", "1.0"], [" can", "1.0"], [" be", "1.0"], [" born", "1.0"], [" deaf", "1.0"], [",", "1.0"], [" but", "1.0"], [" it", "1.0"], [" is", "1.0"], [" not", "1.0"], [" possible", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" deaf", "1.0"]]}, "459": {"qid": "9e78c01675a198781cca", "term": "Michael Bloomberg", "description": "American billionaire businessman and politician, former mayor of New York City", "question": "Can Michael Bloomberg fund the debt of Micronesia for a decade?", "answer": true, "facts": ["Michael Bloomberg is worth an estimated 60 billion dollars as of 2020.", "Micronesia has annual expenses of nearly 200 million dollars."], "decomposition": ["What is Micheal Bloomberg's worth?", "What is the annual expense for Micronesia?", "Is #1 greater than #2?"], "evidence": [[[["Michael Bloomberg-2"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Michael Bloomberg-2"]], [["Micronesia-31"], "no_evidence"], ["no_evidence", "operation"]], [[["Michael Bloomberg-2"]], [["Economy of the Federated States of Micronesia-7"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average IQ of the world's population?\nFacts: The average IQ of the world's population is 100.\nAnswer (True or False): True\nQuestion: What is the average height of the world's population?\nFacts: The average height of the world's population is 5'9\".\nAnswer (True or False): True\nQuestion: What", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can Michael Bloomberg fund the debt of Micronesia for a decade?\nFacts: Michael Bloomberg is worth an estimated 60 billion dollars as of 2020. Micronesia has annual expenses of nearly 200 million dollars.\nAnswer (True or False):", "ct_subject": "Michael Bloomberg is worth an estimated 60 billion dollars as of 2020. Micronesia has annual expenses of nearly 200 million dollars.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9996024"], [" is", "1.0"], [" the", "1.0"], [" average", "0.9861394"], [" IQ", "0.12332642"], [" of", "1.0"], [" the", "0.99999905"], [" world", "0.9999932"], ["'s", "0.99999976"], [" population", "0.8845996"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" IQ", "1.0"], [" of", "1.0"], [" the", "1.0"], [" world", "1.0"], ["'s", "1.0"], [" population", "1.0"], [" is", "1.0"], [" 100", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" height", "0.9999999"], [" of", "1.0"], [" the", "1.0"], [" world", "1.0"], ["'s", "1.0"], [" population", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" height", "1.0"], [" of", "1.0"], [" the", "1.0"], [" world", "1.0"], ["'s", "1.0"], [" population", "1.0"], [" is", "1.0"], [" 5", "0.9999913"], ["'", "0.99945"], ["9", "0.94761056"], ["\".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"]]}, "460": {"qid": "c69b07a0cff831ce6791", "term": "Reiki", "description": "Pseudoscientific healing technique", "question": "Would a physician be unlikely to recommend Reiki?", "answer": true, "facts": ["Physicians typically only recommend treatments that have a significant amount of clinical evidence behind them.", "Reiki has no clinical evidence of effectiveness, and there is no scientific basis for its utility."], "decomposition": ["What is the basis for physician's recommendations?", "Does Reiki have #1?"], "evidence": [[[["Evidence-based medicine-1"]], [["Reiki-1", "Reiki-2"], "operation"]], [[["Physician-1"]], [["Reiki-2"], "no_evidence", "operation"]], [[["Physician-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"soul\" and a \"spirit\"?\nFacts: A soul is a part of the human body that is separate from the body. A spirit is a part of the human body that is not separate from the body.\nAnswer (True or False): True\nQuestion: What is the difference between a \"s", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a physician be unlikely to recommend Reiki?\nFacts: Physicians typically only recommend treatments that have a significant amount of clinical evidence behind them. Reiki has no clinical evidence of effectiveness, and there is no scientific basis for its utility.\nAnswer (True or False):", "ct_subject": "Physicians typically only recommend treatments that have a significant amount of clinical evidence behind them. Reiki has no clinical evidence of effectiveness, and there is no scientific basis for its utility.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99687344"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9999348"], [" between", "1.0"], [" a", "0.99999344"], [" \"", "1.0"], ["s", "0.9826896"], ["oul", "1.0"], ["\"", "0.9989673"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["spirit", "0.97813934"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9897636"], [" soul", "1.0"], [" is", "1.0"], [" a", "0.9999049"], [" part", "1.0"], [" of", "1.0"], [" the", "0.9922368"], [" human", "0.9999043"], [" body", "1.0"], [" that", "0.99082947"], [" is", "1.0"], [" separate", "0.9965898"], [" from", "1.0"], [" the", "1.0"], [" body", "0.9999708"], [".", "1.0"], [" A", "1.0"], [" spirit", "1.0"], [" is", "1.0"], [" a", "1.0"], [" part", "1.0"], [" of", "1.0"], [" the", "1.0"], [" human", "1.0"], [" body", "0.9997528"], [" that", "1.0"], [" is", "1.0"], [" not", "0.997488"], [" separate", "1.0"], [" from", "1.0"], [" the", "1.0"], [" body", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"]]}, "461": {"qid": "2251a4d6e090572a63d7", "term": "Bengal fox", "description": "species of mammal", "question": "Could Ryan Crouser throw a bengal fox with ease?", "answer": true, "facts": ["Ryan Crouser is a professional shot putter who won the gold medal at the 2016 Olympics.", "The men's shot weighs 16.01 pounds.", "The typical weight of a Bengal fox is between 5 to 9 pounds."], "decomposition": ["What sport is Ryan Crouser a professional in?", "How much does the equipment for #1 weigh?", "How much does a Bengal fox weigh?", "Is #2 greater than #3?"], "evidence": [[[["Ryan Crouser-1"]], [["Shot put-11"]], [["Bengal fox-2"]], ["operation"]], [[["Ryan Crouser-1"]], [["Shot put-11"]], [["Bengal fox-2"]], ["operation"]], [[["Ryan Crouser-1"]], [["Shot put-11"], "no_evidence"], [["Bengal fox-2"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human?\nFacts: The average weight of a human is about 200 pounds.\nAnswer (True or False): True\nQuestion: What is the average weight of a Bengal tiger?\nFacts: The average weight of a Bengal tiger is between", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could Ryan Crouser throw a bengal fox with ease?\nFacts: Ryan Crouser is a professional shot putter who won the gold medal at the 2016 Olympics. The men's shot weighs 16.01 pounds. The typical weight of a Bengal fox is between 5 to 9 pounds.\nAnswer (True or False):", "ct_subject": "Ryan Crouser is a professional shot putter who won the gold medal at the 2016 Olympics. The men's shot weighs 16.01 pounds. The typical weight of a Bengal fox is between 5 to 9 pounds.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999243"], [" is", "1.0"], [" the", "1.0"], [" average", "0.99987423"], [" weight", "0.9857453"], [" of", "1.0"], [" a", "1.0"], [" human", "0.9992685"], ["?", "0.99989164"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" weight", "0.9994357"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" is", "1.0"], [" about", "0.9893544"], [" 200", "0.9821212"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "1.0"], [" of", "1.0"], [" a", "1.0"], [" Bengal", "0.055683073"], [" tiger", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" weight", "1.0"], [" of", "1.0"], [" a", "1.0"], [" Bengal", "1.0"], [" tiger", "1.0"], [" is", "1.0"], [" between", "1.0"]]}, "462": {"qid": "c2c6e32ccdd81e5df7f6", "term": "Paralympic Games", "description": "Major international sport event for people with disabilities", "question": "Would Jimmy Vee be eligible to compete in the Paralympic Games?", "answer": true, "facts": ["Jimmy Vee is a dwarf.", "Dwarfism is defined as someone who is medically short-statured.", "Short stature due to a bone deficiency is one of the categories for paralympic athletes."], "decomposition": ["What disability does Jimmy Vee suffer from?", "What is the medical definition of #1?", "Is #2 one of the categories for the paralympic athletes?"], "evidence": [[[["Jimmy Vee-5"]], [["Dwarfism-1"]], [["Paralympic Games-1"]]], [[["Jimmy Vee-5"]], [["Dwarfism-2"]], [["Paralympic Games-42"], "operation"]], [[["Jimmy Vee-5"]], [["Dwarfism-1"]], [["Paralympic Games-4"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: The first person to be born in the United States was a woman named Mary Ann Cotton.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Jimmy Vee be eligible to compete in the Paralympic Games?\nFacts: Jimmy Vee is a dwarf. Dwarfism is defined as someone who is medically short-statured. Short stature due to a bone deficiency is one of the categories for paralympic athletes.\nAnswer (True or False):", "ct_subject": "Jimmy Vee is a dwarf. Dwarfism is defined as someone who is medically short-statured. Short stature due to a bone deficiency is one of the categories for paralympic athletes.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "0.98593336"], [" of", "1.0"], [" the", "1.0"], [" first", "0.57829016"], [" person", "0.99913293"], [" to", "1.0"], [" be", "0.99999845"], [" born", "0.99968374"], [" in", "0.99998605"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9833973"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" a", "0.98784596"], [" woman", "1.0"], [" named", "1.0"], [" Mary", "0.9767646"], [" Ann", "0.99999833"], [" Cotton", "0.9999994"], [".", "1.0"], ["\n", "0.9999999"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"]]}, "463": {"qid": "826cdebd34c07f92fca5", "term": "Tsar", "description": "title given to a male monarch in Russia, Bulgaria and Serbia", "question": "Would a duke hypothetically be subservient to a Tsar?", "answer": true, "facts": ["The Tsar was the highest ruler in several eastern countries.", "A duke was a title given to important european nobles.", "Dukes ranked below princes, kings, and queens.", "Tsars were the equivalents of English Kings."], "decomposition": ["What is the equivalent of a Tsar in English hierarchy/royalty?", "Do dukes rank below #1?"], "evidence": [[[["Tsar-1"]], [["Duke-1"]]], [[["Tsar-1"]], [["Duke-1"], "operation"]], [[["Tsar-6"]], [["Duke-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a king and a duke?\nFacts: A king was the highest ranking person in a country. A duke was the equivalent of an English King.\nAnswer (True or False): True\nQuestion: What is the difference between a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a duke hypothetically be subservient to a Tsar?\nFacts: The Tsar was the highest ruler in several eastern countries. A duke was a title given to important european nobles. Dukes ranked below princes, kings, and queens. Tsars were the equivalents of English Kings.\nAnswer (True or False):", "ct_subject": "The Tsar was the highest ruler in several eastern countries. A duke was a title given to important european nobles. Dukes ranked below princes, kings, and queens. Tsars were the equivalents of English Kings.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99997854"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" king", "0.5796189"], [" and", "1.0"], [" a", "1.0"], [" d", "1.0"], ["uke", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999958"], [" king", "1.0"], [" was", "0.621785"], [" the", "0.99999714"], [" highest", "1.0"], [" ranking", "0.9994832"], [" person", "0.6939059"], [" in", "1.0"], [" a", "1.0"], [" country", "1.0"], [".", "1.0"], [" A", "1.0"], [" d", "1.0"], ["uke", "1.0"], [" was", "1.0"], [" the", "0.9957099"], [" equivalent", "1.0"], [" of", "1.0"], [" an", "0.97446364"], [" English", "1.0"], [" King", "0.9999999"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"]]}, "464": {"qid": "bda0fd7cd1f5a90b72ec", "term": "Chinese calendar", "description": "Lunisolar calendar from China", "question": "Are quadrupeds represented on Chinese calendar?", "answer": true, "facts": ["Quadrupeds are animals that walk on four legs.", "The Chinese calendar has a number of symbols including monkeys, goats, and tigers.", "Tigers have four paws and balance themselves by walking on their toes."], "decomposition": ["What are the symbols of the Chinese calendar?", "What is the defining characteristic of quadrapeds?", "Do any of the animals in #1 have #2?"], "evidence": [[[["Chinese calendar-1", "Earthly Branches-6"]], [["Quadrupedalism-1"]], [["Ox-10"], "operation"]], [[["Chinese zodiac-20"]], [["Quadrupedalism-1"]], ["operation"]], [[["Chinese zodiac-5"]], [["Quadrupedalism-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: The first person to be born in the United States was a woman named Mary Ann Cotton.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the United States?\nF", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are quadrupeds represented on Chinese calendar?\nFacts: Quadrupeds are animals that walk on four legs. The Chinese calendar has a number of symbols including monkeys, goats, and tigers. Tigers have four paws and balance themselves by walking on their toes.\nAnswer (True or False):", "ct_subject": "Quadrupeds are animals that walk on four legs. The Chinese calendar has a number of symbols including monkeys, goats, and tigers. Tigers have four paws and balance themselves by walking on their toes.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9627617"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999324"], [" of", "1.0"], [" the", "1.0"], [" first", "0.5758638"], [" person", "0.87668985"], [" to", "1.0"], [" be", "0.9981382"], [" born", "0.9999472"], [" in", "1.0"], [" the", "1.0"], [" United", "0.99999917"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.359883"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" a", "0.9963194"], [" woman", "0.9999999"], [" named", "1.0"], [" Mary", "0.41569176"], [" Ann", "0.9999844"], [" Cotton", "1.0"], [".", "1.0"], ["\n", "0.9999759"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"]]}, "465": {"qid": "089b0eb6cdf0fe53a863", "term": "Scottish people", "description": "ethnic inhabitants of Scotland", "question": "Are Scottish people descended from Mary, Queen of Scots part French?", "answer": true, "facts": ["Mary, Queen of Scots was Queen of Scotland in the 1500s.", "Mary, Queen of Scots was the daughter of Mary of Guise.", "Mary of Guise was born to a French nobleman, and her mother was French as well."], "decomposition": ["Who was the mother of Mary, Queen of Scots?", "Who were the parents of #1?", "Were #2 French?"], "evidence": [[[["Mary of Guise-1"]], [["Antoinette de Bourbon-1", "Claude, Duke of Guise-1"]], ["operation"]], [[["Mary, Queen of Scots-5"]], [["Mary of Guise-2"]], [["Claude, Duke of Guise-1"], "operation"]], [[["Mary, Queen of Scots-5"]], [["Mary of Guise-2"]], [["Antoinette de Bourbon-1", "Claude, Duke of Guise-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there more people in the world today that have the same DNA as the first people to live on Earth?\nFacts: There are more people alive today than there were at the time of the first humans to live on Earth.\nAnswer (True or False): True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are Scottish people descended from Mary, Queen of Scots part French?\nFacts: Mary, Queen of Scots was Queen of Scotland in the 1500s. Mary, Queen of Scots was the daughter of Mary of Guise. Mary of Guise was born to a French nobleman, and her mother was French as well.\nAnswer (True or False):", "ct_subject": "Mary, Queen of Scots was Queen of Scotland in the 1500s. Mary, Queen of Scots was the daughter of Mary of Guise. Mary of Guise was born to a French nobleman, and her mother was French as well.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "0.9995958"], [" more", "0.16463605"], [" people", "1.0"], [" in", "0.9998871"], [" the", "1.0"], [" world", "0.99999976"], [" today", "1.0"], [" that", "0.9995944"], [" have", "0.998497"], [" the", "0.788219"], [" same", "1.0"], [" DNA", "0.94475234"], [" as", "1.0"], [" the", "1.0"], [" first", "0.9755692"], [" people", "0.9958966"], [" to", "0.9999471"], [" live", "0.93349475"], [" on", "0.9999999"], [" Earth", "0.9999682"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "0.9993311"], [" are", "1.0"], [" more", "1.0"], [" people", "1.0"], [" alive", "0.9997906"], [" today", "1.0"], [" than", "0.98616284"], [" there", "1.0"], [" were", "1.0"], [" at", "0.9997445"], [" the", "1.0"], [" time", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" humans", "0.72934115"], [" to", "1.0"], [" live", "1.0"], [" on", "1.0"], [" Earth", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"]]}, "466": {"qid": "140258f12a2ed0221ea5", "term": "Eleventh grade", "description": "educational year", "question": "Would a student in eleventh grade be unable to run for president of the United States?", "answer": true, "facts": ["Students in the eleventh grade are typically 16\u201317 years of age.", "To serve as president, one must be at least 35 years old."], "decomposition": ["What is the minimum age one must be to run for president?", "Would a typical eleventh grader be way younger than #1?"], "evidence": [[[["President of the United States-5"]], [["Eleventh grade-1"], "operation"]], [[["Age of candidacy-60"]], [["Education in Alberta-13"], "operation"]], [[["President of the United States-5"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human being?\nFacts: The average human being weighs approximately 100 pounds.\nAnswer (True or False): True\nQuestion: What is the average height of a human being?\nFacts: The average human being is approximately 5 feet, 8 inches tall.\nAnswer (True or False): True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a student in eleventh grade be unable to run for president of the United States?\nFacts: Students in the eleventh grade are typically 16\u201317 years of age. To serve as president, one must be at least 35 years old.\nAnswer (True or False):", "ct_subject": "Students in the eleventh grade are typically 16\u201317 years of age. To serve as president, one must be at least 35 years old.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999356"], [" is", "1.0"], [" the", "1.0"], [" average", "0.9888198"], [" weight", "0.99993837"], [" of", "1.0"], [" a", "0.9999702"], [" human", "0.9995258"], [" being", "0.99290174"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.9883718"], [" being", "0.99999976"], [" weighs", "0.9999939"], [" approximately", "0.96999717"], [" 100", "0.9975553"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" height", "0.99999917"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.9999646"], [" being", "1.0"], [" is", "1.0"], [" approximately", "1.0"], [" 5", "0.9994734"], [" feet", "0.99944013"], [",", "1.0"], [" 8", "0.9986671"], [" inches", "1.0"], [" tall", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"]]}, "467": {"qid": "c1fe5e55bb99845e364a", "term": "Sternum", "description": "flat bone in the middle front part of the rib cage", "question": "Is sternum connected to any of the bones mentioned in James Weldon Johnson's Dem Bones?", "answer": true, "facts": ["Dem Bones is a spiritual song composed by James Weldon Johnson.", "The lyrics to Dem Bones mention the hip bone, back bone, and knee bone among others.", "The back bone is the spine, which is connected to the first 7 rib sets.", "The sternum, or breastbone, is a flat bone at the front center of the chest.", "The sternum and ribs are connected and make up the ribcage."], "decomposition": ["Which bones were mentioned in the spiritual song Dem Bones by James Weldon Johnson?", "Is the sternum connected to any of #1?"], "evidence": [[[["Dem Bones-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Dem Bones-1"], "no_evidence"], [["Sternum-5"], "operation"]], [[["Dem Bones-3"], "no_evidence"], [["Sternum-2"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the human body a living organism?\nFacts: The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is sternum connected to any of the bones mentioned in James Weldon Johnson's Dem Bones?\nFacts: Dem Bones is a spiritual song composed by James Weldon Johnson. The lyrics to Dem Bones mention the hip bone, back bone, and knee bone among others. The back bone is the spine, which is connected to the first 7 rib sets. The sternum, or breastbone, is a flat bone at the front center of the chest. The sternum and ribs are connected and make up the ribcage.\nAnswer (True or False):", "ct_subject": "Dem Bones is a spiritual song composed by James Weldon Johnson. The lyrics to Dem Bones mention the hip bone, back bone, and knee bone among others. The back bone is the spine, which is connected to the first 7 rib sets. The sternum, or breastbone, is a flat bone at the front center of the chest. The sternum and ribs are connected and make up the ribcage.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99999976"], [" the", "1.0"], [" human", "0.13883679"], [" body", "0.93481755"], [" a", "0.6246514"], [" living", "0.6792663"], [" organism", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"]]}, "468": {"qid": "76821a3d4561f872b607", "term": "Futurama", "description": "American animated sitcom for the Fox Broadcasting Company and Comedy Central", "question": "Has the creator of Futurama lived in multiple centuries?", "answer": true, "facts": ["The creator of Futurama is Matt Groening.", "Matt Groening was born in 1954.", "The 20th (twentieth) century was a century that began on January 1, 1901 and ended on December 31, 2000.", "The 21st (twenty-first) century began on January 1, 2001, and will end on December 31, 2100."], "decomposition": ["Who is the creator of Futurama?", "How old is #1?", "What is 2020 minus #2?", "When did the most recent new century begin?", "Is #4 between #3 and 2020?"], "evidence": [[[["Futurama-1"]], [["Matt Groening-1"]], ["operation"], [["2000-1"]], ["operation"]], [[["Futurama-1"]], [["Matt Groening-1"]], ["operation"], [["21st century-1"]], ["operation"]], [[["Futurama-1"]], [["Matt Groening-1"]], ["operation"], [["21st century-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the universe infinite?\nFacts: The universe is infinite in size. The universe is infinite in time. The universe is infinite in energy. The universe", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Has the creator of Futurama lived in multiple centuries?\nFacts: The creator of Futurama is Matt Groening. Matt Groening was born in 1954. The 20th (twentieth) century was a century that began on January 1, 1901 and ended on December 31, 2000. The 21st (twenty-first) century began on January 1, 2001, and will end on December 31, 2100.\nAnswer (True or False):", "ct_subject": "The creator of Futurama is Matt Groening. Matt Groening was born in 1954. The 20th (twentieth) century was a century that began on January 1, 1901 and ended on December 31, 2000. The 21st (twenty-first) century began on January 1, 2001, and will end on December 31, 2100.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.7390537"], [" the", "0.9998338"], [" universe", "0.87361383"], [" infinite", "0.7265244"], ["?", "0.99999833"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" universe", "1.0"], [" is", "1.0"], [" infinite", "1.0"], [" in", "0.9988543"], [" size", "1.0"], [".", "1.0"], [" The", "1.0"], [" universe", "1.0"], [" is", "1.0"], [" infinite", "0.99999976"], [" in", "1.0"], [" time", "1.0"], [".", "1.0"], [" The", "1.0"], [" universe", "1.0"], [" is", "1.0"], [" infinite", "1.0"], [" in", "1.0"], [" energy", "0.9999974"], [".", "1.0"], [" The", "1.0"], [" universe", "1.0"]]}, "469": {"qid": "e0e27f835ab064c59bb4", "term": "French Riviera", "description": "Riviera", "question": "Did Pink Floyd have a song about the French Riviera?", "answer": true, "facts": ["Pink Floyd included the song San Tropez on the album Meddle", "San Tropez, also known as Saint Tropez, is a town located on the French Riviera"], "decomposition": ["What songs by Pink Floyd are named after towns in France?", "What are the names of the major towns in the French Riviera?", "Are any towns in #1 also in #2?"], "evidence": [[[["San Tropez (song)-1"]], [["French Riviera-1"]], ["operation"]], [[["San Tropez (song)-1"]], [["Saint-Tropez-1"]], ["operation"]], [[["San Tropez (song)-1"]], [["French Riviera-4"]], [["Saint-Tropez-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Beatles have a song about the French Riviera?\nFacts: The Beatles included the song Strawberry Fields in their album Sgt. Pepper's Lonely Hearts Club Band, also known as The Beatles' White Album, is a collection of songs written by the Beatles and released in 1967\nAnswer (True or False): True\nQuestion: Did the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Pink Floyd have a song about the French Riviera?\nFacts: Pink Floyd included the song San Tropez on the album Meddle San Tropez, also known as Saint Tropez, is a town located on the French Riviera\nAnswer (True or False):", "ct_subject": "Pink Floyd included the song San Tropez on the album Meddle San Tropez, also known as Saint Tropez, is a town located on the French Riviera", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99635494"], [" the", "1.0"], [" Beatles", "1.0"], [" have", "1.0"], [" a", "1.0"], [" song", "1.0"], [" about", "1.0"], [" the", "1.0"], [" French", "1.0"], [" Riv", "1.0"], ["iera", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Beatles", "1.0"], [" included", "1.0"], [" the", "1.0"], [" song", "1.0"], [" Strawberry", "0.9956092"], [" Fields", "1.0"], [" in", "0.9996784"], [" their", "0.9344148"], [" album", "1.0"], [" Sgt", "0.9838372"], [".", "1.0"], [" Pepper", "1.0"], ["'s", "1.0"], [" Lonely", "1.0"], [" Hearts", "1.0"], [" Club", "1.0"], [" Band", "1.0"], [",", "0.7811128"], [" also", "1.0"], [" known", "1.0"], [" as", "1.0"], [" The", "0.9989502"], [" Beatles", "0.9993911"], ["'", "0.99999976"], [" White", "0.99999964"], [" Album", "1.0"], [",", "0.9999957"], [" is", "1.0"], [" a", "1.0"], [" collection", "0.7887972"], [" of", "1.0"], [" songs", "1.0"], [" written", "0.9998267"], [" by", "1.0"], [" the", "1.0"], [" Beatles", "1.0"], [" and", "0.9999764"], [" released", "0.91709435"], [" in", "1.0"], [" 1967", "0.1323217"], ["\n", "0.92058915"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"]]}, "470": {"qid": "506c7e618094c78b7614", "term": "Microsoft Excel", "description": "Spreadsheet editor, part of Microsoft Office", "question": "Is electricity necessary to balance an account in Microsoft Excel?", "answer": true, "facts": ["Microsoft Excel is a computer program", "Computers require a power source"], "decomposition": ["Which devices can run Microsoft Excel software?", "Do all of #1 require electricity to work?"], "evidence": [[[["Microsoft Excel-1"]], [["Smartphone-2"], "operation"]], [[["Microsoft Excel-25"]], [["Electricity-4"]]], [[["Microsoft Excel-1", "Spreadsheet-1"]], [["Computer-43", "Digital electronics-6"], "no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it true that the first person to walk on the moon was Neil Armstrong?\nFacts: Neil Armstrong was the first person to walk on the moon.\nAnswer (True or False): True\nQuestion: Is it true that the first person to fly a plane was Charles Lindbergh?\nFacts: Charles Lindbergh was the first person to fly a plane.\nAnswer (True or False): True\nQuestion: Is it true that the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is electricity necessary to balance an account in Microsoft Excel?\nFacts: Microsoft Excel is a computer program Computers require a power source\nAnswer (True or False):", "ct_subject": "Microsoft Excel is a computer program Computers require a power source", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.97739786"], [" it", "0.999835"], [" true", "0.98170257"], [" that", "1.0"], [" the", "1.0"], [" first", "0.9999908"], [" person", "0.9999188"], [" to", "1.0"], [" walk", "0.62310535"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" was", "1.0"], [" Neil", "1.0"], [" Armstrong", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "0.9999422"], ["acts", "1.0"], [":", "1.0"], [" Neil", "1.0"], [" Armstrong", "1.0"], [" was", "1.0"], [" the", "0.9999894"], [" first", "1.0"], [" person", "0.9999981"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [".", "0.95931125"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" true", "1.0"], [" that", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" fly", "0.99792045"], [" a", "0.915647"], [" plane", "1.0"], [" was", "1.0"], [" Charles", "1.0"], [" Lind", "1.0"], ["ber", "1.0"], ["gh", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Charles", "1.0"], [" Lind", "1.0"], ["ber", "1.0"], ["gh", "1.0"], [" was", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" fly", "1.0"], [" a", "1.0"], [" plane", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" true", "1.0"], [" that", "1.0"], [" the", "1.0"]]}, "471": {"qid": "363a55b705110a878be1", "term": "Goofy", "description": "Disney cartoon character", "question": "If Goofy were a pet, would he need heartworm prevention?", "answer": true, "facts": ["Goofy is an anthropomorphic dog character. ", "Dogs require regular heartworm prevention. "], "decomposition": ["What kind of animal is Goofy?", "Does a #1 require regular heartworm prevention?"], "evidence": [[[["Goofy-1"]], [["Dog-18"]]], [[["Goofy-1"]], [["Dog health-50"]]], [[["Goofy-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: If you were a dog, would you want to be a dog?\nFacts: Dogs are social animals.  Dogs are not interested in being a dog. \nAnswer (True or False): True\nQuestion: If you were a dog, would you want to be a dog?\nFacts: Dogs are social animals.  Dogs are not interested in being a dog. \nAnswer (", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: If Goofy were a pet, would he need heartworm prevention?\nFacts: Goofy is an anthropomorphic dog character.  Dogs require regular heartworm prevention. \nAnswer (True or False):", "ct_subject": "Goofy is an anthropomorphic dog character.  Dogs require regular heartworm prevention. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" If", "0.92682284"], [" you", "0.7976405"], [" were", "0.99979407"], [" a", "0.96261495"], [" dog", "0.3806732"], [",", "1.0"], [" would", "1.0"], [" you", "1.0"], [" want", "0.7502669"], [" to", "1.0"], [" be", "1.0"], [" a", "1.0"], [" dog", "0.9999987"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Dogs", "1.0"], [" are", "1.0"], [" social", "0.99936"], [" animals", "1.0"], [".", "1.0"], [" ", "1.0"], [" Dogs", "1.0"], [" are", "1.0"], [" not", "0.9996213"], [" interested", "0.99546945"], [" in", "1.0"], [" being", "1.0"], [" a", "0.99616635"], [" dog", "0.8265281"], [".", "1.0"], [" ", "1.0"], ["\n", "0.99987066"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" If", "1.0"], [" you", "1.0"], [" were", "1.0"], [" a", "1.0"], [" dog", "0.99999845"], [",", "1.0"], [" would", "1.0"], [" you", "1.0"], [" want", "1.0"], [" to", "1.0"], [" be", "1.0"], [" a", "1.0"], [" dog", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Dogs", "1.0"], [" are", "1.0"], [" social", "1.0"], [" animals", "1.0"], [".", "1.0"], [" ", "1.0"], [" Dogs", "1.0"], [" are", "1.0"], [" not", "1.0"], [" interested", "1.0"], [" in", "1.0"], [" being", "1.0"], [" a", "1.0"], [" dog", "1.0"], [".", "1.0"], [" ", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"]]}, "472": {"qid": "95bd44fab517139182f2", "term": "Alan Greenspan", "description": "13th Chairman of the Federal Reserve in the United States", "question": "Has Alan Greenspan lived through at least a baker's dozen of president's?", "answer": true, "facts": ["A baker's dozen refers to the number 13.", "Alan Greenspan was born in 1926 and turned 94 years old in March of 2020.", "There have been 16 different president's from 1926-2020."], "decomposition": ["When was Alan Greenspan born?", "What number is represented by a baker's dozen?", "How many US presidents have served since #1?", "Is #3 greater than or equal to #2?"], "evidence": [[[["Alan Greenspan-1"]], [["Dozen-7"]], [["Calvin Coolidge-1", "Donald Trump-1"]], ["operation"]], [[["Alan Greenspan-1"]], [["Dozen-7"]], [["Calvin Coolidge-1", "Donald Trump-1"]], ["operation"]], [[["Alan Greenspan-1"]], [["Dozen-7"]], ["no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average age of a person who has been a member of the United States Congress?\nFacts: The average age of a member of the United States Congress is 63.\nAnswer (True or False): True\nQuestion: What is the average age of a person who has been a member", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Has Alan Greenspan lived through at least a baker's dozen of president's?\nFacts: A baker's dozen refers to the number 13. Alan Greenspan was born in 1926 and turned 94 years old in March of 2020. There have been 16 different president's from 1926-2020.\nAnswer (True or False):", "ct_subject": "A baker's dozen refers to the number 13. Alan Greenspan was born in 1926 and turned 94 years old in March of 2020. There have been 16 different president's from 1926-2020.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999449"], [" is", "1.0"], [" the", "1.0"], [" average", "0.99800557"], [" age", "0.9656642"], [" of", "1.0"], [" a", "1.0"], [" person", "0.97523785"], [" who", "0.9999993"], [" has", "1.0"], [" been", "0.9999994"], [" a", "0.85181445"], [" member", "1.0"], [" of", "1.0"], [" the", "1.0"], [" United", "0.9991086"], [" States", "1.0"], [" Congress", "0.9999994"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" age", "1.0"], [" of", "1.0"], [" a", "1.0"], [" member", "1.0"], [" of", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" Congress", "1.0"], [" is", "1.0"], [" 63", "0.2616272"], [".", "0.9725763"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" age", "1.0"], [" of", "1.0"], [" a", "1.0"], [" person", "1.0"], [" who", "1.0"], [" has", "1.0"], [" been", "1.0"], [" a", "1.0"], [" member", "1.0"]]}, "473": {"qid": "6042b48035952d0e1a61", "term": "Friday", "description": "day of the week", "question": "Would an astrologer focus on the densest terrestrial planet for a Friday horoscope?", "answer": true, "facts": ["Friday is associated with Venus in astrology", "Venus is the densest of the terrestrial planets "], "decomposition": ["What astrological body is associated with Friday?", "Which is the densest terrestrial planet?", "Is #2 the same as #1?"], "evidence": [[[["Planetary hours-4"]], [["Venus-19"]], [["Planetary hours-4", "Venus-19"]]], [[["Friday-3"]], [["Outline of Venus-2"]], ["operation"]], [[["Friday-12"]], [["Venus-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the planet that is farthest from the Sun?\nFacts: Mercury\nAnswer (True or False): False\nQuestion: What is the name of the planet that is farthest from the Sun?\nFacts: Venus\nAnswer (True or False): False\nQuestion: What is the name of the planet that is farthest from the Sun?\nFacts", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would an astrologer focus on the densest terrestrial planet for a Friday horoscope?\nFacts: Friday is associated with Venus in astrology Venus is the densest of the terrestrial planets \nAnswer (True or False):", "ct_subject": "Friday is associated with Venus in astrology Venus is the densest of the terrestrial planets ", "good_gpt2_prompt": true, "token_probs": [[" True", "0.999998"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999999"], [" is", "1.0"], [" the", "1.0"], [" name", "0.67224413"], [" of", "1.0"], [" the", "1.0"], [" planet", "1.0"], [" that", "0.9999999"], [" is", "1.0"], [" fart", "0.99901354"], ["hest", "1.0"], [" from", "1.0"], [" the", "1.0"], [" Sun", "0.95458657"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Mercury", "0.99823415"], ["\n", "0.99372673"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" False", "0.9999808"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" planet", "1.0"], [" that", "1.0"], [" is", "1.0"], [" fart", "1.0"], ["hest", "1.0"], [" from", "1.0"], [" the", "1.0"], [" Sun", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "0.99999905"], ["acts", "1.0"], [":", "1.0"], [" Venus", "0.99999976"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" False", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" planet", "1.0"], [" that", "1.0"], [" is", "1.0"], [" fart", "1.0"], ["hest", "1.0"], [" from", "1.0"], [" the", "1.0"], [" Sun", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"]]}, "474": {"qid": "36edeeb054e722511fcd", "term": "Elizabeth II", "description": "Queen of the United Kingdom and the other Commonwealth realms", "question": "Was Elizabeth II the Queen during the Persian Gulf War?", "answer": true, "facts": ["Elizabeth II became Queen in 1952.", "The Persian Gulf War occurred 1990-1991."], "decomposition": ["When did Elizabeth II become the Queen?", "When was the Persian Gulf War?", "Was Elizabeth II alive in #2?", "Is #2 after #1?", "Are the answers to #3 and #4 both yes?"], "evidence": [[[["Elizabeth II-3"]], [["Persian Gulf-3"]], [["Elizabeth II-1"]], ["operation"], ["operation"]], [[["Head of the Commonwealth-8"]], [["Gulf War-1"]], [["Elizabeth II-41"]], ["operation"], ["operation"]], [[["Elizabeth II-3"]], [["Gulf War-1"]], [["Elizabeth II-42"]], ["operation"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Was the Titanic the largest ship ever built?\nFacts: The Titanic was the largest ship ever built. The Titanic was the largest ship ever built. The Titanic was the largest ship ever built. The Titanic was the largest ship ever built. The Titanic was the largest ship ever built. The Titanic was the largest ship ever built. The Titanic was the largest ship ever built. The Titanic was the largest ship ever built. The Titanic was the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Elizabeth II the Queen during the Persian Gulf War?\nFacts: Elizabeth II became Queen in 1952. The Persian Gulf War occurred 1990-1991.\nAnswer (True or False):", "ct_subject": "Elizabeth II became Queen in 1952. The Persian Gulf War occurred 1990-1991.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "0.8443088"], [" the", "1.0"], [" Titanic", "0.98463666"], [" the", "0.9995357"], [" largest", "0.99992263"], [" ship", "1.0"], [" ever", "0.2371036"], [" built", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Titanic", "1.0"], [" was", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" ship", "1.0"], [" ever", "1.0"], [" built", "1.0"], [".", "0.9999541"], [" The", "0.9794667"], [" Titanic", "0.99995077"], [" was", "1.0"], [" the", "0.99999964"], [" largest", "1.0"], [" ship", "1.0"], [" ever", "1.0"], [" built", "1.0"], [".", "1.0"], [" The", "0.72468764"], [" Titanic", "1.0"], [" was", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" ship", "1.0"], [" ever", "1.0"], [" built", "1.0"], [".", "1.0"], [" The", "1.0"], [" Titanic", "1.0"], [" was", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" ship", "1.0"], [" ever", "1.0"], [" built", "1.0"], [".", "1.0"], [" The", "1.0"], [" Titanic", "1.0"], [" was", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" ship", "1.0"], [" ever", "1.0"], [" built", "1.0"], [".", "1.0"], [" The", "1.0"], [" Titanic", "1.0"], [" was", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" ship", "1.0"], [" ever", "1.0"], [" built", "1.0"], [".", "1.0"], [" The", "1.0"], [" Titanic", "1.0"], [" was", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" ship", "1.0"], [" ever", "1.0"], [" built", "1.0"], [".", "1.0"], [" The", "1.0"], [" Titanic", "1.0"], [" was", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" ship", "1.0"], [" ever", "1.0"], [" built", "1.0"], [".", "1.0"], [" The", "1.0"], [" Titanic", "1.0"], [" was", "1.0"], [" the", "1.0"]]}, "475": {"qid": "1a11133f240556f239de", "term": "Ocelot", "description": "Small wild cat", "question": "Could an ocelot outrun a kindergartner? ", "answer": true, "facts": ["An ocelot can run up to 61 kilometers per hour.", "Kindergartners are human children usually aged five to six years old.", "Human children under 10 are not likely to exceed 20 kilometers per hour."], "decomposition": ["How quickly can an ocelot run?", "How old is a kindergartner?", "How quickly can someone run at the age of #2?", "Is #1 greater than #3?"], "evidence": [[[["Ocelot-25", "Tiger-54"], "no_evidence"], [["Kindergarten-1"]], [["Running-43"], "no_evidence"], ["operation"]], [["no_evidence"], [["Kindergarten-89"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Ocelot-25"], "no_evidence"], [["Kindergarten-1"]], ["no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human?\nFacts: The average weight of a human is about 100 kilograms.\nAnswer (True or False): True\nQuestion: What is the average height of a human?\nFacts: The average height of a human is about 1.7 meters.\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could an ocelot outrun a kindergartner? \nFacts: An ocelot can run up to 61 kilometers per hour. Kindergartners are human children usually aged five to six years old. Human children under 10 are not likely to exceed 20 kilometers per hour.\nAnswer (True or False):", "ct_subject": "An ocelot can run up to 61 kilometers per hour. Kindergartners are human children usually aged five to six years old. Human children under 10 are not likely to exceed 20 kilometers per hour.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9982552"], [" is", "1.0"], [" the", "1.0"], [" average", "0.86868376"], [" weight", "0.99969053"], [" of", "1.0"], [" a", "0.9999614"], [" human", "1.0"], ["?", "0.8504647"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" weight", "0.99029595"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" is", "1.0"], [" about", "0.89110374"], [" 100", "0.99978215"], [" kilograms", "0.98967"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" height", "1.0"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" height", "1.0"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 1", "1.0"], [".", "1.0"], ["7", "0.9496629"], [" meters", "1.0"], [".", "1.0"], ["\n", "1.0"]]}, "476": {"qid": "a843854a5eabf03cf56f", "term": "Music", "description": "form of art using sound and silence", "question": "Can music be used as a weapon?", "answer": true, "facts": ["Music is an art form whose medium is sound.", "Music can help elevate or subdue emotions.", "People connect to music through the sound.", "The military uses loud music to cause psychological disorientation and confusion ", "The military calls the use of loud disorienting music part of psychological operations. "], "decomposition": ["In what ways does the military used music in operations?", "Is any of #1 as a weapon?"], "evidence": [[[["Music in psychological operations-1"]], [["Music in psychological operations-2"], "operation"]], [[["Manuel Noriega-44"], "no_evidence"], ["operation"]], [[["Music in psychological operations-2"]], [["Weapon-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake's mother\"?\nFacts: A snowflake is a single snowflake. A snowflake's mother is a single snowflake.\nAnswer (True or False", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can music be used as a weapon?\nFacts: Music is an art form whose medium is sound. Music can help elevate or subdue emotions. People connect to music through the sound. The military uses loud music to cause psychological disorientation and confusion  The military calls the use of loud disorienting music part of psychological operations. \nAnswer (True or False):", "ct_subject": "Music is an art form whose medium is sound. Music can help elevate or subdue emotions. People connect to music through the sound. The military uses loud music to cause psychological disorientation and confusion  The military calls the use of loud disorienting music part of psychological operations. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9858642"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "0.9999995"], [" \"", "0.99999964"], ["s", "0.59354603"], ["now", "0.8316566"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.99986136"], ["now", "1.0"], ["flake", "1.0"], ["'s", "0.9889475"], [" mother", "0.9976382"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.99999785"], [" snow", "0.99984884"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "0.81596076"], [" snow", "0.9998752"], ["flake", "1.0"], [".", "0.9999448"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], ["'s", "1.0"], [" mother", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [".", "1.0"], ["\n", "0.9999999"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"]]}, "477": {"qid": "9e6de7de5577b59708ed", "term": "San Antonio", "description": "City in Texas, United States", "question": "Was San Antonio the site of a major battle in the 19th century?", "answer": true, "facts": ["The Alamo is located in San Antonio.", "The Alamo was the site of a major battle during the Texan Revolution against Mexico in 1836."], "decomposition": ["Where did the most notable battle during the Texas Revolution take place?", "Is #1 located in San Antonio in present day US?", "Did the Texas revolution happen during the 19th century?", "Are #2 and #3 positive?"], "evidence": [[[["Battle of the Alamo-1"]], [["Battle of the Alamo-1"]], [["19th century-1"]], ["operation"]], [[["Battle of the Alamo-1"]], [["Battle of the Alamo-1"]], [["19th century-1", "Battle of the Alamo-1"]], ["operation"]], [[["Battle of the Alamo-1"]], ["operation"], ["operation"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Was the Alamo the site of a major battle in the 19th century?\nFacts: The Alamo is located in San Antonio. The Alamo was the site of a major battle during the Texan Revolution against Mexico in 1836.\nAnswer (True or False): True\nQuestion: Was the Alamo the site of a major battle", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was San Antonio the site of a major battle in the 19th century?\nFacts: The Alamo is located in San Antonio. The Alamo was the site of a major battle during the Texan Revolution against Mexico in 1836.\nAnswer (True or False):", "ct_subject": "The Alamo is located in San Antonio. The Alamo was the site of a major battle during the Texan Revolution against Mexico in 1836.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "0.80011773"], [" the", "1.0"], [" Al", "0.99951816"], ["amo", "1.0"], [" the", "0.9999181"], [" site", "0.99998367"], [" of", "1.0"], [" a", "1.0"], [" major", "0.9999716"], [" battle", "1.0"], [" in", "1.0"], [" the", "1.0"], [" 19", "1.0"], ["th", "1.0"], [" century", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Al", "1.0"], ["amo", "1.0"], [" is", "0.9999949"], [" located", "1.0"], [" in", "1.0"], [" San", "1.0"], [" Antonio", "1.0"], [".", "1.0"], [" The", "1.0"], [" Al", "1.0"], ["amo", "1.0"], [" was", "1.0"], [" the", "1.0"], [" site", "1.0"], [" of", "1.0"], [" a", "1.0"], [" major", "1.0"], [" battle", "1.0"], [" during", "1.0"], [" the", "1.0"], [" Tex", "1.0"], ["an", "1.0"], [" Revolution", "1.0"], [" against", "1.0"], [" Mexico", "1.0"], [" in", "1.0"], [" 18", "1.0"], ["36", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "1.0"], [" the", "1.0"], [" Al", "1.0"], ["amo", "1.0"], [" the", "1.0"], [" site", "1.0"], [" of", "1.0"], [" a", "1.0"], [" major", "1.0"], [" battle", "1.0"]]}, "478": {"qid": "14806a45b707fc56e4dd", "term": "Phobos (moon)", "description": "natural satellite of Mars", "question": "Would you have to wear a coat when on Phobos?", "answer": true, "facts": ["A coat is a garment used to keep a person warm.", "The surface temperatures on Phobos range from about \u22124 \u00b0C (25 \u00b0F) on the sunlit side to \u2212112 \u00b0C (\u2212170 \u00b0F) on the shadowed side."], "decomposition": ["What is the average temperature on Phobos?", "At what temperature would people need to start wearing coats to stay warm?", "Is #1 below #2?"], "evidence": [[[["Phobos (moon)-4"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Phobos (moon)-4"]], [["Coat-1"], "no_evidence"], ["operation"]], [[["Phobos (moon)-4"]], [["Coat-1"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the planet that orbits Mars?\nFacts: Mars is the second planet from the sun. It is the only planet in the solar system that has a day length of 24 hours.\nAnswer (True or False): True\nQuestion:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would you have to wear a coat when on Phobos?\nFacts: A coat is a garment used to keep a person warm. The surface temperatures on Phobos range from about \u22124 \u00b0C (25 \u00b0F) on the sunlit side to \u2212112 \u00b0C (\u2212170 \u00b0F) on the shadowed side.\nAnswer (True or False):", "ct_subject": "A coat is a garment used to keep a person warm. The surface temperatures on Phobos range from about \u22124 \u00b0C (25 \u00b0F) on the sunlit side to \u2212112 \u00b0C (\u2212170 \u00b0F) on the shadowed side.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99995184"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" planet", "1.0"], [" that", "0.9760946"], [" orbits", "0.9995351"], [" Mars", "0.99983597"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Mars", "0.99999964"], [" is", "1.0"], [" the", "1.0"], [" second", "0.9999553"], [" planet", "1.0"], [" from", "1.0"], [" the", "1.0"], [" sun", "0.9999999"], [".", "0.9999993"], [" It", "0.9997856"], [" is", "1.0"], [" the", "1.0"], [" only", "0.9999441"], [" planet", "1.0"], [" in", "1.0"], [" the", "0.9968719"], [" solar", "1.0"], [" system", "1.0"], [" that", "0.9986992"], [" has", "0.9999999"], [" a", "1.0"], [" day", "0.99986863"], [" length", "0.9999894"], [" of", "1.0"], [" 24", "1.0"], [" hours", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"]]}, "479": {"qid": "06724dc213e0dae715f5", "term": "Amazon (company)", "description": "American electronic commerce and cloud computing company", "question": "Could one Amazon share ever buy twenty year Netflix subscription?", "answer": true, "facts": ["Amazon stock has reached as high as $2,500 a share as of June 2020.", "The basic Netflix subscription package costs $8.99 a month as of 2020."], "decomposition": ["What is the cost of a monthly Netflix subscription?", "How many months are there in a year?", "What is #2 multiplied by 20 and then multiplied by #1?", "What is the highest price Amazon stock has ever reached?", "Is #4 greater than #3?"], "evidence": [[[["Netflix-49"]], [["Month-38"]], ["operation"], ["no_evidence"], ["operation"]], [[["Netflix-55"]], [["Year-47"]], ["operation"], [["Amazon (company)-78"], "no_evidence"], ["no_evidence", "operation"]], [[["Netflix-49"]], [["Fiscal year-73"]], ["operation"], ["no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average age of a person in the United States?\nFacts: The average age of a person in the United States is 37.\nAnswer (True or False): True\nQuestion: What is the average age of a person in the United States?\nFacts: The average age of a person in the United States is 37.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could one Amazon share ever buy twenty year Netflix subscription?\nFacts: Amazon stock has reached as high as $2,500 a share as of June 2020. The basic Netflix subscription package costs $8.99 a month as of 2020.\nAnswer (True or False):", "ct_subject": "Amazon stock has reached as high as $2,500 a share as of June 2020. The basic Netflix subscription package costs $8.99 a month as of 2020.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9992537"], [" is", "1.0"], [" the", "1.0"], [" average", "0.9998029"], [" age", "0.9857686"], [" of", "1.0"], [" a", "1.0"], [" person", "1.0"], [" in", "0.5910854"], [" the", "1.0"], [" United", "0.99999213"], [" States", "1.0"], ["?", "0.9999987"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" age", "1.0"], [" of", "1.0"], [" a", "1.0"], [" person", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "1.0"], [" 37", "0.7246722"], [".", "1.0"], ["\n", "0.99918324"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" age", "0.9929403"], [" of", "1.0"], [" a", "1.0"], [" person", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" age", "1.0"], [" of", "1.0"], [" a", "1.0"], [" person", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "1.0"], [" 37", "1.0"], [".", "1.0"]]}, "480": {"qid": "22c9aaeaf0c412ba2201", "term": "Mary, mother of Jesus", "description": "religious figure and mother of Jesus of Nazareth", "question": "Would Mary, mother of Jesus have hypothetically prayed to Artemis if she was Greek?", "answer": true, "facts": ["Mary, Mother of Jesus is known for being a virgin and giving birth to Christ.", "Artemis was the Greek goddess of the hunt, childbirth, and virgins."], "decomposition": ["What is Greek Artemis god of?", "What were the things Mary, mother of Jesus was well known for?", "Is any of #2 included in #1?"], "evidence": [[[["Artemis-1"]], [["Mary, mother of Jesus-14"], "no_evidence"], ["operation"]], [[["Artemis-1"]], [["Mary, mother of Jesus-2"]], ["operation"]], [[["Artemis-16"]], [["Mary, mother of Jesus-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city of Rome?\nFacts: Rome is the capital of the Roman Empire.\nAnswer (True or False): True\nQuestion: What is the name of the city of Rome?\nFacts: Rome is the capital of the Roman Empire.\nAnswer (True or False): True\nQuestion", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Mary, mother of Jesus have hypothetically prayed to Artemis if she was Greek?\nFacts: Mary, Mother of Jesus is known for being a virgin and giving birth to Christ. Artemis was the Greek goddess of the hunt, childbirth, and virgins.\nAnswer (True or False):", "ct_subject": "Mary, Mother of Jesus is known for being a virgin and giving birth to Christ. Artemis was the Greek goddess of the hunt, childbirth, and virgins.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.98078716"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99999964"], [" of", "1.0"], [" the", "1.0"], [" city", "0.054765202"], [" of", "0.9921989"], [" Rome", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "0.99999976"], [" is", "0.98716795"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [".", "1.0"], ["\n", "0.99999976"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"]]}, "481": {"qid": "2c1620d67936c9229270", "term": "Nikola Tesla", "description": "Serbian American inventor", "question": "Is there radiation where Nikola Tesla once worked?", "answer": true, "facts": ["Nikola Tesla built a facility called the Wardenclyffe Tower in Shoreham, New York", "Shoreham was the site of a nuclear power plant in the '70s and '80s"], "decomposition": ["What facility did Nikola Tesla build?", "Where is #1 located?", "Did #2 use to be the site of a nuclear power plant?"], "evidence": [[[["Wardenclyffe Tower-12"]], [["Wardenclyffe Tower-1"]], [["Shoreham Nuclear Power Plant-1"], "operation"]], [[["Wardenclyffe Tower-1"]], [["Wardenclyffe Tower-1"]], [["Shoreham Nuclear Power Plant-4", "Shoreham, New York-9"]]], [[["Wardenclyffe Tower-1"]], [["Wardenclyffe Tower-1"]], [["Shoreham Nuclear Power Plant-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is there a secret society called the Illuminati?\nFacts: The Illuminati is a secret society that has existed since the 18th century.\nAnswer (True or False): True\nQuestion: Is there a secret society called the Freemasons?\nFacts: The Freemasons are a secret society that has existed since the 17th century.\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is there radiation where Nikola Tesla once worked?\nFacts: Nikola Tesla built a facility called the Wardenclyffe Tower in Shoreham, New York Shoreham was the site of a nuclear power plant in the '70s and '80s\nAnswer (True or False):", "ct_subject": "Nikola Tesla built a facility called the Wardenclyffe Tower in Shoreham, New York Shoreham was the site of a nuclear power plant in the '70s and '80s", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.96485263"], [" there", "1.0"], [" a", "1.0"], [" secret", "0.78760487"], [" society", "0.9984567"], [" called", "0.99994445"], [" the", "1.0"], [" Illuminati", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99999475"], [" Illuminati", "1.0"], [" is", "1.0"], [" a", "1.0"], [" secret", "1.0"], [" society", "1.0"], [" that", "1.0"], [" has", "0.9917327"], [" existed", "0.99954045"], [" since", "0.37246242"], [" the", "1.0"], [" 18", "0.004508028"], ["th", "1.0"], [" century", "1.0"], [".", "1.0"], ["\n", "0.996385"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" there", "1.0"], [" a", "1.0"], [" secret", "1.0"], [" society", "1.0"], [" called", "1.0"], [" the", "1.0"], [" Freem", "1.0"], ["asons", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Freem", "1.0"], ["asons", "1.0"], [" are", "1.0"], [" a", "1.0"], [" secret", "1.0"], [" society", "1.0"], [" that", "1.0"], [" has", "1.0"], [" existed", "1.0"], [" since", "1.0"], [" the", "1.0"], [" 17", "0.6199028"], ["th", "1.0"], [" century", "1.0"], [".", "1.0"], ["\n", "1.0"]]}, "482": {"qid": "e9b635db671e0c1be8d9", "term": "Pancreas", "description": "A glandular organ that plays a role in the digestive and endocrine systems of vertebrates.", "question": "Can pancreas removal cause bankruptcy?", "answer": true, "facts": ["Pancreas removal is a medical procedure.", "Medical procedures are expensive in come countries. ", "Expensive procedures can cause debt.", "Debt can cause bankruptcy. "], "decomposition": ["What medical procedures are involved when a pancreas be removed?", "In what places are #1 sometimes directly paid for by the patient?", "Among any of #2, what consequences exist for medical debt?", "Is bankruptcy included in #3?"], "evidence": [[[["General surgery-1"]], [["Health care systems by country-55"]], [["Medical debt-4"]], ["operation"]], [[["Pancreatectomy-4"]], [["Medical debt-4"]], [["Medical debt-4"]], ["operation"]], [[["Pancreas-37", "Pancreas-44"], "no_evidence"], [["Health care in the United States-4"], "no_evidence"], [["Health care in the United States-14"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"sick\" and a \"healthy\" person?\nFacts: A healthy person is one who is not sick. A sick person is one who is sick. A sick person is one who is sick. A sick person is one who is sick. A sick person is one who is sick. A sick person is one who is sick. A", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can pancreas removal cause bankruptcy?\nFacts: Pancreas removal is a medical procedure. Medical procedures are expensive in come countries.  Expensive procedures can cause debt. Debt can cause bankruptcy. \nAnswer (True or False):", "ct_subject": "Pancreas removal is a medical procedure. Medical procedures are expensive in come countries.  Expensive procedures can cause debt. Debt can cause bankruptcy. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.2951345"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.81021047"], [" between", "1.0"], [" a", "0.99999964"], [" \"", "0.99990535"], ["s", "0.857866"], ["ick", "0.6584218"], ["\"", "1.0"], [" and", "0.9999192"], [" a", "0.9999912"], [" \"", "1.0"], ["healthy", "0.99999976"], ["\"", "1.0"], [" person", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" healthy", "0.6970998"], [" person", "1.0"], [" is", "1.0"], [" one", "0.9999964"], [" who", "0.9999"], [" is", "1.0"], [" not", "0.9999261"], [" sick", "1.0"], [".", "1.0"], [" A", "1.0"], [" sick", "1.0"], [" person", "1.0"], [" is", "1.0"], [" one", "1.0"], [" who", "1.0"], [" is", "1.0"], [" sick", "1.0"], [".", "1.0"], [" A", "0.9999999"], [" sick", "0.99977916"], [" person", "1.0"], [" is", "1.0"], [" one", "1.0"], [" who", "1.0"], [" is", "1.0"], [" sick", "1.0"], [".", "1.0"], [" A", "1.0"], [" sick", "1.0"], [" person", "1.0"], [" is", "1.0"], [" one", "1.0"], [" who", "1.0"], [" is", "1.0"], [" sick", "1.0"], [".", "1.0"], [" A", "1.0"], [" sick", "1.0"], [" person", "1.0"], [" is", "1.0"], [" one", "1.0"], [" who", "1.0"], [" is", "1.0"], [" sick", "1.0"], [".", "1.0"], [" A", "1.0"], [" sick", "1.0"], [" person", "1.0"], [" is", "1.0"], [" one", "1.0"], [" who", "1.0"], [" is", "1.0"], [" sick", "1.0"], [".", "1.0"], [" A", "1.0"]]}, "483": {"qid": "441d83eccca7714ba2a7", "term": "Harlem Renaissance", "description": "African-American cultural movement in New York City in the 1920s", "question": "Could Al Capone have read works from the Harlem Renaissance?", "answer": true, "facts": ["The Harlem Renaissance occurred during the 1920s.", "Al Capone lived through the 1920s."], "decomposition": ["When was the Harlem Renaissance?", "Was Al Capone able to read during #1?"], "evidence": [[[["Harlem Renaissance-1"]], [["Al Capone-1"], "operation"]], [[["Harlem Renaissance-1"]], [["Al Capone-1"], "operation"]], [[["Harlem Renaissance-41"], "no_evidence"], [["Al Capone-3"], "no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Beatles have a secret song that was never released?\nFacts: The Beatles had a secret song that was never released.\nAnswer (True or False): True\nQuestion: Did the Beatles have a secret song that was never released?\nFacts: The Beatles had a secret song that was never released.\nAnswer (True or False): True\nQuestion: Did the Beatles have a secret song that was never", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could Al Capone have read works from the Harlem Renaissance?\nFacts: The Harlem Renaissance occurred during the 1920s. Al Capone lived through the 1920s.\nAnswer (True or False):", "ct_subject": "The Harlem Renaissance occurred during the 1920s. Al Capone lived through the 1920s.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99931526"], [" the", "1.0"], [" Beatles", "0.9955438"], [" have", "1.0"], [" a", "1.0"], [" secret", "0.99999964"], [" song", "0.99999535"], [" that", "0.9990645"], [" was", "0.9962568"], [" never", "0.99999106"], [" released", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Beatles", "1.0"], [" had", "0.9500117"], [" a", "1.0"], [" secret", "0.999652"], [" song", "1.0"], [" that", "1.0"], [" was", "1.0"], [" never", "1.0"], [" released", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999999"], [" the", "1.0"], [" Beatles", "1.0"], [" have", "1.0"], [" a", "1.0"], [" secret", "1.0"], [" song", "1.0"], [" that", "1.0"], [" was", "1.0"], [" never", "1.0"], [" released", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Beatles", "1.0"], [" had", "1.0"], [" a", "1.0"], [" secret", "1.0"], [" song", "1.0"], [" that", "1.0"], [" was", "1.0"], [" never", "1.0"], [" released", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" Beatles", "1.0"], [" have", "1.0"], [" a", "1.0"], [" secret", "1.0"], [" song", "1.0"], [" that", "1.0"], [" was", "1.0"], [" never", "1.0"]]}, "484": {"qid": "5e39653ec4524dce45c9", "term": "Suburb", "description": "Human settlement that is part of or near to a larger city", "question": "Do suburbs encourage the use of cars?", "answer": true, "facts": ["Suburbs are generally built outside of walking distance from city centers.", "City centers contain jobs and stores.", "Suburb dwellers need to access jobs and stores to survive."], "decomposition": ["How far are suburbs usually situated from city centres?", "Is #1 usually greater than reasonable walking distance?"], "evidence": [[[["Suburb-1"], "no_evidence"], ["operation"]], [[["Suburb-1"]], [["Commuting-13"]]], [[["Suburb-1"], "no_evidence"], [["Suburb-2"], "no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Do you have to be a member of the Communist Party to be a Communist?\nFacts: The Communist Party is a political party that advocates for the interests of the working class. It is not a religious organization.\nAnswer (True or False): True\nQuestion: Do you have to be a member of the Communist Party to be a Communist?\nFacts", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do suburbs encourage the use of cars?\nFacts: Suburbs are generally built outside of walking distance from city centers. City centers contain jobs and stores. Suburb dwellers need to access jobs and stores to survive.\nAnswer (True or False):", "ct_subject": "Suburbs are generally built outside of walking distance from city centers. City centers contain jobs and stores. Suburb dwellers need to access jobs and stores to survive.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Do", "0.95082223"], [" you", "0.99954057"], [" have", "0.9714082"], [" to", "1.0"], [" be", "1.0"], [" a", "0.9999999"], [" member", "1.0"], [" of", "1.0"], [" the", "0.99999976"], [" Communist", "1.0"], [" Party", "1.0"], [" to", "1.0"], [" be", "1.0"], [" a", "1.0"], [" Communist", "0.99984574"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9985386"], [" Communist", "1.0"], [" Party", "1.0"], [" is", "0.9780873"], [" a", "1.0"], [" political", "1.0"], [" party", "1.0"], [" that", "0.9778153"], [" advocates", "0.9999995"], [" for", "0.99944407"], [" the", "1.0"], [" interests", "0.9999999"], [" of", "1.0"], [" the", "1.0"], [" working", "1.0"], [" class", "1.0"], [".", "1.0"], [" It", "0.42429814"], [" is", "1.0"], [" not", "1.0"], [" a", "1.0"], [" religious", "0.99995863"], [" organization", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Do", "0.9999999"], [" you", "1.0"], [" have", "1.0"], [" to", "1.0"], [" be", "1.0"], [" a", "1.0"], [" member", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Communist", "1.0"], [" Party", "1.0"], [" to", "1.0"], [" be", "1.0"], [" a", "1.0"], [" Communist", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"]]}, "485": {"qid": "a77402c5a20c3a5a8676", "term": "Moose", "description": "A genus of mammals belonging to the deer, muntjac, roe deer, reindeer, and moose family of ruminants", "question": "Would a moose hypothetically be too much for a minotaur to devour whole?", "answer": true, "facts": ["A minotaur was a mythological beast with the head of a bull and body of a human.", "Human stomachs process about three to four pounds of food a day.", "A moose can weigh up to 1500 pounds.", "Bulls can consume around 33 pounds of food a day."], "decomposition": ["What is the body structure of a minotaur?", "What kind of stomach do they have due to #1?", "What is the average weight of a moose?", "How much food weight can #2 process per day?", "Is #3 far greater than #4?"], "evidence": [[[["Minotaur-7"]], [["Minotaur-7"], "no_evidence"], [["Alaska moose-3"]], ["no_evidence"], ["operation"]], [[["Minotaur-1"]], [["Human digestive system-36"]], [["Moose-40"]], [["Moose-41"]], ["operation"]], [[["Minotaur-1"]], [["Stomach-1", "Stomach-3"]], [["Moose-40"]], [["Food energy-14"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the largest animal in the world?\nFacts: The largest animal in the world is the blue whale. It can weigh up to 100,000 pounds.\nAnswer (True or False): True\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a moose hypothetically be too much for a minotaur to devour whole?\nFacts: A minotaur was a mythological beast with the head of a bull and body of a human. Human stomachs process about three to four pounds of food a day. A moose can weigh up to 1500 pounds. Bulls can consume around 33 pounds of food a day.\nAnswer (True or False):", "ct_subject": "A minotaur was a mythological beast with the head of a bull and body of a human. Human stomachs process about three to four pounds of food a day. A moose can weigh up to 1500 pounds. Bulls can consume around 33 pounds of food a day.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99987674"], [" is", "1.0"], [" the", "1.0"], [" largest", "0.99924845"], [" animal", "0.9999447"], [" in", "0.9126942"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" largest", "1.0"], [" animal", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" is", "1.0"], [" the", "1.0"], [" blue", "1.0"], [" whale", "1.0"], [".", "1.0"], [" It", "0.9943136"], [" can", "0.6427619"], [" weigh", "0.99961734"], [" up", "1.0"], [" to", "1.0"], [" 100", "0.99998796"], [",", "1.0"], ["000", "1.0"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"]]}, "486": {"qid": "d4803e3857fb8b51df5b", "term": "Fraktur", "description": "Typeface", "question": "Does Fraktur have a sordid history?", "answer": true, "facts": ["Fraktur is a type of font that originated in Germany.", "Fraktur was used on official Nazi documents.", "Fraktur was used on the cover of Hitler's Mein Kampf."], "decomposition": ["What is Fraktur?", "Which group in Germany used #1 for their official documents?", "Did #2 have a sordid past?"], "evidence": [[[["Fraktur-1"]], [["Fraktur-10"]], ["operation"]], [[["Fraktur-1"]], [["Fraktur-10"]], [["The Holocaust-1"]]], [[["Fraktur-1"]], [["Fraktur-10"]], [["Jewish ghettos in German-occupied Poland-3"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the word \"Fraktur\" a corruption of the word \"fraktur\" which means \"to write\" in Turkish?\nFacts: Fraktur is a type of font that originated in Germany. Fraktur was used on official Nazi documents. Fraktur was used on the cover of Hitler's Me", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Fraktur have a sordid history?\nFacts: Fraktur is a type of font that originated in Germany. Fraktur was used on official Nazi documents. Fraktur was used on the cover of Hitler's Mein Kampf.\nAnswer (True or False):", "ct_subject": "Fraktur is a type of font that originated in Germany. Fraktur was used on official Nazi documents. Fraktur was used on the cover of Hitler's Mein Kampf.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.7829472"], [" the", "0.999966"], [" word", "0.98419696"], [" \"", "1.0"], ["F", "0.9999994"], ["rak", "1.0"], ["tur", "1.0"], ["\"", "1.0"], [" a", "1.0"], [" corruption", "0.99962807"], [" of", "1.0"], [" the", "0.9972638"], [" word", "1.0"], [" \"", "1.0"], ["f", "0.9415795"], ["rak", "1.0"], ["tur", "0.99999917"], ["\"", "0.9999999"], [" which", "0.87123513"], [" means", "1.0"], [" \"", "1.0"], ["to", "0.9986619"], [" write", "0.98161453"], ["\"", "0.99722445"], [" in", "0.99999976"], [" Turkish", "0.91421896"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" F", "0.5216811"], ["rak", "1.0"], ["tur", "1.0"], [" is", "1.0"], [" a", "1.0"], [" type", "1.0"], [" of", "1.0"], [" font", "1.0"], [" that", "1.0"], [" originated", "1.0"], [" in", "1.0"], [" Germany", "1.0"], [".", "1.0"], [" F", "1.0"], ["rak", "1.0"], ["tur", "1.0"], [" was", "1.0"], [" used", "1.0"], [" on", "1.0"], [" official", "1.0"], [" Nazi", "1.0"], [" documents", "1.0"], [".", "1.0"], [" F", "1.0"], ["rak", "1.0"], ["tur", "1.0"], [" was", "1.0"], [" used", "1.0"], [" on", "1.0"], [" the", "1.0"], [" cover", "1.0"], [" of", "1.0"], [" Hitler", "1.0"], ["'s", "1.0"], [" Me", "1.0"]]}, "487": {"qid": "87829d2b3eb19ca11821", "term": "Andes", "description": "Mountain range in South America", "question": "Was the Peak of the Andes hidden from the view of the Colossus of Rhodes?", "answer": true, "facts": ["The highest point of the Andes is almost 23,000 feet high.", "The Colossus of Rhodes, a massive ancient statue, was around 108 feet tall.", "The Andes are located in South America.", "The Colossus of Rhodes was found in ancient Greece."], "decomposition": ["How high is the peak of the Andes?", "How high is the Colossus of Rhodes?", "Where are the Andes located?", "Where is the Colossus of Rhodes located?", "Is #1 higher than #2 and are #3 and #4 different countries?"], "evidence": [[[["Andes-3"]], [["Colossus of Rhodes-1"]], [["Andes-1"]], [["Colossus of Rhodes-1"]], ["operation"]], [[["Andes-3"]], [["Colossus of Rhodes-1"]], [["Andes-1"]], [["Colossus of Rhodes-1"]], ["operation"]], [[["Andes-3"]], [["Colossus of Rhodes-1"]], [["Andes-1"]], [["Colossus of Rhodes-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Was the Great Pyramid of Giza built by the same people who built the Great Pyramid of Khufu?\nFacts: The Great Pyramid of Giza was built by the same people who built the Great Pyramid of Khufu. The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was the Peak of the Andes hidden from the view of the Colossus of Rhodes?\nFacts: The highest point of the Andes is almost 23,000 feet high. The Colossus of Rhodes, a massive ancient statue, was around 108 feet tall. The Andes are located in South America. The Colossus of Rhodes was found in ancient Greece.\nAnswer (True or False):", "ct_subject": "The highest point of the Andes is almost 23,000 feet high. The Colossus of Rhodes, a massive ancient statue, was around 108 feet tall. The Andes are located in South America. The Colossus of Rhodes was found in ancient Greece.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "0.2669982"], [" the", "1.0"], [" Great", "0.99803334"], [" Pyramid", "1.0"], [" of", "1.0"], [" G", "1.0"], ["iza", "1.0"], [" built", "1.0"], [" by", "0.9999763"], [" the", "1.0"], [" same", "1.0"], [" people", "0.999907"], [" who", "0.9999981"], [" built", "1.0"], [" the", "1.0"], [" Great", "1.0"], [" Pyramid", "1.0"], [" of", "1.0"], [" Kh", "0.9210604"], ["uf", "1.0"], ["u", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Great", "1.0"], [" Pyramid", "1.0"], [" of", "1.0"], [" G", "1.0"], ["iza", "1.0"], [" was", "0.99992955"], [" built", "1.0"], [" by", "1.0"], [" the", "1.0"], [" same", "1.0"], [" people", "1.0"], [" who", "1.0"], [" built", "1.0"], [" the", "1.0"], [" Great", "1.0"], [" Pyramid", "1.0"], [" of", "1.0"], [" Kh", "1.0"], ["uf", "1.0"], ["u", "1.0"], [".", "1.0"], [" The", "0.9994299"]]}, "488": {"qid": "9ad9cdadd2d69de7cfff", "term": "LG Electronics", "description": "South Korean multinational electronics company", "question": "Is LG Electronics located in a city with an official bird that has a purplish/blue tail?", "answer": true, "facts": ["LG Electronics is headquarted in Seoul.", "The official bird of Seoul is the Korean magpie.", "The Korean magpie has a purplish/blue colored tail."], "decomposition": ["Which city is LG Electronics headquarters located in?", "Which bird is officially associated with #1", "Is the tail color of #2 purplish-blue?"], "evidence": [[[["LG Electronics-1"]], [["Oriental magpie-1"], "no_evidence"], [["Oriental magpie-5"], "operation"]], [[["LG Electronics-1"]], [["Oriental magpie-1"]], [["Oriental magpie-5"]]], [[["LG Electronics-1"]], [["Oriental magpie-1"], "no_evidence"], [["Oriental magpie-5"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city in which the United States Mint is located the same as the name of the city in which the United States Mint is located in the United States?\nFacts: The United States Mint is located in Philadelphia. The official name of the city in which the United States Mint is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is LG Electronics located in a city with an official bird that has a purplish/blue tail?\nFacts: LG Electronics is headquarted in Seoul. The official bird of Seoul is the Korean magpie. The Korean magpie has a purplish/blue colored tail.\nAnswer (True or False):", "ct_subject": "LG Electronics is headquarted in Seoul. The official bird of Seoul is the Korean magpie. The Korean magpie has a purplish/blue colored tail.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999988"], [" the", "0.9998815"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "0.9491266"], [" which", "0.9999989"], [" the", "0.9995579"], [" United", "0.99889034"], [" States", "1.0"], [" Mint", "0.85422343"], [" is", "1.0"], [" located", "1.0"], [" the", "0.999691"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" name", "0.99999976"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" which", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" Mint", "0.99261755"], [" is", "1.0"], [" located", "1.0"], [" in", "1.0"], [" the", "0.99995744"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "0.96056384"], [" States", "1.0"], [" Mint", "1.0"], [" is", "1.0"], [" located", "1.0"], [" in", "1.0"], [" Philadelphia", "1.0"], [".", "0.9998764"], [" The", "1.0"], [" official", "0.9999987"], [" name", "1.0"], [" of", "1.0"], [" the", "0.9998468"], [" city", "1.0"], [" in", "1.0"], [" which", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" Mint", "1.0"], [" is", "1.0"]]}, "489": {"qid": "2541f27af7753e1ec7c1", "term": "Pea", "description": "species of plant", "question": "Will twenty pea pods contents cover entire chess board?", "answer": true, "facts": ["Pea pods on average have 5 to 6 peas inside.", "A standard chess board has 64 squares."], "decomposition": ["On average, how many peas do twenty pea pods contain?", "How many squares does a standard chess board have?", "Is #1 greater than #2?"], "evidence": [[[["Pea-1"], "no_evidence"], [["Chessboard-3"]], ["no_evidence", "operation"]], [[["Pea-4"], "no_evidence"], [["Chessboard-3"]], ["operation"]], [[["Pea-1"], "no_evidence"], [["Chess-1"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human?\nFacts: The average weight of a human is about 100 pounds.\nAnswer (True or False): True\nQuestion: What is the average weight of a horse?\nFacts: The average weight of a horse is about 150 pounds.\nAnswer (True or False): True\nQuestion: What is the average weight of a cow?\nFacts: The average", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Will twenty pea pods contents cover entire chess board?\nFacts: Pea pods on average have 5 to 6 peas inside. A standard chess board has 64 squares.\nAnswer (True or False):", "ct_subject": "Pea pods on average have 5 to 6 peas inside. A standard chess board has 64 squares.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9963091"], [" is", "1.0"], [" the", "1.0"], [" average", "0.9975885"], [" weight", "1.0"], [" of", "1.0"], [" a", "0.9999994"], [" human", "1.0"], ["?", "0.9999975"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.999998"], [" average", "1.0"], [" weight", "0.9255576"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" is", "1.0"], [" about", "0.99999917"], [" 100", "0.63614494"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "0.10244028"], [" of", "1.0"], [" a", "1.0"], [" horse", "0.98016626"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" weight", "1.0"], [" of", "1.0"], [" a", "1.0"], [" horse", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 150", "0.97643363"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "0.9999974"], [" of", "1.0"], [" a", "1.0"], [" cow", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"]]}, "490": {"qid": "b2c17613452eb229fa92", "term": "Phobos (moon)", "description": "natural satellite of Mars", "question": "Is Phobos (moon) name origin similar to Roman god Pavor?", "answer": true, "facts": ["Phobos (moon) derives its name from ancient Greek mythology.", "Phobos was the god of fear.", "In Roman mythology, Pavor or Terror is known as the personification of fear."], "decomposition": ["What was Phobos (moon) named after?", "What is #1 referred to in Roman mythology?", "Is #2 the same as Pavor or Terror?"], "evidence": [[[["Phobos (moon)-2"]], [["Phobos (mythology)-2"]], ["operation"]], [[["Phobos (moon)-6"]], [["Phobos (mythology)-2"]], ["operation"]], [[["Phobos (mythology)-1"]], [["Phobos (mythology)-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the moon the same as the name of the planet Mars?\nFacts: The name of the moon is the same as the name of the planet Mars.\nAnswer (True or False): True\nQuestion: Is the name of the moon the same as the name of the planet Venus?", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Phobos (moon) name origin similar to Roman god Pavor?\nFacts: Phobos (moon) derives its name from ancient Greek mythology. Phobos was the god of fear. In Roman mythology, Pavor or Terror is known as the personification of fear.\nAnswer (True or False):", "ct_subject": "Phobos (moon) derives its name from ancient Greek mythology. Phobos was the god of fear. In Roman mythology, Pavor or Terror is known as the personification of fear.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "0.999949"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" moon", "0.9318163"], [" the", "0.9613256"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" planet", "0.9988211"], [" Mars", "0.99992335"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" is", "1.0"], [" the", "0.99608946"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" planet", "1.0"], [" Mars", "1.0"], [".", "1.0"], ["\n", "0.9996045"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" moon", "0.9835435"], [" the", "1.0"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" planet", "1.0"], [" Venus", "1.0"], ["?", "1.0"]]}, "491": {"qid": "64354cb6ee6d987d4f0f", "term": "Ice", "description": "water frozen into the solid state", "question": "Would a diet of ice eventually kill a person?", "answer": true, "facts": ["Humans can survive without water for four days.", "Ice can be melted into water, which consists of hydrogen and oxygen, using a simple cigarette lighter.", "Humans can survive without food for 30 to 40 days on average.", "Humans need carbohydrates, proteins, and fats that are contained in foods.", "Water does not contain fat, carbohydrates or protein."], "decomposition": ["Ice is the solid state of what?", "What nutrients are needed to sustain human life?", "Are most of #2 absent from #1?"], "evidence": [[[["Ice-7"]], [["Table of food nutrients-1"]], [["Table of food nutrients-1"], "operation"]], [[["Ice-1"]], [["Nutrient-1"]], [["Water-1"], "operation"]], [[["Ice-1"]], [["Food-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a human and a dog?\nFacts: Dogs are mammals. Humans are primates. Dogs are mammals. Humans are primates.\nAnswer (True or False): True\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a diet of ice eventually kill a person?\nFacts: Humans can survive without water for four days. Ice can be melted into water, which consists of hydrogen and oxygen, using a simple cigarette lighter. Humans can survive without food for 30 to 40 days on average. Humans need carbohydrates, proteins, and fats that are contained in foods. Water does not contain fat, carbohydrates or protein.\nAnswer (True or False):", "ct_subject": "Humans can survive without water for four days. Ice can be melted into water, which consists of hydrogen and oxygen, using a simple cigarette lighter. Humans can survive without food for 30 to 40 days on average. Humans need carbohydrates, proteins, and fats that are contained in foods. Water does not contain fat, carbohydrates or protein.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9994844"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.8725051"], [" between", "1.0"], [" a", "1.0"], [" human", "0.981987"], [" and", "1.0"], [" a", "1.0"], [" dog", "0.9999918"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Dogs", "0.99967515"], [" are", "0.9999999"], [" mammals", "1.0"], [".", "0.99803823"], [" Humans", "1.0"], [" are", "1.0"], [" primates", "0.98299325"], [".", "1.0"], [" Dogs", "0.07052758"], [" are", "0.9999981"], [" mammals", "0.9999517"], [".", "1.0"], [" Humans", "1.0"], [" are", "1.0"], [" primates", "1.0"], [".", "1.0"], ["\n", "0.9996314"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"]]}, "492": {"qid": "bd3caa68cb957d27b0e2", "term": "Lighthouse of Alexandria", "description": "Ancient lighthouse in Egypt", "question": "Were Greeks essential to crafting Egyptian Lighthouse of Alexandria?", "answer": true, "facts": ["The Lighthouse of Alexandria was an impressive monument in Egypt.", "The Lighthouse of Alexandria was built by pharaoh Ptolemy II.", "Ptolemy II was the son of Ptolemy I Soter.", "Ptolemy I Soter was a Greek bodyguard of Alexander the Great and became pharaoh of Egypt."], "decomposition": ["Who built the Lighthouse of Alexandria?", "Who was #1's father?", "Was #2 Greek?"], "evidence": [[[["Lighthouse of Alexandria-7", "Sostratus of Cnidus-1"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Lighthouse of Alexandria-6"]], [["Ptolemy I Soter-2"]], [["Philip II of Macedon-1"], "operation"]], [[["Lighthouse of Alexandria-6"]], [["Ptolemy I Soter-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Romans build the Great Wall of China?\nFacts: The Great Wall of China was built by the Chinese. The Great Wall of China was built by the Chinese. The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Were Greeks essential to crafting Egyptian Lighthouse of Alexandria?\nFacts: The Lighthouse of Alexandria was an impressive monument in Egypt. The Lighthouse of Alexandria was built by pharaoh Ptolemy II. Ptolemy II was the son of Ptolemy I Soter. Ptolemy I Soter was a Greek bodyguard of Alexander the Great and became pharaoh of Egypt.\nAnswer (True or False):", "ct_subject": "The Lighthouse of Alexandria was an impressive monument in Egypt. The Lighthouse of Alexandria was built by pharaoh Ptolemy II. Ptolemy II was the son of Ptolemy I Soter. Ptolemy I Soter was a Greek bodyguard of Alexander the Great and became pharaoh of Egypt.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.7707514"], [" the", "1.0"], [" Romans", "0.9999641"], [" build", "0.99947625"], [" the", "1.0"], [" Great", "1.0"], [" Wall", "1.0"], [" of", "1.0"], [" China", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Great", "1.0"], [" Wall", "1.0"], [" of", "1.0"], [" China", "1.0"], [" was", "1.0"], [" built", "1.0"], [" by", "1.0"], [" the", "1.0"], [" Chinese", "0.99941814"], [".", "1.0"], [" The", "1.0"], [" Great", "1.0"], [" Wall", "1.0"], [" of", "1.0"], [" China", "1.0"], [" was", "1.0"], [" built", "1.0"], [" by", "1.0"], [" the", "1.0"], [" Chinese", "1.0"], [".", "1.0"], [" The", "0.99999964"]]}, "493": {"qid": "4c495564c4165b2d5d16", "term": "Vitamin C", "description": "nutrient found in citrus fruits and other foods", "question": "Would drinking a glass of lemonade provide Vitamin C?", "answer": true, "facts": ["Lemonade is made from lemons, sugar, and water.", "Lemons are a citrus fruit.", "Citrus fruit is high in Vitamin C."], "decomposition": ["What are the ingredients of lemonade?", "Are any of #1 high in Vitamin C?"], "evidence": [[[["Lemonade-1"]], ["no_evidence"]], [[["Lemonade-2"]], [["Lemon-6"], "operation"]], [[["Lemonade-2"]], [["Lemon-12"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"cotton ball\" and a \"cotton baller\"?\nFacts: A cotton ball is a small, round, soft, cotton ball. A cotton baller is a large, round, soft, cotton ball.\nAnswer (True or False): True\nQuestion: What is the difference between a \"cotton ball\" and a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would drinking a glass of lemonade provide Vitamin C?\nFacts: Lemonade is made from lemons, sugar, and water. Lemons are a citrus fruit. Citrus fruit is high in Vitamin C.\nAnswer (True or False):", "ct_subject": "Lemonade is made from lemons, sugar, and water. Lemons are a citrus fruit. Citrus fruit is high in Vitamin C.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999987"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.98763007"], [" between", "1.0"], [" a", "0.9999999"], [" \"", "0.9999963"], ["c", "0.24879424"], ["otton", "0.43581367"], [" ball", "0.99138343"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["otton", "1.0"], [" ball", "1.0"], ["er", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" cotton", "1.0"], [" ball", "1.0"], [" is", "1.0"], [" a", "1.0"], [" small", "0.020832254"], [",", "0.96534234"], [" round", "0.9999864"], [",", "1.0"], [" soft", "0.999998"], [",", "0.9993247"], [" cotton", "0.99631196"], [" ball", "1.0"], [".", "1.0"], [" A", "1.0"], [" cotton", "1.0"], [" ball", "1.0"], ["er", "1.0"], [" is", "1.0"], [" a", "1.0"], [" large", "0.9999995"], [",", "1.0"], [" round", "1.0"], [",", "1.0"], [" soft", "1.0"], [",", "1.0"], [" cotton", "1.0"], [" ball", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["otton", "1.0"], [" ball", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"]]}, "494": {"qid": "b9d5010aaef9115f77e7", "term": "Tom Cruise", "description": "American actor and producer", "question": "Could Tom Cruise explain mental auditing?", "answer": true, "facts": ["Mental auditing is a practice within the church of Scientology.", "Tom Cruise is a long standing member of the church of Scientology and is high in the ranks."], "decomposition": ["What church practices mental auditing?", "Is Tom Cruise a member of #1?"], "evidence": [[[["Auditing (Scientology)-1", "Auditing (Scientology)-2"]], ["no_evidence", "operation"]], [[["Scientology beliefs and practices-1"]], [["Tom Cruise-36"]]], [[["Auditing (Scientology)-1"]], [["Tom Cruise-4"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between the \"Mormon\" church and the \"Christian\" church?\nFacts: The Mormon church is a cult. The Christian church is a religion.\nAnswer (True or False): True\nQuestion: What is the difference between the \"Mormon\" church and the \"Christian\" church?\nFacts: The Mormon church is a cult. The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could Tom Cruise explain mental auditing?\nFacts: Mental auditing is a practice within the church of Scientology. Tom Cruise is a long standing member of the church of Scientology and is high in the ranks.\nAnswer (True or False):", "ct_subject": "Mental auditing is a practice within the church of Scientology. Tom Cruise is a long standing member of the church of Scientology and is high in the ranks.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.6568016"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9996884"], [" between", "1.0"], [" the", "0.9406842"], [" \"", "0.9960517"], ["M", "0.5027411"], ["ormon", "0.99999976"], ["\"", "0.999967"], [" church", "0.9386182"], [" and", "1.0"], [" the", "1.0"], [" \"", "1.0"], ["Christian", "0.9996141"], ["\"", "1.0"], [" church", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Mormon", "0.9999901"], [" church", "1.0"], [" is", "1.0"], [" a", "1.0"], [" cult", "0.9973953"], [".", "0.99999976"], [" The", "1.0"], [" Christian", "0.99998486"], [" church", "1.0"], [" is", "1.0"], [" a", "1.0"], [" religion", "0.99999976"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" the", "1.0"], [" \"", "1.0"], ["M", "1.0"], ["ormon", "1.0"], ["\"", "1.0"], [" church", "1.0"], [" and", "1.0"], [" the", "1.0"], [" \"", "1.0"], ["Christian", "1.0"], ["\"", "1.0"], [" church", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Mormon", "1.0"], [" church", "1.0"], [" is", "1.0"], [" a", "1.0"], [" cult", "1.0"], [".", "1.0"], [" The", "1.0"]]}, "495": {"qid": "fe5428059eda37cc96c2", "term": "Mount Sharp", "description": "mountain on Mars", "question": "Are human footprints absent from Mount Sharp?", "answer": true, "facts": ["Mount Sharp is located on Mars.", "Human beings have not traveled to Mars.", "Human footprints could only be present if human feet touched down on Mount Sharp."], "decomposition": ["Where is Mount Sharp?", "What would produce a human footprint?", "Have #2 never traveled to #1?"], "evidence": [[[["Mount Sharp-1"]], [["Footprint-1"]], ["operation"]], [[["Mount Sharp-1"]], [["Footprint-1"]], [["Human mission to Mars-73"], "operation"]], [[["Mount Sharp-1"]], [["Footprint-1"]], [["Human mission to Mars-2"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the moon made of cheese?\nFacts: The moon is made of rock and ice. Cheese is made of milk and water.\nAnswer (True or False): True\nQuestion: Is the moon made of cheese?\nFacts: The moon is made of rock and ice. Cheese is made of milk and water.\nAnswer (True or False): True\nQuestion:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are human footprints absent from Mount Sharp?\nFacts: Mount Sharp is located on Mars. Human beings have not traveled to Mars. Human footprints could only be present if human feet touched down on Mount Sharp.\nAnswer (True or False):", "ct_subject": "Mount Sharp is located on Mars. Human beings have not traveled to Mars. Human footprints could only be present if human feet touched down on Mount Sharp.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.5760533"], [" the", "0.9999007"], [" moon", "0.9803898"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" moon", "1.0"], [" is", "1.0"], [" made", "1.0"], [" of", "1.0"], [" rock", "0.90043545"], [" and", "0.70674103"], [" ice", "0.99999297"], [".", "1.0"], [" Cheese", "0.99999547"], [" is", "1.0"], [" made", "1.0"], [" of", "1.0"], [" milk", "1.0"], [" and", "0.46791676"], [" water", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" moon", "0.9848962"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "0.99999964"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" moon", "1.0"], [" is", "1.0"], [" made", "1.0"], [" of", "1.0"], [" rock", "1.0"], [" and", "1.0"], [" ice", "1.0"], [".", "1.0"], [" Cheese", "1.0"], [" is", "1.0"], [" made", "1.0"], [" of", "1.0"], [" milk", "1.0"], [" and", "1.0"], [" water", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"]]}, "496": {"qid": "9e7b4c746b598d1521e0", "term": "Geometry", "description": "Branch of mathematics that studies the shape, size and position of objects", "question": "Do carpenters understand geometry?", "answer": true, "facts": ["Carpenters work in building and maintaining structures such as homes, buildings, and gazebos.", "In order to build a home, one must be able to follow the geometry in the blueprints. "], "decomposition": ["What kind of buildings/structures do carpenters help in constructing?", "Do #1 require knowledge of geometry to carry out?"], "evidence": [[[["Carpentry-1"]], [["Geometry-38"], "operation"]], [[["Carpentry-1"]], [["Geometry-1"], "operation"]], [[["Carpentry-1"]], [["Geometry-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"carpenter\" and a \"carpenter\"?\nFacts: A carpenter is a person who builds structures such as homes, buildings, and gazebos. A Carpenter is a person who builds structures such as homes, buildings, and gazebos.\nAnswer (True or", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do carpenters understand geometry?\nFacts: Carpenters work in building and maintaining structures such as homes, buildings, and gazebos. In order to build a home, one must be able to follow the geometry in the blueprints. \nAnswer (True or False):", "ct_subject": "Carpenters work in building and maintaining structures such as homes, buildings, and gazebos. In order to build a home, one must be able to follow the geometry in the blueprints. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9900003"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.99995005"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.1601471"], ["c", "0.9219138"], ["arp", "1.0"], ["enter", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["car", "0.1586768"], ["penter", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9732871"], [" car", "0.9999962"], ["penter", "1.0"], [" is", "1.0"], [" a", "0.9999982"], [" person", "1.0"], [" who", "1.0"], [" builds", "0.9831603"], [" structures", "0.74311787"], [" such", "0.26673314"], [" as", "1.0"], [" homes", "1.0"], [",", "1.0"], [" buildings", "1.0"], [",", "1.0"], [" and", "1.0"], [" g", "1.0"], ["az", "1.0"], ["eb", "1.0"], ["os", "1.0"], [".", "1.0"], [" A", "1.0"], [" Carpenter", "0.99999785"], [" is", "1.0"], [" a", "1.0"], [" person", "1.0"], [" who", "1.0"], [" builds", "0.99999976"], [" structures", "1.0"], [" such", "1.0"], [" as", "1.0"], [" homes", "1.0"], [",", "1.0"], [" buildings", "1.0"], [",", "1.0"], [" and", "1.0"], [" g", "1.0"], ["az", "1.0"], ["eb", "1.0"], ["os", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"]]}, "497": {"qid": "9390df61207ef77f8ba0", "term": "Swan", "description": "large water bird", "question": "Would WWF be angrier if you killed koala instead of black swan?", "answer": true, "facts": ["The WWF is an international organization that works for the preservation of animals.", "Black swans are designated as least concern species meaning they are not close to being endangered.", "Koalas are designated as  vulnerable to extinction\u2014just a step above endangered."], "decomposition": ["What is the black swan's listing on the IUCN red list?", "How is the Koala listed on the IUCN red list?", "What does WWF represent?", "Considering #3, is #2 in more dire straits than #1?"], "evidence": [[[["Black swan-24"]], [["Koala-3"]], [["World Wide Fund for Nature-1"]], ["operation"]], [[["Black swan-24"]], [["Koala-44"]], [["World Wide Fund for Nature-10"]], ["operation"]], [[["Black swan-13", "IUCN Red List-1"]], [["Koala-50", "Vulnerable species-1"], "no_evidence"], [["World Wide Fund for Nature-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"cute\" and a \"cute-ish\" animal?\nFacts: A \"cute\" animal is one that is cute to look at. A \"cute-ish\" animal is one that is cute", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would WWF be angrier if you killed koala instead of black swan?\nFacts: The WWF is an international organization that works for the preservation of animals. Black swans are designated as least concern species meaning they are not close to being endangered. Koalas are designated as vulnerable to extinction\u2014just a step above endangered.\nAnswer (True or False):", "ct_subject": "The WWF is an international organization that works for the preservation of animals. Black swans are designated as least concern species meaning they are not close to being endangered. Koalas are designated as vulnerable to extinction\u2014just a step above endangered.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999926"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.4780145"], ["c", "0.6936859"], ["ute", "0.9978131"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["ute", "1.0"], ["-", "0.99948704"], ["ish", "1.0"], ["\"", "1.0"], [" animal", "0.99988365"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" \"", "0.9825912"], ["c", "1.0"], ["ute", "1.0"], ["\"", "1.0"], [" animal", "1.0"], [" is", "1.0"], [" one", "1.0"], [" that", "1.0"], [" is", "1.0"], [" cute", "1.0"], [" to", "0.9857845"], [" look", "0.7368011"], [" at", "1.0"], [".", "0.9999994"], [" A", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["ute", "1.0"], ["-", "1.0"], ["ish", "1.0"], ["\"", "1.0"], [" animal", "1.0"], [" is", "1.0"], [" one", "1.0"], [" that", "1.0"], [" is", "1.0"], [" cute", "1.0"]]}, "498": {"qid": "5416d6ace8e5bef7e467", "term": "Tick", "description": "order of arachnids", "question": "Could a nymph tick pass through a standard hole punch?", "answer": true, "facts": ["A nymph tick is the size of a poppy seed.", "A poppy seed is around 1mm in size.", "The ISO 838 standards set a hole punch size at 6 mm."], "decomposition": ["What is a nymph tick comparable in size to?", "How big around is #1?", "What is the diameter of a standard hole punch?", "Is #3 greater than or equal to #2?"], "evidence": [[[["Ixodes pacificus-11"]], [["Ixodes pacificus-11"], "no_evidence"], [["Hole punch-6"]], ["operation"]], [[["Tick-1"], "no_evidence"], [["Tick-1"]], [["Hole punch-10"]], ["operation"]], [[["Tick-1"], "no_evidence"], [["Tick-1"]], [["Hole punch-6"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a standard hole punch and a standard punch?\nFacts: A standard hole punch is a hole punch that is used to punch holes in standard paper. A standard punch is a hole punch that is used to punch holes in standard paper.\nAnswer (True or False): True\nQuestion: What is the difference", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could a nymph tick pass through a standard hole punch?\nFacts: A nymph tick is the size of a poppy seed. A poppy seed is around 1mm in size. The ISO 838 standards set a hole punch size at 6 mm.\nAnswer (True or False):", "ct_subject": "A nymph tick is the size of a poppy seed. A poppy seed is around 1mm in size. The ISO 838 standards set a hole punch size at 6 mm.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99920005"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9920438"], [" between", "1.0"], [" a", "1.0"], [" standard", "0.99986696"], [" hole", "0.9999999"], [" punch", "1.0"], [" and", "1.0"], [" a", "1.0"], [" standard", "0.52509606"], [" punch", "0.99999917"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999994"], [" standard", "1.0"], [" hole", "0.95568377"], [" punch", "1.0"], [" is", "1.0"], [" a", "0.9999999"], [" hole", "0.99998224"], [" punch", "1.0"], [" that", "1.0"], [" is", "0.73473257"], [" used", "0.9949473"], [" to", "0.9996443"], [" punch", "1.0"], [" holes", "1.0"], [" in", "1.0"], [" standard", "0.62787634"], [" paper", "0.065319896"], [".", "1.0"], [" A", "1.0"], [" standard", "1.0"], [" punch", "1.0"], [" is", "1.0"], [" a", "1.0"], [" hole", "0.99999964"], [" punch", "1.0"], [" that", "1.0"], [" is", "1.0"], [" used", "1.0"], [" to", "1.0"], [" punch", "1.0"], [" holes", "1.0"], [" in", "1.0"], [" standard", "0.9999994"], [" paper", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"]]}, "499": {"qid": "f8bf74ed2a1f4005a37f", "term": "Clouded leopard", "description": "species of mammal found from the Himalayan foothills through mainland Southeast Asia into China", "question": "Would a clouded leopard encounter an awake pangolin?", "answer": true, "facts": ["Pangolins and clouded leopards have an overlap of ranges", "Pangolins are nocturnal", "Clouded leopards are nocturnal"], "decomposition": ["What is the range of the clouded leopard?", "What time of day is the clouded leopard active?", "What is the range of the pangolin?", "What time of day is the pangolin active?", "Do #1 and #3 overlap while #2 and #4 overlap?"], "evidence": [[[["Clouded leopard-1"]], [["Clouded leopard-24"]], [["Pangolin-1"]], [["Pangolin-2"]], [["Clouded leopard-1", "Clouded leopard-24", "Pangolin-1", "Pangolin-2"]]], [[["Clouded leopard-1"]], [["Clouded leopard-24"]], [["Pangolin-1"], "no_evidence"], [["Pangolin-2"]], ["no_evidence", "operation"]], [[["Clouded leopard-1"]], [["Clouded leopard-24"]], [["Pangolin-1"]], [["Pangolin-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance a clouded leopard has ever traveled?\nFacts: The longest distance a clouded leopard has ever traveled is about 1,000 miles.\nAnswer (True or False): True\nQuestion: What is the longest distance a clouded leopard has ever traveled?\nFacts: The longest distance a clouded", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a clouded leopard encounter an awake pangolin?\nFacts: Pangolins and clouded leopards have an overlap of ranges Pangolins are nocturnal Clouded leopards are nocturnal\nAnswer (True or False):", "ct_subject": "Pangolins and clouded leopards have an overlap of ranges Pangolins are nocturnal Clouded leopards are nocturnal", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99988985"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.7681108"], [" distance", "0.9999852"], [" a", "0.9998801"], [" cloud", "1.0"], ["ed", "1.0"], [" le", "1.0"], ["opard", "1.0"], [" has", "0.9919275"], [" ever", "0.9999999"], [" traveled", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99878687"], [" longest", "0.9999794"], [" distance", "1.0"], [" a", "1.0"], [" cloud", "1.0"], ["ed", "1.0"], [" le", "1.0"], ["opard", "1.0"], [" has", "1.0"], [" ever", "1.0"], [" traveled", "1.0"], [" is", "1.0"], [" about", "0.99999416"], [" 1", "0.9999894"], [",", "1.0"], ["000", "0.99999094"], [" miles", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" cloud", "0.9999937"], ["ed", "1.0"], [" le", "1.0"], ["opard", "1.0"], [" has", "1.0"], [" ever", "1.0"], [" traveled", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" cloud", "1.0"], ["ed", "1.0"]]}, "500": {"qid": "ffa3242f6eacd0271ac2", "term": "Alligator", "description": "Genus of large reptiles", "question": "Would a crocodile survive longer in Great Salt Lake than alligator?", "answer": true, "facts": ["The Great Salt Lake is a Utah lake composed of salt water.", "Crocodiles natural habitat is salt water and they can breathe underwater for hours.", "Alligators have a natural habitat of fresh water."], "decomposition": ["What kind of water habitat is the Great Salt Lake?", "Which water habitats are crocodiles adapted to survive in?", "Which water habitats are alligators adapted to survive in?", "Is #1 included in #2 and excluded from #3?"], "evidence": [[[["Great Salt Lake-1"]], [["Crocodile-2", "Crocodile-3"]], [["Alligator-8"]], ["operation"]], [[["Great Salt Lake-22"]], [["Crocodile-3"]], [["Alligator-8"]], ["operation"]], [[["Great Salt Lake-1"]], [["Crocodile-2"]], [["Crocodile-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance a person can walk in one day?\nFacts: The longest distance a person can walk in one day is about 2,000 miles.\nAnswer (True or False): True\nQuestion: What is the longest distance a person can walk in one year?\nFacts: The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a crocodile survive longer in Great Salt Lake than alligator?\nFacts: The Great Salt Lake is a Utah lake composed of salt water. Crocodiles natural habitat is salt water and they can breathe underwater for hours. Alligators have a natural habitat of fresh water.\nAnswer (True or False):", "ct_subject": "The Great Salt Lake is a Utah lake composed of salt water. Crocodiles natural habitat is salt water and they can breathe underwater for hours. Alligators have a natural habitat of fresh water.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.999329"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.9857222"], [" distance", "0.9999801"], [" a", "0.89805347"], [" person", "0.99552095"], [" can", "0.9948096"], [" walk", "0.99999976"], [" in", "0.99999034"], [" one", "0.5222631"], [" day", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999999"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" person", "1.0"], [" can", "1.0"], [" walk", "1.0"], [" in", "1.0"], [" one", "1.0"], [" day", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 2", "0.98039424"], [",", "1.0"], ["000", "0.9999989"], [" miles", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" person", "1.0"], [" can", "1.0"], [" walk", "1.0"], [" in", "1.0"], [" one", "1.0"], [" year", "0.9980963"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"]]}, "501": {"qid": "6982e0eb7df792f606c9", "term": "Art dealer", "description": "person that buys and sells works of art", "question": "Can an art dealer buy Boeing 737-800 with a Da Vinci painting?", "answer": true, "facts": ["The Boeing 737-800 plane costs 106 million dollars in 2019.", "Salvator Mundi, a painting attributed to Leonardo Da Vinci, is the most expensive painting ever sold.", "Salvator Mundi sold for over 450 million dollars."], "decomposition": ["How much does a  Boeing 737-800 cost?", "How much did Da Vinci's highest priced painting sell for?", "Is #2 more than #1?"], "evidence": [[[["Boeing 737 Next Generation-42"], "no_evidence"], [["Leonardo da Vinci-92"]], ["no_evidence", "operation"]], [["no_evidence"], [["Salvator Mundi (Leonardo)-13"]], ["operation"]], [[["Boeing 737 Next Generation-39"]], [["Leonardo da Vinci-92"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the most expensive car ever sold?\nFacts: The most expensive car ever sold is a Ferrari F430. The Ferrari F430 costs $1.5 million.\nAnswer (True or False): True\nQuestion: What is the most expensive house ever", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can an art dealer buy Boeing 737-800 with a Da Vinci painting?\nFacts: The Boeing 737-800 plane costs 106 million dollars in 2019. Salvator Mundi, a painting attributed to Leonardo Da Vinci, is the most expensive painting ever sold. Salvator Mundi sold for over 450 million dollars.\nAnswer (True or False):", "ct_subject": "The Boeing 737-800 plane costs 106 million dollars in 2019. Salvator Mundi, a painting attributed to Leonardo Da Vinci, is the most expensive painting ever sold. Salvator Mundi sold for over 450 million dollars.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9928054"], [" is", "1.0"], [" the", "1.0"], [" most", "0.89375067"], [" expensive", "1.0"], [" car", "0.99989164"], [" ever", "0.98929685"], [" sold", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" most", "1.0"], [" expensive", "1.0"], [" car", "1.0"], [" ever", "1.0"], [" sold", "1.0"], [" is", "0.9999646"], [" a", "0.9984937"], [" Ferrari", "0.9586483"], [" F", "0.497272"], ["430", "1.0"], [".", "0.78212726"], [" The", "0.99472576"], [" Ferrari", "1.0"], [" F", "1.0"], ["430", "1.0"], [" costs", "0.9463407"], [" $", "0.9995945"], ["1", "1.0"], [".", "0.9999862"], ["5", "0.9995746"], [" million", "1.0"], [".", "0.9999994"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" most", "1.0"], [" expensive", "1.0"], [" house", "0.9999999"], [" ever", "1.0"]]}, "502": {"qid": "000275e7aa81e30885e7", "term": "Numerology", "description": "any study of the purported divine, mystical or other special relationship between a number and some coinciding observed (or perceived) events", "question": "Has numerology helped shape hotel layouts?", "answer": true, "facts": ["Numerology is the study of numbers and how they relate to events.", "Numbers such as 3 and 7 hold biblical significance.", "Numbers such as 6 and 13 are said to be unlucky.", "The thirteenth floor is a designation of a level of a multi-level building that is often omitted in countries where the number 13 is considered unlucky.", "Many hotels do not have thirteenth floors because of the enduring superstition."], "decomposition": ["What numbers are often considered unlucky?", "What number is usually omitted in numbering hotel floors?", "Is #2 part of #1?"], "evidence": [[[["13 (number)-14"], "no_evidence"], [["13 (number)-14"]], ["operation"]], [[["13 (number)-14"]], [["13 (number)-14", "Thirteenth floor-2"]], ["operation"]], [[["13 (number)-16", "Number of the Beast-1"], "no_evidence"], [["Thirteenth floor-7"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the number of the year?\nFacts: The number of the year is 365.2422 days.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Has numerology helped shape hotel layouts?\nFacts: Numerology is the study of numbers and how they relate to events. Numbers such as 3 and 7 hold biblical significance. Numbers such as 6 and 13 are said to be unlucky. The thirteenth floor is a designation of a level of a multi-level building that is often omitted in countries where the number 13 is considered unlucky. Many hotels do not have thirteenth floors because of the enduring superstition.\nAnswer (True or False):", "ct_subject": "Numerology is the study of numbers and how they relate to events. Numbers such as 3 and 7 hold biblical significance. Numbers such as 6 and 13 are said to be unlucky. The thirteenth floor is a designation of a level of a multi-level building that is often omitted in countries where the number 13 is considered unlucky. Many hotels do not have thirteenth floors because of the enduring superstition.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99662566"], [" is", "1.0"], [" the", "1.0"], [" number", "0.5715587"], [" of", "1.0"], [" the", "1.0"], [" year", "0.99960047"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" number", "0.9999993"], [" of", "1.0"], [" the", "1.0"], [" year", "1.0"], [" is", "1.0"], [" 365", "1.0"], [".", "1.0"], ["24", "0.9999995"], ["22", "0.9808553"], [" days", "0.9998596"], [".", "1.0"]]}, "503": {"qid": "4e694a71502c2e750c00", "term": "Anchovy", "description": "Family of fishes", "question": "Are anchovies associated with Italian food?", "answer": true, "facts": ["Pizza is an Italian food.", "Anchovies are occasionally used as a pizza topping."], "decomposition": ["What food item are anchovies sometimes eaten with?", "Is #1 an Italian food?"], "evidence": [[[["Anchovies as food-5"]], [["Anchovies as food-5"], "operation"]], [[["Anchovies as food-3"]], [["Pizza-1"], "operation"]], [[["Anchovies as food-3"]], [["Pizza-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any other animals that have been associated with the name of a city?\nFacts: There are many other animals that have been associated with the name of a city. For example, the name of the city of Rome is derived from the Roman god of agriculture, Romulus.\nAnswer (True or False): True\nQuestion: Are there any other animals that have been associated with the name of a country?\nFacts: There are many", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are anchovies associated with Italian food?\nFacts: Pizza is an Italian food. Anchovies are occasionally used as a pizza topping.\nAnswer (True or False):", "ct_subject": "Pizza is an Italian food. Anchovies are occasionally used as a pizza topping.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "0.9999974"], [" any", "0.9989918"], [" other", "0.9980592"], [" animals", "0.99998784"], [" that", "1.0"], [" have", "0.849079"], [" been", "0.9999999"], [" associated", "0.96602774"], [" with", "1.0"], [" the", "1.0"], [" name", "0.26439992"], [" of", "0.9999958"], [" a", "1.0"], [" city", "0.9735806"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "0.99999607"], [" are", "1.0"], [" many", "0.99999976"], [" other", "0.9988753"], [" animals", "1.0"], [" that", "1.0"], [" have", "1.0"], [" been", "1.0"], [" associated", "1.0"], [" with", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" a", "1.0"], [" city", "1.0"], [".", "1.0"], [" For", "0.32623914"], [" example", "1.0"], [",", "1.0"], [" the", "1.0"], [" name", "0.66544616"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "0.9999856"], [" is", "0.99998367"], [" derived", "0.99999857"], [" from", "1.0"], [" the", "1.0"], [" Roman", "0.26656306"], [" god", "0.9999851"], [" of", "1.0"], [" agriculture", "0.8529865"], [",", "1.0"], [" Rom", "0.96935564"], ["ulus", "1.0"], [".", "1.0"], ["\n", "0.99932337"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "1.0"], [" any", "1.0"], [" other", "0.91774493"], [" animals", "1.0"], [" that", "1.0"], [" have", "1.0"], [" been", "1.0"], [" associated", "1.0"], [" with", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" a", "1.0"], [" country", "0.48901165"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "1.0"], [" are", "1.0"], [" many", "1.0"]]}, "504": {"qid": "9527809204a14d94be67", "term": "Onion", "description": "vegetable", "question": "Can chemicals in onion help create a thermonuclear bomb?", "answer": true, "facts": ["A thermonuclear bomb, also called a hydrogen bomb, uses hydrogen under high temperatures to create an explosive reaction.", "While chopping onions, cells inside the onion are broken and the gas that comes out forms sulfenic acid.", "Sulfenic acid is composed of several elements including hydrogen."], "decomposition": ["What elements are used in a thermonuclear bomb?", "When onions are chopped what gas is released?", "What elements are found in #2?", "Is #1 a subset of #3?"], "evidence": [[[["Thermonuclear weapon-2"]], [["Onion-30"], "no_evidence"], [["Syn-Propanethial-S-oxide-1"], "no_evidence"], ["operation"]], [[["Thermonuclear weapon-1"], "no_evidence"], [["Onion-30"]], ["operation"], ["no_evidence", "operation"]], [[["Thermonuclear weapon-2"]], [["Onion-30"]], [["Syn-Propanethial-S-oxide-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"salt\" and a \"salt and pepper\"?\nFacts: Salt is a mineral that is found in the earth's crust. Salt is a mineral that is found in the earth's crust.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can chemicals in onion help create a thermonuclear bomb?\nFacts: A thermonuclear bomb, also called a hydrogen bomb, uses hydrogen under high temperatures to create an explosive reaction. While chopping onions, cells inside the onion are broken and the gas that comes out forms sulfenic acid. Sulfenic acid is composed of several elements including hydrogen.\nAnswer (True or False):", "ct_subject": "A thermonuclear bomb, also called a hydrogen bomb, uses hydrogen under high temperatures to create an explosive reaction. While chopping onions, cells inside the onion are broken and the gas that comes out forms sulfenic acid. Sulfenic acid is composed of several elements including hydrogen.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.8685714"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.99999404"], [" between", "1.0"], [" a", "0.9999999"], [" \"", "0.999767"], ["s", "0.9923074"], ["alt", "0.9919641"], ["\"", "0.99999785"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["alt", "1.0"], [" and", "0.9912881"], [" pepper", "1.0"], ["\"?", "0.99186397"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Salt", "0.99790287"], [" is", "1.0"], [" a", "1.0"], [" mineral", "0.99993193"], [" that", "1.0"], [" is", "0.99999857"], [" found", "0.99999785"], [" in", "1.0"], [" the", "0.9999999"], [" earth", "1.0"], ["'s", "0.9991013"], [" crust", "1.0"], [".", "1.0"], [" Salt", "0.9999504"], [" is", "0.9999893"], [" a", "0.9796998"], [" mineral", "0.9999994"], [" that", "1.0"], [" is", "1.0"], [" found", "1.0"], [" in", "1.0"], [" the", "1.0"], [" earth", "1.0"], ["'s", "1.0"], [" crust", "1.0"], [".", "1.0"]]}, "505": {"qid": "bad3d5551705406fe58d", "term": "Snowboarding", "description": "winter sport", "question": "Would it be difficult to snowboard on Venus?", "answer": true, "facts": ["Snowboarding involves descending a snow-covered slope while standing on a snowboard.", "Snow is formed by the freezing of water.", "Water has a freezing point of 32\u00b0F.", "Venus has a mean surface temperature of 737 K (464 \u00b0C; 867 \u00b0F)."], "decomposition": ["What kind of surface is suitable for snowboarding?", "What temperature range facilitates the formation of #1?", "What is the average surface temperature on Venus?", "Is #3 within #2?"], "evidence": [[[["Snowboarding-1"]], [["Snow-16"]], [["Venus-2"]], ["operation"]], [[["Snowboarding-1"]], [["Freezing-5"], "no_evidence"], [["Venus-2"]], ["operation"]], [[["Snowboarding-1"]], [["Snow-3"], "no_evidence"], [["Venus-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human?\nFacts: The average human weight is about 100 pounds (45 kg).\nAnswer (True or False): True\nQuestion: What is the average height of a human?\nFacts: The average human", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would it be difficult to snowboard on Venus?\nFacts: Snowboarding involves descending a snow-covered slope while standing on a snowboard. Snow is formed by the freezing of water. Water has a freezing point of 32\u00b0F. Venus has a mean surface temperature of 737 K (464 \u00b0C; 867 \u00b0F).\nAnswer (True or False):", "ct_subject": "Snowboarding involves descending a snow-covered slope while standing on a snowboard. Snow is formed by the freezing of water. Water has a freezing point of 32\u00b0F. Venus has a mean surface temperature of 737 K (464 \u00b0C; 867 \u00b0F).", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999917"], [" is", "1.0"], [" the", "1.0"], [" average", "0.32241786"], [" weight", "0.991397"], [" of", "1.0"], [" a", "0.999997"], [" human", "1.0"], ["?", "0.99998355"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.19915219"], [" weight", "0.99999225"], [" is", "1.0"], [" about", "0.9999075"], [" 100", "0.9747322"], [" pounds", "0.99999714"], [" (", "0.9816653"], ["45", "1.0"], [" kg", "0.99999833"], [").", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" height", "0.999746"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.9384368"]]}, "506": {"qid": "d7482b2dc4028be17b36", "term": "Pope Alexander VI", "description": "Pope of the Catholic Church 1492\u20131503", "question": "Were any of despised Pope Alexander VI's descendants canonized?", "answer": true, "facts": ["Pope Alexander the VI was a controversial pope born as Rodrigo Borgia.", "Rodrigo Borgia had several children including the despised Juan Borgia who was murdered in 1497.", "Juan Borgia's grandson, Francis Borgia, was a Jesuit priest and the third Superior General of the Society of Jesus.", "Canonization is the process by which the Catholic Church names someone a saint.", "Francis Borgia was named a Catholic saint in June 1670."], "decomposition": ["What dynastic house was Pope Alexander VI a member of?", "Were any members of #1 canonized?", "Was #2 a direct descendent of Alexander VI?"], "evidence": [[[["Pope Alexander VI-2"]], [["Francis Borgia, 4th Duke of Gand\u00eda-1"]], ["operation"]], [[["Pope Alexander VI-2"]], [["Francis Borgia, 4th Duke of Gand\u00eda-1"]], ["operation"]], [[["House of Borgia-2"]], [["Francis Borgia, 4th Duke of Gand\u00eda-1"]], [["House of Borgia-21"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Was the Pope Alexander VI canonized?\nFacts: Pope Alexander VI was a controversial", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Were any of despised Pope Alexander VI's descendants canonized?\nFacts: Pope Alexander the VI was a controversial pope born as Rodrigo Borgia. Rodrigo Borgia had several children including the despised Juan Borgia who was murdered in 1497. Juan Borgia's grandson, Francis Borgia, was a Jesuit priest and the third Superior General of the Society of Jesus. Canonization is the process by which the Catholic Church names someone a saint. Francis Borgia was named a Catholic saint in June 1670.\nAnswer (True or False):", "ct_subject": "Pope Alexander the VI was a controversial pope born as Rodrigo Borgia. Rodrigo Borgia had several children including the despised Juan Borgia who was murdered in 1497. Juan Borgia's grandson, Francis Borgia, was a Jesuit priest and the third Superior General of the Society of Jesus. Canonization is the process by which the Catholic Church names someone a saint. Francis Borgia was named a Catholic saint in June 1670.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "0.6702662"], [" the", "1.0"], [" Pope", "0.74215746"], [" Alexander", "0.9258787"], [" VI", "1.0"], [" canon", "0.8541144"], ["ized", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Pope", "1.0"], [" Alexander", "1.0"], [" VI", "1.0"], [" was", "1.0"], [" a", "0.9004409"], [" controversial", "1.0"]]}, "507": {"qid": "765ef4b32efee9947e5f", "term": "Charles Manson", "description": "American criminal, cult leader", "question": "Did any killer Manson band members were named for exceed Charles Manson's kills?", "answer": true, "facts": ["Many of the members of the band Marilyn Manson combined the names of a model or actress with a serial killer.", "Marilyn Manson band memberTwiggy Ramirez took his name from model Twiggy and serial killer Richard Ramirez.", "Richard Ramirez was charged with 13 counts of murder.", "Charles Manson was charged with 9 counts of murder."], "decomposition": ["What serial killers were members of the Manson band named after?", "How many counts of murder was Charles Manson charged with?", "Were the murder charges of any of #1 greater than #2?"], "evidence": [[[["Marilyn Manson (band)-4"]], [["Charles Manson-1"]], [["John Wayne Gacy-4"]]], [[["Charles Manson-1"], "no_evidence"], [["Charles Manson-1"]], ["operation"]], [[["Marilyn Manson (band)-1", "Marilyn Manson (band)-4"]], [["Charles Manson-1"]], [["Ted Bundy-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Beatles ever meet Charles Manson?\nFacts: The Beatles were in the studio recording their first album when they were approached by a man who said he was a friend of Charles", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did any killer Manson band members were named for exceed Charles Manson's kills?\nFacts: Many of the members of the band Marilyn Manson combined the names of a model or actress with a serial killer. Marilyn Manson band memberTwiggy Ramirez took his name from model Twiggy and serial killer Richard Ramirez. Richard Ramirez was charged with 13 counts of murder. Charles Manson was charged with 9 counts of murder.\nAnswer (True or False):", "ct_subject": "Many of the members of the band Marilyn Manson combined the names of a model or actress with a serial killer. Marilyn Manson band memberTwiggy Ramirez took his name from model Twiggy and serial killer Richard Ramirez. Richard Ramirez was charged with 13 counts of murder. Charles Manson was charged with 9 counts of murder.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99217236"], [" the", "0.9999517"], [" Beatles", "1.0"], [" ever", "0.92184937"], [" meet", "0.6313822"], [" Charles", "0.99993813"], [" Manson", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Beatles", "1.0"], [" were", "0.99883896"], [" in", "0.9169676"], [" the", "1.0"], [" studio", "0.9999949"], [" recording", "1.0"], [" their", "0.9999987"], [" first", "0.9999994"], [" album", "1.0"], [" when", "1.0"], [" they", "1.0"], [" were", "0.9971294"], [" approached", "0.99788207"], [" by", "1.0"], [" a", "0.9553925"], [" man", "0.9999994"], [" who", "0.99990666"], [" said", "0.18304731"], [" he", "1.0"], [" was", "1.0"], [" a", "0.9960139"], [" friend", "0.3752035"], [" of", "1.0"], [" Charles", "1.0"]]}, "508": {"qid": "406d6897eb20e5740d3f", "term": "Brewing", "description": "production of beer", "question": "Should Peter Griffin be an expert at the craft of brewing?", "answer": true, "facts": ["Peter Griffin is an employee of a brewery in Quahog. ", "Peter has worked at the brewery for many years and is expected to be familiar with how beer is made."], "decomposition": ["Where does Peter Griffin work?", "Is #1 a brewery?"], "evidence": [[[["Peter Griffin-2"]], ["operation"]], [[["Jungle Love (Family Guy)-3"]], ["operation"]], [[["Peter Griffin-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the capital of the state of Rhode Island?\nFacts: The city of Providence is the capital of Rhode Island.\nAnswer (True or False): True\nQuestion: What is the name of the city in the United States that is the capital of the state of Rhode Island", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Should Peter Griffin be an expert at the craft of brewing?\nFacts: Peter Griffin is an employee of a brewery in Quahog.  Peter has worked at the brewery for many years and is expected to be familiar with how beer is made.\nAnswer (True or False):", "ct_subject": "Peter Griffin is an employee of a brewery in Quahog.  Peter has worked at the brewery for many years and is expected to be familiar with how beer is made.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999993"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99976736"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9999583"], [" in", "0.49932095"], [" the", "0.9944237"], [" United", "0.99999976"], [" States", "1.0"], [" that", "1.0"], [" is", "0.9994991"], [" the", "1.0"], [" capital", "0.9994295"], [" of", "1.0"], [" the", "0.9998975"], [" state", "1.0"], [" of", "1.0"], [" Rhode", "0.9998017"], [" Island", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99998176"], [" city", "0.99928313"], [" of", "0.76008093"], [" Providence", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" Rhode", "1.0"], [" Island", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" Rhode", "0.9999703"], [" Island", "1.0"]]}, "509": {"qid": "ee1ccc88aa46999db8da", "term": "James Brown", "description": "American singer, songwriter, producer and bandleader from South Carolina", "question": "Could James Brown's ex-wives hold a doubles game of tennis?", "answer": true, "facts": ["James Brown had four ex-wives", "Doubles tennis requires two players per team, with two teams playing against each other"], "decomposition": ["How many people are required for tennis doubles?", "How many ex-wives did James Brown have?", "Is #2 greater than or equal to #1?"], "evidence": [[[["Tennis games-23"]], [["James Brown-62"]], ["operation"]], [[["Types of tennis match-3"]], [["James Brown-62"]], ["operation"]], [[["Tennis-75"]], [["James Brown-62"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: The first person to be born in the United States was a woman named Mary Ann Cotton.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: The first person to be born in the United States", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could James Brown's ex-wives hold a doubles game of tennis?\nFacts: James Brown had four ex-wives Doubles tennis requires two players per team, with two teams playing against each other\nAnswer (True or False):", "ct_subject": "James Brown had four ex-wives Doubles tennis requires two players per team, with two teams playing against each other", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9995271"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9792748"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9978654"], [" person", "0.8424524"], [" to", "1.0"], [" be", "0.890135"], [" born", "0.90989995"], [" in", "1.0"], [" the", "1.0"], [" United", "0.124473855"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9930352"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" a", "0.999607"], [" woman", "0.9996105"], [" named", "1.0"], [" Mary", "0.99283534"], [" Ann", "1.0"], [" Cotton", "1.0"], [".", "0.9999982"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"]]}, "510": {"qid": "a98478f4c66dea748297", "term": "Reality", "description": "Sum or aggregate of all that is real or existent", "question": "Could Plato have agreed with the beliefs of Jainism?", "answer": true, "facts": ["One principle of reality in Jainism is karma, or asrava.", "Jainism began around 500 B.C.", "Plato was born around 428 B.C., so he was alive while Jainism existed.", "Plato believed in karma and reincarnation."], "decomposition": ["What are the major beliefs in Jainism?", "What were Plato's major beliefs?", "When did Jainism begin?", "When was Plato born?", "Is there an overlap between #1 and #2, and is #4 more recent than #3?"], "evidence": [[[["Jainism-1"]], [["Plato-43"]], [["Jainism-1"]], [["Plato-1"]], ["operation"]], [[["Jainism-4"], "no_evidence"], [["Plato-3"], "no_evidence"], [["Jainism-61"]], [["Plato-1"]], ["operation"]], [[["Jainism-2"]], [["Plato-3", "Plato-39", "Plato-43"], "no_evidence"], [["Jainism-1"]], [["Plato-1"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the ancient Egyptians believe in the existence of the Loch Ness Monster?\nFacts: The Loch Ness Monster is a mythological creature that has been reported in the Scottish Highlands for centuries. The Loch Ness Monster is a mythological creature that has been reported", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could Plato have agreed with the beliefs of Jainism?\nFacts: One principle of reality in Jainism is karma, or asrava. Jainism began around 500 B.C. Plato was born around 428 B.C., so he was alive while Jainism existed. Plato believed in karma and reincarnation.\nAnswer (True or False):", "ct_subject": "One principle of reality in Jainism is karma, or asrava. Jainism began around 500 B.C. Plato was born around 428 B.C., so he was alive while Jainism existed. Plato believed in karma and reincarnation.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.98670095"], [" the", "1.0"], [" ancient", "0.97860783"], [" Egyptians", "0.999997"], [" believe", "0.8086487"], [" in", "1.0"], [" the", "0.99998903"], [" existence", "0.9999502"], [" of", "1.0"], [" the", "0.8208669"], [" Loch", "0.10764353"], [" Ness", "1.0"], [" Monster", "0.9999999"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Loch", "0.99422836"], [" Ness", "1.0"], [" Monster", "1.0"], [" is", "1.0"], [" a", "1.0"], [" myth", "1.0"], ["ological", "0.98756975"], [" creature", "1.0"], [" that", "1.0"], [" has", "0.9983347"], [" been", "1.0"], [" reported", "1.0"], [" in", "0.9993093"], [" the", "0.8645912"], [" Scottish", "0.7636843"], [" Highlands", "1.0"], [" for", "0.91794586"], [" centuries", "1.0"], [".", "1.0"], [" The", "1.0"], [" Loch", "1.0"], [" Ness", "1.0"], [" Monster", "1.0"], [" is", "0.99999905"], [" a", "0.99911183"], [" myth", "0.70701295"], ["ological", "1.0"], [" creature", "1.0"], [" that", "1.0"], [" has", "1.0"], [" been", "1.0"], [" reported", "1.0"]]}, "511": {"qid": "f9fcf86196d1847b2f0b", "term": "2008 Summer Olympics", "description": "Games of the XXIX Olympiad, held in Beijing in 2008", "question": "Could you drive a Rowe 550 to the 2008 Summer Olympics?", "answer": true, "facts": ["The Rowe 550 was a car produced by the Chinese SAIC motor company.", "The Rowe 550 debuted at the 2007 Shanghai Auto Show.", "The 2008 Beijing Summer Olympics happened in the Capital of the People's Republic of China."], "decomposition": ["When was the Roewe 550 launched?", "Did the 2008 Summer Olympics hold before or during #1?"], "evidence": [[[["Roewe 550-1"]], [["2008 Summer Olympics-1"], "operation"]], [[["Roewe 550-1"]], [["2008 Summer Olympics-1"], "operation"]], [[["Roewe 550-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be killed by a shark?\nFacts: The first person to be killed by a shark was a man named John H. Brown. Brown was killed by a shark while swimming in the waters off the coast of Florida.\nAnswer (True or False):", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could you drive a Rowe 550 to the 2008 Summer Olympics?\nFacts: The Rowe 550 was a car produced by the Chinese SAIC motor company. The Rowe 550 debuted at the 2007 Shanghai Auto Show. The 2008 Beijing Summer Olympics happened in the Capital of the People's Republic of China.\nAnswer (True or False):", "ct_subject": "The Rowe 550 was a car produced by the Chinese SAIC motor company. The Rowe 550 debuted at the 2007 Shanghai Auto Show. The 2008 Beijing Summer Olympics happened in the Capital of the People's Republic of China.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999976"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999988"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9447713"], [" person", "0.9060976"], [" to", "1.0"], [" be", "0.9999075"], [" killed", "0.49512175"], [" by", "0.9999994"], [" a", "1.0"], [" shark", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" killed", "1.0"], [" by", "1.0"], [" a", "1.0"], [" shark", "1.0"], [" was", "1.0"], [" a", "1.0"], [" man", "0.9994623"], [" named", "1.0"], [" John", "0.99999714"], [" H", "0.9129695"], [".", "1.0"], [" Brown", "0.7662728"], [".", "1.0"], [" Brown", "0.99999905"], [" was", "1.0"], [" killed", "0.9999622"], [" by", "1.0"], [" a", "1.0"], [" shark", "0.21340021"], [" while", "0.99974936"], [" swimming", "0.90313405"], [" in", "0.9898775"], [" the", "1.0"], [" waters", "0.9999981"], [" off", "0.9918046"], [" the", "1.0"], [" coast", "1.0"], [" of", "1.0"], [" Florida", "0.9966453"], [".", "0.9368199"], ["\n", "0.9999995"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"]]}, "512": {"qid": "6ef0bafcb42de140de17", "term": "Eagle", "description": "large carnivore bird", "question": "Can shooting bald eagle get a person more prison time than Michael Vick?", "answer": true, "facts": ["Michael Vick spent 21 months in prison for an illegal dog fighting ring.", "Shooting a bald eagle carries a penalty of up to two years in prison for a second conviction."], "decomposition": ["How long of a penalty is it for shooting a bald eagle?", "How many months did Michael Vick serve in prison?", "Is #1 longer than #2?"], "evidence": [[[["Bald and Golden Eagle Protection Act-18"]], [["Michael Vick-2"]], ["operation"]], [[["Bald and Golden Eagle Protection Act-4"]], [["Michael Vick-2"]], ["no_evidence", "operation"]], [[["Bald and Golden Eagle Protection Act-18"]], [["Michael Vick-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake's mother\"?\nFacts: A \"snowflake\" is a person who is easily offended. A \"snowflake's mother\" is a person who is easily offended.\nAnswer (True or False): True\nQuestion: What is the difference", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can shooting bald eagle get a person more prison time than Michael Vick?\nFacts: Michael Vick spent 21 months in prison for an illegal dog fighting ring. Shooting a bald eagle carries a penalty of up to two years in prison for a second conviction.\nAnswer (True or False):", "ct_subject": "Michael Vick spent 21 months in prison for an illegal dog fighting ring. Shooting a bald eagle carries a penalty of up to two years in prison for a second conviction.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9990233"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.75940907"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.9999881"], ["s", "0.8449849"], ["now", "0.99999917"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["'s", "0.9961849"], [" mother", "0.9992924"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" \"", "0.8141383"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" person", "0.999997"], [" who", "1.0"], [" is", "1.0"], [" easily", "1.0"], [" offended", "1.0"], [".", "0.99999917"], [" A", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["'s", "1.0"], [" mother", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" person", "1.0"], [" who", "1.0"], [" is", "1.0"], [" easily", "1.0"], [" offended", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"]]}, "513": {"qid": "760a0d26a50972656345", "term": "Ginger", "description": "Species of plant", "question": "Could the Port of Baltimore handle the entire world's cargo production of ginger each year?", "answer": true, "facts": ["In 2018, the world production of ginger was 2.8 million tons.", "The Port of Baltimore handles about 2.8 million tons of cargo per fiscal quarter. ", "A fiscal quarter is shorter than a year."], "decomposition": ["How much cargo does the Port of Baltimore handle each fiscal quarter?", "How much ginger cargo is produced each year?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Port of Baltimore-19"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Port of Baltimore-18"]], [["Ginger-8"]], ["operation"]], [[["Port of Baltimore-19"]], [["Ginger-8"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the largest city in the United States?\nFacts: The largest city in the United States is New York City.\nAnswer (True or False): True\nQuestion: What is the largest city in the United States?\nFacts: The largest city in the United", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could the Port of Baltimore handle the entire world's cargo production of ginger each year?\nFacts: In 2018, the world production of ginger was 2.8 million tons. The Port of Baltimore handles about 2.8 million tons of cargo per fiscal quarter.  A fiscal quarter is shorter than a year.\nAnswer (True or False):", "ct_subject": "In 2018, the world production of ginger was 2.8 million tons. The Port of Baltimore handles about 2.8 million tons of cargo per fiscal quarter.  A fiscal quarter is shorter than a year.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999697"], [" is", "1.0"], [" the", "1.0"], [" largest", "0.59326565"], [" city", "0.98905903"], [" in", "1.0"], [" the", "1.0"], [" United", "0.99992573"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" largest", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "1.0"], [" New", "1.0"], [" York", "1.0"], [" City", "1.0"], [".", "1.0"], ["\n", "0.9999999"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" largest", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"]]}, "514": {"qid": "7f8417d42ef9ea1e5a8c", "term": "Watermelon", "description": "A large fruit with a smooth hard rind, of the gourd family", "question": "Are more watermelons grown in Brazil than Antarctica?", "answer": true, "facts": ["Watermelons are plants grown in climates from tropical to temperate, needing temperatures higher than about 25 \u00b0C (77 \u00b0F) to thrive.", "The climate of Antarctica is the coldest on Earth.", "The climate of Brazil comprises a wide range of weather conditions across a large area and varied topography, but most of the country is tropical."], "decomposition": ["Which climate is suitable for the cultivation of watermelon?", "What are the prevalent climatic conditions in Brazil?", "What are the prevalent climatic conditions in Antarctica?", "Is #2 more similar to #1 than #3?"], "evidence": [[[["Watermelon-2"]], [["Brazil-47"]], [["Antarctica-42"]], ["operation"]], [[["Watermelon-15"]], [["Climate of Brazil-5"]], [["Antarctica-42"]], ["operation"]], [[["Watermelon-2"]], [["Brazil-47"]], [["Antarctica-42"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are more people in the United States today related to the first Americans than to the first Europeans?\nFacts: The first Americans were the first Europeans. The first Europeans were the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are more watermelons grown in Brazil than Antarctica?\nFacts: Watermelons are plants grown in climates from tropical to temperate, needing temperatures higher than about 25 \u00b0C (77 \u00b0F) to thrive. The climate of Antarctica is the coldest on Earth. The climate of Brazil comprises a wide range of weather conditions across a large area and varied topography, but most of the country is tropical.\nAnswer (True or False):", "ct_subject": "Watermelons are plants grown in climates from tropical to temperate, needing temperatures higher than about 25 \u00b0C (77 \u00b0F) to thrive. The climate of Antarctica is the coldest on Earth. The climate of Brazil comprises a wide range of weather conditions across a large area and varied topography, but most of the country is tropical.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.9999932"], [" more", "0.85244286"], [" people", "1.0"], [" in", "0.99249625"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" today", "1.0"], [" related", "1.0"], [" to", "1.0"], [" the", "1.0"], [" first", "0.87587166"], [" Americans", "0.41695452"], [" than", "1.0"], [" to", "0.9999999"], [" the", "1.0"], [" first", "1.0"], [" Europeans", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" Americans", "1.0"], [" were", "1.0"], [" the", "1.0"], [" first", "0.99986815"], [" Europeans", "0.99976426"], [".", "0.9892663"], [" The", "1.0"], [" first", "1.0"], [" Europeans", "1.0"], [" were", "1.0"], [" the", "1.0"]]}, "515": {"qid": "5170cfed313d2de67942", "term": "The Tonight Show Starring Jimmy Fallon", "description": "American late-night talk show", "question": "On August 20, 2020,  does The Tonight Show Starring Jimmy Fallon air after moonset EST?", "answer": true, "facts": ["On August 20th, The Tonight Show Starring Jimmy Fallon airs at 11:35PM", "On August 20th, the moon on the east coast of the USA will set around 9PM"], "decomposition": ["The Tonight Show Starring Jimmy Fallon airs at 11:35 p.m. ET/PT.", "On August 20th, the moon on the east coast of the USA  set 9PM", "Does #1 occur after #2?"], "evidence": [[[["The Tonight Show Starring Jimmy Fallon-2"]], [["Moonlight-1", "Sunset-1"], "no_evidence"], ["operation"]], [[["The Tonight Show Starring Jimmy Fallon-2"]], [["Lunar phase-16"]], [["Lunar phase-16", "The Tonight Show Starring Jimmy Fallon-2"], "no_evidence"]], [[["The Tonight Show-30"], "no_evidence"], [["Moonrise-1"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the year 2000?\nFacts: The first person to be born in the year 2000 was born on January 1, 2000.\nAnswer (True or False): True\nQuestion: What is the name of the first person to", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: On August 20, 2020,  does The Tonight Show Starring Jimmy Fallon air after moonset EST?\nFacts: On August 20th, The Tonight Show Starring Jimmy Fallon airs at 11:35PM On August 20th, the moon on the east coast of the USA will set around 9PM\nAnswer (True or False):", "ct_subject": "On August 20th, The Tonight Show Starring Jimmy Fallon airs at 11:35PM On August 20th, the moon on the east coast of the USA will set around 9PM", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999815"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999871"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9203755"], [" person", "0.9199852"], [" to", "1.0"], [" be", "0.9999405"], [" born", "0.9999788"], [" in", "0.999998"], [" the", "1.0"], [" year", "0.9999999"], [" 2000", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "0.99999726"], [" born", "0.99999964"], [" on", "0.9824723"], [" January", "0.99921346"], [" 1", "1.0"], [",", "1.0"], [" 2000", "0.9803106"], [".", "0.9999765"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"]]}, "516": {"qid": "9d72d2a919f84ce9c597", "term": "Zoology", "description": "Study of the animal kingdom", "question": "Is zoology unconcerned with strigoi?", "answer": true, "facts": ["Zoology is the study of the behavior and classification of animals.", "Strigoi are spirits that can transform into animals in Romanian mythology.", "Zoology is based on science and fossils."], "decomposition": ["What does the study of zoology entail?", "What kind of creatures are the strigoi?", "Is #2 unrelated to #1"], "evidence": [[[["Zoology-3"]], [["Strigoi-5"]], [["Strigoi-5", "Zoology-3"], "operation"]], [[["Zoology-1"]], [["Strigoi-1"]], ["operation"]], [[["Zoology-1"]], [["Strigoi-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the Roman Empire the greatest empire in history?\nFacts: The Roman Empire was the greatest empire in history. The Roman Empire was the greatest empire in history. The Roman Empire was the greatest empire in history. The Roman Empire was the greatest empire in history. The Roman Empire was the greatest empire in history. The Roman Empire was the greatest", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is zoology unconcerned with strigoi?\nFacts: Zoology is the study of the behavior and classification of animals. Strigoi are spirits that can transform into animals in Romanian mythology. Zoology is based on science and fossils.\nAnswer (True or False):", "ct_subject": "Zoology is the study of the behavior and classification of animals. Strigoi are spirits that can transform into animals in Romanian mythology. Zoology is based on science and fossils.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99999964"], [" the", "0.99997604"], [" Roman", "0.98022574"], [" Empire", "0.99999964"], [" the", "0.99997044"], [" greatest", "0.507033"], [" empire", "0.9999999"], [" in", "1.0"], [" history", "0.99995315"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [" was", "1.0"], [" the", "1.0"], [" greatest", "0.98813325"], [" empire", "1.0"], [" in", "1.0"], [" history", "1.0"], [".", "1.0"], [" The", "0.9927492"], [" Roman", "1.0"], [" Empire", "1.0"], [" was", "1.0"], [" the", "0.9926025"], [" greatest", "0.9995352"], [" empire", "1.0"], [" in", "1.0"], [" history", "1.0"], [".", "1.0"], [" The", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [" was", "1.0"], [" the", "1.0"], [" greatest", "1.0"], [" empire", "1.0"], [" in", "1.0"], [" history", "1.0"], [".", "1.0"], [" The", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [" was", "1.0"], [" the", "1.0"], [" greatest", "1.0"], [" empire", "1.0"], [" in", "1.0"], [" history", "1.0"], [".", "1.0"], [" The", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [" was", "1.0"], [" the", "1.0"], [" greatest", "1.0"], [" empire", "1.0"], [" in", "1.0"], [" history", "1.0"], [".", "1.0"], [" The", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [" was", "1.0"], [" the", "1.0"], [" greatest", "1.0"]]}, "517": {"qid": "54713361a75dbc02912c", "term": "Kidney", "description": "internal organ in most animals, including vertebrates and some invertebrates", "question": "Is it safe to eat kidney?", "answer": true, "facts": ["Traditional British cuisine includes \"Kidney Pie\", or \"Steak and Kidney Pie\".", "Kidney Pie contains kidney."], "decomposition": ["What are the various kinds of meat safe for human consumption?", "Is kidney included in #1?"], "evidence": [[[["Offal-95"]], ["operation"]], [[["Meat-1"]], ["no_evidence"]], [[["Meat-17"], "operation"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it safe to eat a raw egg?\nFacts: Raw eggs are not safe to eat.\nAnswer (True or False): True\nQuestion: Is it safe to eat raw meat?\nFacts: Raw meat is not safe to eat.\nAnswer (True or False): True\nQuestion: Is it safe to eat raw fish?\nFacts: Raw fish is not safe to eat.\nAnswer", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is it safe to eat kidney?\nFacts: Traditional British cuisine includes \"Kidney Pie\", or \"Steak and Kidney Pie\". Kidney Pie contains kidney.\nAnswer (True or False):", "ct_subject": "Traditional British cuisine includes \"Kidney Pie\", or \"Steak and Kidney Pie\". Kidney Pie contains kidney.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999912"], [" it", "1.0"], [" safe", "1.0"], [" to", "1.0"], [" eat", "0.99999607"], [" a", "0.26934314"], [" raw", "0.9993561"], [" egg", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Raw", "1.0"], [" eggs", "1.0"], [" are", "0.9611489"], [" not", "0.99960655"], [" safe", "1.0"], [" to", "1.0"], [" eat", "1.0"], [".", "1.0"], ["\n", "0.99992526"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" safe", "1.0"], [" to", "1.0"], [" eat", "1.0"], [" raw", "0.9679215"], [" meat", "0.99999654"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Raw", "1.0"], [" meat", "1.0"], [" is", "1.0"], [" not", "1.0"], [" safe", "1.0"], [" to", "1.0"], [" eat", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" safe", "1.0"], [" to", "1.0"], [" eat", "1.0"], [" raw", "1.0"], [" fish", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Raw", "1.0"], [" fish", "1.0"], [" is", "1.0"], [" not", "1.0"], [" safe", "1.0"], [" to", "1.0"], [" eat", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"]]}, "518": {"qid": "20744bd481b4245333fa", "term": "Gorillaz", "description": "British virtual band", "question": "Has Gorillaz creator been in more bands than Bernard Sumner?", "answer": true, "facts": ["Gorillaz was created by Damon Albarn.", "Damon Albarn has been in five bands: Gorillaz, Blur, The Good, the Bad & the Queen, Elastica, and DRC Music.", "Bernard Sumner has been in three bands: New Order, Joy Division, and Electronic, Bad Lieutenant."], "decomposition": ["Who was the primary creator of Gorillaz?", "How many bands has #1 been a member of?", "How many bands has Bernard Sumner been a member of?", "Is #2 greater than #3?"], "evidence": [[[["Damon Albarn-1"]], [["Damon Albarn-1", "Damon Albarn-3"]], [["Bernard Sumner-1", "Bernard Sumner-2", "Bernard Sumner-6"]], ["operation"]], [[["Gorillaz-1"]], [["Blur (band)-1", "Gorillaz-1", "Rocket Juice & the Moon-1", "The Good, the Bad & the Queen-1"]], [["Bad Lieutenant (band)-1", "Bernard Sumner-4", "Electronic (band)-1", "Joy Division-1"]], ["operation"]], [[["Damon Albarn-1"]], [["Damon Albarn-1", "Damon Albarn-3"]], [["Bernard Sumner-3", "Bernard Sumner-4", "Bernard Sumner-5", "Bernard Sumner-6"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to ever be born in the year 2000?\nFacts: The first person to ever be born in the year 2000 was born on January 1,", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Has Gorillaz creator been in more bands than Bernard Sumner?\nFacts: Gorillaz was created by Damon Albarn. Damon Albarn has been in five bands: Gorillaz, Blur, The Good, the Bad & the Queen, Elastica, and DRC Music. Bernard Sumner has been in three bands: New Order, Joy Division, and Electronic, Bad Lieutenant.\nAnswer (True or False):", "ct_subject": "Gorillaz was created by Damon Albarn. Damon Albarn has been in five bands: Gorillaz, Blur, The Good, the Bad & the Queen, Elastica, and DRC Music. Bernard Sumner has been in three bands: New Order, Joy Division, and Electronic, Bad Lieutenant.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.98198384"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99998355"], [" of", "1.0"], [" the", "1.0"], [" first", "0.37685576"], [" person", "0.99261487"], [" to", "1.0"], [" ever", "0.8313191"], [" be", "0.9961681"], [" born", "0.9999987"], [" in", "1.0"], [" the", "0.9999999"], [" year", "1.0"], [" 2000", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "0.88578755"], [" person", "1.0"], [" to", "1.0"], [" ever", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "1.0"], [" born", "0.99175125"], [" on", "0.9474723"], [" January", "0.99509925"], [" 1", "1.0"], [",", "1.0"]]}, "519": {"qid": "89a3281f33b15c7fba3f", "term": "The Hague", "description": "City and municipality in South Holland, Netherlands", "question": "Does Abdulqawi Yusuf go to the Hague on a typical work day?", "answer": true, "facts": ["Abdulqawi Yusuf is the current president of the International Court of Justice", "The International Court of Justice is headquartered in The Hague"], "decomposition": ["What organization does Abdulqawi Yusuf's work for?", "Where is #1 headquartered?"], "evidence": [[[["Abdulqawi Yusuf-1"]], [["International Court of Justice-3"], "operation"]], [[["Abdulqawi Yusuf-1"]], [["International Court of Justice-3"]]], [[["Abdulqawi Yusuf-1"]], [["United Nations-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the country that is the largest producer of oil in the world?\nFacts: Saudi Arabia\nAnswer (True or False): True\nQuestion: What is the name of the country that is the largest producer of oil in the world?\nFacts: Saudi Arabia\nAnswer (True or False): True\nQuestion: What is the name of", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Abdulqawi Yusuf go to the Hague on a typical work day?\nFacts: Abdulqawi Yusuf is the current president of the International Court of Justice The International Court of Justice is headquartered in The Hague\nAnswer (True or False):", "ct_subject": "Abdulqawi Yusuf is the current president of the International Court of Justice The International Court of Justice is headquartered in The Hague", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9997907"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" country", "0.35170913"], [" that", "0.99999666"], [" is", "0.9980501"], [" the", "1.0"], [" largest", "0.99999464"], [" producer", "0.9942002"], [" of", "1.0"], [" oil", "0.99765587"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Saudi", "0.8857658"], [" Arabia", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "0.990475"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" country", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" producer", "1.0"], [" of", "1.0"], [" oil", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Saudi", "1.0"], [" Arabia", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "0.9988071"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"]]}, "520": {"qid": "da526afb80b0174a19b0", "term": "Honey badger", "description": "species of mammal", "question": "Would a honey badger fit inside an oven?", "answer": true, "facts": [" Adult honey badgers measure 23 to 28 cm (9.1 to 11.0 in) in shoulder height and 55\u201377 cm (22\u201330 in) in body length, with the tail adding another 12\u201330 cm (4.7\u201311.8 in).", "Thirty-inch ovens are the standard for most homes and most kitchens. The inside dimensions of the oven are approximately 25 inches wide and 16 inches high. The oven will be approximately 16 inches deep. "], "decomposition": ["What is the average dimmension of a honey badger?", "What are the dimensions of an average oven?", "Is #1 less than #2?"], "evidence": [[[["Honey badger-12"], "no_evidence"], ["no_evidence"], ["no_evidence"]], [[["Honey badger-12"], "no_evidence"], [["Oven-6"], "no_evidence"], ["operation"]], [[["Honey badger-12"]], [["Oven-6"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a honey badger fit inside an oven?\nFacts:  Adult honey badgers measure 23 to 28 cm (9.1 to 11.0 in) in shoulder height and 55\u201377 cm (22\u201330 in) in body length, with the tail adding another 12\u201330 cm (4.7\u201311.8 in). Thirty-inch ovens are the standard for most homes and most kitchens. The inside dimensions of the oven are approximately 25 inches wide and 16 inches high. The oven will be approximately 16 inches deep. \nAnswer (True or False):", "ct_subject": " Adult honey badgers measure 23 to 28 cm (9.1 to 11.0 in) in shoulder height and 55\u201377 cm (22\u201330 in) in body length, with the tail adding another 12\u201330 cm (4.7\u201311.8 in). Thirty-inch ovens are the standard for most homes and most kitchens. The inside dimensions of the oven are approximately 25 inches wide and 16 inches high. The oven will be approximately 16 inches deep. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999509"], [" is", "1.0"], [" the", "1.0"], [" average", "0.9030131"], [" weight", "1.0"], [" of", "1.0"], [" a", "1.0"], [" human", "0.99999964"]]}, "521": {"qid": "7d2ebef168d86d7ea5a2", "term": "Great Wall of China", "description": "wall along the historical northern borders of China", "question": "Could the Great Wall of China connect the Dodgers to the White Sox?", "answer": true, "facts": ["The Dodgers are a baseball team located in Los Angeles", "The White Sox are a baseball team located in Chicago", "The distance between Los Angeles and Chicago is 2,015 miles", "The length of the main section of the Great Wall of China is 2,145 miles"], "decomposition": ["Where are the Dodgers located?", "Where are the White Sox located?", "What is the distance between #1 and #2?", "How long is The Great Wall of China?", "Is #4 larger than #3?"], "evidence": [[[["Los Angeles Dodgers-1"]], [["Chicago White Sox-1"]], [["Southwest Chief-1"]], [["Great Wall of China-3"]], ["operation"]], [[["Los Angeles Dodgers-1"]], [["Chicago White Sox-1"]], ["no_evidence"], [["Great Wall of China-3"]], ["no_evidence", "operation"]], [[["Los Angeles Dodgers-1"]], [["Chicago White Sox-1"]], [["Southwest Chief-1"]], [["Ming Great Wall-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance between two points on the Earth?\nFacts: The distance between the Earth and the Moon is about 238,000 miles. The distance between the Earth and the Sun is about 93 million miles.\nAnswer (True or False): True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could the Great Wall of China connect the Dodgers to the White Sox?\nFacts: The Dodgers are a baseball team located in Los Angeles The White Sox are a baseball team located in Chicago The distance between Los Angeles and Chicago is 2,015 miles The length of the main section of the Great Wall of China is 2,145 miles\nAnswer (True or False):", "ct_subject": "The Dodgers are a baseball team located in Los Angeles The White Sox are a baseball team located in Chicago The distance between Los Angeles and Chicago is 2,015 miles The length of the main section of the Great Wall of China is 2,145 miles", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9133405"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.9999995"], [" distance", "1.0"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"], [" on", "0.9979019"], [" the", "0.9991259"], [" Earth", "0.99922967"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" distance", "1.0"], [" between", "1.0"], [" the", "0.99995685"], [" Earth", "0.9277496"], [" and", "1.0"], [" the", "1.0"], [" Moon", "0.9946148"], [" is", "1.0"], [" about", "0.94970256"], [" 238", "0.9999957"], [",", "1.0"], ["000", "1.0"], [" miles", "1.0"], [".", "0.75235057"], [" The", "1.0"], [" distance", "1.0"], [" between", "1.0"], [" the", "1.0"], [" Earth", "1.0"], [" and", "1.0"], [" the", "1.0"], [" Sun", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 93", "1.0"], [" million", "0.99999976"], [" miles", "1.0"], [".", "1.0"], ["\n", "0.97699285"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"]]}, "522": {"qid": "2be20073ad0ad58502eb", "term": "Halloween", "description": "Holiday celebrated October 31", "question": "If a baby was born on Halloween would they be a Scorpio?", "answer": true, "facts": ["Halloween is a holiday where people dress up and happens on October 31 each year.", "The zodiac sign of Scorpio encompasses the dates from October 23 to November 22."], "decomposition": ["On what date does Halloween occur each year?", "What dates are included in the Zodiac sign of Scorpio?", "Does #1 fall in the date span listed in #2?"], "evidence": [[[["Halloween-1"]], [["Scorpio (astrology)-1"]], ["operation"]], [[["Halloween-1"]], [["Scorpio (astrology)-1"]], ["operation"]], [[["Halloween-12"], "no_evidence"], [["Zodiac-31"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the constellation Orion?\nFacts: The constellation Orion is the brightest star in the night sky. It is the brightest star in the sky and is the second brightest star in the sky after Sirius.\nAnswer (True or False): True\nQuestion: What is the name of the constellation Scorpio?\nFacts", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: If a baby was born on Halloween would they be a Scorpio?\nFacts: Halloween is a holiday where people dress up and happens on October 31 each year. The zodiac sign of Scorpio encompasses the dates from October 23 to November 22.\nAnswer (True or False):", "ct_subject": "Halloween is a holiday where people dress up and happens on October 31 each year. The zodiac sign of Scorpio encompasses the dates from October 23 to November 22.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.6696976"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9779744"], [" of", "1.0"], [" the", "1.0"], [" constellation", "0.9999993"], [" Orion", "0.9899385"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9995995"], [" constellation", "0.9999999"], [" Orion", "1.0"], [" is", "1.0"], [" the", "0.99999225"], [" brightest", "0.99967813"], [" star", "1.0"], [" in", "1.0"], [" the", "1.0"], [" night", "0.9998203"], [" sky", "1.0"], [".", "1.0"], [" It", "0.9999999"], [" is", "1.0"], [" the", "0.99997985"], [" brightest", "0.98745745"], [" star", "1.0"], [" in", "1.0"], [" the", "1.0"], [" sky", "0.7827054"], [" and", "0.99999464"], [" is", "0.9999682"], [" the", "0.9999988"], [" second", "0.99655735"], [" brightest", "1.0"], [" star", "0.99999976"], [" in", "1.0"], [" the", "1.0"], [" sky", "0.9994783"], [" after", "0.9999994"], [" Sirius", "0.99888533"], [".", "1.0"], ["\n", "0.99999225"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" constellation", "1.0"], [" Scorp", "0.23047018"], ["io", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"]]}, "523": {"qid": "55a0154c4a5dd692c046", "term": "Mercedes-Benz", "description": "automobile brand of Daimler AG", "question": "Was Mercedes-Benz associated with the Nazis?", "answer": true, "facts": ["During the 1930s, Mercedes-Benz produced the 770 model.", "The 770 was popular with Nazis, and Adolf Hitler used them as his personal vehicle."], "decomposition": ["Which Mercedes-Benz model was made during the 1930s?", "Was #1 popular among the Nazis?"], "evidence": [[[["Mercedes-Benz-6"]], ["operation"]], [[["Mercedes-Benz 770-1"]], [["Mercedes-Benz 770-1"]]], [[["Mercedes-Benz 770-6"]], [["Mercedes-Benz-6"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Nazis use the swastika as a symbol?\nFacts: The swastika was used by the Nazis as a symbol of their ideology.\nAnswer (True or False): True\nQuestion: Did the Nazis use the swastika as a symbol?\nFacts: The swastika was used by the Nazis as a symbol of their ideology.\nAnswer (True or False", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Mercedes-Benz associated with the Nazis?\nFacts: During the 1930s, Mercedes-Benz produced the 770 model. The 770 was popular with Nazis, and Adolf Hitler used them as his personal vehicle.\nAnswer (True or False):", "ct_subject": "During the 1930s, Mercedes-Benz produced the 770 model. The 770 was popular with Nazis, and Adolf Hitler used them as his personal vehicle.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.47888425"], [" the", "1.0"], [" Nazis", "0.99991107"], [" use", "0.9999901"], [" the", "1.0"], [" swast", "0.85217357"], ["ika", "1.0"], [" as", "1.0"], [" a", "1.0"], [" symbol", "1.0"], ["?", "0.99170154"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" swast", "1.0"], ["ika", "1.0"], [" was", "0.9985207"], [" used", "0.9995974"], [" by", "1.0"], [" the", "1.0"], [" Nazis", "0.99845886"], [" as", "1.0"], [" a", "1.0"], [" symbol", "1.0"], [" of", "0.9993376"], [" their", "0.8902733"], [" ideology", "0.99986446"], [".", "0.99999976"], ["\n", "0.99999845"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9958528"], [" the", "1.0"], [" Nazis", "1.0"], [" use", "1.0"], [" the", "1.0"], [" swast", "0.99935013"], ["ika", "1.0"], [" as", "1.0"], [" a", "1.0"], [" symbol", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" swast", "1.0"], ["ika", "1.0"], [" was", "1.0"], [" used", "1.0"], [" by", "1.0"], [" the", "1.0"], [" Nazis", "1.0"], [" as", "1.0"], [" a", "1.0"], [" symbol", "1.0"], [" of", "1.0"], [" their", "1.0"], [" ideology", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"]]}, "524": {"qid": "fab710522fe2f188579d", "term": "Buzz Aldrin", "description": "American astronaut; second person to walk on the Moon", "question": "Could Buzz Aldrin have owned a computer?", "answer": true, "facts": ["Buzz Aldrin was born in 1930 and is still alive in 2020. ", "Home computers were first available for sale in 1977. "], "decomposition": ["When were personal computers made available to the public?", "When was Buzz Aldrin born?", "Is #2 well before #1?"], "evidence": [[[["Personal computer-10"]], [["Buzz Aldrin-1"]], [["Buzz Aldrin-1"], "operation"]], [[["Personal computer-15"]], [["Buzz Aldrin-1"]], ["operation"]], [[["Personal computer-7"]], [["Buzz Aldrin-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the first man on the moon land on the moon?\nFacts: The first man on the moon landed on the moon on July 20, 1969. \nAnswer (True or False): True\nQuestion: Did the first man on the moon land on the moon?\nFacts: The first man on the moon landed on the moon on July 20, 1969. \nAnswer (True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could Buzz Aldrin have owned a computer?\nFacts: Buzz Aldrin was born in 1930 and is still alive in 2020.  Home computers were first available for sale in 1977. \nAnswer (True or False):", "ct_subject": "Buzz Aldrin was born in 1930 and is still alive in 2020.  Home computers were first available for sale in 1977. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.6493484"], [" the", "1.0"], [" first", "0.7165113"], [" man", "0.90092945"], [" on", "0.99982375"], [" the", "1.0"], [" moon", "1.0"], [" land", "0.9734036"], [" on", "1.0"], [" the", "0.96722066"], [" moon", "1.0"], ["?", "0.9806613"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" man", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" landed", "0.99999535"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" on", "0.9998553"], [" July", "1.0"], [" 20", "1.0"], [",", "1.0"], [" 1969", "1.0"], [".", "1.0"], [" ", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9944131"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" land", "0.99970466"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" man", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" landed", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" on", "1.0"], [" July", "1.0"], [" 20", "1.0"], [",", "1.0"], [" 1969", "1.0"], [".", "1.0"], [" ", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"]]}, "525": {"qid": "15271da5457a404509c4", "term": "Force", "description": "Any action that tends to maintain or alter the motion of an object", "question": "Can a cheetah generate enough force to topple Big Show?", "answer": true, "facts": ["Big Show is a professional wrestler that weighs 383 pounds.", "Force is equal to mass times acceleration.", "An adult Cheetah weighs around 160 pounds.", "An adult Cheetah can run up to 58 MPH."], "decomposition": ["How much does Big Show weigh?", "How much does a cheetah weigh?", "How fast can a cheetah run?", "Is the force produced by a mass of #2 and a speed of #3 enough to knock over something that weighs #1?"], "evidence": [[["no_evidence"], [["Cheetah-1"]], [["Cheetah-1"]], ["operation"]], [[["Big Show-4"], "no_evidence"], [["Cheetah-1"]], [["Cheetah-1"]], [["Acceleration-13"], "operation"]], [[["Big Show-4"], "no_evidence"], [["Cheetah-1"]], [["Cheetah-15"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human?\nFacts: The average human weight is about 190 pounds.\nAnswer (True or False): True\nQuestion: What is the average weight of a human male?\nFacts: The average human male weight is about 190 pounds.\nAnswer (True or", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can a cheetah generate enough force to topple Big Show?\nFacts: Big Show is a professional wrestler that weighs 383 pounds. Force is equal to mass times acceleration. An adult Cheetah weighs around 160 pounds. An adult Cheetah can run up to 58 MPH.\nAnswer (True or False):", "ct_subject": "Big Show is a professional wrestler that weighs 383 pounds. Force is equal to mass times acceleration. An adult Cheetah weighs around 160 pounds. An adult Cheetah can run up to 58 MPH.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99918765"], [" is", "1.0"], [" the", "1.0"], [" average", "0.32745937"], [" weight", "0.9998709"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], ["?", "0.9988224"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.9999169"], [" weight", "0.8810063"], [" is", "1.0"], [" about", "0.99732447"], [" 190", "0.48867592"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "0.9923969"], [" of", "1.0"], [" a", "1.0"], [" human", "0.9995714"], [" male", "0.628418"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "1.0"], [" male", "1.0"], [" weight", "0.9999999"], [" is", "1.0"], [" about", "1.0"], [" 190", "0.8756929"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"]]}, "526": {"qid": "4752f67ef92a9195ee36", "term": "Elijah", "description": "Biblical prophet", "question": "Is Elijah part of a Jewish holiday?", "answer": true, "facts": ["The Jewish holiday Passover involves a traditional ceremonial dinner.", "During the ceremony, it is customary to fill an extra cup with wine and put it at the center of the table.", "The door is then opened so the prophet Elijah can visit."], "decomposition": ["How is Elijah venerated according to Jewish custom?", "Does #1 include venerating Elijah at a holiday?"], "evidence": [[[["Elijah-49"], "no_evidence"], [["Elijah-43", "Elijah-44"], "operation"]], [[["Elijah-49"]], ["no_evidence"]], [[["Passover Seder-62"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city of Jerusalem the same as the name of the city of Rome?\nFacts: The name of the city of Jerusalem is the same as the name of the city of Rome.\nAnswer (True or False): True\nQuestion: Is the name of the city of Jerusalem", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Elijah part of a Jewish holiday?\nFacts: The Jewish holiday Passover involves a traditional ceremonial dinner. During the ceremony, it is customary to fill an extra cup with wine and put it at the center of the table. The door is then opened so the prophet Elijah can visit.\nAnswer (True or False):", "ct_subject": "The Jewish holiday Passover involves a traditional ceremonial dinner. During the ceremony, it is customary to fill an extra cup with wine and put it at the center of the table. The door is then opened so the prophet Elijah can visit.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99999964"], [" the", "0.9992235"], [" name", "0.9212647"], [" of", "0.9999995"], [" the", "1.0"], [" city", "0.9997998"], [" of", "1.0"], [" Jerusalem", "0.99999523"], [" the", "0.9965056"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" name", "0.9999962"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "0.9896819"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" name", "0.9999995"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Jerusalem", "1.0"], [" is", "1.0"], [" the", "1.0"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Jerusalem", "0.9912519"]]}, "527": {"qid": "24418db1d18a7fdf79f3", "term": "Basil", "description": "species of plant", "question": "Is basil safe from Hypervitaminosis D?", "answer": true, "facts": ["Hypervitaminosis D is a rare disease caused by having too much vitamin D.", "Basil contains many vitamins including Vitamin A, B, C, E, and K."], "decomposition": ["Hypervitaminosis D is caused by eating too much of what vitamin?", "Does basil contain #1?", "Is it safe to avoid #1 by eating #2?"], "evidence": [[[["Hypervitaminosis D-1"]], ["no_evidence"], ["operation"]], [[["Hypervitaminosis D-1"]], [["Vitamin D-52"]], ["operation"]], [[["Hypervitaminosis D-1"]], ["no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it true that the sun is the source of all life on earth?\nFacts: The sun is the source of all life on earth.\nAnswer (True or False): True\nQuestion: Is it true that the sun is the source of all life on earth?\nFacts: The sun is the source of all life on earth.\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is basil safe from Hypervitaminosis D?\nFacts: Hypervitaminosis D is a rare disease caused by having too much vitamin D. Basil contains many vitamins including Vitamin A, B, C, E, and K.\nAnswer (True or False):", "ct_subject": "Hypervitaminosis D is a rare disease caused by having too much vitamin D. Basil contains many vitamins including Vitamin A, B, C, E, and K.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "0.99995244"], [" true", "0.9999995"], [" that", "1.0"], [" the", "1.0"], [" sun", "0.9852504"], [" is", "1.0"], [" the", "0.9999981"], [" source", "0.9982674"], [" of", "1.0"], [" all", "1.0"], [" life", "1.0"], [" on", "1.0"], [" earth", "0.9987696"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" sun", "1.0"], [" is", "1.0"], [" the", "0.9997658"], [" source", "1.0"], [" of", "1.0"], [" all", "0.9948754"], [" life", "1.0"], [" on", "1.0"], [" earth", "1.0"], [".", "0.9999999"], ["\n", "0.99813664"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" true", "1.0"], [" that", "1.0"], [" the", "1.0"], [" sun", "0.9999747"], [" is", "1.0"], [" the", "1.0"], [" source", "1.0"], [" of", "1.0"], [" all", "1.0"], [" life", "1.0"], [" on", "1.0"], [" earth", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" sun", "1.0"], [" is", "1.0"], [" the", "1.0"], [" source", "1.0"], [" of", "1.0"], [" all", "1.0"], [" life", "1.0"], [" on", "1.0"], [" earth", "1.0"], [".", "1.0"], ["\n", "1.0"]]}, "528": {"qid": "ee284d0d4524904d39cb", "term": "Northern Mariana Islands", "description": "American-dependent insular area in the western Pacific", "question": "Is Mark Cuban able to visit Northern Mariana Islands without a passport?", "answer": true, "facts": ["Citizens of the United States can visit Northern Mariana Islands without a passport.", "Mark Cuban is an American citizen."], "decomposition": ["Citizens of what countries can visit the Northern Mariana Islands without a passport?", "What country is Mark Cuban a citizen of?", "Is #2 included in #1?"], "evidence": [[[["Northern Mariana Islands-1"], "no_evidence"], [["Brian Cuban-2"], "no_evidence"], ["operation"]], [[["Northern Mariana Islands-50"], "no_evidence"], [["Mark Cuban-3"]], ["no_evidence", "operation"]], [[["Northern Mariana Islands-1"]], [["Mark Cuban-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is there a difference between a \"citizen\" and a \"citizen of the United States\"?\nFacts: A citizen of the United States is a person who is a citizen of the United States. A citizen of the United States is a person who is a citizen of the United States.\nAnswer (True or False): True\nQuestion: Is there a difference between a \"citizen", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Mark Cuban able to visit Northern Mariana Islands without a passport?\nFacts: Citizens of the United States can visit Northern Mariana Islands without a passport. Mark Cuban is an American citizen.\nAnswer (True or False):", "ct_subject": "Citizens of the United States can visit Northern Mariana Islands without a passport. Mark Cuban is an American citizen.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99852645"], [" there", "0.011586762"], [" a", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "0.9961295"], [" \"", "0.9999988"], ["c", "0.86413985"], ["itizen", "0.9999999"], ["\"", "0.99977285"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "0.9996195"], ["itizen", "1.0"], [" of", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.90309477"], [" citizen", "0.9521601"], [" of", "0.9879298"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "1.0"], [" a", "1.0"], [" person", "1.0"], [" who", "1.0"], [" is", "0.9999299"], [" a", "1.0"], [" citizen", "0.9619809"], [" of", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [".", "1.0"], [" A", "1.0"], [" citizen", "1.0"], [" of", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "1.0"], [" a", "0.99999714"], [" person", "1.0"], [" who", "1.0"], [" is", "1.0"], [" a", "1.0"], [" citizen", "1.0"], [" of", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" there", "0.9999993"], [" a", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["itizen", "1.0"]]}, "529": {"qid": "e263728bd940c5c71bdf", "term": "Hydropower", "description": "energy derived from falling or running water", "question": "Is chaff produced by hydropower?", "answer": true, "facts": ["Chaff is excess material from milled grain.", "Some mills use hydropower to mill grain."], "decomposition": ["Where does Chaff come from?", "Do some #1's use hydropower to do it's function?"], "evidence": [[[["Chaff-1"]], [["Winnowing (sedimentology)-1"]]], [[["Chaff-1"]], [["Hydropower-1"], "operation"]], [[["Chaff-5"]], [["Hydropower-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the moon made of cheese?\nFacts: The moon is made of cheese.\nAnswer (True or False): True\nQuestion: Is the moon made of cheese?\nFacts: The moon is made of cheese.\nAnswer (True or False): True\nQuestion: Is the moon made of cheese?\nFacts: The moon is made of cheese.\nAnswer (True or False): True\nQuestion: Is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is chaff produced by hydropower?\nFacts: Chaff is excess material from milled grain. Some mills use hydropower to mill grain.\nAnswer (True or False):", "ct_subject": "Chaff is excess material from milled grain. Some mills use hydropower to mill grain.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999827"], [" the", "0.97674245"], [" moon", "0.57368374"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" moon", "1.0"], [" is", "1.0"], [" made", "1.0"], [" of", "1.0"], [" cheese", "0.99999785"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" moon", "0.95445514"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" moon", "1.0"], [" is", "1.0"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" moon", "1.0"], [" is", "1.0"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"]]}, "530": {"qid": "77f814c0e9766c9cdb4d", "term": "Apollo 13", "description": "A failed crewed mission to land on the Moon", "question": "Were deaths from Apollo 13 mission eclipsed by other space missions?", "answer": true, "facts": ["Apollo 13 was a failed Moon landing in which the crew had faulty oxygen resources.", "All of the Apollo 13 crew returned safely to earth.", "The Challenger was a space shuttle that malfunctioned, killing all seven passengers.", "The Space Shuttle Columbia disaster had seven casualties."], "decomposition": ["How many astronauts died on the Apollo 13 mission?", "How many astronauts died during the launch of the Challenger space shuttle?", "How many astronauts died aboard the Columbia space shuttle?", "Are #2 and #3 both greater than #1?"], "evidence": [[[["Apollo 13-3"]], [["Space Shuttle Challenger disaster-1"]], [["Space Shuttle Columbia disaster-1"]], ["operation"]], [[["Apollo 13-55"]], [["Space Shuttle Challenger disaster-1"]], [["Space Shuttle Columbia disaster-1"]], ["operation"]], [[["Apollo 13-52"]], [["Space Shuttle Challenger-1"]], [["Space Shuttle Columbia-24"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the year 2000?\nFacts: The first person to be born in the year 2000 was born in the year 2000.\nAnswer (True or False): True\nQuestion: What is the name", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Were deaths from Apollo 13 mission eclipsed by other space missions?\nFacts: Apollo 13 was a failed Moon landing in which the crew had faulty oxygen resources. All of the Apollo 13 crew returned safely to earth. The Challenger was a space shuttle that malfunctioned, killing all seven passengers. The Space Shuttle Columbia disaster had seven casualties.\nAnswer (True or False):", "ct_subject": "Apollo 13 was a failed Moon landing in which the crew had faulty oxygen resources. All of the Apollo 13 crew returned safely to earth. The Challenger was a space shuttle that malfunctioned, killing all seven passengers. The Space Shuttle Columbia disaster had seven casualties.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.8739735"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9825203"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9439238"], [" person", "0.99639624"], [" to", "1.0"], [" be", "0.99975497"], [" born", "0.9384509"], [" in", "1.0"], [" the", "0.99298567"], [" year", "0.111024156"], [" 2000", "0.9999993"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "0.8562272"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "1.0"], [" born", "0.9989986"], [" in", "0.9284494"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"]]}, "531": {"qid": "b52d3a7244abcf58ac29", "term": "Final Fantasy VI", "description": "1994 video game", "question": "Is Final Fantasy VI closer to beginning than end of its franchise?", "answer": true, "facts": ["Final Fantasy VI is the sixth entry into the Final Fantasy series of video games.", "There are 15 total games in the main Final Fantasy series of video games as of 2020."], "decomposition": ["How many releases have been made in the Final Fantasy franchise?", "Final Fantasy VI comes in what position in the series?", "What is the absolute difference between #1 and the cardinal value of #2?", "What is the absolute difference between 1 and the cardinal value of #2?", "Is #3 greater than #4?"], "evidence": [[[["Final Fantasy-9"]], ["operation"], ["operation"], ["operation"], ["operation"]], [[["Final Fantasy-1"]], [["Final Fantasy-6"]], ["operation"], ["operation"], ["operation"]], [[["Final Fantasy-1"]], [["Final Fantasy VI-1"]], ["operation"], ["operation"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the character in the movie \"The Princess Bride\" a reference to the character in the book?\nFacts: The name of the character in the movie \"The Princess Bride\" is named Buttercup. The name of the character in the book is named Buttercup.\nAnswer (True or False): True\nQuestion:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Final Fantasy VI closer to beginning than end of its franchise?\nFacts: Final Fantasy VI is the sixth entry into the Final Fantasy series of video games. There are 15 total games in the main Final Fantasy series of video games as of 2020.\nAnswer (True or False):", "ct_subject": "Final Fantasy VI is the sixth entry into the Final Fantasy series of video games. There are 15 total games in the main Final Fantasy series of video games as of 2020.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9998185"], [" the", "0.13018805"], [" name", "0.99999857"], [" of", "1.0"], [" the", "1.0"], [" character", "0.9424717"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "1.0"], ["The", "1.0"], [" Princess", "0.5127133"], [" Bride", "1.0"], ["\"", "1.0"], [" a", "0.9053104"], [" reference", "1.0"], [" to", "1.0"], [" the", "1.0"], [" character", "0.9612635"], [" in", "1.0"], [" the", "1.0"], [" book", "1.0"], ["?", "0.99275607"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" name", "0.9979705"], [" of", "1.0"], [" the", "1.0"], [" character", "1.0"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "0.99999666"], ["The", "1.0"], [" Princess", "1.0"], [" Bride", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" named", "0.8706055"], [" Butter", "1.0"], ["cup", "1.0"], [".", "1.0"], [" The", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" character", "1.0"], [" in", "1.0"], [" the", "1.0"], [" book", "1.0"], [" is", "0.99998343"], [" named", "1.0"], [" Butter", "1.0"], ["cup", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"]]}, "532": {"qid": "a16ec88f41867d97f60a", "term": "Charles Manson", "description": "American criminal, cult leader", "question": "Has Don King killed more people than Charles Manson did with his own hands in 1971?", "answer": true, "facts": ["Charles Manson is famous for a series of murders in 1971.", "Charles Manson's cult was responsible for seven deaths in 1971 but he was not present during the murders.", "Boxing promoter Don King has been charged with killing two people in incidents 13 years apart and settled out of court.."], "decomposition": ["How many people did Charles Manson actually kill?", "Don King has been charged with killing how many people?", "Is #2 larger than #1?"], "evidence": [[[["Charles Manson-1"]], [["Don King (boxing promoter)-3"]], [["Charles Manson-1", "Don King (boxing promoter)-3"], "operation"]], [[["Charles Manson-1"]], [["Don King (boxing promoter)-1"]], ["operation"]], [[["Charles Manson-1"], "no_evidence"], [["Don King (boxing promoter)-3"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city of New York City the same as the name of the city of New York?\nFacts: The name of the city of New York is New York City. The name of the city of", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Has Don King killed more people than Charles Manson did with his own hands in 1971?\nFacts: Charles Manson is famous for a series of murders in 1971. Charles Manson's cult was responsible for seven deaths in 1971 but he was not present during the murders. Boxing promoter Don King has been charged with killing two people in incidents 13 years apart and settled out of court..\nAnswer (True or False):", "ct_subject": "Charles Manson is famous for a series of murders in 1971. Charles Manson's cult was responsible for seven deaths in 1971 but he was not present during the murders. Boxing promoter Don King has been charged with killing two people in incidents 13 years apart and settled out of court..", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.7759648"], [" the", "0.7653349"], [" name", "0.99475753"], [" of", "0.99997544"], [" the", "1.0"], [" city", "0.999997"], [" of", "0.9999646"], [" New", "0.9782785"], [" York", "1.0"], [" City", "1.0"], [" the", "0.9575478"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], ["?", "0.87479764"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99741805"], [" name", "0.99999845"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" is", "0.9998568"], [" New", "0.9745398"], [" York", "1.0"], [" City", "0.99641067"], [".", "1.0"], [" The", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"]]}, "533": {"qid": "3f09ce8b018df4527e20", "term": "Sea turtle", "description": "superfamily of reptiles", "question": "Are sea turtles enjoying life during quarantine?", "answer": true, "facts": ["Sea turtles nest on beaches", "Quarantine due to COVID has resulted in far fewer people using beaches", "More sea turtles have been able to nest and reproduce on beaches during quarantine"], "decomposition": ["What elements comprise \"enjoying life\" for a sea turtle?", "Where do the elements in #1 occur?", "How prevalent were humans in the areas in #2 pre-Covid-19?", "In the areas in #2, are humans less prevalent now than in #3?"], "evidence": [[[["Sea turtle-11"], "no_evidence"], [["Sea turtle-45"]], [["Sea turtle-13"]], ["operation"]], [[["Sea turtle-14"], "no_evidence"], [["Sea turtle migration-6"], "no_evidence"], ["no_evidence"], [["Sea turtle-13"], "no_evidence", "operation"]], [[["Green sea turtle-30"]], [["Sea turtle-12"]], [["Tourism-47"], "no_evidence"], [["Sea turtle-13"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any other animals that are not native to the United States that are still here?\nFacts: Yes, there are many animals that are not native to the United States that are still here.\nAnswer (True or False): True\nQuestion: Are there any animals that are not native to the United States that are not endangered?\nFacts: Yes,", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are sea turtles enjoying life during quarantine?\nFacts: Sea turtles nest on beaches Quarantine due to COVID has resulted in far fewer people using beaches More sea turtles have been able to nest and reproduce on beaches during quarantine\nAnswer (True or False):", "ct_subject": "Sea turtles nest on beaches Quarantine due to COVID has resulted in far fewer people using beaches More sea turtles have been able to nest and reproduce on beaches during quarantine", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.99746156"], [" there", "0.9999999"], [" any", "0.6980272"], [" other", "0.59371376"], [" animals", "0.9999999"], [" that", "1.0"], [" are", "0.9591637"], [" not", "0.99403185"], [" native", "0.99880147"], [" to", "1.0"], [" the", "0.9999161"], [" United", "1.0"], [" States", "1.0"], [" that", "0.9999907"], [" are", "0.99971944"], [" still", "0.9800867"], [" here", "0.6989586"], ["?", "0.999972"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Yes", "0.9958883"], [",", "0.9999572"], [" there", "1.0"], [" are", "1.0"], [" many", "0.99933124"], [" animals", "0.9662475"], [" that", "1.0"], [" are", "0.9999925"], [" not", "1.0"], [" native", "1.0"], [" to", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" are", "1.0"], [" still", "1.0"], [" here", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.9940088"], [" there", "1.0"], [" any", "1.0"], [" animals", "0.99999976"], [" that", "1.0"], [" are", "1.0"], [" not", "1.0"], [" native", "1.0"], [" to", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" are", "1.0"], [" not", "0.999956"], [" endangered", "0.9999895"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Yes", "1.0"], [",", "1.0"]]}, "534": {"qid": "f5daea01f2eab51d3ceb", "term": "Big Ben", "description": "Bell within the clock tower at the Palace of Westminster in London, England", "question": "Can a blind person tell time by Big Ben?", "answer": true, "facts": ["Big Ben is a striking clock", "A blind person can hear the time the bell tolls"], "decomposition": ["What type of clock is Big Ben?", "How does #1 indicate a new hour?", "Can a blind person hear #2?"], "evidence": [[[["Big Ben-1"]], [["Striking clock-8"]], [["Striking clock-9"]]], [[["Big Ben-1"]], [["Big Ben-47"]], ["operation"]], [[["Big Ben-1"]], [["Big Ben-50"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance between two points on Earth?\nFacts: The distance between London and New York is about 3,000 miles.\nAnswer (True or False): True\nQuestion: What is the longest distance between two points on the Moon?\nFacts: The distance between the Moon and Earth is about 1,400 miles.\nAnswer (True or False): True\nQuestion: What is the longest distance between two points", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can a blind person tell time by Big Ben?\nFacts: Big Ben is a striking clock A blind person can hear the time the bell tolls\nAnswer (True or False):", "ct_subject": "Big Ben is a striking clock A blind person can hear the time the bell tolls", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99945337"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.99995816"], [" distance", "0.9995432"], [" between", "0.99968016"], [" two", "1.0"], [" points", "1.0"], [" on", "0.9989177"], [" Earth", "0.9991197"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" distance", "0.999074"], [" between", "1.0"], [" London", "0.1203241"], [" and", "1.0"], [" New", "0.9999901"], [" York", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 3", "0.9745944"], [",", "1.0"], ["000", "0.999395"], [" miles", "1.0"], [".", "1.0"], ["\n", "0.99999976"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"], [" on", "1.0"], [" the", "1.0"], [" Moon", "0.98876405"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" distance", "1.0"], [" between", "1.0"], [" the", "1.0"], [" Moon", "1.0"], [" and", "1.0"], [" Earth", "0.99410313"], [" is", "1.0"], [" about", "1.0"], [" 1", "0.74514973"], [",", "1.0"], ["400", "0.82837063"], [" miles", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"]]}, "535": {"qid": "4649ef4a279369c83a9f", "term": "Alan Rickman", "description": "British actor", "question": "Do many fans of J.K Rowling know who Alan Rickman is?", "answer": true, "facts": ["J.K Rowling wrote the Harry Potter series.", "Alan Rickman performed the role of Severus Snape throughout all 8 Harry Potter films."], "decomposition": ["What is JK Rowling most famous for?", "What characters has actor Alan Rickman played?", "What characters appear in #1?", "Is at least one character from #2 also listed in #3?"], "evidence": [[[["J. K. Rowling-1"]], [["Alan Rickman-2"]], [["Harry Potter (film series)-30"], "no_evidence"], ["operation"]], [[["J. K. Rowling-1"]], [["Alan Rickman-2"]], [["Severus Snape-3"]], ["operation"]], [[["J. K. Rowling-1"]], [["Alan Rickman-15"]], [["Severus Snape-47"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the character in the movie \"The Avengers\"?\nFacts: The character in the movie \"The Avengers\" is named \"Tony Stark\".\nAnswer (True or False): True\nQuestion: What is the name of the character in the movie \"The Avengers\"?\nFacts: The character in the movie \"The Avengers\" is named \"Tony", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do many fans of J.K Rowling know who Alan Rickman is?\nFacts: J.K Rowling wrote the Harry Potter series. Alan Rickman performed the role of Severus Snape throughout all 8 Harry Potter films.\nAnswer (True or False):", "ct_subject": "J.K Rowling wrote the Harry Potter series. Alan Rickman performed the role of Severus Snape throughout all 8 Harry Potter films.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9986834"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999988"], [" of", "1.0"], [" the", "1.0"], [" character", "0.049192313"], [" in", "0.9999999"], [" the", "1.0"], [" movie", "0.9999994"], [" \"", "1.0"], ["The", "1.0"], [" Avengers", "0.9998753"], ["\"?", "0.9241119"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" character", "0.999992"], [" in", "0.99999976"], [" the", "1.0"], [" movie", "1.0"], [" \"", "0.9999999"], ["The", "1.0"], [" Avengers", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" named", "1.0"], [" \"", "0.9999943"], ["Tony", "0.99999976"], [" Stark", "1.0"], ["\".", "0.9804135"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" character", "1.0"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "1.0"], ["The", "1.0"], [" Avengers", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" character", "1.0"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "1.0"], ["The", "1.0"], [" Avengers", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" named", "1.0"], [" \"", "1.0"], ["Tony", "1.0"]]}, "536": {"qid": "cc266a3e29a3b8442b14", "term": "Billionaire", "description": "person who has a net worth of at least one billion (1,000,000,000) units of a given currency", "question": "Would a 900,000 pound net worth person be an American billionaire if they exchange currency June 2020?", "answer": true, "facts": ["The exchange rate in June of 2020 between dollars and pounds is 1 Euro= 1.23 dollar.", "900000 pounds is equal to about 1,107,000.00"], "decomposition": ["What is the minimum amount one must have to be called a billionaire?", "As of  June 2020, how many dollars make a pound?", "Is #2 times 900000 at least equal to #1?"], "evidence": [[[["Billionaire-1"]], [["Pound sterling-62"], "no_evidence"], ["operation"]], [[["Billionaire-1"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Billionaire-1"]], [["Pound sterling-62"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human being?\nFacts: The average human being weighs about 70 pounds.\nAnswer (True or False): True\nQuestion: What is the average height of a human being?\nFacts: The average height of a human being is about 5 feet, 8 inches", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a 900,000 pound net worth person be an American billionaire if they exchange currency June 2020?\nFacts: The exchange rate in June of 2020 between dollars and pounds is 1 Euro= 1.23 dollar. 900000 pounds is equal to about 1,107,000.00\nAnswer (True or False):", "ct_subject": "The exchange rate in June of 2020 between dollars and pounds is 1 Euro= 1.23 dollar. 900000 pounds is equal to about 1,107,000.00", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999597"], [" is", "1.0"], [" the", "1.0"], [" average", "0.99999833"], [" weight", "0.9998306"], [" of", "1.0"], [" a", "0.99999404"], [" human", "0.9999969"], [" being", "0.9985927"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.97357213"], [" being", "0.9997372"], [" weighs", "0.99993575"], [" about", "0.999997"], [" 70", "0.51349515"], [" pounds", "0.999995"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" height", "0.7354784"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" height", "0.16612509"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "1.0"], [" is", "1.0"], [" about", "0.9998627"], [" 5", "1.0"], [" feet", "0.99998677"], [",", "0.9999013"], [" 8", "0.9745514"], [" inches", "1.0"]]}, "537": {"qid": "6d4b524b0f555ce5a7ae", "term": "Missionary", "description": "member of a religious group sent into an area to do evangelism", "question": "Is there a popular Broadway character who is a missionary?", "answer": true, "facts": ["The Book of Mormon is a popular Broadway musical.", "The two main characters are Elder Price and Elder Cunningham.", "They are Mormon missionaries sent to Africa to share their religion with the villagers."], "decomposition": ["Who are the two main characters in the popular Broadway musical \"Book of Mormon\"?", "Are #1 missionaries?"], "evidence": [[[["The Book of Mormon (musical)-3"]], ["operation"]], [[["The Book of Mormon (musical)-3"]], ["operation"]], [[["The Book of Mormon (musical)-32"]], [["The Book of Mormon (musical)-3"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is there a popular Broadway musical that is a missionary?\nFacts: The Book of Mormon is a popular Broadway musical. The two main characters are Elder Price and Elder Cunningham. They are Mormon missionaries sent to Africa to share their religion with the villagers.\nAnswer (True or False): True\nQuestion: Is there a popular Broadway musical that", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is there a popular Broadway character who is a missionary?\nFacts: The Book of Mormon is a popular Broadway musical. The two main characters are Elder Price and Elder Cunningham. They are Mormon missionaries sent to Africa to share their religion with the villagers.\nAnswer (True or False):", "ct_subject": "The Book of Mormon is a popular Broadway musical. The two main characters are Elder Price and Elder Cunningham. They are Mormon missionaries sent to Africa to share their religion with the villagers.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999999"], [" there", "1.0"], [" a", "1.0"], [" popular", "1.0"], [" Broadway", "0.99996173"], [" musical", "1.0"], [" that", "0.99999905"], [" is", "1.0"], [" a", "0.9999995"], [" missionary", "0.99999595"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Book", "1.0"], [" of", "1.0"], [" Mormon", "1.0"], [" is", "1.0"], [" a", "1.0"], [" popular", "1.0"], [" Broadway", "1.0"], [" musical", "1.0"], [".", "1.0"], [" The", "1.0"], [" two", "1.0"], [" main", "1.0"], [" characters", "1.0"], [" are", "1.0"], [" Elder", "1.0"], [" Price", "1.0"], [" and", "1.0"], [" Elder", "1.0"], [" Cunningham", "1.0"], [".", "1.0"], [" They", "1.0"], [" are", "1.0"], [" Mormon", "1.0"], [" missionaries", "1.0"], [" sent", "1.0"], [" to", "1.0"], [" Africa", "1.0"], [" to", "1.0"], [" share", "1.0"], [" their", "1.0"], [" religion", "1.0"], [" with", "1.0"], [" the", "1.0"], [" villagers", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" there", "1.0"], [" a", "1.0"], [" popular", "1.0"], [" Broadway", "1.0"], [" musical", "1.0"], [" that", "1.0"]]}, "538": {"qid": "d216d1e93117da3934e2", "term": "Tower of London", "description": "A historic castle on the north bank of the River Thames in central London", "question": "Would Robert Stack have been interested in Tower of London during 1400s for his 14 season show?", "answer": true, "facts": ["Robert Stack was an actor best known for Unsolved Mysteries which lasted for 14 seasons before being rebooted by Netflix.", "The Tower of London is a historic building in London.", "Unsolved Mysteries explored unexplained phenomenon and mysterious events.", "The heirs of Edward IV mysteriously vanished from the Tower of London in the 1400s and were presumed muredered."], "decomposition": ["What is the defining feature of Robert Stack's 14 season show?", "What events happened at the Tower of London in the 1400's?", "Do any of the events in #2 have the characteristic in #1?"], "evidence": [[[["Robert Stack-35"]], [["Tower of London-2", "Tower of London-3"]], ["operation"]], [[["Robert Stack-1"]], [["Tower of London-34"]], ["operation"]], [[["Unsolved Mysteries-1"]], [["Princes in the Tower-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city of London?\nFacts: London is the capital of England. The city is located in the south", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Robert Stack have been interested in Tower of London during 1400s for his 14 season show?\nFacts: Robert Stack was an actor best known for Unsolved Mysteries which lasted for 14 seasons before being rebooted by Netflix. The Tower of London is a historic building in London. Unsolved Mysteries explored unexplained phenomenon and mysterious events. The heirs of Edward IV mysteriously vanished from the Tower of London in the 1400s and were presumed muredered.\nAnswer (True or False):", "ct_subject": "Robert Stack was an actor best known for Unsolved Mysteries which lasted for 14 seasons before being rebooted by Netflix. The Tower of London is a historic building in London. Unsolved Mysteries explored unexplained phenomenon and mysterious events. The heirs of Edward IV mysteriously vanished from the Tower of London in the 1400s and were presumed muredered.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999794"], [" is", "0.9999999"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "0.893452"], [" of", "0.9973271"], [" London", "0.86475635"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" London", "0.99804187"], [" is", "1.0"], [" the", "0.99882215"], [" capital", "1.0"], [" of", "1.0"], [" England", "1.0"], [".", "0.91784626"], [" The", "0.99611074"], [" city", "0.99931324"], [" is", "0.9712738"], [" located", "1.0"], [" in", "1.0"], [" the", "0.8825118"], [" south", "0.99393785"]]}, "539": {"qid": "f8e6087cd52c925fedad", "term": "Tony Bennett", "description": "American singer", "question": "Could ancient Tony Bennett have a baby in 2020?", "answer": true, "facts": ["Tony Bennett is a legendary singer who will turn 94 years old in August 2020.", "Ramjit Raghav, the oldest man to have a baby, had his first child at age 94.", "Ramjit Raghav had his second child at age 96."], "decomposition": ["How old was Tony Bennett in 2020?", "How old was the oldest man to father a child?", "Is #1 less than #2?"], "evidence": [[[["Tony Bennett-1"]], [["Ramjit Raghav-1"]], ["operation"]], [[["Tony Bennett-1"]], [["Ramjit Raghav-1"]], ["operation"]], [[["Tony Bennett-1"]], [["Ramjit Raghav-4"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human?\nFacts: The average weight of a human is about 100 pounds.\nAnswer (True or False): True\nQuestion: What is the average height of a human?\nFacts: The average height of a human is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could ancient Tony Bennett have a baby in 2020?\nFacts: Tony Bennett is a legendary singer who will turn 94 years old in August 2020. Ramjit Raghav, the oldest man to have a baby, had his first child at age 94. Ramjit Raghav had his second child at age 96.\nAnswer (True or False):", "ct_subject": "Tony Bennett is a legendary singer who will turn 94 years old in August 2020. Ramjit Raghav, the oldest man to have a baby, had his first child at age 94. Ramjit Raghav had his second child at age 96.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9998858"], [" is", "1.0"], [" the", "1.0"], [" average", "0.9796881"], [" weight", "0.993536"], [" of", "1.0"], [" a", "0.9999989"], [" human", "0.9964223"], ["?", "0.98001605"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" weight", "0.08892778"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" is", "1.0"], [" about", "0.9999976"], [" 100", "0.9997602"], [" pounds", "1.0"], [".", "1.0"], ["\n", "0.9999995"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" height", "1.0"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" height", "1.0"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" is", "1.0"]]}, "540": {"qid": "5090f573b09ac3050824", "term": "Eagle", "description": "large carnivore bird", "question": "Are eagles and young bears both used as labels for skills-training youth groups?", "answer": true, "facts": ["A young bear is a cub.", "Boy Scouts is a skill-training youth group that includes divisions such as Cub Scouts and Eagle Scouts"], "decomposition": ["What is the name of a young bear?", "What is a popular skill training group for boys? ", "Are #1 and eagles names for groups in #2?"], "evidence": [[[["Bear-27"]], [["Boy Scouts of America-2"]], [["Boy Scouts of America-22", "Boy Scouts of America-26"], "operation"]], [[["Bear-37"]], [["Boy Scouts of America-1", "Scouting-1"]], [["Cub Scout-1", "Eagle Scout (Boy Scouts of America)-1"]]], [[["Bear-27"]], [["Scout (Scouting)-1"]], [["Eagle Scout (Boy Scouts of America)-1", "Scout (Scouting)-5"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any other animals that are used as labels for skills-training youth groups?\nFacts: There are many other animals that are used as labels for skills-training youth groups.\nAnswer (True or False): True\nQuestion: Are there any other animals that are used as labels for skills-training youth groups?\nFacts: There are many", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are eagles and young bears both used as labels for skills-training youth groups?\nFacts: A young bear is a cub. Boy Scouts is a skill-training youth group that includes divisions such as Cub Scouts and Eagle Scouts\nAnswer (True or False):", "ct_subject": "A young bear is a cub. Boy Scouts is a skill-training youth group that includes divisions such as Cub Scouts and Eagle Scouts", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.9998342"], [" there", "0.9999982"], [" any", "0.9998888"], [" other", "0.999798"], [" animals", "1.0"], [" that", "1.0"], [" are", "0.5807361"], [" used", "0.9587362"], [" as", "1.0"], [" labels", "1.0"], [" for", "1.0"], [" skills", "1.0"], ["-", "1.0"], ["training", "1.0"], [" youth", "1.0"], [" groups", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "0.99993396"], [" are", "1.0"], [" many", "0.90872663"], [" other", "0.9987025"], [" animals", "1.0"], [" that", "0.9999788"], [" are", "1.0"], [" used", "1.0"], [" as", "1.0"], [" labels", "1.0"], [" for", "1.0"], [" skills", "1.0"], ["-", "1.0"], ["training", "1.0"], [" youth", "1.0"], [" groups", "1.0"], [".", "1.0"], ["\n", "0.9997422"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "1.0"], [" any", "1.0"], [" other", "1.0"], [" animals", "1.0"], [" that", "1.0"], [" are", "1.0"], [" used", "1.0"], [" as", "1.0"], [" labels", "1.0"], [" for", "1.0"], [" skills", "1.0"], ["-", "1.0"], ["training", "1.0"], [" youth", "1.0"], [" groups", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "1.0"], [" are", "1.0"], [" many", "1.0"]]}, "541": {"qid": "673fb9299daae81cba69", "term": "HIV", "description": "Human retrovirus, cause of AIDS", "question": "Is it safe to share silverware with an HIV positive person?", "answer": true, "facts": ["HIV is transmitted through blood and mucous membrane contact, not saliva.", "Silverware is used in the mouth and contacts saliva but not other bodily fluids. "], "decomposition": ["How is HIV transmitted?", "What comes in contact with silverware when you use it?", "Is there any overlap between #1 and #2?"], "evidence": [[[["HIV-1"]], [["Household silver-3"], "no_evidence"], ["operation"]], [[["HIV-1"]], [["Eating utensil etiquette-5"]], [["Management of HIV/AIDS-63"], "operation"]], [[["HIV-1"]], [["Cutlery-1"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it safe to share a toothbrush with an HIV positive person?\nFacts: HIV is transmitted through blood and mucous membrane contact, not saliva. Toothbrushes are used in the mouth and contacts saliva but not other bodily fluids. \nAnswer (True or False): True\nQuestion: Is it safe to share a toothbrush with an HIV", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is it safe to share silverware with an HIV positive person?\nFacts: HIV is transmitted through blood and mucous membrane contact, not saliva. Silverware is used in the mouth and contacts saliva but not other bodily fluids. \nAnswer (True or False):", "ct_subject": "HIV is transmitted through blood and mucous membrane contact, not saliva. Silverware is used in the mouth and contacts saliva but not other bodily fluids. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99999726"], [" it", "1.0"], [" safe", "1.0"], [" to", "1.0"], [" share", "0.9999999"], [" a", "1.0"], [" tooth", "0.9999999"], ["brush", "1.0"], [" with", "1.0"], [" an", "0.9999995"], [" HIV", "1.0"], [" positive", "1.0"], [" person", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" HIV", "1.0"], [" is", "1.0"], [" transmitted", "1.0"], [" through", "1.0"], [" blood", "1.0"], [" and", "1.0"], [" muc", "1.0"], ["ous", "1.0"], [" membrane", "1.0"], [" contact", "1.0"], [",", "1.0"], [" not", "1.0"], [" saliva", "1.0"], [".", "1.0"], [" Tooth", "1.0"], ["br", "1.0"], ["ushes", "1.0"], [" are", "1.0"], [" used", "1.0"], [" in", "1.0"], [" the", "1.0"], [" mouth", "1.0"], [" and", "1.0"], [" contacts", "0.99990475"], [" saliva", "1.0"], [" but", "1.0"], [" not", "1.0"], [" other", "1.0"], [" bodily", "1.0"], [" fluids", "1.0"], [".", "1.0"], [" ", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" safe", "1.0"], [" to", "1.0"], [" share", "1.0"], [" a", "1.0"], [" tooth", "0.99999976"], ["brush", "1.0"], [" with", "1.0"], [" an", "1.0"], [" HIV", "1.0"]]}, "542": {"qid": "24aa6050c1ed0405e663", "term": "Baptism", "description": "Christian rite of admission and adoption, almost invariably with the use of water", "question": "Can Immersion Baptism lead to a death like Jeff Buckley's?", "answer": true, "facts": ["Immersion Baptism is the practice of submerging people underwater for a religious ritual.", "Jeff Buckley was an acclaimed singer that died of drowning in 1997.", "A baby in Moldova died from Immersion Baptism in 2010."], "decomposition": ["How did Jeff Buckley die?", "How is immersion baptism performed?", "Are the circumstances surrounding #1 similar to that of #2?"], "evidence": [[[["Jeff Buckley-3"]], [["Immersion baptism-1"]], ["operation"]], [[["Jeff Buckley-36"]], [["Immersion baptism-4"]], [["Immersion baptism-4"], "operation"]], [[["Jeff Buckley-3"]], [["Immersion baptism-2"]], [["Drowning-1", "Swimming-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the Bible a true book?\nFacts: The Bible is a collection of books written by various authors over a period of time. The Bible is not a single book.\nAnswer (True or False): True\nQuestion: Is the Bible a true book?\nFacts:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can Immersion Baptism lead to a death like Jeff Buckley's?\nFacts: Immersion Baptism is the practice of submerging people underwater for a religious ritual. Jeff Buckley was an acclaimed singer that died of drowning in 1997. A baby in Moldova died from Immersion Baptism in 2010.\nAnswer (True or False):", "ct_subject": "Immersion Baptism is the practice of submerging people underwater for a religious ritual. Jeff Buckley was an acclaimed singer that died of drowning in 1997. A baby in Moldova died from Immersion Baptism in 2010.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9962657"], [" the", "0.9991335"], [" Bible", "1.0"], [" a", "0.19887486"], [" true", "0.9901889"], [" book", "1.0"], ["?", "0.9999981"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Bible", "1.0"], [" is", "1.0"], [" a", "0.22904445"], [" collection", "1.0"], [" of", "1.0"], [" books", "0.9984913"], [" written", "0.9999995"], [" by", "0.99998915"], [" various", "0.74338496"], [" authors", "1.0"], [" over", "1.0"], [" a", "0.98480046"], [" period", "1.0"], [" of", "1.0"], [" time", "0.99999154"], [".", "1.0"], [" The", "0.9999999"], [" Bible", "1.0"], [" is", "1.0"], [" not", "0.99999917"], [" a", "0.9360095"], [" single", "1.0"], [" book", "1.0"], [".", "1.0"], ["\n", "0.99999833"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "0.9999999"], [" Bible", "1.0"], [" a", "0.8538405"], [" true", "1.0"], [" book", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"]]}, "543": {"qid": "8f3bbafdb649f391fea3", "term": "Arnold Schwarzenegger", "description": "Austrian-American actor, businessman, bodybuilder and politician", "question": "Would Arnold Schwarzenegger be unable to run for President of the US?", "answer": true, "facts": ["Arnold Schwarzenegger was born in Austria.", "One requirement of running for the US Presidency is to have been born in the USA."], "decomposition": ["Where was Arnold Schwarzenegger born?", "Is #1 in the US?"], "evidence": [[[["Arnold Schwarzenegger-11"]], ["operation"]], [[["Arnold Schwarzenegger-6"]], [["Thal, Styria-1"], "operation"]], [[["Arnold Schwarzenegger-6"]], [["Styria-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the USA?\nFacts: The first person to be born in the USA was John Smith.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the USA?\nFacts: The first person to be born in the USA was John Smith.\nAnswer (True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Arnold Schwarzenegger be unable to run for President of the US?\nFacts: Arnold Schwarzenegger was born in Austria. One requirement of running for the US Presidency is to have been born in the USA.\nAnswer (True or False):", "ct_subject": "Arnold Schwarzenegger was born in Austria. One requirement of running for the US Presidency is to have been born in the USA.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999963"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9832755"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9159338"], [" person", "0.5978166"], [" to", "1.0"], [" be", "0.9897322"], [" born", "0.9929931"], [" in", "1.0"], [" the", "1.0"], [" USA", "0.98896396"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.68191284"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" USA", "1.0"], [" was", "1.0"], [" John", "0.6843727"], [" Smith", "0.9862546"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" USA", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" USA", "1.0"], [" was", "1.0"], [" John", "1.0"], [" Smith", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"]]}, "544": {"qid": "30ffdbc264ee1084a9b5", "term": "Capsaicin", "description": "chemical compound", "question": "Is capsaicin associated with cooking?", "answer": true, "facts": ["Capsaicin occurs naturally in chilies and other peppers.", "It is the substance that makes the peppers spicy.", "Peppers are routinely used in cooking in many cultures."], "decomposition": ["What is capsaicin a highly active component of?", "Is #1 used in cooking? "], "evidence": [[[["Capsaicin-1"]], [["Chili pepper-1"]]], [[["Capsaicin-1"]], [["Chili pepper-1"]]], [[["Capsaicin-2"]], [["Capsicum-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it true that the first man on the moon was a woman?\nFacts: The first woman on the moon was a man named Sally Ride. She was the first American woman in space.\nAnswer (True or False): True\nQuestion: Is it true that the first man on the moon was a woman?\nFacts: The first woman on the moon", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is capsaicin associated with cooking?\nFacts: Capsaicin occurs naturally in chilies and other peppers. It is the substance that makes the peppers spicy. Peppers are routinely used in cooking in many cultures.\nAnswer (True or False):", "ct_subject": "Capsaicin occurs naturally in chilies and other peppers. It is the substance that makes the peppers spicy. Peppers are routinely used in cooking in many cultures.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999989"], [" it", "0.14102107"], [" true", "1.0"], [" that", "1.0"], [" the", "1.0"], [" first", "0.99998677"], [" man", "0.35916826"], [" on", "0.9969919"], [" the", "0.9424935"], [" moon", "1.0"], [" was", "1.0"], [" a", "0.9999994"], [" woman", "0.9990947"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99469334"], [" first", "1.0"], [" woman", "0.9899261"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" was", "1.0"], [" a", "0.9932019"], [" man", "0.9612498"], [" named", "0.78115326"], [" Sally", "1.0"], [" Ride", "1.0"], [".", "1.0"], [" She", "0.7448797"], [" was", "1.0"], [" the", "1.0"], [" first", "1.0"], [" American", "1.0"], [" woman", "1.0"], [" in", "0.5380731"], [" space", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" true", "1.0"], [" that", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "0.9998808"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" was", "1.0"], [" a", "1.0"], [" woman", "0.9999988"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" woman", "0.98949"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"]]}, "545": {"qid": "042642af049331a87b87", "term": "German Shepherd", "description": "Dog breed", "question": "Would a German Shepherd be welcome in an airport?", "answer": true, "facts": ["Airports in the US must be compliant with the ADA allowing for service dogs as medical equipment.", "Police security often use dogs like German Shepherds to search for drugs at airports.", "Some airlines have special travel accommodations for dogs."], "decomposition": ["What measures are used to provide security in airports?", "Which of #1 involve the use of animals?", "Are German Shepherds used as #2?"], "evidence": [[[["Airport security-2"]], [["Detection dog-1"]], [["German Shepherd-2"], "operation"]], [[["Airport security-7"]], [["Airport security-7"]], ["operation"]], [[["Airport security-7"]], [["Police dog-1"]], [["Police dog-11"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake's\" hair?\nFacts: A snowflake is a single snowflake. A snowflake's hair is a single hair.\nAnswer (True or False): True\nQuestion: What is the difference between a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a German Shepherd be welcome in an airport?\nFacts: Airports in the US must be compliant with the ADA allowing for service dogs as medical equipment. Police security often use dogs like German Shepherds to search for drugs at airports. Some airlines have special travel accommodations for dogs.\nAnswer (True or False):", "ct_subject": "Airports in the US must be compliant with the ADA allowing for service dogs as medical equipment. Police security often use dogs like German Shepherds to search for drugs at airports. Some airlines have special travel accommodations for dogs.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999905"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.99999547"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.07862067"], ["now", "1.0"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["'s", "0.9646621"], ["\"", "0.99244505"], [" hair", "0.8245014"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.80790347"], [" snow", "0.99993765"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "0.99837935"], [" snow", "0.9789007"], ["flake", "1.0"], [".", "0.9327329"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], ["'s", "1.0"], [" hair", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "1.0"], [" hair", "0.9998431"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"]]}, "546": {"qid": "d402074eec03cd7ea06e", "term": "Underworld", "description": "The mythic Relm of the Dead, located far underground (aka, Hades; Underworld)", "question": "Would Hades and Osiris hypothetically compete for real estate in the Underworld?", "answer": true, "facts": ["Hades was the Greek god of death and the Underworld.", "Osiris was the Egyptian god of the Underworld."], "decomposition": ["What was Hades the God of?", "What was Osiris the God of?", "Is there any overlap between #1 and #2?"], "evidence": [[[["Hades-1"]], [["Osiris-1"]], ["operation"]], [[["Hades-1"]], [["Osiris-1"]], ["operation"]], [[["Hades-1"]], [["Osiris-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: John Adams was the first person to be born in the United States.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: John Adams was the first person to be born in the United States.\nAnswer (", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Hades and Osiris hypothetically compete for real estate in the Underworld?\nFacts: Hades was the Greek god of death and the Underworld. Osiris was the Egyptian god of the Underworld.\nAnswer (True or False):", "ct_subject": "Hades was the Greek god of death and the Underworld. Osiris was the Egyptian god of the Underworld.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99982554"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999957"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9972357"], [" person", "0.9450042"], [" to", "1.0"], [" be", "0.94989496"], [" born", "0.9985732"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "0.9987503"], [" Adams", "0.9999999"], [" was", "0.99998724"], [" the", "0.99999535"], [" first", "1.0"], [" person", "1.0"], [" to", "0.999387"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "0.99903095"], [" Adams", "1.0"], [" was", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"]]}, "547": {"qid": "45c4f7aad64b5fdd76e9", "term": "Moustache", "description": "Facial hair grown on the upper lip", "question": "Would a Rockette look odd with a moustache? ", "answer": true, "facts": ["The Rockettes are an American dance group made up of all women.", "In America, it is uncommon to see a woman with a moustache."], "decomposition": ["What groups of people make up the Rockettes?", "Would #1 look odd with a moustache?"], "evidence": [[[["The Rockettes-2"], "no_evidence"], ["operation"]], [[["The Rockettes-5"]], [["Facial hair-5"]]], [[["The Rockettes-1"], "no_evidence"], [["Facial hair-5"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest word in the English language?\nFacts: The longest word in the English language is \"the\".\nAnswer (True or False): True\nQuestion: What is the longest word in the English language?\nFacts: The longest word in the English language is \"the\".\nAnswer (True or False): True\nQuestion: What", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a Rockette look odd with a moustache? \nFacts: The Rockettes are an American dance group made up of all women. In America, it is uncommon to see a woman with a moustache.\nAnswer (True or False):", "ct_subject": "The Rockettes are an American dance group made up of all women. In America, it is uncommon to see a woman with a moustache.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.6340304"], [" word", "0.891234"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], [" is", "1.0"], [" \"", "0.99999917"], ["the", "0.9999846"], ["\".", "0.99999905"], ["\n", "0.9725634"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.99999917"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], [" is", "1.0"], [" \"", "1.0"], ["the", "1.0"], ["\".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"]]}, "548": {"qid": "15885efe6f91724c16d9", "term": "Audi", "description": "Automotive manufacturing subsidiary of Volkswagen Group", "question": "Is sound barrier too much for Audi R8 V-10 Plus to break?", "answer": true, "facts": ["Audi R8 V-10 Plus is the fastest car produced by Audi.", "The Audi R8 V-10 Plus has a top speed of 205 MPH.", "To break the sound barrier, a jet must reach a speed of 770 MPH."], "decomposition": ["What is the top speed of an Audi R8 V-10 Plus?", "What speed must be obtained to break the sound barrier?", "Is #2 greater than #1?"], "evidence": [[[["Audi R8 (Type 4S)-11"]], [["Sound barrier-2"]], [["Audi R8 (Type 4S)-11", "Speed of sound-1"]]], [[["Audi S8-5"], "no_evidence"], [["Sound barrier-11"]], ["operation"]], [[["Audi R8-1"], "no_evidence"], [["Speed of sound-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it true that the first person to fly across the Atlantic Ocean was a man named Charles Lindbergh?\nFacts: Charles Lindbergh was a pilot. He flew across the Atlantic Ocean in 1927.\nAnswer (True or False): True\nQuestion", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is sound barrier too much for Audi R8 V-10 Plus to break?\nFacts: Audi R8 V-10 Plus is the fastest car produced by Audi. The Audi R8 V-10 Plus has a top speed of 205 MPH. To break the sound barrier, a jet must reach a speed of 770 MPH.\nAnswer (True or False):", "ct_subject": "Audi R8 V-10 Plus is the fastest car produced by Audi. The Audi R8 V-10 Plus has a top speed of 205 MPH. To break the sound barrier, a jet must reach a speed of 770 MPH.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9321889"], [" it", "0.99595743"], [" true", "0.99511814"], [" that", "1.0"], [" the", "1.0"], [" first", "0.999951"], [" person", "0.9418258"], [" to", "1.0"], [" fly", "0.5284505"], [" across", "0.9999933"], [" the", "1.0"], [" Atlantic", "1.0"], [" Ocean", "1.0"], [" was", "1.0"], [" a", "1.0"], [" man", "0.9664965"], [" named", "0.99992573"], [" Charles", "0.99999976"], [" Lind", "1.0"], ["ber", "1.0"], ["gh", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Charles", "1.0"], [" Lind", "1.0"], ["ber", "1.0"], ["gh", "1.0"], [" was", "1.0"], [" a", "0.9998134"], [" pilot", "0.9878419"], [".", "0.99999845"], [" He", "1.0"], [" flew", "0.9825959"], [" across", "0.9940202"], [" the", "1.0"], [" Atlantic", "1.0"], [" Ocean", "1.0"], [" in", "1.0"], [" 1927", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"]]}, "549": {"qid": "b110458494fa12444ff5", "term": "Kanji", "description": "adopted logographic Chinese characters used in the modern Japanese writing system", "question": "Can printing books in kanji instead of the Roman alphabet save trees?", "answer": true, "facts": ["The Kanji writing system uses less space to communicate ideas than an alphabet system.", "Trees are killed to make pages for books."], "decomposition": ["What type of writing system is kanji?", "Does #1 take up less space than an alphabet?"], "evidence": [[[["Logogram-1"]], [["Logogram-2"], "operation"]], [[["Kanji-1"]], [["Logogram-1"], "no_evidence"]], [[["Kanji-1"]], [["Kanji-49", "Kanji-80"], "no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake\"?\nFacts: A snowflake is a single crystal of ice. A snowflake is a single crystal of ice.\nAnswer (True or False): True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake\"?\nFacts: A snow", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can printing books in kanji instead of the Roman alphabet save trees?\nFacts: The Kanji writing system uses less space to communicate ideas than an alphabet system. Trees are killed to make pages for books.\nAnswer (True or False):", "ct_subject": "The Kanji writing system uses less space to communicate ideas than an alphabet system. Trees are killed to make pages for books.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.98631024"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.912694"], [" between", "1.0"], [" a", "0.9999833"], [" \"", "0.9942153"], ["s", "0.92349374"], ["now", "1.0"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.99999845"], ["now", "1.0"], ["flake", "1.0"], ["\"?", "0.99901843"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999999"], [" snow", "1.0"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "0.9669126"], [" crystal", "0.48436448"], [" of", "0.9650253"], [" ice", "1.0"], [".", "0.95602685"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" is", "1.0"], [" a", "0.9964887"], [" single", "1.0"], [" crystal", "1.0"], [" of", "1.0"], [" ice", "0.99894804"], [".", "1.0"], ["\n", "0.9999999"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" snow", "1.0"]]}, "550": {"qid": "27b84ef72c557aa8fc14", "term": "Watermelon", "description": "A large fruit with a smooth hard rind, of the gourd family", "question": "Is watermelon safe for people with a tricarboxylic acid allergy?", "answer": true, "facts": ["Tricarboxylic acid as an acid that manifests itself in fruits as citric acid.", "Citric acid can be found in citrus fruits such as oranges and lemon.", "Watermelon is not a citrus fruit."], "decomposition": ["What is the most common example of a tricarboxylic acid?", "Which kind of fruits is #1 usually present in?", "Is watermelon excluded from #2?"], "evidence": [[[["Tricarboxylic acid-1"]], [["Citric acid-1"]], [["Citrus-1", "Watermelon-2"], "operation"]], [[["Tricarboxylic acid-2"]], [["Citric acid-14"]], [["Citric acid-14", "Watermelon-8"], "operation"]], [[["Tricarboxylic acid-1"]], [["Citric acid-14"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it true that the first person to walk on the moon was a woman?\nFacts: No. The first person to walk on the moon was Neil Armstrong.\nAnswer (True or False): True\nQuestion: Is it true that the first person to walk on the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is watermelon safe for people with a tricarboxylic acid allergy?\nFacts: Tricarboxylic acid as an acid that manifests itself in fruits as citric acid. Citric acid can be found in citrus fruits such as oranges and lemon. Watermelon is not a citrus fruit.\nAnswer (True or False):", "ct_subject": "Tricarboxylic acid as an acid that manifests itself in fruits as citric acid. Citric acid can be found in citrus fruits such as oranges and lemon. Watermelon is not a citrus fruit.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999994"], [" it", "0.99999464"], [" true", "0.9999993"], [" that", "1.0"], [" the", "1.0"], [" first", "0.99993265"], [" person", "0.9997074"], [" to", "1.0"], [" walk", "0.12747692"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" was", "1.0"], [" a", "0.9988733"], [" woman", "0.6898935"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" No", "0.20498513"], [".", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" was", "1.0"], [" Neil", "0.9989109"], [" Armstrong", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" true", "1.0"], [" that", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" walk", "0.5627805"], [" on", "1.0"], [" the", "1.0"]]}, "551": {"qid": "0431a3fc727855c8ad83", "term": "Dalai Lama", "description": "Tibetan Buddhist spiritual teacher", "question": "Can the Dalai Lama fit in a car?", "answer": true, "facts": ["The Dalai Lama is a person.", "Cars are designed for people to sit in them."], "decomposition": ["What type of being is the Dalai Lama?", "Who are cars designed for?", "Is #1 the same as #2?"], "evidence": [[[["Dalai Lama-1"]], [["Car controls-23"]], ["operation"]], [[["Dalai Lama-1"]], [["Car-42"]], ["operation"]], [[["Dalai Lama-1", "Person-1"]], [["Car-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a man and a woman?\nFacts: A man is a human being. A woman is a human being.\nAnswer (True or False): True\nQuestion: What is the difference between a man and a woman?\nFacts: A man is a human being. A woman is a human being.\nAnswer (True or False): True\nQuestion: What is the difference between a man and a woman", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can the Dalai Lama fit in a car?\nFacts: The Dalai Lama is a person. Cars are designed for people to sit in them.\nAnswer (True or False):", "ct_subject": "The Dalai Lama is a person. Cars are designed for people to sit in them.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999963"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.997738"], [" between", "1.0"], [" a", "1.0"], [" man", "0.9960061"], [" and", "1.0"], [" a", "1.0"], [" woman", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" man", "1.0"], [" is", "1.0"], [" a", "1.0"], [" human", "0.9138831"], [" being", "1.0"], [".", "1.0"], [" A", "1.0"], [" woman", "1.0"], [" is", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "1.0"], [".", "0.9999999"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" man", "1.0"], [" and", "1.0"], [" a", "1.0"], [" woman", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" man", "1.0"], [" is", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "1.0"], [".", "1.0"], [" A", "1.0"], [" woman", "1.0"], [" is", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" man", "1.0"], [" and", "1.0"], [" a", "1.0"], [" woman", "1.0"]]}, "552": {"qid": "02eb6aacd72d1b93a83a", "term": "Vulcan (mythology)", "description": "Ancient Roman god of fire, volcanoes, and metalworking", "question": "Does the Roman god Vulcan have a Greek equivalent?", "answer": true, "facts": ["Vulcan is the Roman god of fire and metalworking.", "Hephaestus is the Greek god of fire and metalworking.", "They are the same mythological figure, one of many characters the Romans borrowed from the Greeks and changed their names."], "decomposition": ["What is the Roman god Vulcan god of?", "Is there a god of #1 in Greek mythology?"], "evidence": [[[["Vulcan (mythology)-17"]], [["Helios-13"]]], [[["Vulcan (mythology)-1"]], [["Vulcan (mythology)-1"]]], [[["Vulcan (mythology)-17"]], [["Hephaestus-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the Roman god of the sky?\nFacts: The Roman god of the sky was called Vulcan.\nAnswer (True or False): True\nQuestion: What is the name of the Roman god of the sea?\nFacts: The Roman god of the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does the Roman god Vulcan have a Greek equivalent?\nFacts: Vulcan is the Roman god of fire and metalworking. Hephaestus is the Greek god of fire and metalworking. They are the same mythological figure, one of many characters the Romans borrowed from the Greeks and changed their names.\nAnswer (True or False):", "ct_subject": "Vulcan is the Roman god of fire and metalworking. Hephaestus is the Greek god of fire and metalworking. They are the same mythological figure, one of many characters the Romans borrowed from the Greeks and changed their names.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.98904175"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" god", "1.0"], [" of", "1.0"], [" the", "1.0"], [" sky", "0.9388988"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Roman", "1.0"], [" god", "1.0"], [" of", "1.0"], [" the", "1.0"], [" sky", "1.0"], [" was", "0.73456603"], [" called", "1.0"], [" Vulcan", "0.32863572"], [".", "1.0"], ["\n", "0.99998474"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" god", "1.0"], [" of", "1.0"], [" the", "1.0"], [" sea", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Roman", "1.0"], [" god", "1.0"], [" of", "1.0"], [" the", "1.0"]]}, "553": {"qid": "e1f93419cb9a2f1d06ca", "term": "Christians", "description": "people who adhere to Christianity", "question": "Does Hammurabi's Code violate Christians Golden Rule?", "answer": true, "facts": ["The Golden Rule of Christianity states to do unto others as you would want them to do to you.", "Hammurabi's Code states an eye for an eye and a tooth for a tooth."], "decomposition": ["What is the golden rule in Christianity? ", "What does the Code of Hammurabi state?", "Is #1 the same meaning as #2?"], "evidence": [[[["Golden Rule-1", "Golden Rule-20"]], [["Code of Hammurabi-15", "Shofetim (parsha)-26"], "no_evidence"], ["operation"]], [[["Golden Rule-1"]], [["Code of Hammurabi-1"]], [["Eye for an eye-1"], "operation"]], [[["Golden Rule-21"]], [["Code of Hammurabi-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the Bible a true history book?\nFacts: The Bible is a collection of stories and myths. The stories are not historical.\nAnswer (True or False): True\nQuestion: Is the Bible a true book of morals?\nFacts: The Bible is a collection of stories and myths. The stories are not moral", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Hammurabi's Code violate Christians Golden Rule?\nFacts: The Golden Rule of Christianity states to do unto others as you would want them to do to you. Hammurabi's Code states an eye for an eye and a tooth for a tooth.\nAnswer (True or False):", "ct_subject": "The Golden Rule of Christianity states to do unto others as you would want them to do to you. Hammurabi's Code states an eye for an eye and a tooth for a tooth.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9925895"], [" the", "0.9984289"], [" Bible", "1.0"], [" a", "0.9999386"], [" true", "1.0"], [" history", "0.9628901"], [" book", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Bible", "1.0"], [" is", "1.0"], [" a", "1.0"], [" collection", "0.9918371"], [" of", "1.0"], [" stories", "0.80634815"], [" and", "0.9981432"], [" myths", "0.35689095"], [".", "0.9999386"], [" The", "0.00078803283"], [" stories", "0.9968573"], [" are", "1.0"], [" not", "1.0"], [" historical", "0.9976031"], [".", "0.999985"], ["\n", "0.9999999"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" Bible", "1.0"], [" a", "0.9999999"], [" true", "1.0"], [" book", "0.99999774"], [" of", "0.9996654"], [" morals", "0.9999999"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Bible", "1.0"], [" is", "1.0"], [" a", "1.0"], [" collection", "1.0"], [" of", "1.0"], [" stories", "1.0"], [" and", "1.0"], [" myths", "1.0"], [".", "1.0"], [" The", "1.0"], [" stories", "1.0"], [" are", "1.0"], [" not", "1.0"], [" moral", "1.0"]]}, "554": {"qid": "8eecf2ff459ef5a9b98a", "term": "Cactus", "description": "Family of mostly succulent plants, adapted to dry environments", "question": "Would an aerodynamic cactus benefit from more frequently closed stomata?", "answer": true, "facts": ["Cactus spines help the plant retain water by reducing air flow around the plant", "Aerodynamic objects have smooth surfaces ", "Crassulacean acid metabolism is used by cactuses ", "Crassulacean acid metabolism is when a plant's stomata stay closed during daylight or times of drought to prevent water loss"], "decomposition": ["What helps cacti conserve water?", "Of #1, what methods do not involve protrusions that might restrict air flow?", "Are closed stoma one of #2?"], "evidence": [[[["Cactus-28", "Cactus-29"]], [["Cactus-34"]], ["operation"]], [[["Cactus-1"]], [["Cactus-35"]], ["operation"]], [[["Cactus-13", "Cactus-14"], "no_evidence"], [["Cactus-13", "Cactus-14"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"cactus\" and a \"cactus-like\" plant?\nFacts: Cactuses are plants that have a spiny, or c", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would an aerodynamic cactus benefit from more frequently closed stomata?\nFacts: Cactus spines help the plant retain water by reducing air flow around the plant Aerodynamic objects have smooth surfaces  Crassulacean acid metabolism is used by cactuses  Crassulacean acid metabolism is when a plant's stomata stay closed during daylight or times of drought to prevent water loss\nAnswer (True or False):", "ct_subject": "Cactus spines help the plant retain water by reducing air flow around the plant Aerodynamic objects have smooth surfaces  Crassulacean acid metabolism is used by cactuses  Crassulacean acid metabolism is when a plant's stomata stay closed during daylight or times of drought to prevent water loss", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99872404"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.97569966"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.22382472"], ["c", "0.9999585"], ["actus", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["actus", "1.0"], ["-", "1.0"], ["like", "1.0"], ["\"", "1.0"], [" plant", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" C", "0.9240669"], ["act", "0.7160387"], ["uses", "0.15282866"], [" are", "1.0"], [" plants", "1.0"], [" that", "1.0"], [" have", "0.99999976"], [" a", "0.984406"], [" sp", "0.8354773"], ["iny", "1.0"], [",", "0.95333433"], [" or", "0.9987191"], [" c", "0.9807022"]]}, "555": {"qid": "a0eb63776f7720ec0bcc", "term": "Parsifal", "description": "opera in three acts by Richard Wagner", "question": "Was the subject of Parsifal taken from British folklore?", "answer": true, "facts": ["Parsifal was loosely based on a poem about Percival", "Percival was a Knight of the Round Table", "King Arthur and the Knights of the Round Table were products of British folklore"], "decomposition": ["What was the opera 'Parsifal' based on?", "Who is the main character in #1?", "Which group is #2 part of?", "Did #3 originate from British folklore?"], "evidence": [[[["Parsifal-1"]], [["Parzival-1"]], [["Percival-1"]], [["Knights of the Round Table-1"]]], [[["Parsifal-1"]], [["Percival-1"]], [["Knights of the Round Table-1"]], ["operation"]], [[["Parsifal-5"]], [["Parsifal-34"]], [["Parsifal-33"]], [["Knight-4"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Was the subject of The Wizard of Oz taken from the Wizard of Oz?\nFacts: The Wizard of Oz was loosely based on a poem about the Wizard of Oz was a product of British folklore\nAnswer (True or False): True\nQuestion: Was the subject of The Wizard of Oz taken from the Wizard of Oz?\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was the subject of Parsifal taken from British folklore?\nFacts: Parsifal was loosely based on a poem about Percival Percival was a Knight of the Round Table King Arthur and the Knights of the Round Table were products of British folklore\nAnswer (True or False):", "ct_subject": "Parsifal was loosely based on a poem about Percival Percival was a Knight of the Round Table King Arthur and the Knights of the Round Table were products of British folklore", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "0.9998454"], [" the", "1.0"], [" subject", "1.0"], [" of", "1.0"], [" The", "0.9993327"], [" Wizard", "0.7722923"], [" of", "1.0"], [" Oz", "1.0"], [" taken", "1.0"], [" from", "1.0"], [" the", "0.99639916"], [" Wizard", "1.0"], [" of", "1.0"], [" Oz", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Wizard", "1.0"], [" of", "1.0"], [" Oz", "1.0"], [" was", "1.0"], [" loosely", "1.0"], [" based", "1.0"], [" on", "1.0"], [" a", "1.0"], [" poem", "1.0"], [" about", "1.0"], [" the", "0.9995511"], [" Wizard", "1.0"], [" of", "1.0"], [" Oz", "1.0"], [" was", "1.0"], [" a", "1.0"], [" product", "1.0"], [" of", "1.0"], [" British", "0.9999999"], [" folklore", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "1.0"], [" the", "1.0"], [" subject", "1.0"], [" of", "1.0"], [" The", "1.0"], [" Wizard", "1.0"], [" of", "1.0"], [" Oz", "1.0"], [" taken", "1.0"], [" from", "1.0"], [" the", "1.0"], [" Wizard", "1.0"], [" of", "1.0"], [" Oz", "1.0"], ["?", "1.0"], ["\n", "1.0"]]}, "556": {"qid": "d06b92172f2854fa1f62", "term": "Parsley", "description": "species of plant, herb", "question": "Is it normal to find parsley in multiple sections of the grocery store?", "answer": true, "facts": ["Parsley is available in both fresh and dry forms.", "Fresh parsley must be kept cool.", "Dry parsley is a shelf stable product."], "decomposition": ["What forms of parsley are consumed?", "What areas of the grocery store carry each of #1?", "Does #2 include places separate from one another?"], "evidence": [[[["Parsley-13", "Parsley-15"]], [["Produce-2"], "no_evidence"], ["operation"]], [[["Parsley-18", "Parsley-3"]], [["Supermarket-3"], "no_evidence"], ["no_evidence"]], [[["Parsley-16", "Parsley-18"], "no_evidence"], [["Canning-1", "Produce-2"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it normal to find a large number of people who have a genetic mutation that causes a disease?\nFacts: The number of people with a genetic mutation that causes a disease is very small.\nAnswer (True or False): True\nQuestion: Is it normal to find a large number of people who have a genetic mutation that causes a disease?\nF", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is it normal to find parsley in multiple sections of the grocery store?\nFacts: Parsley is available in both fresh and dry forms. Fresh parsley must be kept cool. Dry parsley is a shelf stable product.\nAnswer (True or False):", "ct_subject": "Parsley is available in both fresh and dry forms. Fresh parsley must be kept cool. Dry parsley is a shelf stable product.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9978148"], [" it", "1.0"], [" normal", "0.9998859"], [" to", "0.99999976"], [" find", "1.0"], [" a", "0.9999758"], [" large", "0.9850011"], [" number", "1.0"], [" of", "1.0"], [" people", "0.998946"], [" who", "0.9055196"], [" have", "0.9525335"], [" a", "0.6460137"], [" genetic", "0.8925817"], [" mutation", "0.99917173"], [" that", "1.0"], [" causes", "1.0"], [" a", "0.753866"], [" disease", "0.785507"], ["?", "0.9999769"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.960649"], [" number", "0.97778314"], [" of", "1.0"], [" people", "1.0"], [" with", "0.999913"], [" a", "1.0"], [" genetic", "1.0"], [" mutation", "1.0"], [" that", "1.0"], [" causes", "1.0"], [" a", "1.0"], [" disease", "1.0"], [" is", "1.0"], [" very", "0.9999987"], [" small", "1.0"], [".", "1.0"], ["\n", "0.9999418"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" normal", "1.0"], [" to", "1.0"], [" find", "1.0"], [" a", "1.0"], [" large", "1.0"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" who", "1.0"], [" have", "1.0"], [" a", "1.0"], [" genetic", "1.0"], [" mutation", "1.0"], [" that", "1.0"], [" causes", "1.0"], [" a", "1.0"], [" disease", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"]]}, "557": {"qid": "626694e8f6691431f311", "term": "Kayak", "description": "small boat propelled with a double-bladed paddle", "question": "Are kayaks used at the summit of Mount Everest?", "answer": true, "facts": ["Kayaks are used to transport people in water.", "The summit of Mount Everest has no running water."], "decomposition": ["What are kayaks typically used on?", "Does the summit of Mount Everest have any #1?"], "evidence": [[[["Kayak-1"]], [["Mount Everest-122"], "operation"]], [[["Whitewater kayaking-1"]], [["Dudh Koshi-2"]]], [[["Kayak-1"]], [["Mount Everest-122"], "no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any people who have been to the moon?\nFacts: There are no people who have been to the moon.\nAnswer (True or False): True\nQuestion: Are there any people who have been to the moon?\nFacts: There are no people who have been to the moon.\nAnswer (True or False): True\nQuestion: Are there any people who have been to the moon?", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are kayaks used at the summit of Mount Everest?\nFacts: Kayaks are used to transport people in water. The summit of Mount Everest has no running water.\nAnswer (True or False):", "ct_subject": "Kayaks are used to transport people in water. The summit of Mount Everest has no running water.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.9963952"], [" there", "1.0"], [" any", "0.99575645"], [" people", "0.9940758"], [" who", "0.9992812"], [" have", "0.9983285"], [" been", "0.9984339"], [" to", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "0.99999034"], [" are", "1.0"], [" no", "1.0"], [" people", "1.0"], [" who", "1.0"], [" have", "1.0"], [" been", "1.0"], [" to", "1.0"], [" the", "1.0"], [" moon", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "1.0"], [" any", "1.0"], [" people", "1.0"], [" who", "1.0"], [" have", "1.0"], [" been", "1.0"], [" to", "1.0"], [" the", "0.99855477"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "1.0"], [" are", "1.0"], [" no", "1.0"], [" people", "1.0"], [" who", "1.0"], [" have", "1.0"], [" been", "1.0"], [" to", "1.0"], [" the", "1.0"], [" moon", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "1.0"], [" any", "1.0"], [" people", "1.0"], [" who", "1.0"], [" have", "1.0"], [" been", "1.0"], [" to", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"]]}, "558": {"qid": "efd8365a1bcecfab14bc", "term": "Orange County, California", "description": "County in California, United States", "question": "Does Orange County, California require airplanes to be quiet?", "answer": true, "facts": ["John Wayne Airport is in Orange County.", "John Wayne Airport is in very close proximity to residential areas.", "There is a General Aviation Noise Ordinance in Orange County, California. ", "Commercial pilots will cut the engine of the aircraft on arrival and departure from Orange County, California. "], "decomposition": ["Which ordinance must airports within or close to Orange County, California abide by?", "What actions do commercial pilots take concerning their engine noise when arriving or departing Orange County, California?", "Does #1 and #2 require that their airplanes make less noise?"], "evidence": [[[["John Wayne Airport-33"]], [["John Wayne Airport-38"]], ["no_evidence"]], [[["Orange County, California-13"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["John Wayne Airport-33"]], [["John Wayne Airport-34"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the largest in the United States?\nFacts: The largest city in the United States is Los Angeles. \nAnswer (True or False): True\nQuestion: What is the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Orange County, California require airplanes to be quiet?\nFacts: John Wayne Airport is in Orange County. John Wayne Airport is in very close proximity to residential areas. There is a General Aviation Noise Ordinance in Orange County, California.  Commercial pilots will cut the engine of the aircraft on arrival and departure from Orange County, California. \nAnswer (True or False):", "ct_subject": "John Wayne Airport is in Orange County. John Wayne Airport is in very close proximity to residential areas. There is a General Aviation Noise Ordinance in Orange County, California.  Commercial pilots will cut the engine of the aircraft on arrival and departure from Orange County, California. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9448068"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999999"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "0.888434"], [" the", "1.0"], [" United", "0.99996865"], [" States", "1.0"], [" that", "1.0"], [" is", "0.38541803"], [" the", "1.0"], [" largest", "0.9977724"], [" in", "0.9890598"], [" the", "0.9997589"], [" United", "0.96529293"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9950039"], [" largest", "0.9999974"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "1.0"], [" Los", "0.999923"], [" Angeles", "1.0"], [".", "0.80871004"], [" ", "0.9294274"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"]]}, "559": {"qid": "404ab9200fe1935e7f4c", "term": "Lullaby", "description": "soothing song, usually sung to young children before they go to sleep", "question": "Is an inappropriate lullaby Love Song from November 11, 2000?", "answer": true, "facts": ["Lullabies are often simple and repetitive.", "Lullabies are used to soothe young children, usually to go to sleep.", "Love Song was a song released by Marilyn Manson on his November 11, 2000 record.", "Marilyn Manson is a heavy metal band.", "The lyrics to Love Song has curse words and scary imagery."], "decomposition": ["Why are lullabies usually sung?", "What were the features of The Love Song by Marilyn Manson?", "Would #2 be unsuitable to achieve #1?"], "evidence": [[[["Lullaby-1"]], [["Holy Wood (In the Shadow of the Valley of Death)-1", "Holy Wood (In the Shadow of the Valley of Death)-21"]], ["operation"]], [[["Lullaby-1"]], ["no_evidence"], ["no_evidence"]], [[["Lullaby-1"]], [["Marilyn Manson-8"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the song \"Love Song\" from the album \"The Pale Emperor\" by Marilyn Manson?\nFacts: The song \"Love Song\" was released on the album \"The Pale", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is an inappropriate lullaby Love Song from November 11, 2000?\nFacts: Lullabies are often simple and repetitive. Lullabies are used to soothe young children, usually to go to sleep. Love Song was a song released by Marilyn Manson on his November 11, 2000 record. Marilyn Manson is a heavy metal band. The lyrics to Love Song has curse words and scary imagery.\nAnswer (True or False):", "ct_subject": "Lullabies are often simple and repetitive. Lullabies are used to soothe young children, usually to go to sleep. Love Song was a song released by Marilyn Manson on his November 11, 2000 record. Marilyn Manson is a heavy metal band. The lyrics to Love Song has curse words and scary imagery.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "0.97170895"], [" name", "0.9342587"], [" of", "1.0"], [" the", "1.0"], [" song", "0.6351686"], [" \"", "1.0"], ["Love", "0.8856147"], [" Song", "1.0"], ["\"", "1.0"], [" from", "1.0"], [" the", "0.9999988"], [" album", "1.0"], [" \"", "1.0"], ["The", "1.0"], [" Pale", "0.92176193"], [" Emperor", "1.0"], ["\"", "1.0"], [" by", "0.99999535"], [" Marilyn", "1.0"], [" Manson", "1.0"], ["?", "0.99999964"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" song", "0.840568"], [" \"", "1.0"], ["Love", "1.0"], [" Song", "1.0"], ["\"", "1.0"], [" was", "0.99998355"], [" released", "1.0"], [" on", "0.99991345"], [" the", "1.0"], [" album", "1.0"], [" \"", "1.0"], ["The", "1.0"], [" Pale", "1.0"]]}, "560": {"qid": "7f139bf51273517efd4e", "term": "Christopher Nolan", "description": "British\u2013American film director, screenwriter, and producer", "question": "Is Christopher Nolan indebted to Bob Kane?", "answer": true, "facts": ["Christopher Nolan rose to fame in large part because of his trilogy of Batman movies released from 2005 to 2012", "Bob Kane was the original artist and co-creator of Batman"], "decomposition": ["Who created the Batman?", "To what films was Christopher Nolan's Hollywood success attributed to from the 2000s to 2010s?", "Did #1 provide the source material to #2?"], "evidence": [[[["Batman-1"]], [["Christopher Nolan-16"]], ["operation"]], [[["Batman-1"]], [["Christopher Nolan-14"]], ["operation"]], [[["Batman-1"]], [["Bruce Wayne (The Dark Knight trilogy)-6"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first movie to be released in the United States?\nFacts: The first movie to be released in the United States was The Wizard of Oz in 1939.\nAnswer (True or False): True\nQuestion: What is the name of the first movie to be released in the United States?\nFacts: The first movie to be released in", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Christopher Nolan indebted to Bob Kane?\nFacts: Christopher Nolan rose to fame in large part because of his trilogy of Batman movies released from 2005 to 2012 Bob Kane was the original artist and co-creator of Batman\nAnswer (True or False):", "ct_subject": "Christopher Nolan rose to fame in large part because of his trilogy of Batman movies released from 2005 to 2012 Bob Kane was the original artist and co-creator of Batman", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.28706226"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999995"], [" of", "1.0"], [" the", "1.0"], [" first", "0.71783596"], [" movie", "0.47877198"], [" to", "0.98366725"], [" be", "0.9999833"], [" released", "1.0"], [" in", "0.9999999"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" movie", "1.0"], [" to", "1.0"], [" be", "1.0"], [" released", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" The", "0.6396338"], [" Wizard", "1.0"], [" of", "1.0"], [" Oz", "1.0"], [" in", "0.9796538"], [" 1939", "1.0"], [".", "0.99999976"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" movie", "1.0"], [" to", "1.0"], [" be", "1.0"], [" released", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" movie", "1.0"], [" to", "1.0"], [" be", "1.0"], [" released", "1.0"], [" in", "1.0"]]}, "561": {"qid": "0d2746ffc1870126ee29", "term": "Cane toad", "description": "World's largest toad", "question": "Would the average Hawaiian male experience more days on Earth compared to a wild cane toad?", "answer": true, "facts": ["Cane toads have a life expectancy of 10 to 15 years in the wild.", "The average life expectancy of a male born in Hawaii is 79.3 years as of 2018."], "decomposition": ["What is the average lifespan of an average Hawaiian male?", "What is the average lifespan of a wild cane toad?", "Is #1 greater than #2?"], "evidence": [[[["Human-58"], "no_evidence"], [["Cane toad-10"]], ["operation"]], [["no_evidence"], [["Cane toad-10"]], ["operation"]], [[["Life expectancy-12"], "no_evidence"], [["Cane toad-10"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human?\nFacts: The average weight of a human is about 100 pounds.\nAnswer (True or False): True\nQuestion: What is the average height of a human?\nFacts: The average height of a human is about 5 feet, 9 inches.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would the average Hawaiian male experience more days on Earth compared to a wild cane toad?\nFacts: Cane toads have a life expectancy of 10 to 15 years in the wild. The average life expectancy of a male born in Hawaii is 79.3 years as of 2018.\nAnswer (True or False):", "ct_subject": "Cane toads have a life expectancy of 10 to 15 years in the wild. The average life expectancy of a male born in Hawaii is 79.3 years as of 2018.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9993223"], [" is", "1.0"], [" the", "1.0"], [" average", "0.99999964"], [" weight", "0.9963987"], [" of", "1.0"], [" a", "1.0"], [" human", "0.99995244"], ["?", "0.99867576"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" weight", "0.87731755"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" is", "1.0"], [" about", "0.9998925"], [" 100", "0.9998636"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" height", "0.9999989"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" height", "1.0"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 5", "0.99999213"], [" feet", "0.99999845"], [",", "0.9980192"], [" 9", "0.54532593"], [" inches", "1.0"], [".", "0.46014485"]]}, "562": {"qid": "53271c927076992cfb21", "term": "Suicide", "description": "Intentional act of causing one's own death", "question": "Is slitting your wrists an unreliable suicide method?", "answer": true, "facts": ["Wrist slitting has only a 6% mortality rate.", "Many people cannot complete the action of slitting their wrists due to pain or shock."], "decomposition": ["How often do people survive attempts to commit suicide by wrist-slitting?", "Does #1 indicate a high chance of survival?"], "evidence": [[["no_evidence"], ["no_evidence"]], [["no_evidence"], ["no_evidence"]], [[["Suicide methods-5"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it true that the first person to walk on the moon was a woman?\nFacts: The first person to walk on the moon was a man.\nAnswer (True or False): True\nQuestion: Is it true that the first person to fly across the Atlantic Ocean was a woman?\nFacts: The first person to fly across the Atlantic Ocean was a man.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is slitting your wrists an unreliable suicide method?\nFacts: Wrist slitting has only a 6% mortality rate. Many people cannot complete the action of slitting their wrists due to pain or shock.\nAnswer (True or False):", "ct_subject": "Wrist slitting has only a 6% mortality rate. Many people cannot complete the action of slitting their wrists due to pain or shock.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9996178"], [" it", "0.9782552"], [" true", "0.9968015"], [" that", "1.0"], [" the", "1.0"], [" first", "0.8009094"], [" person", "0.6528469"], [" to", "1.0"], [" walk", "0.99791974"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" was", "1.0"], [" a", "0.9999825"], [" woman", "0.9985551"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999802"], [" first", "1.0"], [" person", "0.99996114"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" was", "1.0"], [" a", "1.0"], [" man", "1.0"], [".", "0.99994683"], ["\n", "0.9986933"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" true", "1.0"], [" that", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" fly", "0.9994128"], [" across", "0.9998554"], [" the", "1.0"], [" Atlantic", "1.0"], [" Ocean", "1.0"], [" was", "1.0"], [" a", "1.0"], [" woman", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" fly", "1.0"], [" across", "1.0"], [" the", "1.0"], [" Atlantic", "1.0"], [" Ocean", "1.0"], [" was", "1.0"], [" a", "1.0"], [" man", "1.0"], [".", "1.0"]]}, "563": {"qid": "3f59cf2d6b48378dbefe", "term": "Woodrow Wilson", "description": "28th president of the United States", "question": "Was Woodrow Wilson sandwiched between two presidents from the opposing party?", "answer": true, "facts": ["Woodrow Wilson was a Democratic president and was between Taft and Harding.", "President William Howard Taft was a Repubican.", "President Warren G. Harding was a Republican."], "decomposition": ["Who was president before Woodrow Wilson?", "Who was president after Woodrow Wilson?", "What is the party of #1?", "What is the party of #2?", "Are #3 and #4 the same as each other and not the same as Wilson's party?"], "evidence": [[[["William Howard Taft-1"]], [["Warren G. Harding-1"]], [["William Howard Taft-3"]], [["Warren G. Harding-1"]], [["Woodrow Wilson-1"], "operation"]], [[["William Howard Taft-1"]], [["Presidency of Warren G. Harding-2"]], [["William Howard Taft-1"]], [["Warren G. Harding-26"]], [["Woodrow Wilson-30"], "operation"]], [[["Woodrow Wilson-2"]], [["Warren G. Harding-1", "Woodrow Wilson-1"]], [["William Howard Taft-3"]], [["Warren G. Harding-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first president of the United States?\nFacts: George Washington.\nAnswer (True or False): True\nQuestion: What is the name of the first president of the United States?\nFacts: George Washington.\nAnswer (True or False): True\nQuestion: What is the name of", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Woodrow Wilson sandwiched between two presidents from the opposing party?\nFacts: Woodrow Wilson was a Democratic president and was between Taft and Harding. President William Howard Taft was a Repubican. President Warren G. Harding was a Republican.\nAnswer (True or False):", "ct_subject": "Woodrow Wilson was a Democratic president and was between Taft and Harding. President William Howard Taft was a Repubican. President Warren G. Harding was a Republican.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.98543394"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99999845"], [" of", "1.0"], [" the", "1.0"], [" first", "0.94729084"], [" president", "0.99999964"], [" of", "0.9961242"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" George", "0.99996233"], [" Washington", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" president", "1.0"], [" of", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" George", "1.0"], [" Washington", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"]]}, "564": {"qid": "dcb503515e6282a1b30f", "term": "Earth's magnetic field", "description": "Magnetic field that extends from the Earth\u2019s inner core to where it meets the solar wind", "question": "Would a compass attuned to Earth's magnetic field be a bad gift for a Christmas elf??", "answer": true, "facts": ["Christmas elves work in Santa's workshop on the North Pole", "Magnetic compasses point to the North Pole ", "If you are on the North Pole a compass will not work"], "decomposition": ["What do compasses do?", "Where do Christmas elves work?", "Would #1 not be effective if you are already at #2?"], "evidence": [[[["Compass-1"]], [["Christmas elf-1"]], [["North Magnetic Pole-1"]]], [[["Compass-1"]], [["North Pole-61"]], ["operation"]], [[["Compass-19", "Compass-20", "Compass-7"]], [["Christmas elf-1", "Christmas elf-7"]], [["Compass-35"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake with a hole in it\"?\nFacts: A snowflake is a single crystal of ice that has been frozen. A snowflake with a hole in it is a single crystal of ice that has been melted.\nAnswer (True or", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a compass attuned to Earth's magnetic field be a bad gift for a Christmas elf??\nFacts: Christmas elves work in Santa's workshop on the North Pole Magnetic compasses point to the North Pole  If you are on the North Pole a compass will not work\nAnswer (True or False):", "ct_subject": "Christmas elves work in Santa's workshop on the North Pole Magnetic compasses point to the North Pole  If you are on the North Pole a compass will not work", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999926"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.8244284"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.8274661"], ["now", "1.0"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], [" with", "0.54282963"], [" a", "1.0"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" snow", "0.99999964"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "0.9999368"], [" crystal", "0.9999573"], [" of", "0.6629257"], [" ice", "1.0"], [" that", "0.96584177"], [" has", "0.9955739"], [" been", "1.0"], [" frozen", "0.9877037"], [".", "1.0"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" with", "1.0"], [" a", "1.0"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "0.99991035"], [" crystal", "1.0"], [" of", "1.0"], [" ice", "0.9999931"], [" that", "1.0"], [" has", "1.0"], [" been", "1.0"], [" melted", "0.8972964"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"]]}, "565": {"qid": "102db3a12d5a45bf7a2a", "term": "RoboCop", "description": "1987 science fiction film directed by Paul Verhoeven", "question": "Is RoboCop director from same country as Gaite Jansen?", "answer": true, "facts": ["Robocop was directed by Paul Verhoeven.", "Paul Verhoeven was born in Amsterdam, Netherlands.", "Gaite Jansen is an actress known for Jett and Peaky Blinders and was born in Rotterdam, Netherlands."], "decomposition": ["Who is the director of the movie RoboCop?", "Where was #1 born?", "Where was Gaite Jansen born?", "Are #2 and #3 the same?"], "evidence": [[[["RoboCop-1"]], [["Paul Verhoeven-1", "Paul Verhoeven-4"]], [["Gaite Jansen-1"]], ["operation"]], [[["RoboCop-1"]], [["Paul Verhoeven-4"]], [["Gaite Jansen-1"]], ["operation"]], [[["RoboCop-13"]], [["Paul Verhoeven-4"]], [["Gaite Jansen-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city in the movie \"The Avengers\" the same as the city in the book \"The Avengers\"?\nFacts: The name of the city in the movie \"The Avengers\" is New York. The name of the city in the book \"The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is RoboCop director from same country as Gaite Jansen?\nFacts: Robocop was directed by Paul Verhoeven. Paul Verhoeven was born in Amsterdam, Netherlands. Gaite Jansen is an actress known for Jett and Peaky Blinders and was born in Rotterdam, Netherlands.\nAnswer (True or False):", "ct_subject": "Robocop was directed by Paul Verhoeven. Paul Verhoeven was born in Amsterdam, Netherlands. Gaite Jansen is an actress known for Jett and Peaky Blinders and was born in Rotterdam, Netherlands.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999844"], [" the", "0.99887997"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9639892"], [" in", "0.30744898"], [" the", "0.9993075"], [" movie", "1.0"], [" \"", "0.9999517"], ["The", "1.0"], [" Avengers", "0.7112362"], ["\"", "1.0"], [" the", "0.38604036"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" city", "0.9945208"], [" in", "1.0"], [" the", "1.0"], [" book", "0.3794754"], [" \"", "0.9999993"], ["The", "1.0"], [" Avengers", "1.0"], ["\"?", "0.9999894"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" name", "0.89999235"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "1.0"], ["The", "1.0"], [" Avengers", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" New", "0.993666"], [" York", "1.0"], [".", "0.9999659"], [" The", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" book", "1.0"], [" \"", "1.0"], ["The", "1.0"]]}, "566": {"qid": "6d62de0fed9d8151b413", "term": "Torah", "description": "First five books of the Hebrew Bible", "question": "Does Happy Gilmore Productions CEO own a Torah?", "answer": true, "facts": ["The CEO of Happy Gilmore Productions is Adam Sandler.", "Adam Sandler's religious beliefs are Judaism. ", "The Torah is the first part of the bible in Judaism."], "decomposition": ["Who is the CEO of Happy Gilmore Productions?", "What religion does #1 follow?", "What religion uses the Torah?", "Is #2 the same as #3?"], "evidence": [[[["Happy Madison Productions-1"], "no_evidence"], [["Adam Sandler-26"]], [["Torah-1"]], ["operation"]], [[["Happy Madison Productions-1"], "no_evidence"], [["Adam Sandler-5"], "no_evidence"], [["Torah-1", "Torah-1"]], ["operation"]], [["no_evidence"], ["no_evidence"], [["Jews-1", "Torah-1"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city of New York City pronounced \"New York\"?\nFacts: The name of the city of New York City is pronounced \"New York\".\nAnswer (True or False): True\nQuestion: Is the name of the city of New York City pronounced \"New York\"?\nFacts: The name of the city of New York City is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Happy Gilmore Productions CEO own a Torah?\nFacts: The CEO of Happy Gilmore Productions is Adam Sandler. Adam Sandler's religious beliefs are Judaism.  The Torah is the first part of the bible in Judaism.\nAnswer (True or False):", "ct_subject": "The CEO of Happy Gilmore Productions is Adam Sandler. Adam Sandler's religious beliefs are Judaism.  The Torah is the first part of the bible in Judaism.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.23223443"], [" the", "0.999987"], [" name", "0.99997663"], [" of", "0.9998517"], [" the", "1.0"], [" city", "0.99998903"], [" of", "0.99998426"], [" New", "0.9999994"], [" York", "1.0"], [" City", "1.0"], [" pronounced", "0.6124517"], [" \"", "1.0"], ["New", "1.0"], [" York", "1.0"], ["\"?", "0.9593366"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9868995"], [" name", "0.99999714"], [" of", "1.0"], [" the", "0.9992331"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" City", "0.596995"], [" is", "1.0"], [" pronounced", "1.0"], [" \"", "1.0"], ["New", "1.0"], [" York", "1.0"], ["\".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999931"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" City", "1.0"], [" pronounced", "1.0"], [" \"", "1.0"], ["New", "1.0"], [" York", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" City", "1.0"], [" is", "1.0"]]}, "567": {"qid": "4ceb5ba3fb64a5f89a59", "term": "ZIP Code", "description": "numeric postal code used in the United States", "question": "Do most people only memorize slightly over half of their ZIP code?", "answer": true, "facts": ["ZIP codes in the US are 9 digits in length. ", "Most forms in the US only require and have space for the first 5 digits of a ZIP code?"], "decomposition": ["How long are zip codes in the US?", "When forms ask for zip codes, how many spaces do they typically request?", "Is #2 less than #1?"], "evidence": [[[["ZIP Code-1"]], [["ZIP Code-10"], "no_evidence"], ["operation"]], [[["ZIP Code-1"]], ["no_evidence"], ["no_evidence", "operation"]], [[["ZIP Code-1"]], [["ZIP Code-10"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human?\nFacts: The average human weight is about 100 pounds.\nAnswer (True or False): True\nQuestion: What is the average height of a human?\nFacts: The average human height is about 6 feet tall.\nAnswer (True or False): True\nQuestion: What is the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do most people only memorize slightly over half of their ZIP code?\nFacts: ZIP codes in the US are 9 digits in length.  Most forms in the US only require and have space for the first 5 digits of a ZIP code?\nAnswer (True or False):", "ct_subject": "ZIP codes in the US are 9 digits in length.  Most forms in the US only require and have space for the first 5 digits of a ZIP code?", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99404114"], [" is", "1.0"], [" the", "1.0"], [" average", "0.5057658"], [" weight", "0.7873305"], [" of", "1.0"], [" a", "0.99999475"], [" human", "1.0"], ["?", "0.9879389"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.9998479"], [" weight", "0.960552"], [" is", "1.0"], [" about", "0.99999225"], [" 100", "0.99902296"], [" pounds", "0.9999995"], [".", "1.0"], ["\n", "0.9999155"], ["Answer", "0.99999976"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" height", "0.9999999"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.99999976"], [" height", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 6", "0.9103387"], [" feet", "1.0"], [" tall", "0.9956463"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"]]}, "568": {"qid": "75b9eafe50ef589e8f78", "term": "Honey badger", "description": "species of mammal", "question": "Would a snake have reasons to fear a honey badger?", "answer": true, "facts": ["Snakes are considered prey to Honey Badgers.", "Honey Badgers have sharp teeth and are carnivorous. "], "decomposition": ["What prey do Honey badgers hunt?", "Are snakes listed in #1?"], "evidence": [[[["Honey badger-21"]], [["Honey badger-21"]]], [[["Honey badger-21"]], ["operation"]], [[["Honey badger-21"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the capital of the state of New York?\nFacts: The city of New York is the capital of the state of New York.\nAnswer (True or False): True\nQuestion: What is the name of the city in the United States that is the capital of the state of New York?\nFacts: The city of New", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a snake have reasons to fear a honey badger?\nFacts: Snakes are considered prey to Honey Badgers. Honey Badgers have sharp teeth and are carnivorous. \nAnswer (True or False):", "ct_subject": "Snakes are considered prey to Honey Badgers. Honey Badgers have sharp teeth and are carnivorous. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999994"], [" is", "1.0"], [" the", "1.0"], [" name", "0.5837262"], [" of", "1.0"], [" the", "1.0"], [" city", "0.61095136"], [" in", "0.6139022"], [" the", "0.98454547"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "0.999514"], [" the", "1.0"], [" capital", "0.9999887"], [" of", "1.0"], [" the", "1.0"], [" state", "0.9999881"], [" of", "1.0"], [" New", "0.1907944"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9880513"], [" city", "0.99869883"], [" of", "0.86047506"], [" New", "1.0"], [" York", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" New", "0.99976546"], [" York", "0.9999765"], ["?", "1.0"], ["\n", "1.0"], ["F", "0.9999999"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"]]}, "569": {"qid": "1c505cad07ffe97f4e41", "term": "Snowshoe", "description": "Footwear for walking easily across snow", "question": "Has Burger King  contributed to a decrease in need for snowshoes?", "answer": true, "facts": ["Burger king serves beef", "Beef farming is associated with increased global temperatures and decreased snowfall"], "decomposition": ["What is the main food item that burger king sells?", "What kind of meat is in #1?", "What does farming for #2 do to the global temperature?", "If #3 occurs, are people less lilely to need snowshoes?"], "evidence": [[[["Burger King-1"]], [["Pork-2"]], ["no_evidence"], ["no_evidence"]], [[["Burger King-1"]], [["Hamburger-1"]], [["Environmental impact of meat production-19"], "no_evidence"], [["Snowshoe-1"], "operation"]], [[["Burger King-1"]], [["Meat-9"]], [["Cattle-90"]], [["Global warming-3"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it true that the first person to use the word \"gay\" was a man?\nFacts: The first person to use the word \"gay\" was a man.\nAnswer (True or False): True\nQuestion: Is it true that the first person to use the word \"gay\" was a man?\nFacts: The first person to use the word \"gay\" was a man.\nAnswer", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Has Burger King  contributed to a decrease in need for snowshoes?\nFacts: Burger king serves beef Beef farming is associated with increased global temperatures and decreased snowfall\nAnswer (True or False):", "ct_subject": "Burger king serves beef Beef farming is associated with increased global temperatures and decreased snowfall", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.80900884"], [" it", "0.30110478"], [" true", "1.0"], [" that", "1.0"], [" the", "1.0"], [" first", "0.9999927"], [" person", "0.82658064"], [" to", "1.0"], [" use", "0.34232402"], [" the", "0.9999999"], [" word", "1.0"], [" \"", "1.0"], ["gay", "0.9930392"], ["\"", "1.0"], [" was", "1.0"], [" a", "1.0"], [" man", "0.9999995"], ["?", "0.66697484"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "0.99517536"], [" person", "0.9999713"], [" to", "1.0"], [" use", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["gay", "1.0"], ["\"", "1.0"], [" was", "1.0"], [" a", "1.0"], [" man", "1.0"], [".", "0.9999999"], ["\n", "0.9999201"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" true", "1.0"], [" that", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" use", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["gay", "1.0"], ["\"", "1.0"], [" was", "1.0"], [" a", "1.0"], [" man", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" use", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["gay", "1.0"], ["\"", "1.0"], [" was", "1.0"], [" a", "1.0"], [" man", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"]]}, "570": {"qid": "0e8b696f5770ad5e3ea7", "term": "Pablo Escobar", "description": "Colombian drug lord (1949\u20131993)", "question": "Did Pablo Escobar's nickname collection outshine Robert Moses Grove's?", "answer": true, "facts": ["Robert Moses Grove was a baseball player nicknamed Lefty Grove.", "Pablo Escobar had several nicknames including: Don Pablo, El Padrino, and El Patr\u00f3n."], "decomposition": ["How many nicknames did Pablo Escobar have?", "How many nicknames did Robert Moses Grove have?", "Is #1 greater than #2?"], "evidence": [[[["Pablo Escobar-28"]], [["Lefty Grove-1"]], [["Lefty Grove-1", "Pablo Escobar-28"]]], [[["Pablo Escobar-1", "Pablo Escobar-28"], "no_evidence"], [["Lefty Grove-1"]], ["operation"]], [[["Pablo Escobar-28"], "no_evidence"], [["Lefty Grove-1"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Beatles' song \"I Want to Hold Your Hand\" outshine the Beatles' song \"I Want to Hold Your Hand\" from the movie \"The Wizard of Oz\"?\nFacts: The Beatles' song \"I Want to Hold Your Hand\" was written by Paul McCartney and George Harrison. The Wizard of Oz was written", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Pablo Escobar's nickname collection outshine Robert Moses Grove's?\nFacts: Robert Moses Grove was a baseball player nicknamed Lefty Grove. Pablo Escobar had several nicknames including: Don Pablo, El Padrino, and El Patr\u00f3n.\nAnswer (True or False):", "ct_subject": "Robert Moses Grove was a baseball player nicknamed Lefty Grove. Pablo Escobar had several nicknames including: Don Pablo, El Padrino, and El Patr\u00f3n.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99993706"], [" the", "1.0"], [" Beatles", "0.4270853"], ["'", "0.99843866"], [" song", "0.9999751"], [" \"", "1.0"], ["I", "0.9176773"], [" Want", "0.9999964"], [" to", "1.0"], [" Hold", "1.0"], [" Your", "1.0"], [" Hand", "1.0"], ["\"", "1.0"], [" out", "0.9828986"], ["shine", "1.0"], [" the", "1.0"], [" Beatles", "0.9997819"], ["'", "1.0"], [" song", "1.0"], [" \"", "1.0"], ["I", "1.0"], [" Want", "1.0"], [" to", "1.0"], [" Hold", "1.0"], [" Your", "1.0"], [" Hand", "1.0"], ["\"", "0.9830914"], [" from", "0.44893205"], [" the", "1.0"], [" movie", "1.0"], [" \"", "1.0"], ["The", "0.7243385"], [" Wizard", "0.99999535"], [" of", "1.0"], [" Oz", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Beatles", "1.0"], ["'", "1.0"], [" song", "1.0"], [" \"", "1.0"], ["I", "1.0"], [" Want", "1.0"], [" to", "1.0"], [" Hold", "1.0"], [" Your", "1.0"], [" Hand", "1.0"], ["\"", "1.0"], [" was", "1.0"], [" written", "1.0"], [" by", "1.0"], [" Paul", "0.9999999"], [" McCartney", "1.0"], [" and", "0.99999976"], [" George", "0.9995072"], [" Harrison", "1.0"], [".", "1.0"], [" The", "1.0"], [" Wizard", "0.8261718"], [" of", "1.0"], [" Oz", "1.0"], [" was", "1.0"], [" written", "0.9999999"]]}, "571": {"qid": "debac3970a4fc11774fd", "term": "Marco Rubio", "description": "United States Senator from Florida", "question": "Could Marco Rubio ride the Candymonium roller coaster at Hershey Park?", "answer": true, "facts": ["The Candymonium roller coaster is restricted to park visitors over 54\" tall (4'6\").", "Marco Rubio is 5'9\" tall."], "decomposition": ["What is the height limit for the Candymonium roller coaster?", "How tall is Marco Rubio?", "Is #2 greater than or equal to #1?"], "evidence": [[[["Candymonium-6"], "no_evidence"], ["no_evidence"], ["operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Candymonium-1"], "no_evidence"], [["Marco Antonio Rubio-1", "Marco Rubio-1"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the closest to the equator?\nFacts: The closest city to the equator is Cape Town, South Africa.\nAnswer (True or False): True\nQuestion: What is the name of the city in the United States that is the closest to the equator?\nFacts", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could Marco Rubio ride the Candymonium roller coaster at Hershey Park?\nFacts: The Candymonium roller coaster is restricted to park visitors over 54\" tall (4'6\"). Marco Rubio is 5'9\" tall.\nAnswer (True or False):", "ct_subject": "The Candymonium roller coaster is restricted to park visitors over 54\" tall (4'6\"). Marco Rubio is 5'9\" tall.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99998"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999999"], [" of", "1.0"], [" the", "1.0"], [" city", "0.99695134"], [" in", "1.0"], [" the", "0.9999914"], [" United", "0.9999926"], [" States", "1.0"], [" that", "1.0"], [" is", "0.997607"], [" the", "1.0"], [" closest", "0.9175997"], [" to", "1.0"], [" the", "1.0"], [" equ", "0.8190885"], ["ator", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" closest", "1.0"], [" city", "1.0"], [" to", "1.0"], [" the", "1.0"], [" equ", "1.0"], ["ator", "1.0"], [" is", "1.0"], [" Cape", "0.99999976"], [" Town", "1.0"], [",", "1.0"], [" South", "1.0"], [" Africa", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" closest", "1.0"], [" to", "1.0"], [" the", "1.0"], [" equ", "0.9999771"], ["ator", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"]]}, "572": {"qid": "2a7eeadb1e045fda4550", "term": "Spinach", "description": "species of plant", "question": "Was the amount of spinach Popeye ate unhealthy?", "answer": true, "facts": ["Popeye was a cartoon character that ate whole cans of spinach to maintain his fighting strength.", "Spinach is high in oxalates which can lead to kidney stones.", "Too much spinach can lead to bloating, gas, fever, and diarrhea."], "decomposition": ["What is spinach high in?", "What does eating too much of #1 do to a body?", "Are #2's bad for a body?"], "evidence": [[[["Spinach-1"]], [["Oxalate-10"]], [["Kidney stone disease-1", "Oxalate-10"]]], [[["Spinach-7"]], [["Oxalate-10"]], ["operation"]], [[["Spinach-7"]], [["Vitamin A-13"]], [["Vitamin A-16"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human?\nFacts: The average human weight is about 100 pounds.\nAnswer (True or False): True\nQuestion: What is the average height of a human?\nFacts: The average human height is about 5 feet, 8 inches", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was the amount of spinach Popeye ate unhealthy?\nFacts: Popeye was a cartoon character that ate whole cans of spinach to maintain his fighting strength. Spinach is high in oxalates which can lead to kidney stones. Too much spinach can lead to bloating, gas, fever, and diarrhea.\nAnswer (True or False):", "ct_subject": "Popeye was a cartoon character that ate whole cans of spinach to maintain his fighting strength. Spinach is high in oxalates which can lead to kidney stones. Too much spinach can lead to bloating, gas, fever, and diarrhea.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.91252023"], [" is", "1.0"], [" the", "1.0"], [" average", "0.15247506"], [" weight", "0.99999964"], [" of", "1.0"], [" a", "0.9999465"], [" human", "0.99483055"], ["?", "0.9774769"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.999977"], [" weight", "0.9257879"], [" is", "1.0"], [" about", "0.9999999"], [" 100", "0.99149406"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" height", "0.9988122"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.9923538"], [" height", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 5", "0.99993956"], [" feet", "1.0"], [",", "0.9273571"], [" 8", "0.88566446"], [" inches", "1.0"]]}, "573": {"qid": "6cbd917db2a8315af17c", "term": "Chlorine", "description": "Chemical element with atomic number 17", "question": "Can you buy chlorine at a dollar store?", "answer": true, "facts": ["Chlorine, when added to water, creates household bleach.", "Household bleach is available at most dollar stores."], "decomposition": ["What type of item is Chlorine?", "Would department would you find #1 in?", "Do dollar stores have #2?"], "evidence": [[[["Chlorine-62"]], [["Cleaning agent-16"], "no_evidence"], ["operation"]], [[["Chlorine-64"]], [["Department store-2"]], [["Dollar Tree-2"]]], [[["Chlorine-4"]], [["Household hardware-1"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"cotton ball\" and a \"cotton baller\"?\nFacts: A cotton ball is a cotton baller is a cotton ball.\nAnswer (True or False): True\nQuestion: What is the difference between a \"cotton ball\" and a \"cotton baller\"?\nFacts: A cotton ball is a cotton baller is a cotton ball.\nAnswer", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can you buy chlorine at a dollar store?\nFacts: Chlorine, when added to water, creates household bleach. Household bleach is available at most dollar stores.\nAnswer (True or False):", "ct_subject": "Chlorine, when added to water, creates household bleach. Household bleach is available at most dollar stores.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9998568"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9999827"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.9962567"], ["c", "0.9538988"], ["otton", "0.100855134"], [" ball", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["otton", "1.0"], [" ball", "0.99999154"], ["er", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999999"], [" cotton", "1.0"], [" ball", "1.0"], [" is", "1.0"], [" a", "1.0"], [" cotton", "0.99531627"], [" ball", "1.0"], ["er", "1.0"], [" is", "1.0"], [" a", "1.0"], [" cotton", "1.0"], [" ball", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["otton", "1.0"], [" ball", "1.0"], ["\"", "0.99999785"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["otton", "1.0"], [" ball", "1.0"], ["er", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" cotton", "1.0"], [" ball", "1.0"], [" is", "1.0"], [" a", "1.0"], [" cotton", "1.0"], [" ball", "1.0"], ["er", "1.0"], [" is", "1.0"], [" a", "1.0"], [" cotton", "1.0"], [" ball", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"]]}, "574": {"qid": "4f286910705ee9e8aceb", "term": "Hypothermia", "description": "A human body core temperature below 35.0\u00b0C", "question": "Would hypothermia be a concern for a human wearing zoot suit on Triton?", "answer": true, "facts": ["A zoot suit was a man's suit of an exaggerated style popular in the 1940s.", "Triton is one of the coldest planets in the solar system.", "Triton is located about 2.8 billion miles from the warmth of the sun.", "Triton has an average temperature of -235.0\u00b0C", "A zoot suit is made of thin material such as cloth."], "decomposition": ["What is the average temperature on Triton?", "What material are zoot suits made of?", "Below which body temperature will hypothermia set in?", "Would clothes made of #2 be unable to keep body temperature above #3 in ambient temperature of #1?"], "evidence": [[[["Triton (moon)-3"]], [["Zoot Suit Riots-2"]], [["Hypothermia-1"]], ["operation"]], [[["Triton (moon)-3"]], [["Zoot Suit Riots-12", "Zoot Suit Riots-2"]], [["Human body temperature-30"]], ["operation"]], [[["Triton (moon)-3"]], [["Zoot suit-15"], "no_evidence"], [["Hypothermia-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the temperature of the sun?\nFacts: The sun is a star.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would hypothermia be a concern for a human wearing zoot suit on Triton?\nFacts: A zoot suit was a man's suit of an exaggerated style popular in the 1940s. Triton is one of the coldest planets in the solar system. Triton is located about 2.8 billion miles from the warmth of the sun. Triton has an average temperature of -235.0\u00b0C A zoot suit is made of thin material such as cloth.\nAnswer (True or False):", "ct_subject": "A zoot suit was a man's suit of an exaggerated style popular in the 1940s. Triton is one of the coldest planets in the solar system. Triton is located about 2.8 billion miles from the warmth of the sun. Triton has an average temperature of -235.0\u00b0C A zoot suit is made of thin material such as cloth.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9974809"], [" is", "1.0"], [" the", "1.0"], [" temperature", "0.92710096"], [" of", "1.0"], [" the", "0.9999298"], [" sun", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" sun", "1.0"], [" is", "0.99999857"], [" a", "0.99999976"], [" star", "1.0"], [".", "0.9999913"]]}, "575": {"qid": "4c7da1c6a6b94f8c44b1", "term": "Saltwater crocodile", "description": "species of reptile", "question": "Are saltwater crocodiles related to alligators?", "answer": true, "facts": ["Crocodiles belong to the family Crocodylinae.", "Alligators belong to the family Alligatoridae.", "Crocodylinae and Alligatoridae both belong to the order Crocodilia."], "decomposition": ["What family do Crocodiles belong to?", "What family do Alligators belong to?", "What order does #1 belong to?", "What order does #2 belong to?", "Are #3 and #4 the same thing?"], "evidence": [[[["Crocodile-1"]], [["Alligator-1"]], [["Crocodile-1"]], [["Alligator-1"]], ["operation"]], [[["Crocodile-35"]], [["American alligator-5"]], [["Crocodile-35"]], [["Alligator-1"]], ["operation"]], [[["Crocodylidae-1"]], [["Alligator-1"]], [["Crocodilia-1"]], [["Crocodilia-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any other animals that have been named after the Roman Emperor?\nFacts: There are many animals that have been named after the Roman Emperor. The most famous of these is the crocodile.\nAnswer (True or False): True\nQuestion: Are there any animals that have been named after the Roman", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are saltwater crocodiles related to alligators?\nFacts: Crocodiles belong to the family Crocodylinae. Alligators belong to the family Alligatoridae. Crocodylinae and Alligatoridae both belong to the order Crocodilia.\nAnswer (True or False):", "ct_subject": "Crocodiles belong to the family Crocodylinae. Alligators belong to the family Alligatoridae. Crocodylinae and Alligatoridae both belong to the order Crocodilia.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "0.92373884"], [" any", "0.996861"], [" other", "0.9751544"], [" animals", "1.0"], [" that", "1.0"], [" have", "0.107777745"], [" been", "0.99990046"], [" named", "0.9973573"], [" after", "1.0"], [" the", "0.9999988"], [" Roman", "1.0"], [" Emperor", "0.99999917"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "0.98137295"], [" are", "1.0"], [" many", "0.99999034"], [" animals", "0.49679953"], [" that", "0.43133715"], [" have", "1.0"], [" been", "1.0"], [" named", "1.0"], [" after", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" Emperor", "1.0"], [".", "1.0"], [" The", "0.999926"], [" most", "1.0"], [" famous", "1.0"], [" of", "0.98873806"], [" these", "1.0"], [" is", "0.9922225"], [" the", "1.0"], [" crocod", "0.96707714"], ["ile", "1.0"], [".", "1.0"], ["\n", "0.9860459"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.8996876"], [" there", "1.0"], [" any", "1.0"], [" animals", "0.99947554"], [" that", "1.0"], [" have", "1.0"], [" been", "1.0"], [" named", "1.0"], [" after", "1.0"], [" the", "1.0"], [" Roman", "1.0"]]}, "576": {"qid": "d0cf00dae29efafa0865", "term": "Funeral", "description": "ceremony for a person who has died", "question": "Do embalmed bodies feel different at funerals?", "answer": true, "facts": ["Embalming fluid fixates into the bodily tissues and replaces the bodily fluid.", "Bodies that have not been embalmed tend to feel soft.", "When embalming fluid fills the body, the body becomes firm."], "decomposition": ["What does Embalming a body do to it?", "Does #1 make a body hard?", "What does a non embalmed body feel like", "Is #2 different from #3?"], "evidence": [[[["Embalming-4"]], ["operation"], ["no_evidence"], ["operation"]], [[["Embalming-37"], "no_evidence"], ["no_evidence", "operation"], ["no_evidence"], ["no_evidence", "operation"]], [[["Embalming-1"]], [["Embalming chemicals-14"]], [["Natural burial-1"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"skeleton\" and a \"body\"?\nFacts: A skeleton is a solid mass of tissue that is used to support the body. A body is a living organism that is used to support the body.\nAnswer (True or False): True\nQuestion: What", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do embalmed bodies feel different at funerals?\nFacts: Embalming fluid fixates into the bodily tissues and replaces the bodily fluid. Bodies that have not been embalmed tend to feel soft. When embalming fluid fills the body, the body becomes firm.\nAnswer (True or False):", "ct_subject": "Embalming fluid fixates into the bodily tissues and replaces the bodily fluid. Bodies that have not been embalmed tend to feel soft. When embalming fluid fills the body, the body becomes firm.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.998708"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "0.9999999"], [" \"", "0.99840564"], ["s", "0.8272124"], ["keleton", "0.69939727"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["body", "0.98025495"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.99999905"], [" skeleton", "1.0"], [" is", "1.0"], [" a", "1.0"], [" solid", "0.9892039"], [" mass", "0.9999888"], [" of", "1.0"], [" tissue", "0.954009"], [" that", "0.96507585"], [" is", "0.9999957"], [" used", "0.9995454"], [" to", "0.9999989"], [" support", "0.9999993"], [" the", "0.9999914"], [" body", "1.0"], [".", "1.0"], [" A", "1.0"], [" body", "1.0"], [" is", "1.0"], [" a", "1.0"], [" living", "1.0"], [" organism", "0.9999448"], [" that", "0.9560346"], [" is", "0.99790525"], [" used", "0.97601867"], [" to", "1.0"], [" support", "1.0"], [" the", "1.0"], [" body", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"]]}, "577": {"qid": "535870a1d73b7288d0e1", "term": "Toronto Star", "description": "Newspaper in Toronto, Ontario, Canada", "question": "Would someone in Boston not receive the Toronto Star?", "answer": true, "facts": ["The Toronto Star is only distributed in Canada.", "Boston is located in the United States."], "decomposition": ["Where is the Toronto Star distributed?", "Where is Boston located?", "Is #2 not in #1?"], "evidence": [[[["Toronto Star-1"]], [["Boston-1"]], ["operation"]], [[["Toronto Star-1"]], [["Boston-1"]], ["operation"]], [[["Toronto Star-1"]], [["Boston-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the capital of the state of Massachusetts?\nFacts: Boston\nAnswer (True or False): True\nQuestion: What is the name of the city in the United States that is the capital of the state of New Hampshire?\nFacts: Concord\nAnswer (True or False): True\nQuestion: What is the name of the city in the United States that is the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would someone in Boston not receive the Toronto Star?\nFacts: The Toronto Star is only distributed in Canada. Boston is located in the United States.\nAnswer (True or False):", "ct_subject": "The Toronto Star is only distributed in Canada. Boston is located in the United States.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99997246"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9986603"], [" of", "1.0"], [" the", "1.0"], [" city", "0.99998116"], [" in", "0.8126535"], [" the", "0.99999976"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "0.92277867"], [" the", "1.0"], [" capital", "0.99999547"], [" of", "1.0"], [" the", "1.0"], [" state", "0.058856316"], [" of", "1.0"], [" Massachusetts", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Boston", "0.9999838"], ["\n", "0.9999999"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "0.98426616"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" New", "0.73614997"], [" Hampshire", "0.93612826"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Concord", "0.9999876"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"]]}, "578": {"qid": "759c7c8babd737e24f1d", "term": "Cinnamon", "description": "spice obtained from the inner bark of several trees from the genus Cinnamomum", "question": "Is a spice grinder ueseless for the cheapest cinnamon sticks?", "answer": true, "facts": ["Different species of cinnamon require different grinding methods to make the spice", "Indonesian cinnamon quills are thick and capable of damaging a spice or coffee grinder.", "Indonesian cinnamon is the most common and cheapest cinnamon in the USA"], "decomposition": ["What type of cinnamon sticks are the cheapest in the US?", "What is the size of #1?", "Because of #2, would it cause damage to a typical spice grinder?"], "evidence": [[[["Cinnamon-2"]], [["Cinnamomum cassia-2"]], ["operation"]], [[["Cinnamon-2"]], [["Cinnamomum cassia-2"]], [["Herb grinder-1"], "operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it true that the first person to use the word \"candy\" was a man named John Harvey Kellogg?\nFacts: Yes, Kellogg was the first person to use the word \"candy\" in the United States.\nAnswer (True or False): True\nQuestion: Is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is a spice grinder ueseless for the cheapest cinnamon sticks?\nFacts: Different species of cinnamon require different grinding methods to make the spice Indonesian cinnamon quills are thick and capable of damaging a spice or coffee grinder. Indonesian cinnamon is the most common and cheapest cinnamon in the USA\nAnswer (True or False):", "ct_subject": "Different species of cinnamon require different grinding methods to make the spice Indonesian cinnamon quills are thick and capable of damaging a spice or coffee grinder. Indonesian cinnamon is the most common and cheapest cinnamon in the USA", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99505556"], [" it", "0.9981311"], [" true", "0.9999931"], [" that", "1.0"], [" the", "1.0"], [" first", "0.99999964"], [" person", "0.9492079"], [" to", "1.0"], [" use", "0.9973713"], [" the", "0.99920136"], [" word", "1.0"], [" \"", "1.0"], ["c", "0.9999988"], ["andy", "0.9999975"], ["\"", "1.0"], [" was", "1.0"], [" a", "1.0"], [" man", "0.9947285"], [" named", "1.0"], [" John", "0.9999964"], [" Harvey", "0.3728268"], [" Kell", "1.0"], ["ogg", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Yes", "0.91631377"], [",", "0.99789596"], [" Kell", "0.9861261"], ["ogg", "1.0"], [" was", "0.99999964"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" use", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "0.9994954"], ["c", "1.0"], ["andy", "1.0"], ["\"", "1.0"], [" in", "1.0"], [" the", "0.9990355"], [" United", "0.99986553"], [" States", "1.0"], [".", "1.0"], ["\n", "0.9999987"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"]]}, "579": {"qid": "07ba78b177df5d2a30c3", "term": "Whole genome sequencing", "description": "A process that determines the complete DNA sequence of an organism's genome at a single time", "question": "Did Rosalind Franklin contribute to work that led to Whole Genome Sequencing?", "answer": true, "facts": ["Rosalind Franklin used specialized photography to capture the first photos of the double helix.", "The double helix is the form that DNA takes.", "Without understanding the structure of DNA, genome sequencing would be impossible."], "decomposition": ["Rosalind Franklin capture the first photo of what?", "What takes the form of #1?", "Is understanding #2 essential to genome sequencing?"], "evidence": [[[["Rosalind Franklin-19"]], [["DNA-1"]], [["Whole genome sequencing-1"], "operation"]], [[["Rosalind Franklin-3"]], [["Rosalind Franklin-3"]], [["Whole genome sequencing-1"], "operation"]], [[["Rosalind Franklin-3"]], [["Rosalind Franklin-3"]], [["Rosalind Franklin-3", "Whole genome sequencing-3"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Wright Brothers fly a plane?\nFacts: The Wright Brothers flew a plane.\nAnswer (True or False): True\nQuestion: Did the Wright Brothers fly a plane?\nFacts: The Wright Brothers flew a plane.\nAnswer (True or False): True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Rosalind Franklin contribute to work that led to Whole Genome Sequencing?\nFacts: Rosalind Franklin used specialized photography to capture the first photos of the double helix. The double helix is the form that DNA takes. Without understanding the structure of DNA, genome sequencing would be impossible.\nAnswer (True or False):", "ct_subject": "Rosalind Franklin used specialized photography to capture the first photos of the double helix. The double helix is the form that DNA takes. Without understanding the structure of DNA, genome sequencing would be impossible.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9997774"], [" the", "1.0"], [" Wright", "0.98784804"], [" Brothers", "1.0"], [" fly", "0.9999726"], [" a", "0.99827385"], [" plane", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Wright", "1.0"], [" Brothers", "1.0"], [" flew", "0.9892223"], [" a", "1.0"], [" plane", "0.99999857"], [".", "1.0"], ["\n", "0.99996555"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" Wright", "1.0"], [" Brothers", "1.0"], [" fly", "1.0"], [" a", "1.0"], [" plane", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Wright", "1.0"], [" Brothers", "1.0"], [" flew", "1.0"], [" a", "1.0"], [" plane", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"]]}, "580": {"qid": "557e389d7efe47c5bc4d", "term": "Blue", "description": "A primary colour between purple and green", "question": "Is the most expensive color in the world Blue?", "answer": true, "facts": ["Blue is a primary color.", "Blue is between violet and green on the visible light spectrum.", "Lapis Lazuli is used to make ultramarine. ", "Ultramarine is a pigment of Blue", "Processing Lapis Lazuli into Ultramarine is the most expensive of color processes."], "decomposition": ["What was the most expensive pigment used by Renaissance painters?", "Is #1 a shade of the color blue?"], "evidence": [[[["Ultramarine-2"]], [["Ultramarine-1"], "operation"]], [[["Blue-2"]], [["Ultramarine-2"], "operation"]], [[["Ultramarine-2"]], [["Ultramarine-2"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the most expensive color in the world Green?\nFacts: Green is a primary color. Green is between blue and violet on the visible light spectrum. Lapis Lazuli is used to make ultramarine.  Ultramarine is a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is the most expensive color in the world Blue?\nFacts: Blue is a primary color. Blue is between violet and green on the visible light spectrum. Lapis Lazuli is used to make ultramarine.  Ultramarine is a pigment of Blue Processing Lapis Lazuli into Ultramarine is the most expensive of color processes.\nAnswer (True or False):", "ct_subject": "Blue is a primary color. Blue is between violet and green on the visible light spectrum. Lapis Lazuli is used to make ultramarine.  Ultramarine is a pigment of Blue Processing Lapis Lazuli into Ultramarine is the most expensive of color processes.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9830221"], [" the", "0.9999291"], [" most", "1.0"], [" expensive", "1.0"], [" color", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" Green", "0.08701313"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Green", "1.0"], [" is", "1.0"], [" a", "1.0"], [" primary", "1.0"], [" color", "1.0"], [".", "1.0"], [" Green", "1.0"], [" is", "1.0"], [" between", "1.0"], [" blue", "0.58839375"], [" and", "1.0"], [" violet", "1.0"], [" on", "1.0"], [" the", "1.0"], [" visible", "1.0"], [" light", "1.0"], [" spectrum", "1.0"], [".", "1.0"], [" Lap", "1.0"], ["is", "1.0"], [" Laz", "1.0"], ["uli", "1.0"], [" is", "1.0"], [" used", "1.0"], [" to", "1.0"], [" make", "1.0"], [" ult", "0.9999999"], ["ram", "1.0"], ["arine", "1.0"], [".", "1.0"], [" ", "0.9996563"], [" Ult", "1.0"], ["ram", "1.0"], ["arine", "1.0"], [" is", "1.0"], [" a", "0.9999999"]]}, "581": {"qid": "0782cde19737531d14fe", "term": "University of Pittsburgh", "description": "American state-related research university located in Pittsburgh, Pennsylvania", "question": "Is University of Pittsburgh easier to enter than FBI?", "answer": true, "facts": ["The University of Pittsburgh has around a 60% acceptance rate.", "The FBI estimated accepting 900 agents out of 16000 applicants in 2019."], "decomposition": ["What percent of applicants does University of Pittsburgh accept?", "How many applications did the FBI get in 2019?", "Out of #2, how many were accepted?", "What is #3 divided by #2?", "Is #1 greater than #4?"], "evidence": [[["no_evidence"], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"], ["no_evidence", "operation"]], [[["University of Pittsburgh-2"], "no_evidence"], [["Federal Bureau of Investigation-59"], "no_evidence"], ["no_evidence"], ["operation"], ["operation"]], [[["University of Pittsburgh-35"], "no_evidence"], [["Federal Bureau of Investigation-61"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average age of a college graduate?\nFacts: The average age of a college graduate is 24.\nAnswer (True or False): True\nQuestion: What is the average age of a college graduate?\nFacts: The average age of a college graduate is 24.\nAnswer (True or False): True\nQuestion: What is the average age of a college graduate?", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is University of Pittsburgh easier to enter than FBI?\nFacts: The University of Pittsburgh has around a 60% acceptance rate. The FBI estimated accepting 900 agents out of 16000 applicants in 2019.\nAnswer (True or False):", "ct_subject": "The University of Pittsburgh has around a 60% acceptance rate. The FBI estimated accepting 900 agents out of 16000 applicants in 2019.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.7235777"], [" is", "1.0"], [" the", "1.0"], [" average", "0.9996965"], [" age", "0.99973196"], [" of", "1.0"], [" a", "1.0"], [" college", "0.9885266"], [" graduate", "0.9986118"], ["?", "0.9358751"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" age", "1.0"], [" of", "1.0"], [" a", "1.0"], [" college", "1.0"], [" graduate", "1.0"], [" is", "1.0"], [" 24", "0.030426333"], [".", "1.0"], ["\n", "0.99903166"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" age", "0.9999162"], [" of", "1.0"], [" a", "1.0"], [" college", "1.0"], [" graduate", "1.0"], ["?", "0.9987924"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" age", "1.0"], [" of", "1.0"], [" a", "1.0"], [" college", "1.0"], [" graduate", "1.0"], [" is", "1.0"], [" 24", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" age", "1.0"], [" of", "1.0"], [" a", "1.0"], [" college", "1.0"], [" graduate", "1.0"], ["?", "1.0"]]}, "582": {"qid": "ed421aaf019e01f38a84", "term": "Landscape architect", "description": "person involved in the planning, design and sometimes direction of a landscape, garden, or distinct space", "question": "Would Persephone be a good consultant to a landscape architect?", "answer": true, "facts": ["Persephone is a vegetation goddess. ", "A vegetation deity is a nature deity whose disappearance and reappearance, or life, death and rebirth, embodies the growth cycle of plants.", "Landscape architects deal with planning and laying out gardens and other plant life."], "decomposition": ["Over what domains does Persephone preside?", "Do landscape architects work with any of #1?"], "evidence": [[[["Persephone-1", "Persephone-2"]], [["Landscape architect-1"], "operation"]], [[["Persephone-1"]], [["Landscaping-5"]]], [[["Persephone-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: John Adams was born on July 4, 1735.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Persephone be a good consultant to a landscape architect?\nFacts: Persephone is a vegetation goddess.  A vegetation deity is a nature deity whose disappearance and reappearance, or life, death and rebirth, embodies the growth cycle of plants. Landscape architects deal with planning and laying out gardens and other plant life.\nAnswer (True or False):", "ct_subject": "Persephone is a vegetation goddess.  A vegetation deity is a nature deity whose disappearance and reappearance, or life, death and rebirth, embodies the growth cycle of plants. Landscape architects deal with planning and laying out gardens and other plant life.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999999"], [" is", "1.0"], [" the", "1.0"], [" name", "0.92235345"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9796357"], [" person", "0.7244954"], [" to", "1.0"], [" be", "0.99993634"], [" born", "0.9916754"], [" in", "1.0"], [" the", "1.0"], [" United", "0.9997372"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "0.9975096"], [" Adams", "0.99281466"], [" was", "0.9999974"], [" born", "0.17306319"], [" on", "0.9993888"], [" July", "0.9802024"], [" 4", "1.0"], [",", "1.0"], [" 17", "1.0"], ["35", "0.9998369"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"]]}, "583": {"qid": "a2a5a06a55c24bf2d408", "term": "Pregnancy", "description": "time when children develop inside the mother's body before birth", "question": "Do women often need new shoes during their pregnancy?", "answer": true, "facts": ["Pregnancy can cause swelling in the feet and legs.", "For safety and comfort, one needs to get new shoes if the size of their feet change."], "decomposition": ["Which signs and symptoms of pregnancy in women affect the lower extremities?", "Do #1 lead to a change in size of affected areas?"], "evidence": [[[["Pregnancy-9"]], ["operation"]], [[["Pregnancy-9"]], ["operation"]], [[["Inferior vena cava syndrome-3"]], [["Edema-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake with a hole in it\"?\nFacts: A snowflake is a solid, round, and shiny object. A snowflake with a hole in it is a solid, round, and shiny object with a hole in it.\nAnswer (True or False): True\nQuestion: What is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do women often need new shoes during their pregnancy?\nFacts: Pregnancy can cause swelling in the feet and legs. For safety and comfort, one needs to get new shoes if the size of their feet change.\nAnswer (True or False):", "ct_subject": "Pregnancy can cause swelling in the feet and legs. For safety and comfort, one needs to get new shoes if the size of their feet change.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999454"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9999534"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.9999987"], ["s", "0.86854327"], ["now", "0.9992797"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.99999917"], ["now", "1.0"], ["flake", "1.0"], [" with", "0.99846375"], [" a", "1.0"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.99999964"], [" snow", "1.0"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" solid", "0.93525314"], [",", "0.9971341"], [" round", "1.0"], [",", "0.9999999"], [" and", "0.9767496"], [" shiny", "0.9996768"], [" object", "1.0"], [".", "1.0"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" with", "1.0"], [" a", "1.0"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], [" is", "1.0"], [" a", "1.0"], [" solid", "0.99999976"], [",", "1.0"], [" round", "1.0"], [",", "1.0"], [" and", "1.0"], [" shiny", "1.0"], [" object", "1.0"], [" with", "0.9727973"], [" a", "1.0"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"]]}, "584": {"qid": "4ce0ad7ba3ee5886a794", "term": "Double-slit experiment", "description": "Physics experiment, showing light can be modelled by both waves and particles", "question": "Can a minor replicate the double-slit experiment?", "answer": true, "facts": ["A minor is a human child.", "The double-slit experiment can theoretically be replicated by any human."], "decomposition": ["What species of living things does 'minor' refer to?", "Can #1 replicate the double-slit experiment?"], "evidence": [[[["Minor (law)-1"]], [["Double-slit experiment-2"]]], [[["Minor (law)-1"]], [["Double-slit experiment-1"], "operation"]], [[["Age of majority-1"]], [["Double-slit experiment-2"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a human and a dog?\nFacts: A dog is a mammal. Humans are mammals.\nAnswer (True or False): True\nQuestion: What is the difference between a human and a horse?\nFacts: A horse is a mammal. Humans are mammals.\nAnswer (True or False): True\nQuestion: What is the difference between a human and a pig?\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can a minor replicate the double-slit experiment?\nFacts: A minor is a human child. The double-slit experiment can theoretically be replicated by any human.\nAnswer (True or False):", "ct_subject": "A minor is a human child. The double-slit experiment can theoretically be replicated by any human.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9961669"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9982147"], [" between", "1.0"], [" a", "0.9999995"], [" human", "0.9573735"], [" and", "1.0"], [" a", "1.0"], [" dog", "0.832267"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" dog", "0.96696424"], [" is", "0.9999999"], [" a", "1.0"], [" mammal", "0.99999523"], [".", "1.0"], [" Humans", "0.9995091"], [" are", "1.0"], [" mammals", "0.90990824"], [".", "1.0"], ["\n", "0.99999905"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" human", "1.0"], [" and", "1.0"], [" a", "1.0"], [" horse", "0.99931395"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" horse", "0.99988925"], [" is", "1.0"], [" a", "1.0"], [" mammal", "1.0"], [".", "1.0"], [" Humans", "1.0"], [" are", "1.0"], [" mammals", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" human", "1.0"], [" and", "1.0"], [" a", "1.0"], [" pig", "0.98595375"], ["?", "1.0"], ["\n", "1.0"]]}, "585": {"qid": "064d8faaa8f164b55270", "term": "Apollo 15", "description": "Fourth crewed mission to land on the Moon", "question": "Would the crew of Apollo 15 have difficulty riding a unicycle?", "answer": true, "facts": ["There were 3 astronauts in the crew of the Apollo 15 mission.", "A unicycle only contains one saddle, and is typically only operated by a single person."], "decomposition": ["What is the maximum number of people that can ride a typical unicycle?", "How many people were on the Apollo 15 crew?", "Is #2 greater than #1?"], "evidence": [[[["Unicycle-1"]], [["Apollo 15-1"]], ["operation"]], [[["Unicycle-1"], "no_evidence"], [["Apollo 15-6"]], ["operation"]], [[["Unicycle-25"]], [["Apollo 15-6"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: John Adams was born on January 19, 1735.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: Thomas Jefferson was born on January 19, 1743", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would the crew of Apollo 15 have difficulty riding a unicycle?\nFacts: There were 3 astronauts in the crew of the Apollo 15 mission. A unicycle only contains one saddle, and is typically only operated by a single person.\nAnswer (True or False):", "ct_subject": "There were 3 astronauts in the crew of the Apollo 15 mission. A unicycle only contains one saddle, and is typically only operated by a single person.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999416"], [" is", "1.0"], [" the", "1.0"], [" name", "0.7372834"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9892307"], [" person", "0.8081406"], [" to", "1.0"], [" be", "0.960883"], [" born", "0.97992545"], [" in", "1.0"], [" the", "1.0"], [" United", "0.9987417"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "0.84871334"], [" Adams", "0.9613822"], [" was", "1.0"], [" born", "0.29199874"], [" on", "0.9987097"], [" January", "0.124274656"], [" 19", "0.2410679"], [",", "1.0"], [" 17", "1.0"], ["35", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Thomas", "0.08914189"], [" Jefferson", "1.0"], [" was", "1.0"], [" born", "1.0"], [" on", "1.0"], [" January", "0.30125025"], [" 19", "1.0"], [",", "1.0"], [" 17", "1.0"], ["43", "1.0"]]}, "586": {"qid": "620957c2018f91328a2e", "term": "Infinitive", "description": "grammatical form", "question": "Is Shakespeare famous because of the infinitive form?", "answer": true, "facts": ["Shakespeare wrote the play Hamlet", "Hamlet contains one of Shakespeare's most famous passages, Hamlet's soliloquy", "Hamlet's soliloquy begins with the line 'To be or not to be', which uses the infinitive form"], "decomposition": ["The use of the infinitive form in \"To be or not to be\" appears in which popular soliloquy?", "Which book contained #1", "Did Williams Shakespeare write #2?"], "evidence": [[[["To be, or not to be-1"]], [["Hamlet-1"]], [["Hamlet-2"], "operation"]], [[["To be, or not to be-1"]], [["To be, or not to be-1"]], [["To be, or not to be-1"]]], [[["To be, or not to be-1"]], [["Hamlet-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the word \"cuckold\" a modern invention?\nFacts: The word \"cuckold\" is a modern invention. The word \"cuckold\" was used in the Middle Ages to describe a man who had sex with his wife's lover. The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Shakespeare famous because of the infinitive form?\nFacts: Shakespeare wrote the play Hamlet Hamlet contains one of Shakespeare's most famous passages, Hamlet's soliloquy Hamlet's soliloquy begins with the line 'To be or not to be', which uses the infinitive form\nAnswer (True or False):", "ct_subject": "Shakespeare wrote the play Hamlet Hamlet contains one of Shakespeare's most famous passages, Hamlet's soliloquy Hamlet's soliloquy begins with the line 'To be or not to be', which uses the infinitive form", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99999976"], [" the", "0.99999607"], [" word", "1.0"], [" \"", "0.44290003"], ["c", "0.9912832"], ["uck", "0.9999747"], ["old", "1.0"], ["\"", "1.0"], [" a", "0.9999993"], [" modern", "0.9999999"], [" invention", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["uck", "1.0"], ["old", "1.0"], ["\"", "1.0"], [" is", "0.99995756"], [" a", "1.0"], [" modern", "0.99999964"], [" invention", "1.0"], [".", "1.0"], [" The", "0.99800915"], [" word", "1.0"], [" \"", "0.50907034"], ["c", "1.0"], ["uck", "1.0"], ["old", "1.0"], ["\"", "1.0"], [" was", "0.9998907"], [" used", "0.53411514"], [" in", "0.9999924"], [" the", "1.0"], [" Middle", "0.11891287"], [" Ages", "1.0"], [" to", "0.9943218"], [" describe", "0.99661213"], [" a", "1.0"], [" man", "1.0"], [" who", "1.0"], [" had", "0.8728952"], [" sex", "0.9240123"], [" with", "1.0"], [" his", "0.9998466"], [" wife", "1.0"], ["'s", "1.0"], [" lover", "0.9700162"], [".", "1.0"], [" The", "0.86733973"]]}, "587": {"qid": "4957e0329698c05832c0", "term": "Seismology", "description": "The scientific study of earthquakes and propagation of elastic waves through a planet", "question": "Did Brad Peyton need to know about seismology?", "answer": true, "facts": ["Brad Peyton directed the movie San Andreas", "San Andreas is an earthquake disaster film", "Seismology is the science of earthquakes and related phenomena"], "decomposition": ["What does the study of seismology involve?", "What was the movie San Andreas primarily about?", "Did Brad Peyton direct San Andreas and is #2 included in #1?"], "evidence": [[[["Seismology-1"]], [["San Andreas (film)-1"]], [["San Andreas (film)-1"], "operation"]], [[["Seismology-1"]], [["San Andreas (film)-1"]], [["Brad Peyton-1"], "operation"]], [[["Seismology-1"]], [["San Andreas (film)-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did Brad Peyton need to know about the history of the United States?\nFacts: Brad Peyton directed the movie San Andreas San Andreas is an earthquake disaster film History of the United States is the history of the United States\nAnswer (True or False): True\nQuestion: Did Brad Peyton need to know about the history of the United States?\nFacts: Brad Peyton directed the movie San Andreas San", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Brad Peyton need to know about seismology?\nFacts: Brad Peyton directed the movie San Andreas San Andreas is an earthquake disaster film Seismology is the science of earthquakes and related phenomena\nAnswer (True or False):", "ct_subject": "Brad Peyton directed the movie San Andreas San Andreas is an earthquake disaster film Seismology is the science of earthquakes and related phenomena", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9992429"], [" Brad", "0.9980724"], [" Peyton", "1.0"], [" need", "1.0"], [" to", "1.0"], [" know", "1.0"], [" about", "1.0"], [" the", "1.0"], [" history", "0.6973834"], [" of", "1.0"], [" the", "1.0"], [" United", "0.9998598"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Brad", "1.0"], [" Peyton", "1.0"], [" directed", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" San", "1.0"], [" Andreas", "1.0"], [" San", "1.0"], [" Andreas", "1.0"], [" is", "1.0"], [" an", "1.0"], [" earthquake", "1.0"], [" disaster", "1.0"], [" film", "1.0"], [" History", "0.9999987"], [" of", "0.9999999"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "1.0"], [" the", "1.0"], [" history", "0.9999999"], [" of", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["\n", "0.98196363"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" Brad", "1.0"], [" Peyton", "1.0"], [" need", "1.0"], [" to", "1.0"], [" know", "1.0"], [" about", "1.0"], [" the", "1.0"], [" history", "1.0"], [" of", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Brad", "1.0"], [" Peyton", "1.0"], [" directed", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" San", "1.0"], [" Andreas", "1.0"], [" San", "1.0"]]}, "588": {"qid": "082af323f414128588b8", "term": "Frigate", "description": "Type of warship", "question": "Are ropes required to operate a frigate?", "answer": true, "facts": ["Frigates are a kind of sailing ship.", "Many features of ships require rope to use."], "decomposition": ["What force powers frigates?", "What characteristic of frigates allows them to use #1?", "Are ropes used to manipulate #2?"], "evidence": [[[["Sailing-1"]], [["Frigate-6"]], [["Sailing ship-35"], "operation"]], [[["Frigate-11"]], [["Sail-1"]], [["Sail-3"], "no_evidence"]], [[["Full-rigged ship-5"], "no_evidence"], [["Rigging-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: John Brown.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: John Brown.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are ropes required to operate a frigate?\nFacts: Frigates are a kind of sailing ship. Many features of ships require rope to use.\nAnswer (True or False):", "ct_subject": "Frigates are a kind of sailing ship. Many features of ships require rope to use.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99945194"], [" is", "1.0"], [" the", "1.0"], [" name", "0.994524"], [" of", "1.0"], [" the", "1.0"], [" first", "0.957143"], [" person", "0.755482"], [" to", "1.0"], [" be", "0.9999584"], [" executed", "0.9992017"], [" in", "0.99950516"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "0.721734"], [" Brown", "1.0"], [".", "0.9988066"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "1.0"], [" Brown", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "0.9999999"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"]]}, "589": {"qid": "b418ba4e11aee26faabc", "term": "Jason", "description": "Greek mythological hero", "question": "Does Jason have anything in common with Dr. Disrespect?", "answer": true, "facts": ["Jason cheated on Medea with Creusa", "Dr. Disrespect cheated on his wife with another woman"], "decomposition": ["Was Jason faithful or unfaithful?", "Was Dr. Disrespect faithful or unfaithful?", "Are #1 and #2 the same?"], "evidence": [[[["Medea-10"], "no_evidence"], [["Dr DisRespect-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Jason-3"]], ["no_evidence"], ["operation"]], [[["Jason-18"]], [["Dr DisRespect-5"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is Jason the son of Zeus?\nFacts: Jason is the son of Zeus and Hera\nAnswer (True or False): True\nQuestion: Is Jason the son of Zeus and Hera?\nFacts: Jason is the son of Zeus and Hera\nAnswer (True or False): True\nQuestion: Is Jason the son of Zeus and Hera?\nFacts: Jason is the son of Zeus and Hera\nAnswer", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Jason have anything in common with Dr. Disrespect?\nFacts: Jason cheated on Medea with Creusa Dr. Disrespect cheated on his wife with another woman\nAnswer (True or False):", "ct_subject": "Jason cheated on Medea with Creusa Dr. Disrespect cheated on his wife with another woman", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9993697"], [" Jason", "0.998142"], [" the", "0.5207055"], [" son", "0.9695221"], [" of", "1.0"], [" Zeus", "0.99814665"], ["?", "0.8978633"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Jason", "1.0"], [" is", "0.9999999"], [" the", "0.9999994"], [" son", "1.0"], [" of", "1.0"], [" Zeus", "1.0"], [" and", "0.9203948"], [" Hera", "0.9999999"], ["\n", "0.99999964"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" Jason", "1.0"], [" the", "1.0"], [" son", "1.0"], [" of", "1.0"], [" Zeus", "1.0"], [" and", "0.9999877"], [" Hera", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Jason", "1.0"], [" is", "1.0"], [" the", "1.0"], [" son", "1.0"], [" of", "1.0"], [" Zeus", "1.0"], [" and", "1.0"], [" Hera", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" Jason", "1.0"], [" the", "1.0"], [" son", "1.0"], [" of", "1.0"], [" Zeus", "1.0"], [" and", "1.0"], [" Hera", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Jason", "1.0"], [" is", "1.0"], [" the", "1.0"], [" son", "1.0"], [" of", "1.0"], [" Zeus", "1.0"], [" and", "1.0"], [" Hera", "1.0"], ["\n", "1.0"], ["Answer", "1.0"]]}, "590": {"qid": "b1e061a88d467fdb632e", "term": "2009", "description": "Year", "question": "Could $1 for each 2009 eclipse buy a copy of TIME magazine in 2020?", "answer": true, "facts": ["The 2020 Newsstand price of TIME magazine is $5.99.", "There were six eclipses in 2009 including 2 solar and 4 lunar eclipses."], "decomposition": ["What was the price of a single issue of TIME magazine in 2020?", "How many solar eclipses were there in 2009?", "How many lunar eclipses were there in 2009?", "What is #2 plus #3?", "Is #4 greater than or equal to #1?"], "evidence": [[[["Time (magazine)-5"], "no_evidence"], [["July 2009 lunar eclipse-6"], "no_evidence"], [["July 2009 lunar eclipse-4"], "no_evidence"], ["operation"], ["operation"]], [[["Time (magazine)-5"], "no_evidence"], [["Solar eclipse of January 26, 1990-1", "Solar eclipse of July 22, 2009-1"]], [["August 2009 lunar eclipse-1"]], ["operation"], ["operation"]], [[["Time (magazine)-22", "Time (magazine)-5"], "no_evidence"], [["Solar eclipse-3"], "no_evidence"], [["Lunar eclipse-23"], "no_evidence"], ["no_evidence", "operation"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in China that is the closest to the North Pole?\nFacts: The closest city to the North Pole is called Lop Nor.\nAnswer (True or False): True\nQuestion: What is the name of the city in China that is the closest to the South Pole?\nFacts: The closest", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could $1 for each 2009 eclipse buy a copy of TIME magazine in 2020?\nFacts: The 2020 Newsstand price of TIME magazine is $5.99. There were six eclipses in 2009 including 2 solar and 4 lunar eclipses.\nAnswer (True or False):", "ct_subject": "The 2020 Newsstand price of TIME magazine is $5.99. There were six eclipses in 2009 including 2 solar and 4 lunar eclipses.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999341"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99917585"], [" of", "1.0"], [" the", "1.0"], [" city", "0.005784265"], [" in", "0.999119"], [" China", "0.9409595"], [" that", "1.0"], [" is", "0.9988166"], [" the", "1.0"], [" closest", "0.12187067"], [" to", "1.0"], [" the", "1.0"], [" North", "1.0"], [" Pole", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" closest", "0.82821923"], [" city", "1.0"], [" to", "1.0"], [" the", "1.0"], [" North", "1.0"], [" Pole", "1.0"], [" is", "1.0"], [" called", "0.9742891"], [" L", "0.9999964"], ["op", "0.99999845"], [" Nor", "0.9999254"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" China", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" closest", "1.0"], [" to", "1.0"], [" the", "1.0"], [" South", "1.0"], [" Pole", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" closest", "1.0"]]}, "591": {"qid": "90474e8efe599628cbc4", "term": "Olympia, Washington", "description": "State capital and city in Washington, United States", "question": "Does Olympia Washington share name with Hephaestus's workshop location?", "answer": true, "facts": ["Olympia Washington, is named after Mount Olympus.", "Mount Olympus is a mountain range in Washington named after the ancient Greek Mount Olympus.", "Hephaestus was the ancient Greek god of the forge and had a workshop on Mount Olympus."], "decomposition": ["Where did Hephaestus have his workshop?", "Olympia, Washington derived it's name from what mountain?", "Is #2 the same as #1?"], "evidence": [[[["Hephaestus-5"]], [["Olympia, Washington-3"]], ["operation"]], [[["Hephaestus-2"]], [["Mount Olympus-1"]], ["operation"]], [[["Hephaestus-5"], "operation"], [["Hephaestus-5"], "operation"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city of Rome derived from the Roman god of war, Mars?\nFacts: The name of Rome is derived from the Roman god of war, Mars. The Roman god of war was known as Mars.\nAnswer (True or False): True\nQuestion", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Olympia Washington share name with Hephaestus's workshop location?\nFacts: Olympia Washington, is named after Mount Olympus. Mount Olympus is a mountain range in Washington named after the ancient Greek Mount Olympus. Hephaestus was the ancient Greek god of the forge and had a workshop on Mount Olympus.\nAnswer (True or False):", "ct_subject": "Olympia Washington, is named after Mount Olympus. Mount Olympus is a mountain range in Washington named after the ancient Greek Mount Olympus. Hephaestus was the ancient Greek god of the forge and had a workshop on Mount Olympus.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99787855"], [" the", "0.9999999"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "0.3265618"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" Roman", "0.8904771"], [" god", "0.99998367"], [" of", "1.0"], [" war", "0.99817216"], [",", "0.2991596"], [" Mars", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" name", "0.9995782"], [" of", "1.0"], [" Rome", "0.99823546"], [" is", "0.99999976"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" god", "1.0"], [" of", "1.0"], [" war", "1.0"], [",", "1.0"], [" Mars", "1.0"], [".", "1.0"], [" The", "0.9881931"], [" Roman", "0.9999994"], [" god", "1.0"], [" of", "1.0"], [" war", "1.0"], [" was", "0.997929"], [" known", "0.91790366"], [" as", "1.0"], [" Mars", "1.0"], [".", "0.9998466"], ["\n", "0.9999999"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"]]}, "592": {"qid": "dd5a2ed28bfb890a14e5", "term": "Spaghetti", "description": "Type of pasta", "question": "Should a Celiac sufferer avoid spaghetti?", "answer": true, "facts": ["Celiac is a disease in which the body cannot process gluten.", "Gluten is a protein found in wheat.", "Spaghetti is made with milled wheat and water."], "decomposition": ["What food ingredients should people with celiac disease avoid?", "What ingredients make up spaghetti?", "Is #2 listed in #1?"], "evidence": [[[["Coeliac disease-2"]], [["Spaghetti-1"]], ["operation"]], [[["Coeliac disease-2"]], [["Spaghetti-1"]], ["operation"]], [[["Coeliac disease-36"]], [["Spaghetti-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake\"?\nFacts: A snowflake is a single crystal. A snowflake is made up of many crystals.\nAnswer (True or False): True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake\"?\nFacts: A", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Should a Celiac sufferer avoid spaghetti?\nFacts: Celiac is a disease in which the body cannot process gluten. Gluten is a protein found in wheat. Spaghetti is made with milled wheat and water.\nAnswer (True or False):", "ct_subject": "Celiac is a disease in which the body cannot process gluten. Gluten is a protein found in wheat. Spaghetti is made with milled wheat and water.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999976"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.99930096"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.6660817"], ["now", "0.9524387"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.98899055"], ["now", "1.0"], ["flake", "1.0"], ["\"?", "0.99787545"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" snow", "0.9999734"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "0.9510307"], [" crystal", "0.9999987"], [".", "0.9750944"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" is", "1.0"], [" made", "0.10328841"], [" up", "0.6058573"], [" of", "1.0"], [" many", "1.0"], [" crystals", "0.9648958"], [".", "1.0"], ["\n", "0.99894327"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"]]}, "593": {"qid": "2eed74f7dc66f5aadcd7", "term": "Saudi Aramco", "description": "Saudi Arabian petroleum and natural gas company", "question": "Was Saudi Aramco started due to an assassination?", "answer": true, "facts": ["Saudi Aramco was formed in response to oil shortages during World War I", "The origins of World War I can be traced to the assassination of Archduke Franz Ferdinand in Sarajevo"], "decomposition": ["Saudi Aramco was formed in response to oil shortages during what major conflict?", "What event is widely acknowledged to have started #1?", "Is #2 an assassination?"], "evidence": [[[["Saudi Aramco-7"]], [["Paris in the Belle \u00c9poque-96"]], [["Assassination-1"]]], [[["Saudi Aramco-7"]], [["Assassination of Archduke Franz Ferdinand-1"]], ["operation"]], [[["Saudi Aramco-7"]], [["Assassination of Archduke Franz Ferdinand-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city of Rome?\nFacts: Rome was the capital of the Roman Empire. The name of the city was changed to Rome after the fall of the Roman Empire.\nAnswer (True or False): True\nQuestion: What is the name of the city of Rome?\nFacts: Rome was the capital", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Saudi Aramco started due to an assassination?\nFacts: Saudi Aramco was formed in response to oil shortages during World War I The origins of World War I can be traced to the assassination of Archduke Franz Ferdinand in Sarajevo\nAnswer (True or False):", "ct_subject": "Saudi Aramco was formed in response to oil shortages during World War I The origins of World War I can be traced to the assassination of Archduke Franz Ferdinand in Sarajevo", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.5453084"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9937097"], [" of", "1.0"], [" the", "1.0"], [" city", "0.6750358"], [" of", "0.9999205"], [" Rome", "0.9520205"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "0.99999285"], [" was", "0.8460995"], [" the", "0.931607"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [".", "0.9999119"], [" The", "0.79012746"], [" name", "0.4169522"], [" of", "0.9999995"], [" the", "1.0"], [" city", "1.0"], [" was", "0.9999821"], [" changed", "1.0"], [" to", "1.0"], [" Rome", "0.99999976"], [" after", "0.8302481"], [" the", "1.0"], [" fall", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [".", "0.9999788"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "0.99986994"], [" was", "1.0"], [" the", "1.0"], [" capital", "1.0"]]}, "594": {"qid": "957adcdf1c676bc082da", "term": "Riksdag", "description": "Legislative body of Sweden", "question": "Is the Riksdag a political entity in Scandinavia?", "answer": true, "facts": ["The Riksdag is the legislative branch of the Swedish government.", "Sweden is part of Scandinavia."], "decomposition": ["What country does the Riksdag belong to?", "Which countries are part of Scandinavia?", "Is #1 included in #2?"], "evidence": [[[["Riksdag-1"]], [["Scandinavia-1"]], ["operation"]], [[["Riksdag-1"]], [["Scandinavia-1"]], ["operation"]], [[["Riksdag-1"]], [["Scandinavia-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the United States a democracy?\nFacts: The United States is a constitutional republic.\nAnswer (True or False): True\nQuestion: Is the United States a republic?\nFacts: The United States is a constitutional monarchy.\nAnswer (True or False): True\nQuestion: Is the United States a republic?\nFacts: The United States is a constitutional monarchy.\nAnswer (", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is the Riksdag a political entity in Scandinavia?\nFacts: The Riksdag is the legislative branch of the Swedish government. Sweden is part of Scandinavia.\nAnswer (True or False):", "ct_subject": "The Riksdag is the legislative branch of the Swedish government. Sweden is part of Scandinavia.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999856"], [" the", "1.0"], [" United", "0.999974"], [" States", "1.0"], [" a", "0.9999981"], [" democracy", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "1.0"], [" a", "1.0"], [" constitutional", "0.97571725"], [" republic", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" a", "1.0"], [" republic", "0.99998474"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "1.0"], [" a", "1.0"], [" constitutional", "0.9999486"], [" monarchy", "0.9943065"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" a", "1.0"], [" republic", "0.36480558"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "1.0"], [" a", "1.0"], [" constitutional", "1.0"], [" monarchy", "0.99952996"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"]]}, "595": {"qid": "1534ef2130cb48d6abb4", "term": "Heart", "description": "organ for the circulation of blood in animal circulatory systems", "question": "Is a jellyfish safe from atherosclerosis?", "answer": true, "facts": ["Atherosclerosis is a condition in which the arteries to the heart are blocked.", "Jellyfish use their guts to circulate nutrients because they do not have hearts."], "decomposition": ["What structures are affected by atherosclerosis?", "What bodily system does #1 contribute to?", "What structures are found in the jellyfish #2?", "Are there structures in common in both #1 and #3?"], "evidence": [[[["Atherosclerosis-1"]], [["Circulatory system-1"]], [["Jellyfish-18"]], ["operation"]], [[["Atherosclerosis-1"]], [["Artery-2"]], [["Jellyfish-18"]], ["operation"]], [[["Atherosclerosis-1"]], [["Artery-2"]], [["Jellyfish-18"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it true that the first man on the moon was Neil Armstrong?\nFacts: Neil Armstrong was the first man on the moon.\nAnswer (True or False): True\nQuestion: Is it true that the first man on the moon was Neil Armstrong?\nFacts: Neil Armstrong was the first man on the moon.\nAnswer (True or False): True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is a jellyfish safe from atherosclerosis?\nFacts: Atherosclerosis is a condition in which the arteries to the heart are blocked. Jellyfish use their guts to circulate nutrients because they do not have hearts.\nAnswer (True or False):", "ct_subject": "Atherosclerosis is a condition in which the arteries to the heart are blocked. Jellyfish use their guts to circulate nutrients because they do not have hearts.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99247146"], [" it", "0.8207586"], [" true", "0.99999917"], [" that", "1.0"], [" the", "0.9999999"], [" first", "0.92325926"], [" man", "0.9871622"], [" on", "0.99380714"], [" the", "0.99997866"], [" moon", "1.0"], [" was", "1.0"], [" Neil", "0.46080676"], [" Armstrong", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "0.9999999"], ["acts", "1.0"], [":", "1.0"], [" Neil", "1.0"], [" Armstrong", "1.0"], [" was", "1.0"], [" the", "0.9151072"], [" first", "1.0"], [" man", "1.0"], [" on", "0.63534266"], [" the", "1.0"], [" moon", "1.0"], [".", "1.0"], ["\n", "0.99982387"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" true", "1.0"], [" that", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "0.9999994"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" was", "1.0"], [" Neil", "1.0"], [" Armstrong", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Neil", "1.0"], [" Armstrong", "1.0"], [" was", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"]]}, "596": {"qid": "0df67cbe1345f90555dd", "term": "Quran", "description": "The central religious text of Islam", "question": "Would Dave Chappelle pray over a Quran?", "answer": true, "facts": ["Dave Chappelle converted to Islam in 1998.", "Dave Chappelle has not deviated from his religious beliefs since 1998 and is a practicing Muslim.", "Practicing Muslims pray often."], "decomposition": ["Which group uses the Quran as their religious text?", "Does Dave Chappelle belong to #1?"], "evidence": [[[["Quran-1"]], [["Dave Chappelle-57"], "operation"]], [[["Quran-1"]], [["Dave Chappelle-57"]]], [[["Quran-1"]], [["Dave Chappelle-57"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a Muslim and a Christian?\nFacts: Muslims believe in the same God as Christians. Muslims believe in the same God as Christians. Muslims believe in the same God as Christians. Muslims believe in the same God as Christians. Muslims believe in the same God as Christians. Muslims believe in the same God as Christians. Muslims", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Dave Chappelle pray over a Quran?\nFacts: Dave Chappelle converted to Islam in 1998. Dave Chappelle has not deviated from his religious beliefs since 1998 and is a practicing Muslim. Practicing Muslims pray often.\nAnswer (True or False):", "ct_subject": "Dave Chappelle converted to Islam in 1998. Dave Chappelle has not deviated from his religious beliefs since 1998 and is a practicing Muslim. Practicing Muslims pray often.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99998856"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9822411"], [" between", "1.0"], [" a", "0.99999917"], [" Muslim", "0.99973196"], [" and", "1.0"], [" a", "1.0"], [" Christian", "0.9999901"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Muslims", "0.9986358"], [" believe", "0.99970657"], [" in", "0.9987237"], [" the", "0.8942857"], [" same", "0.99999976"], [" God", "1.0"], [" as", "1.0"], [" Christians", "1.0"], [".", "1.0"], [" Muslims", "1.0"], [" believe", "1.0"], [" in", "0.9999976"], [" the", "1.0"], [" same", "1.0"], [" God", "1.0"], [" as", "1.0"], [" Christians", "1.0"], [".", "1.0"], [" Muslims", "0.86807024"], [" believe", "1.0"], [" in", "1.0"], [" the", "1.0"], [" same", "1.0"], [" God", "1.0"], [" as", "1.0"], [" Christians", "1.0"], [".", "1.0"], [" Muslims", "0.9999461"], [" believe", "1.0"], [" in", "1.0"], [" the", "1.0"], [" same", "1.0"], [" God", "1.0"], [" as", "1.0"], [" Christians", "1.0"], [".", "1.0"], [" Muslims", "0.9993248"], [" believe", "1.0"], [" in", "1.0"], [" the", "1.0"], [" same", "1.0"], [" God", "1.0"], [" as", "1.0"], [" Christians", "1.0"], [".", "1.0"], [" Muslims", "0.99954647"], [" believe", "1.0"], [" in", "1.0"], [" the", "1.0"], [" same", "1.0"], [" God", "1.0"], [" as", "1.0"], [" Christians", "1.0"], [".", "1.0"], [" Muslims", "0.99995077"]]}, "597": {"qid": "16bb34f451c9620e422c", "term": "Justin Timberlake", "description": "American singer, record producer, and actor", "question": "Can Justin Timberlake ride Shipwreck Falls at Six Flags?", "answer": true, "facts": ["Shipwreck Falls is a boat ride at Six Flags", "The minimum height for Shipwreck Falls is 42\"", "Justin Timberlake is 73\" tall"], "decomposition": ["What is Shipwreck Falls?", "What is the minimum height required to ride #1?", "How tall is Justin Timberlake?", "Is #3 bigger than #2?"], "evidence": [[[["Shipwreck Falls-1"]], ["no_evidence"], [["Justin Timberlake-1"], "no_evidence"], ["operation"]], [[["Shipwreck Falls-1"]], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Shipwreck Falls-1"]], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the tallest building in the world?\nFacts: The tallest building in the world is the Burj Khalifa in Dubai, United Arab Emirates. The Burj Khalifa is the tallest building in the world.\nAnswer (True or False): True\nQuestion: What is the tallest building in the world?\nFacts: The tallest building in the world is the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can Justin Timberlake ride Shipwreck Falls at Six Flags?\nFacts: Shipwreck Falls is a boat ride at Six Flags The minimum height for Shipwreck Falls is 42\" Justin Timberlake is 73\" tall\nAnswer (True or False):", "ct_subject": "Shipwreck Falls is a boat ride at Six Flags The minimum height for Shipwreck Falls is 42\" Justin Timberlake is 73\" tall", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.81661403"], [" is", "1.0"], [" the", "1.0"], [" tallest", "0.10819996"], [" building", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" tallest", "1.0"], [" building", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" is", "1.0"], [" the", "1.0"], [" Bur", "1.0"], ["j", "1.0"], [" Khal", "1.0"], ["ifa", "1.0"], [" in", "1.0"], [" Dubai", "1.0"], [",", "0.98543173"], [" United", "0.9999392"], [" Arab", "1.0"], [" Emirates", "1.0"], [".", "1.0"], [" The", "0.9918888"], [" Bur", "1.0"], ["j", "1.0"], [" Khal", "1.0"], ["ifa", "1.0"], [" is", "1.0"], [" the", "0.99998677"], [" tallest", "1.0"], [" building", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [".", "0.9999926"], ["\n", "0.9997273"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" tallest", "0.9981743"], [" building", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" tallest", "1.0"], [" building", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" is", "1.0"], [" the", "1.0"]]}, "598": {"qid": "68f04375b2ac70c73759", "term": "D\u00fcsseldorf", "description": "Place in North Rhine-Westphalia, Germany", "question": "Does D\u00fcsseldorf have only a small number of smoggy days each year?", "answer": true, "facts": ["Mercer's 2012 Quality of Living survey ranked D\u00fcsseldorf the sixth most livable city in the world.", "Clean air is an important attribute for a livable city.", "Smog is a term for air pollution."], "decomposition": ["What is another term for smog?", "What is D\u00fcsseldorf ranked as in Mercer's 2012 Quality of Living survey?", "To be #2, does a country need to have limited #1 days a year?"], "evidence": [[[["Smog-1"]], [["Global Liveability Ranking-2"], "no_evidence"], ["operation"]], [[["Smog-1"]], [["D\u00fcsseldorf-2"]], ["operation"]], [[["Smog-1"]], [["D\u00fcsseldorf-2"]], [["Most livable cities-7"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average temperature in D\u00fcsseldorf?\nFacts: D\u00fcsseldorf is the coldest city in Germany. The average temperature in D\u00fcsseldorf is -1.5\u00b0C.\nAnswer", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does D\u00fcsseldorf have only a small number of smoggy days each year?\nFacts: Mercer's 2012 Quality of Living survey ranked D\u00fcsseldorf the sixth most livable city in the world. Clean air is an important attribute for a livable city. Smog is a term for air pollution.\nAnswer (True or False):", "ct_subject": "Mercer's 2012 Quality of Living survey ranked D\u00fcsseldorf the sixth most livable city in the world. Clean air is an important attribute for a livable city. Smog is a term for air pollution.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.98307854"], [" is", "1.0"], [" the", "1.0"], [" average", "0.93601054"], [" temperature", "0.9803494"], [" in", "1.0"], [" D", "1.0"], ["\u00fc", "1.0"], ["s", "1.0"], ["se", "1.0"], ["ld", "1.0"], ["orf", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" D", "0.601429"], ["\u00fc", "1.0"], ["s", "1.0"], ["se", "1.0"], ["ld", "1.0"], ["orf", "1.0"], [" is", "0.9912459"], [" the", "0.99212986"], [" cold", "0.9999999"], ["est", "1.0"], [" city", "1.0"], [" in", "1.0"], [" Germany", "0.99999976"], [".", "1.0"], [" The", "0.99999595"], [" average", "1.0"], [" temperature", "1.0"], [" in", "0.9999999"], [" D", "1.0"], ["\u00fc", "1.0"], ["s", "1.0"], ["se", "1.0"], ["ld", "1.0"], ["orf", "1.0"], [" is", "1.0"], [" -", "1.0"], ["1", "0.74740684"], [".", "1.0"], ["5", "0.98594475"], ["\u00b0", "0.9903328"], ["C", "1.0"], [".", "0.99999917"], ["\n", "0.9999999"], ["Answer", "1.0"]]}, "599": {"qid": "6491c2e5de52c190eff8", "term": "Saint", "description": "one who has been recognized for having an exceptional degree of holiness, sanctity, and virtue", "question": "Can a false pope become a saint?", "answer": true, "facts": ["A false pope, or antipope, is someone that tries to claim they are the true pope but the church rejects them.", "Hippolytus (c. 170\u2013235 AD) headed a schismatic group as a rival to the Bishop of Rome, thus becoming an antipope.", "Hippolytus (c. 170\u2013235 AD) was named a saint in the Roman Catholic Church."], "decomposition": ["Which actions could make the Catholic church consider one a false pope or antipope?", "What role did Hippolytus (c. 170\u2013235 AD) play in the schismatic group against the Bishop of Rome?", "Is #2 a form of #1 and he still became saint?"], "evidence": [[[["Antipope-1"]], [["Hippolytus of Rome-1"]], [["Hippolytus of Rome-24"], "operation"]], [[["Antipope-1"], "no_evidence"], [["Hippolytus of Rome-1"]], [["Hippolytus of Rome-2"], "operation"]], [[["Antipope-1"]], [["Antipope-4"]], [["Antipope-16", "Saint-7"], "no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a Catholic and a Protestant?\nFacts: Catholics believe in the Holy Trinity, the Holy Spirit, and the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can a false pope become a saint?\nFacts: A false pope, or antipope, is someone that tries to claim they are the true pope but the church rejects them. Hippolytus (c. 170\u2013235 AD) headed a schismatic group as a rival to the Bishop of Rome, thus becoming an antipope. Hippolytus (c. 170\u2013235 AD) was named a saint in the Roman Catholic Church.\nAnswer (True or False):", "ct_subject": "A false pope, or antipope, is someone that tries to claim they are the true pope but the church rejects them. Hippolytus (c. 170\u2013235 AD) headed a schismatic group as a rival to the Bishop of Rome, thus becoming an antipope. Hippolytus (c. 170\u2013235 AD) was named a saint in the Roman Catholic Church.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99996805"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9999995"], [" between", "1.0"], [" a", "0.99999917"], [" Catholic", "0.9817785"], [" and", "1.0"], [" a", "1.0"], [" Protestant", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Catholics", "1.0"], [" believe", "0.9999771"], [" in", "0.999987"], [" the", "1.0"], [" Holy", "0.9999999"], [" Trinity", "1.0"], [",", "1.0"], [" the", "0.12920912"], [" Holy", "0.99457574"], [" Spirit", "0.9999995"], [",", "1.0"], [" and", "0.9977725"], [" the", "1.0"]]}, "600": {"qid": "36f063a2fc1338b5a21d", "term": "Starch", "description": "glucose polymer used as energy store in plants", "question": "Can a wheelbarrow full of starch kill hyperglycemics?", "answer": true, "facts": ["Hyperglycemia is a condition in which people have higher than normal blood glucose levels.", "Starch is a compound made by plants that is made of numerous glucose units.", "An excess of glucose can lead to diabetic complications and can result ind death.", "The average wheelbarrow can hold up to 1200 pounds."], "decomposition": ["What is hyperglycemia?", "What is starch made of?", "How much can the average wheelbarrow hold?", "Could #3 of #2 potentially be fatal to someone who has #1?"], "evidence": [[[["Hyperglycemia-1"]], [["Starch-1"]], [["Wheelbarrow-2"]], [["Hyperglycemia-21"]]], [[["Hyperglycemia-1"]], [["Starch-1"]], [["Wheelbarrow-2"]], [["Hyperglycemia-2"], "operation"]], [[["Hyperglycemia-1"]], [["Starch-1"]], [["Wheelbarrow-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human body?\nFacts: The average human body weight is about 100 pounds.\nAnswer (True or False): True\nQuestion: What is the average weight", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can a wheelbarrow full of starch kill hyperglycemics?\nFacts: Hyperglycemia is a condition in which people have higher than normal blood glucose levels. Starch is a compound made by plants that is made of numerous glucose units. An excess of glucose can lead to diabetic complications and can result ind death. The average wheelbarrow can hold up to 1200 pounds.\nAnswer (True or False):", "ct_subject": "Hyperglycemia is a condition in which people have higher than normal blood glucose levels. Starch is a compound made by plants that is made of numerous glucose units. An excess of glucose can lead to diabetic complications and can result ind death. The average wheelbarrow can hold up to 1200 pounds.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.53815943"], [" is", "1.0"], [" the", "1.0"], [" average", "0.8903609"], [" weight", "1.0"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" body", "0.035906255"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "1.0"], [" body", "1.0"], [" weight", "1.0"], [" is", "1.0"], [" about", "0.9999963"], [" 100", "0.99788195"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "0.9962022"]]}, "601": {"qid": "363e5889466d85bba2ca", "term": "Giraffe", "description": "Tall African ungulate", "question": "Do giraffes require special facilities at zoos?", "answer": true, "facts": ["Giraffes are much taller than other land animals.", "Giraffe shelters at zoos must be built larger than shelters for other animals to accommodate their height."], "decomposition": ["What is the most distinctive feature of a giraffe?", "Does #1 make it necessary for them to have different facilities from other animals at a zoo?"], "evidence": [[[["Giraffe-16"]], [["Giraffe-16"]]], [[["Giraffe-2"]], [["West African giraffe-4"], "no_evidence", "operation"]], [[["Giraffe-2"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"giant\" and a \"giantess\"?\nFacts: A \"giant\" is a person who is over 6 feet tall. A \"giantess\" is a person who is over 4 feet tall.\nAnswer (True or False): True\nQuestion: What is the difference between a \"giant\" and", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do giraffes require special facilities at zoos?\nFacts: Giraffes are much taller than other land animals. Giraffe shelters at zoos must be built larger than shelters for other animals to accommodate their height.\nAnswer (True or False):", "ct_subject": "Giraffes are much taller than other land animals. Giraffe shelters at zoos must be built larger than shelters for other animals to accommodate their height.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9806326"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9937238"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.9987513"], ["g", "0.894622"], ["iant", "0.9994578"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["g", "1.0"], ["iant", "1.0"], ["ess", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" \"", "0.999974"], ["g", "1.0"], ["iant", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" person", "0.93818724"], [" who", "1.0"], [" is", "1.0"], [" over", "0.6959525"], [" 6", "1.0"], [" feet", "1.0"], [" tall", "1.0"], [".", "1.0"], [" A", "1.0"], [" \"", "1.0"], ["g", "1.0"], ["iant", "1.0"], ["ess", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" person", "1.0"], [" who", "1.0"], [" is", "1.0"], [" over", "1.0"], [" 4", "0.9999224"], [" feet", "1.0"], [" tall", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["g", "1.0"], ["iant", "1.0"], ["\"", "1.0"], [" and", "1.0"]]}, "602": {"qid": "8aa5c9ba2ae74151dd6e", "term": "Krishna", "description": "Major deity in Hinduism", "question": "Was Krishna skilled at using the bow?", "answer": true, "facts": ["Lord Krishna was known as the eighth manifestation of the god Vishnu.", "Vishnu had a trove of weapons including the Sudarshana Chakra and Sharanga.", "Sharanga was a celestial bow and a favored weapon of Vishnu."], "decomposition": ["Which Hindu god was Krishna known to be a manifestation of?", "Which weapons belonging to #1 were among his favorite?", "Is the bow included in #2?"], "evidence": [[[["Krishna-1"]], [["Sharanga-1"]], [["Sharanga-1"]]], [[["Krishna-1"]], [["Sharanga-1"]], ["operation"]], [[["Krishna-1"]], [["Krishna-24", "Krishna-36", "Sharanga-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Was Jesus Christ a prophet?\nFacts: Jesus Christ was born in Bethlehem, in the land of Judah. He was the son of Joseph and Mary. He was crucified on the cross. He was buried in a tomb in Jerusalem. He rose from the dead. He ascended to heaven. He is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Krishna skilled at using the bow?\nFacts: Lord Krishna was known as the eighth manifestation of the god Vishnu. Vishnu had a trove of weapons including the Sudarshana Chakra and Sharanga. Sharanga was a celestial bow and a favored weapon of Vishnu.\nAnswer (True or False):", "ct_subject": "Lord Krishna was known as the eighth manifestation of the god Vishnu. Vishnu had a trove of weapons including the Sudarshana Chakra and Sharanga. Sharanga was a celestial bow and a favored weapon of Vishnu.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "0.99812955"], [" Jesus", "0.9902346"], [" Christ", "0.99999666"], [" a", "0.99957985"], [" prophet", "0.96377766"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Jesus", "1.0"], [" Christ", "1.0"], [" was", "1.0"], [" born", "1.0"], [" in", "0.99999976"], [" Bethlehem", "0.9999926"], [",", "0.999818"], [" in", "0.96397436"], [" the", "1.0"], [" land", "1.0"], [" of", "1.0"], [" Judah", "0.0045891725"], [".", "1.0"], [" He", "0.96236384"], [" was", "1.0"], [" the", "0.9734433"], [" son", "1.0"], [" of", "1.0"], [" Joseph", "0.9999912"], [" and", "1.0"], [" Mary", "1.0"], [".", "1.0"], [" He", "0.99999964"], [" was", "1.0"], [" cruc", "1.0"], ["ified", "1.0"], [" on", "1.0"], [" the", "0.9996221"], [" cross", "0.99999857"], [".", "0.99999225"], [" He", "1.0"], [" was", "0.9839267"], [" buried", "0.99997544"], [" in", "1.0"], [" a", "0.96593875"], [" tomb", "1.0"], [" in", "1.0"], [" Jerusalem", "0.99692565"], [".", "1.0"], [" He", "0.99999607"], [" rose", "0.8383409"], [" from", "1.0"], [" the", "1.0"], [" dead", "1.0"], [".", "0.5664553"], [" He", "0.99999785"], [" ascended", "0.99999964"], [" to", "0.96519"], [" heaven", "1.0"], [".", "1.0"], [" He", "1.0"], [" is", "0.9999995"]]}, "603": {"qid": "17b8fa74f450c4d0e56b", "term": "Alice in Wonderland (1951 film)", "description": "1951 American animated musical fantasy film produced by Walt Disney Productions", "question": "Does Disney's Alice in Wonderland involve the celebration of a holiday?", "answer": true, "facts": ["In the movie, Alice meets the Mad Hatter.", "The Mad Hatter is having a tea party to celebrate his Unbirthday.", "The Unbirthday is a holiday which happens every day of the year which is not the subject's actual birthday."], "decomposition": ["What celebrations were featured in the Disney movie Alice in Wonderland?", "Is any of #1 an holiday?"], "evidence": [[[["Alice in Wonderland (1951 film)-7"]], ["operation"]], [[["Alice in Wonderland (franchise)-14"]], [["Birthday-1"], "operation"]], [[["Alice in Wonderland (1951 film)-9"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the character in the movie \"The Wizard of Oz\"?\nFacts: The Wizard of Oz is a character in the movie \"The Wizard of Oz\". The Wizard of Oz is a character in the movie \"The Wizard of Oz\". The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Disney's Alice in Wonderland involve the celebration of a holiday?\nFacts: In the movie, Alice meets the Mad Hatter. The Mad Hatter is having a tea party to celebrate his Unbirthday. The Unbirthday is a holiday which happens every day of the year which is not the subject's actual birthday.\nAnswer (True or False):", "ct_subject": "In the movie, Alice meets the Mad Hatter. The Mad Hatter is having a tea party to celebrate his Unbirthday. The Unbirthday is a holiday which happens every day of the year which is not the subject's actual birthday.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99373007"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" character", "0.9629707"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "0.9996767"], ["The", "1.0"], [" Wizard", "0.99939656"], [" of", "1.0"], [" Oz", "1.0"], ["\"?", "0.9998715"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Wizard", "0.9947984"], [" of", "1.0"], [" Oz", "1.0"], [" is", "1.0"], [" a", "0.99997234"], [" character", "0.9935869"], [" in", "1.0"], [" the", "1.0"], [" movie", "0.988167"], [" \"", "1.0"], ["The", "1.0"], [" Wizard", "1.0"], [" of", "1.0"], [" Oz", "1.0"], ["\".", "0.99999654"], [" The", "0.9999958"], [" Wizard", "0.8877475"], [" of", "1.0"], [" Oz", "1.0"], [" is", "1.0"], [" a", "1.0"], [" character", "1.0"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "1.0"], ["The", "1.0"], [" Wizard", "1.0"], [" of", "1.0"], [" Oz", "1.0"], ["\".", "1.0"], [" The", "0.8289864"]]}, "604": {"qid": "d9f255a491a0e4048210", "term": "Separation of church and state", "description": "principle to separate religious and civil institutions", "question": "Does USA fail separation of church and state in multiple ways?", "answer": true, "facts": ["Separation of church ad state refers to keeping God and religion out of state matters.", "Presidents of the United States are sworn in by placing their hand on a bible.", "The US currency contains the words, \"In God We Trust.\"", "The Pledge of Allegiance states, \"One Nation Under God.\""], "decomposition": ["How are US Presidents sworn in?", "What is the inscription on the US currency?", "What does the Pledge of Allegiance state?", "Do #1, #2 and #3 contain references to religion/the chuch?"], "evidence": [[[["President of the United States-46"]], [["In God We Trust-2"]], [["Pledge of Allegiance-1"]], ["operation"]], [[["United States presidential inauguration-23"]], [["In God We Trust-15"]], [["Pledge of Allegiance-1"]], ["operation"]], [[["Oath of office of the President of the United States-12"]], [["In God We Trust-3"]], [["Pledge of Allegiance-43"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the United States a democracy?\nFacts: The United States is a republic. The US Constitution is the supreme law of the land. The US Congress is the only body that can propose legislation. The US Supreme Court", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does USA fail separation of church and state in multiple ways?\nFacts: Separation of church ad state refers to keeping God and religion out of state matters. Presidents of the United States are sworn in by placing their hand on a bible. The US currency contains the words, \"In God We Trust.\" The Pledge of Allegiance states, \"One Nation Under God.\"\nAnswer (True or False):", "ct_subject": "Separation of church ad state refers to keeping God and religion out of state matters. Presidents of the United States are sworn in by placing their hand on a bible. The US currency contains the words, \"In God We Trust.\" The Pledge of Allegiance states, \"One Nation Under God.\"", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.7381046"], [" the", "0.9997701"], [" United", "0.9999745"], [" States", "1.0"], [" a", "0.9985012"], [" democracy", "0.99025506"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "1.0"], [" a", "0.9999999"], [" republic", "0.8709304"], [".", "1.0"], [" The", "0.9999994"], [" US", "0.9988059"], [" Constitution", "1.0"], [" is", "0.9842791"], [" the", "0.9932775"], [" supreme", "1.0"], [" law", "1.0"], [" of", "1.0"], [" the", "1.0"], [" land", "1.0"], [".", "1.0"], [" The", "1.0"], [" US", "1.0"], [" Congress", "1.0"], [" is", "1.0"], [" the", "1.0"], [" only", "0.99937457"], [" body", "1.0"], [" that", "0.9999881"], [" can", "1.0"], [" propose", "0.9954145"], [" legislation", "0.6167005"], [".", "1.0"], [" The", "0.9999789"], [" US", "1.0"], [" Supreme", "0.99300355"], [" Court", "1.0"]]}, "605": {"qid": "97c91d5613b99fd4098f", "term": "Gorilla", "description": "Genus of mammals", "question": "Are gorillas closely related to humans?", "answer": true, "facts": ["Gorillas are part of the animal family Hominidae.", "Hominidae also includes the genus Homo, which only contains the human species."], "decomposition": ["What animal family are Gorillas part of?", "Are humans also part of #1?"], "evidence": [[[["Hominidae-1"]], ["operation"]], [[["Gorilla-1"]], [["Primate-2"]]], [[["Hominidae-1"]], [["Hominidae-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any animals that are not related to humans?\nFacts: There are many animals that are not related to humans. For example, there are many animals that are not related to the horse family.\nAnswer (True or False): True\nQuestion: Are there any animals that are not related to humans?\nFacts: There are many animals that are not related to the horse family", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are gorillas closely related to humans?\nFacts: Gorillas are part of the animal family Hominidae. Hominidae also includes the genus Homo, which only contains the human species.\nAnswer (True or False):", "ct_subject": "Gorillas are part of the animal family Hominidae. Hominidae also includes the genus Homo, which only contains the human species.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "0.99994147"], [" any", "0.9998167"], [" animals", "0.901908"], [" that", "1.0"], [" are", "1.0"], [" not", "0.9999999"], [" related", "1.0"], [" to", "1.0"], [" humans", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "1.0"], [" are", "1.0"], [" many", "1.0"], [" animals", "1.0"], [" that", "1.0"], [" are", "1.0"], [" not", "1.0"], [" related", "1.0"], [" to", "1.0"], [" humans", "1.0"], [".", "1.0"], [" For", "0.9950984"], [" example", "1.0"], [",", "1.0"], [" there", "0.91585827"], [" are", "1.0"], [" many", "1.0"], [" animals", "0.99179804"], [" that", "1.0"], [" are", "1.0"], [" not", "1.0"], [" related", "1.0"], [" to", "1.0"], [" the", "0.98786896"], [" horse", "0.99999785"], [" family", "0.99997056"], [".", "0.99995315"], ["\n", "0.3238811"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "1.0"], [" any", "1.0"], [" animals", "1.0"], [" that", "1.0"], [" are", "1.0"], [" not", "1.0"], [" related", "1.0"], [" to", "1.0"], [" humans", "0.9999981"], ["?", "0.9999932"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "1.0"], [" are", "1.0"], [" many", "1.0"], [" animals", "1.0"], [" that", "1.0"], [" are", "1.0"], [" not", "1.0"], [" related", "1.0"], [" to", "1.0"], [" the", "1.0"], [" horse", "1.0"], [" family", "1.0"]]}, "606": {"qid": "fc9339f21f44a3841a7c", "term": "Koala", "description": "An arboreal herbivorous marsupial native to Australia.", "question": "Do Koalas prefer Eucalyptus over meat?", "answer": true, "facts": ["Koalas are herbivores.", "Koalas main dietary staple is eucalyptus "], "decomposition": ["What kind of diet do Koalas follow?", "Are Eucalyptus part of #1?"], "evidence": [[[["Koala-2"]], ["operation"]], [[["Koala-2"]], [["Eucalypt-5", "Koala-2"]]], [[["Koala-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Do you know what the word \"Kiwi\" means?\nFacts: The word \"Kiwi\" is derived from the Maori word \"Kiwi\" which means \"bird of paradise\".\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in New Zealand?\nFacts: The first person to be born in New Zealand", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do Koalas prefer Eucalyptus over meat?\nFacts: Koalas are herbivores. Koalas main dietary staple is eucalyptus \nAnswer (True or False):", "ct_subject": "Koalas are herbivores. Koalas main dietary staple is eucalyptus ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Do", "0.75882345"], [" you", "0.9123316"], [" know", "0.99984527"], [" what", "0.97009754"], [" the", "0.99995244"], [" word", "1.0"], [" \"", "1.0"], ["K", "0.9998154"], ["iw", "0.99999976"], ["i", "1.0"], ["\"", "1.0"], [" means", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.81426984"], [" word", "1.0"], [" \"", "1.0"], ["K", "1.0"], ["iw", "1.0"], ["i", "1.0"], ["\"", "1.0"], [" is", "0.99997735"], [" derived", "0.9863876"], [" from", "1.0"], [" the", "1.0"], [" Ma", "0.99993217"], ["ori", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["K", "0.99997616"], ["iw", "1.0"], ["i", "1.0"], ["\"", "0.99999845"], [" which", "0.9995695"], [" means", "1.0"], [" \"", "1.0"], ["bird", "0.9847239"], [" of", "1.0"], [" paradise", "1.0"], ["\".", "0.9999999"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.29649886"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9976017"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9995478"], [" person", "0.61587507"], [" to", "1.0"], [" be", "1.0"], [" born", "0.9999988"], [" in", "1.0"], [" New", "0.7687196"], [" Zealand", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" New", "1.0"], [" Zealand", "1.0"]]}, "607": {"qid": "a61431fb7fe82f6b2ee0", "term": "Julian calendar", "description": "solar calendar in use from imperial Rome until after the Reformation", "question": "Did Saint Augustine use the Julian calendar?", "answer": true, "facts": ["The Julian calendar was in use from 45 BC to the late 16th century AD", "Saint Augustine lived from 354 AD to 430 AD"], "decomposition": ["During what years was the Julian calendar used?", "When did Saint Augustine live?", "Is #2 during the time period listed in #1?"], "evidence": [[[["Julian calendar-1", "Julian calendar-2"]], [["Augustine of Hippo-1"]], ["operation"]], [[["Julian calendar-1", "Julian calendar-2"]], [["Augustine of Hippo-1"]], ["operation"]], [[["Julian calendar-2"]], [["Augustine of Hippo-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Roman Catholic Church use the Julian calendar?\nFacts: The Julian calendar was in use from 45 BC to the late 16th century AD\nAnswer (True or False): True\nQuestion: Did the Roman Catholic Church use the Gregorian calendar?\nFacts: The Gregorian calendar was in use from 1582 to the early 20th century AD\nAnswer (True or False): True\nQuestion", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Saint Augustine use the Julian calendar?\nFacts: The Julian calendar was in use from 45 BC to the late 16th century AD Saint Augustine lived from 354 AD to 430 AD\nAnswer (True or False):", "ct_subject": "The Julian calendar was in use from 45 BC to the late 16th century AD Saint Augustine lived from 354 AD to 430 AD", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99995303"], [" the", "1.0"], [" Roman", "1.0"], [" Catholic", "0.9572628"], [" Church", "1.0"], [" use", "1.0"], [" the", "1.0"], [" Julian", "1.0"], [" calendar", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Julian", "0.13962708"], [" calendar", "1.0"], [" was", "1.0"], [" in", "1.0"], [" use", "1.0"], [" from", "1.0"], [" 45", "1.0"], [" BC", "1.0"], [" to", "1.0"], [" the", "1.0"], [" late", "1.0"], [" 16", "1.0"], ["th", "1.0"], [" century", "1.0"], [" AD", "1.0"], ["\n", "0.95360017"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999999"], [" the", "1.0"], [" Roman", "1.0"], [" Catholic", "1.0"], [" Church", "1.0"], [" use", "1.0"], [" the", "1.0"], [" Greg", "1.0"], ["orian", "1.0"], [" calendar", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Greg", "1.0"], ["orian", "1.0"], [" calendar", "1.0"], [" was", "1.0"], [" in", "1.0"], [" use", "1.0"], [" from", "1.0"], [" 15", "1.0"], ["82", "1.0"], [" to", "0.97069216"], [" the", "1.0"], [" early", "0.8325389"], [" 20", "0.9972849"], ["th", "1.0"], [" century", "1.0"], [" AD", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"]]}, "608": {"qid": "1515242f123df1362ad7", "term": "Elk", "description": "Large antlered species of deer from North America and east Asia", "question": "Would a body builder prefer an elk burger over a beef burger?", "answer": true, "facts": ["Bodybuilders want to build muscle and keep fat low", "Elk meat is leaner than beef", "Elk meat has higher protein than beef", "Protein helps build muscle"], "decomposition": ["Which nutrients are more important for a body builder's diet?", "How is an elk burger different from a beef burger in terms of nutrients?", "Considering #1 and #2 would an elk burger be a better source of #1?"], "evidence": [[[["Bodybuilding-31", "Bodybuilding-41"]], [["Elk-3"]], [["Bodybuilding-31", "Elk-3"]]], [[["Bodybuilding-41"]], [["Elk-3"]], ["operation"]], [[["Bodybuilding-39"]], [["Elk-3"]], [["Elk-3"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"sausage\" and a \"burger\"?\nFacts: A sausage is a meat patty that has been cooked and then sliced. A burger is a meat patty that has been cooked and then grilled.\nAnswer (True or False): True\nQuestion: What is the difference between a \"sausage\" and", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a body builder prefer an elk burger over a beef burger?\nFacts: Bodybuilders want to build muscle and keep fat low Elk meat is leaner than beef Elk meat has higher protein than beef Protein helps build muscle\nAnswer (True or False):", "ct_subject": "Bodybuilders want to build muscle and keep fat low Elk meat is leaner than beef Elk meat has higher protein than beef Protein helps build muscle", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999297"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9468719"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.8291272"], ["s", "0.8523305"], ["aus", "0.9780017"], ["age", "1.0"], ["\"", "0.9987645"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["bur", "0.9999982"], ["ger", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999989"], [" sausage", "0.9999976"], [" is", "1.0"], [" a", "1.0"], [" meat", "0.98866165"], [" pat", "0.9999862"], ["ty", "1.0"], [" that", "0.9586676"], [" has", "0.9987495"], [" been", "1.0"], [" cooked", "1.0"], [" and", "0.98905635"], [" then", "0.99957174"], [" sliced", "0.9999999"], [".", "0.99980515"], [" A", "1.0"], [" burger", "1.0"], [" is", "1.0"], [" a", "1.0"], [" meat", "1.0"], [" pat", "1.0"], ["ty", "1.0"], [" that", "1.0"], [" has", "1.0"], [" been", "1.0"], [" cooked", "1.0"], [" and", "1.0"], [" then", "1.0"], [" grilled", "0.99088573"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["aus", "1.0"], ["age", "1.0"], ["\"", "1.0"], [" and", "1.0"]]}, "609": {"qid": "4454659b2638172ab151", "term": "Mercury (element)", "description": "Chemical element with atomic number 80", "question": "Can you transport a coin along a sea of mercury?", "answer": true, "facts": ["The density of an object determines if it will float.", "An object will float if it is less dense than the liquid it is placed in.", "Mercury is liquid at room temperature.", "The density of mercury is 13.56 g/cm3.", "The density of a penny is 7.15 g/cm3."], "decomposition": ["What is the density of mercury?", "What is the density of a typical coin?", "Is #2 less than #1?", "Considering #3 and the principle of flotation, will the coin float along mercury sea surface?"], "evidence": [[[["Mercury (element)-7"]], [["Quarter (United States coin)-3"]], ["operation"], [["Archimedes' principle-3"], "operation"]], [[["Mercury (element)-7"]], ["no_evidence"], ["no_evidence", "operation"], [["Buoyancy-1"], "operation"]], [[["Mercury (element)-7"]], [["Coin-2", "Metal-9"]], ["operation"], [["Buoyancy-2"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a man and a woman?\nFacts: A man is a male human being. A woman is a female human being.\nAnswer (True or False): True\nQuestion: What", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can you transport a coin along a sea of mercury?\nFacts: The density of an object determines if it will float. An object will float if it is less dense than the liquid it is placed in. Mercury is liquid at room temperature. The density of mercury is 13.56 g/cm3. The density of a penny is 7.15 g/cm3.\nAnswer (True or False):", "ct_subject": "The density of an object determines if it will float. An object will float if it is less dense than the liquid it is placed in. Mercury is liquid at room temperature. The density of mercury is 13.56 g/cm3. The density of a penny is 7.15 g/cm3.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9997484"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.30881628"], [" between", "1.0"], [" a", "1.0"], [" man", "0.8140031"], [" and", "1.0"], [" a", "1.0"], [" woman", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" man", "1.0"], [" is", "1.0"], [" a", "1.0"], [" male", "1.0"], [" human", "0.96286404"], [" being", "1.0"], [".", "1.0"], [" A", "1.0"], [" woman", "1.0"], [" is", "1.0"], [" a", "1.0"], [" female", "1.0"], [" human", "1.0"], [" being", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"]]}, "610": {"qid": "3e9e3ccc9fc4d44a3eb4", "term": "Astronaut", "description": "Person who commands, pilots, or serves as a crew member of a spacecraft", "question": "Can actress Danica McKellar skip astronaut education requirements?", "answer": true, "facts": ["Astronaut's are required to have a bachelor's degree in engineering, biological science, physical science, computer science, or mathematics.", "Actress Danica McKellar graduated summa cum laude from UCLA with a degree in Mathematics."], "decomposition": ["Astronauts can have any one of which degrees?", "What degree does Danica McKellar have?", "Is #2 included in #1?"], "evidence": [[[["NASA Astronaut Corps-10"]], [["Danica McKellar-5"]], ["operation"]], [[["NASA Astronaut Corps-10"]], [["Danica McKellar-5"]], ["operation"]], [[["NASA Astronaut Corps-10"]], [["Danica McKellar-5"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human?\nFacts: The average weight of a human is about 200 pounds.\nAnswer (True or False): True\nQuestion: What is the average height of a human?\nFacts: The average height of a human is about 5 feet, 8", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can actress Danica McKellar skip astronaut education requirements?\nFacts: Astronaut's are required to have a bachelor's degree in engineering, biological science, physical science, computer science, or mathematics. Actress Danica McKellar graduated summa cum laude from UCLA with a degree in Mathematics.\nAnswer (True or False):", "ct_subject": "Astronaut's are required to have a bachelor's degree in engineering, biological science, physical science, computer science, or mathematics. Actress Danica McKellar graduated summa cum laude from UCLA with a degree in Mathematics.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99986494"], [" is", "1.0"], [" the", "1.0"], [" average", "0.9493301"], [" weight", "0.6998102"], [" of", "1.0"], [" a", "0.9999993"], [" human", "0.99999666"], ["?", "0.99753726"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" weight", "0.65673393"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" is", "1.0"], [" about", "0.9640713"], [" 200", "0.95336694"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" height", "1.0"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" height", "1.0"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 5", "0.99941707"], [" feet", "0.9916971"], [",", "0.98032504"], [" 8", "0.92318386"]]}, "611": {"qid": "b2dddce981183d5a083d", "term": "Robert Downey Jr.", "description": "American actor", "question": "Did Robert Downey Jr. possess same caliber gun as Resident Evil's Barry Burton?", "answer": true, "facts": ["Robert Downey Jr. was arrested in 1996 n drug and weapons charges and possessed a .357 Magnum.", "Barry Burton, a character in the Resident Evil series, used a Colt Python.", "The Colt Python is a type of .357 Magnum revolver."], "decomposition": ["What type of gun did Robert Downey Jr. have when he was arrested?", "What gun does Barry Burton use?", "What type of gun is #2?", "Is #1 the same as #3?"], "evidence": [[[["Robert Downey Jr.-13"]], [["Resident Evil 3: Nemesis-10"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Robert Downey Jr.-13"]], [["Resident Evil 3: Nemesis-10"], "no_evidence"], [["Colt Python-1"]], ["operation"]], [[["Robert Downey Jr.-13"]], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the original Star Wars trilogy have a female protagonist?\nFacts: The original Star Wars trilogy had a female protagonist.\nAnswer (True or False): True\nQuestion: Did the original Star Wars trilogy have a male protagonist?\nFacts", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Robert Downey Jr. possess same caliber gun as Resident Evil's Barry Burton?\nFacts: Robert Downey Jr. was arrested in 1996 n drug and weapons charges and possessed a .357 Magnum. Barry Burton, a character in the Resident Evil series, used a Colt Python. The Colt Python is a type of .357 Magnum revolver.\nAnswer (True or False):", "ct_subject": "Robert Downey Jr. was arrested in 1996 n drug and weapons charges and possessed a .357 Magnum. Barry Burton, a character in the Resident Evil series, used a Colt Python. The Colt Python is a type of .357 Magnum revolver.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99536926"], [" the", "1.0"], [" original", "0.89222336"], [" Star", "0.99999964"], [" Wars", "1.0"], [" trilogy", "1.0"], [" have", "0.9999243"], [" a", "1.0"], [" female", "0.6074544"], [" protagonist", "0.50213623"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99907386"], [" original", "1.0"], [" Star", "1.0"], [" Wars", "1.0"], [" trilogy", "1.0"], [" had", "1.0"], [" a", "1.0"], [" female", "1.0"], [" protagonist", "1.0"], [".", "1.0"], ["\n", "0.5598126"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" original", "1.0"], [" Star", "1.0"], [" Wars", "1.0"], [" trilogy", "1.0"], [" have", "1.0"], [" a", "1.0"], [" male", "0.9999999"], [" protagonist", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"]]}, "612": {"qid": "3f7daff5574045d7b737", "term": "Olive", "description": "Species of plant", "question": "Would Bugs Bunny harm an olive tree in the real world?", "answer": true, "facts": ["Bugs Bunny is an anthropomorphic gray and white rabbit.", "Rabbits eat the bark of olive trees and can do considerable damage, especially to young trees."], "decomposition": ["What kind of animal is Bugs Bunny?", "Do #1 eat and damage the bark of olive trees?"], "evidence": [[[["Bugs Bunny-2"]], [["Olive-73"]]], [[["Bugs Bunny-2"]], [["Olive-73"], "operation"]], [[["Bugs Bunny-2"]], [["Olive-73"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: The first person to be executed in the United States was John Brown.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: The first person to be executed in the United", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Bugs Bunny harm an olive tree in the real world?\nFacts: Bugs Bunny is an anthropomorphic gray and white rabbit. Rabbits eat the bark of olive trees and can do considerable damage, especially to young trees.\nAnswer (True or False):", "ct_subject": "Bugs Bunny is an anthropomorphic gray and white rabbit. Rabbits eat the bark of olive trees and can do considerable damage, especially to young trees.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999046"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9563386"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9993548"], [" person", "0.9813719"], [" to", "1.0"], [" be", "0.99992037"], [" executed", "0.9932375"], [" in", "0.9999652"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9619828"], [" first", "1.0"], [" person", "1.0"], [" to", "0.91436005"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" John", "0.99928397"], [" Brown", "1.0"], [".", "0.3381864"], ["\n", "0.9999299"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999999"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"]]}, "613": {"qid": "f61620c8fca3a1530998", "term": "Daily Mirror", "description": "British daily tabloid newspaper owned by Reach plc.", "question": "Can a copy of The Daily Mirror sustain a campfire?", "answer": true, "facts": ["The Daily Mirror is a British tabloid made of paper.", "Kindling helps sustain a fire because easily combustible help a fire keep igniting.", "Paper is capable of igniting and burning easily."], "decomposition": ["What kind of product is The Daily Mirror?", "Is #1 made of combustible material?"], "evidence": [[[["Daily Mirror-1"]], ["no_evidence"]], [[["Daily Mirror-1"]], [["Newsprint-1"], "no_evidence", "operation"]], [[["Daily Mirror-1"]], [["Combustibility and flammability-2"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance between two points on Earth?\nFacts: The longest distance between two points on Earth is the distance between the Earth and the Sun.\nAnswer (True or False): True\nQuestion: What is the longest distance between two points on the Moon?\nFacts: The longest distance between two points", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can a copy of The Daily Mirror sustain a campfire?\nFacts: The Daily Mirror is a British tabloid made of paper. Kindling helps sustain a fire because easily combustible help a fire keep igniting. Paper is capable of igniting and burning easily.\nAnswer (True or False):", "ct_subject": "The Daily Mirror is a British tabloid made of paper. Kindling helps sustain a fire because easily combustible help a fire keep igniting. Paper is capable of igniting and burning easily.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9997408"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.48185953"], [" distance", "0.99912435"], [" between", "0.97873425"], [" two", "1.0"], [" points", "1.0"], [" on", "0.9958943"], [" Earth", "0.98283523"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "0.599549"], [" distance", "1.0"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"], [" on", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" the", "0.4353538"], [" distance", "1.0"], [" between", "1.0"], [" the", "0.9995316"], [" Earth", "0.9995555"], [" and", "1.0"], [" the", "1.0"], [" Sun", "1.0"], [".", "1.0"], ["\n", "0.98453647"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "0.9999995"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"], [" on", "1.0"], [" the", "1.0"], [" Moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"]]}, "614": {"qid": "848941fb02c8818a4e1f", "term": "Porsche", "description": "automotive brand manufacturing subsidiary of Volkswagen", "question": "Could a Porsche 992 Turbo S defeat Usain Bolt in a 100 meter sprint?", "answer": true, "facts": ["The Porsche 992 Turbo S can accelerate to 62 mph in 2.7 seconds.", "Usain Bolt's top speed ever measured is 27.79 mph."], "decomposition": ["What is the max speed of a Porsche 992 Turbo S?", "What is Bolt's top speed?", "Is #1 faster than #2?"], "evidence": [[[["Porsche 992-8"]], [["Usain Bolt-106"]], ["operation"]], [[["Porsche 992-8"]], [["Footspeed-4"]], ["operation"]], [[["Porsche 992-8"]], [["Usain Bolt-106"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance a human being has ever walked?\nFacts: The longest distance a human being has ever walked is about 2,000 miles.\nAnswer (True or False): True\nQuestion: What is the longest distance a human being has ever flown?\nFacts: The longest distance a human being has ever flown", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could a Porsche 992 Turbo S defeat Usain Bolt in a 100 meter sprint?\nFacts: The Porsche 992 Turbo S can accelerate to 62 mph in 2.7 seconds. Usain Bolt's top speed ever measured is 27.79 mph.\nAnswer (True or False):", "ct_subject": "The Porsche 992 Turbo S can accelerate to 62 mph in 2.7 seconds. Usain Bolt's top speed ever measured is 27.79 mph.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999541"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.67667097"], [" distance", "1.0"], [" a", "0.9999546"], [" human", "0.7014597"], [" being", "0.2718312"], [" has", "1.0"], [" ever", "1.0"], [" walked", "0.86852044"], ["?", "0.99999976"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "0.99999654"], [" has", "1.0"], [" ever", "1.0"], [" walked", "1.0"], [" is", "1.0"], [" about", "0.99700934"], [" 2", "0.9996611"], [",", "1.0"], ["000", "0.9999999"], [" miles", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "1.0"], [" has", "1.0"], [" ever", "1.0"], [" flown", "0.99954"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "1.0"], [" has", "1.0"], [" ever", "1.0"], [" flown", "1.0"]]}, "615": {"qid": "46f53b9e0008ad1110ed", "term": "Cuban Revolution", "description": "Revolution in Cuba between 1953 and 1959", "question": "During the Cuban revolution, did the US experience a population boom?", "answer": true, "facts": ["After WWII, the US experienced a baby boom.", "WWII ended in 1945."], "decomposition": ["When was the Cuban Revolution?", "When did the United States experience a rapid growth in its population?", "Does some or all of #2 overlap with #1?"], "evidence": [[[["Cuban Revolution-8"]], [["Baby boom-3"]], [["Baby boom-3", "Cuban Revolution-8"], "operation"]], [[["Cuban Revolution-1"]], [["Mid-twentieth century baby boom-1", "Mid-twentieth century baby boom-3"]], ["operation"]], [[["Cuban Revolution-1"]], [["Mid-twentieth century baby boom-12"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average IQ of a person in the US?\nFacts: The average IQ of a person in the US is 100.\nAnswer (True or False): True\nQuestion: What is the average IQ of a person in the US?\nFacts: The average IQ of a person in the US is 100.\nAnswer (True or False): True\nQuestion: What is the average IQ of a person in the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: During the Cuban revolution, did the US experience a population boom?\nFacts: After WWII, the US experienced a baby boom. WWII ended in 1945.\nAnswer (True or False):", "ct_subject": "After WWII, the US experienced a baby boom. WWII ended in 1945.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99063325"], [" is", "1.0"], [" the", "1.0"], [" average", "0.9999906"], [" IQ", "0.9972799"], [" of", "1.0"], [" a", "0.52351743"], [" person", "0.9759936"], [" in", "0.96570665"], [" the", "1.0"], [" US", "0.9978137"], ["?", "0.99998677"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" IQ", "1.0"], [" of", "1.0"], [" a", "1.0"], [" person", "1.0"], [" in", "1.0"], [" the", "1.0"], [" US", "1.0"], [" is", "1.0"], [" 100", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" IQ", "0.99999726"], [" of", "1.0"], [" a", "1.0"], [" person", "1.0"], [" in", "1.0"], [" the", "1.0"], [" US", "0.9998903"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" IQ", "1.0"], [" of", "1.0"], [" a", "1.0"], [" person", "1.0"], [" in", "1.0"], [" the", "1.0"], [" US", "1.0"], [" is", "1.0"], [" 100", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" IQ", "1.0"], [" of", "1.0"], [" a", "1.0"], [" person", "1.0"], [" in", "1.0"], [" the", "1.0"]]}, "616": {"qid": "e4115d4b10cb3447c7a2", "term": "Meatball", "description": "dish made from ground meat rolled into a small ball-like form", "question": "Can you buy furniture and meatballs in the same store?", "answer": true, "facts": ["IKEA is well known for selling cheap, modern furniture.", "IKEA is famous for serving meatballs at their in-store restaurants."], "decomposition": ["What is IKEA known for selling?", "What are some delicacies IKEA is known to serve at their in-store restaurants?", "Is meatballs included in #2 and #1 furniture?"], "evidence": [[[["IKEA-1"]], [["IKEA-15"]], [["IKEA-15"]]], [[["IKEA-1"]], [["IKEA-12"]], ["operation"]], [[["IKEA-1"]], [["IKEA-12"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"carpet\" and a \"carpeted floor\"?\nFacts: A carpet is a material that is made of a single layer of fabric. A carpeted floor is a material that is made of multiple layers of fabric.\nAnswer (True or False): True\nQuestion: What is the difference between a \"carp", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can you buy furniture and meatballs in the same store?\nFacts: IKEA is well known for selling cheap, modern furniture. IKEA is famous for serving meatballs at their in-store restaurants.\nAnswer (True or False):", "ct_subject": "IKEA is well known for selling cheap, modern furniture. IKEA is famous for serving meatballs at their in-store restaurants.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9998615"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.35636652"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.97930247"], ["c", "0.35501727"], ["arp", "0.991344"], ["et", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "0.89082104"], ["arp", "1.0"], ["et", "1.0"], ["ed", "1.0"], [" floor", "0.71403396"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" carpet", "1.0"], [" is", "1.0"], [" a", "1.0"], [" material", "0.052813526"], [" that", "1.0"], [" is", "1.0"], [" made", "0.9964981"], [" of", "0.93897843"], [" a", "0.9990753"], [" single", "0.99347407"], [" layer", "1.0"], [" of", "1.0"], [" fabric", "0.96570975"], [".", "1.0"], [" A", "1.0"], [" carpet", "1.0"], ["ed", "1.0"], [" floor", "1.0"], [" is", "1.0"], [" a", "0.9803051"], [" material", "1.0"], [" that", "1.0"], [" is", "1.0"], [" made", "1.0"], [" of", "1.0"], [" multiple", "0.99986076"], [" layers", "1.0"], [" of", "1.0"], [" fabric", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["arp", "1.0"]]}, "617": {"qid": "39764cc9679c2e0e6435", "term": "2000", "description": "Year", "question": "Was there fear leading up to the year 2000?", "answer": true, "facts": ["Many computer programs were not designed with the year 2000 in mind.", "People were worried that computers would crash all over the world when the year 2000 arrived.", "Financial and electrical systems require computers to function.", "Without financial and electrical systems there could be global chaos."], "decomposition": ["What concerns did people have about computing systems as 2000 approached?", "Did #1 involve a widespread fear of malfunction?"], "evidence": [[[["Year 2000 problem-1"]], [["Year 2000 problem-23"], "operation"]], [[["Year 2000 problem-1"]], [["Year 2000 problem-15"], "operation"]], [[["Year 2000 problem-1"]], [["Year 2000 problem-23"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first computer?\nFacts: The first computer was called the ENIAC. It was built in 1946 by the National Bureau of Standards.\nAnswer (True or False): True\nQuestion: What is the name of the first computer?\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was there fear leading up to the year 2000?\nFacts: Many computer programs were not designed with the year 2000 in mind. People were worried that computers would crash all over the world when the year 2000 arrived. Financial and electrical systems require computers to function. Without financial and electrical systems there could be global chaos.\nAnswer (True or False):", "ct_subject": "Many computer programs were not designed with the year 2000 in mind. People were worried that computers would crash all over the world when the year 2000 arrived. Financial and electrical systems require computers to function. Without financial and electrical systems there could be global chaos.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999852"], [" is", "1.0"], [" the", "1.0"], [" name", "0.38539264"], [" of", "1.0"], [" the", "1.0"], [" first", "0.79027"], [" computer", "0.7060545"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" computer", "1.0"], [" was", "1.0"], [" called", "0.77004534"], [" the", "1.0"], [" EN", "0.97685975"], ["I", "1.0"], ["AC", "1.0"], [".", "1.0"], [" It", "0.99803895"], [" was", "1.0"], [" built", "0.99997234"], [" in", "0.9989285"], [" 1946", "0.99621373"], [" by", "0.9771588"], [" the", "1.0"], [" National", "0.99800044"], [" Bureau", "1.0"], [" of", "1.0"], [" Standards", "1.0"], [".", "0.99991906"], ["\n", "0.97669286"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" computer", "1.0"], ["?", "0.99999535"], ["\n", "1.0"]]}, "618": {"qid": "e6d3973ed3feb8a42928", "term": "Bee", "description": "Clade of insects", "question": "Can Africanized bees be considered multicultural?", "answer": true, "facts": ["Multicultural refers to a blend of several cultures within one organism.", "Africanized bees, also killer bees are a result of crossbreeding.", "Africanized bees are a mix of East African lowland honey bees and European honey bee subspecies such as the Italian honey bee and the Iberian honey bee. "], "decomposition": ["What is the definition of multicultural?", "What are Africanized bees a result of?", "What types of bees were part of #2?", "Is #3 an example of #1?"], "evidence": [[[["Multiculturalism-1"]], [["Africanized bee-1"]], [["Africanized bee-1"]], [["Africanized bee-1", "Multiculturalism-1"]]], [[["Multiracial people-1"]], [["Honey bee-21"]], [["Africanized bee-1"]], ["operation"]], [[["Multiculturalism-1"]], [["Africanized bee-1"]], [["African bee-1", "Western honey bee-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a bee and a wasp?\nFacts: Bees are insects. Wasps are arachnids.\nAnswer (True or False): True\nQuestion: What is the difference between a bee and", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can Africanized bees be considered multicultural?\nFacts: Multicultural refers to a blend of several cultures within one organism. Africanized bees, also killer bees are a result of crossbreeding. Africanized bees are a mix of East African lowland honey bees and European honey bee subspecies such as the Italian honey bee and the Iberian honey bee. \nAnswer (True or False):", "ct_subject": "Multicultural refers to a blend of several cultures within one organism. Africanized bees, also killer bees are a result of crossbreeding. Africanized bees are a mix of East African lowland honey bees and European honey bee subspecies such as the Italian honey bee and the Iberian honey bee. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.96597177"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" bee", "0.99979466"], [" and", "0.99940455"], [" a", "1.0"], [" was", "1.0"], ["p", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Bees", "0.9999443"], [" are", "1.0"], [" insects", "1.0"], [".", "0.9998981"], [" Was", "1.0"], ["ps", "1.0"], [" are", "1.0"], [" ar", "0.98131573"], ["ach", "1.0"], ["n", "1.0"], ["ids", "1.0"], [".", "1.0"], ["\n", "0.17762357"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "0.99999976"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" bee", "1.0"], [" and", "1.0"]]}, "619": {"qid": "42c947475244ed02629e", "term": "Oyster", "description": "salt-water bivalve mollusc", "question": "Can oysters be used in guitar manufacturing?", "answer": true, "facts": ["Oysters produce nacre", "Nacre is also known as mother of pearl", "Mother of pearl is commonly used as an inlay on guitar fretboards, headstocks, and soundboards"], "decomposition": ["What non-food products are derived from oysters?", "Which of #1 are used for decoration?", "What materials are used to decorate a guitar?", "Is there overlap between #2 and #3?"], "evidence": [[[["Nacre-1", "Nacre-17"], "no_evidence"], [["Oyster-2"], "no_evidence"], [["Guitar-45"], "no_evidence"], ["operation"]], [[["Oyster-9"]], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Oyster-6"]], [["Oyster-2"]], [["Inlay (guitar)-1"]], [["Inlay (guitar)-1", "Oyster-2"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake\" diamond?\nFacts: A snowflake is a diamond with a single snowflake pattern. A snowflake diamond is a diamond with a single snowflake pattern and a single snowflake cut.\nAnswer (True or False): True\nQuestion: What is the difference", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can oysters be used in guitar manufacturing?\nFacts: Oysters produce nacre Nacre is also known as mother of pearl Mother of pearl is commonly used as an inlay on guitar fretboards, headstocks, and soundboards\nAnswer (True or False):", "ct_subject": "Oysters produce nacre Nacre is also known as mother of pearl Mother of pearl is commonly used as an inlay on guitar fretboards, headstocks, and soundboards", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99992216"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9913442"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.6055498"], ["now", "0.99576664"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.9999999"], ["now", "1.0"], ["flake", "1.0"], ["\"", "0.9980459"], [" diamond", "0.9864954"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" snow", "0.96570957"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" diamond", "0.88995826"], [" with", "0.066590995"], [" a", "1.0"], [" single", "0.99999964"], [" snow", "0.9725648"], ["flake", "1.0"], [" pattern", "0.993933"], [".", "0.9999999"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" diamond", "0.9911567"], [" is", "1.0"], [" a", "1.0"], [" diamond", "1.0"], [" with", "1.0"], [" a", "0.9992656"], [" single", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" pattern", "1.0"], [" and", "0.9137871"], [" a", "0.9999933"], [" single", "1.0"], [" snow", "0.99999964"], ["flake", "1.0"], [" cut", "0.9671039"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"]]}, "620": {"qid": "29a634ea52cba72c13e4", "term": "Brussels sprout", "description": "vegetable", "question": "Are Brussels sprout particularly good for adrenal fatigue?", "answer": true, "facts": ["Adenal fatigue is a disorder in which the body does not produce enough hormones and people get tired.", "Brussels sprout are foods rich in vitamin C.", "When stress levels rise, the adrenal glands require more Vitamin C and it is used very quickly."], "decomposition": ["What vitamins are found in abundance in Brussels sprouts?", "What vitamins do the adrenal glands require when a body is under stress?", "Is #2 found in #1?"], "evidence": [[[["Brussels sprout-12"]], [["Adrenaline-29"]], ["operation"]], [[["Brussels sprout-12"]], [["Adrenal gland-2"], "no_evidence"], ["no_evidence"]], [[["Brussels sprout-12"]], [["Adrenal fatigue-1", "Adrenal gland-2"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it true that the first man to walk on the moon was a woman?\nFacts: The first man to walk on the moon was a man named Neil Armstrong. He was the first man to walk on the moon on July 20, 1969.\nAnswer (True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are Brussels sprout particularly good for adrenal fatigue?\nFacts: Adenal fatigue is a disorder in which the body does not produce enough hormones and people get tired. Brussels sprout are foods rich in vitamin C. When stress levels rise, the adrenal glands require more Vitamin C and it is used very quickly.\nAnswer (True or False):", "ct_subject": "Adenal fatigue is a disorder in which the body does not produce enough hormones and people get tired. Brussels sprout are foods rich in vitamin C. When stress levels rise, the adrenal glands require more Vitamin C and it is used very quickly.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.306457"], [" it", "0.99999964"], [" true", "0.99999547"], [" that", "1.0"], [" the", "1.0"], [" first", "0.9972337"], [" man", "0.5678803"], [" to", "0.99873835"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" was", "1.0"], [" a", "1.0"], [" woman", "0.6540027"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.98975253"], [" first", "1.0"], [" man", "0.9413617"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" was", "1.0"], [" a", "1.0"], [" man", "0.8422762"], [" named", "0.34300464"], [" Neil", "1.0"], [" Armstrong", "1.0"], [".", "1.0"], [" He", "0.99886477"], [" was", "1.0"], [" the", "0.9999999"], [" first", "1.0"], [" man", "0.9999876"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" on", "0.98898494"], [" July", "1.0"], [" 20", "1.0"], [",", "1.0"], [" 1969", "1.0"], [".", "1.0"], ["\n", "0.9999994"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"]]}, "621": {"qid": "b331cd0701aa703e188b", "term": "Veto", "description": "legal power to unilaterally stop an official action, especially the enactment of legislation", "question": "Can the US branch of government that has power over the military also have the power to veto?", "answer": true, "facts": ["The US President is the commander in chief of the US military.", "The executive branch of the USA includes the President.", "The President has power to veto."], "decomposition": ["What US branch has power over the military?", "Who has the power to veto?", "Is #2 part of #1?"], "evidence": [[[["Article Two of the United States Constitution-1", "Article Two of the United States Constitution-4", "Executive (government)-5"]], [["Federal government of the United States-18", "Veto-1"]], ["operation"]], [[["Federal government of the United States-17"]], [["Federal government of the United States-17", "Federal government of the United States-18"]], [["Federal government of the United States-17"]]], [[["Federal jurisdiction (United States)-4"]], [["Veto-34"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"citizen\" and a \"citizen of the United States\"?\nFacts: A citizen is a person who is a US citizen. A citizen of the United States is a person who is a US citizen.\nAnswer (True or False): True\nQuestion: What is the difference between a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can the US branch of government that has power over the military also have the power to veto?\nFacts: The US President is the commander in chief of the US military. The executive branch of the USA includes the President. The President has power to veto.\nAnswer (True or False):", "ct_subject": "The US President is the commander in chief of the US military. The executive branch of the USA includes the President. The President has power to veto.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9866263"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "0.9998876"], [" \"", "1.0"], ["c", "0.93910897"], ["itizen", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["itizen", "1.0"], [" of", "1.0"], [" the", "1.0"], [" United", "0.9901428"], [" States", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" citizen", "0.7812133"], [" is", "0.99999905"], [" a", "1.0"], [" person", "1.0"], [" who", "1.0"], [" is", "0.9998933"], [" a", "1.0"], [" US", "1.0"], [" citizen", "1.0"], [".", "1.0"], [" A", "1.0"], [" citizen", "1.0"], [" of", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "1.0"], [" a", "1.0"], [" person", "1.0"], [" who", "1.0"], [" is", "1.0"], [" a", "1.0"], [" US", "1.0"], [" citizen", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"]]}, "622": {"qid": "d9e23006aef632b6d65f", "term": "Kane (wrestler)", "description": "American professional wrestler, actor, businessman, and politician", "question": "Have any murderers outlasted Kane's Royal Rumble record?", "answer": true, "facts": ["The longest Kane lasted in the Royal Rumble was 53:46 in 2001.", "Chris Benoit lasted over an hour in the 2004 Royal Rumble.", "Chris Benoit murdered his own wife and son in 2007."], "decomposition": ["What is Kane's Royal Rumble record?", "Which wrestlers have a Royal Rumble record longer than #1?", "Are any of the wrestlers listed in #2 a murderer?"], "evidence": [[[["Royal Rumble (2001)-13"], "no_evidence"], [["Royal Rumble (2004)-19"]], [["Chris Benoit-3"], "operation"]], [[["Royal Rumble (2014)-36"]], [["Royal Rumble match-30"]], [["Chris Benoit-3"]]], [[["Kane (wrestler)-3"]], [["Chris Benoit-2"], "no_evidence"], [["Chris Benoit-3"], "no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest match in WWE history?\nFacts: The longest match in WWE history was the 2000 Royal Rumble. The longest match in WWE history was the 2000 Royal Rumble. The longest match in WWE history was the 2000 Royal Rumble. The longest match in WWE history was the 2000 Royal Rumble. The longest match in", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Have any murderers outlasted Kane's Royal Rumble record?\nFacts: The longest Kane lasted in the Royal Rumble was 53:46 in 2001. Chris Benoit lasted over an hour in the 2004 Royal Rumble. Chris Benoit murdered his own wife and son in 2007.\nAnswer (True or False):", "ct_subject": "The longest Kane lasted in the Royal Rumble was 53:46 in 2001. Chris Benoit lasted over an hour in the 2004 Royal Rumble. Chris Benoit murdered his own wife and son in 2007.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99013734"], [" is", "0.99999905"], [" the", "1.0"], [" longest", "1.0"], [" match", "0.99859315"], [" in", "1.0"], [" WWE", "0.9987167"], [" history", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" match", "1.0"], [" in", "1.0"], [" WWE", "1.0"], [" history", "1.0"], [" was", "0.99839455"], [" the", "0.078598164"], [" 2000", "0.951565"], [" Royal", "1.0"], [" Rumble", "1.0"], [".", "0.9651129"], [" The", "0.9977787"], [" longest", "0.9999666"], [" match", "1.0"], [" in", "1.0"], [" WWE", "1.0"], [" history", "1.0"], [" was", "1.0"], [" the", "1.0"], [" 2000", "1.0"], [" Royal", "1.0"], [" Rumble", "1.0"], [".", "1.0"], [" The", "0.9994992"], [" longest", "1.0"], [" match", "1.0"], [" in", "1.0"], [" WWE", "1.0"], [" history", "1.0"], [" was", "1.0"], [" the", "1.0"], [" 2000", "1.0"], [" Royal", "1.0"], [" Rumble", "1.0"], [".", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" match", "1.0"], [" in", "1.0"], [" WWE", "1.0"], [" history", "1.0"], [" was", "1.0"], [" the", "1.0"], [" 2000", "1.0"], [" Royal", "1.0"], [" Rumble", "1.0"], [".", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" match", "1.0"], [" in", "1.0"]]}, "623": {"qid": "ff0c2df8c385ec5189dc", "term": "Sloth", "description": "tree dwelling animal noted for slowness", "question": "Could a sloth hypothetically watch an entire episode of Scrubs underwater?", "answer": true, "facts": ["Sloths can hold their breath underwater for up to 40 minutes.", "The running time of a Scrubs episode is between 20-23 minutes."], "decomposition": ["How long can sloths hold their breath underwater?", "How long is an episode of Scrubs?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Sloth-20"]], [["Scrubs (TV series)-69"]], ["operation"]], [[["Sloth-20"]], ["no_evidence"], ["no_evidence"]], [[["Sloth-20"]], ["no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance a human has ever walked?\nFacts: The longest distance a human has ever walked is about 2,000 miles.\nAnswer (True or False): True\nQuestion: What is the longest distance a human has ever walked?\nFacts: The longest distance a human has ever walked is about 2,000 miles.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could a sloth hypothetically watch an entire episode of Scrubs underwater?\nFacts: Sloths can hold their breath underwater for up to 40 minutes. The running time of a Scrubs episode is between 20-23 minutes.\nAnswer (True or False):", "ct_subject": "Sloths can hold their breath underwater for up to 40 minutes. The running time of a Scrubs episode is between 20-23 minutes.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9572084"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.99997616"], [" distance", "0.9988952"], [" a", "0.9999865"], [" human", "0.984335"], [" has", "0.98685026"], [" ever", "0.9999999"], [" walked", "0.99997807"], ["?", "0.99999547"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" human", "1.0"], [" has", "1.0"], [" ever", "1.0"], [" walked", "1.0"], [" is", "1.0"], [" about", "0.9998816"], [" 2", "0.96761954"], [",", "1.0"], ["000", "0.9999987"], [" miles", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" human", "1.0"], [" has", "1.0"], [" ever", "1.0"], [" walked", "0.8109761"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" human", "1.0"], [" has", "1.0"], [" ever", "1.0"], [" walked", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 2", "1.0"], [",", "1.0"], ["000", "1.0"], [" miles", "1.0"], [".", "1.0"]]}, "624": {"qid": "fa454eaf401bb6222bf3", "term": "Referee", "description": "person of authority, in a variety of sports, who is responsible for presiding over the game from a neutral point of view", "question": "Is the referee at a soccer match highly visible against the field?", "answer": true, "facts": ["Referees uniforms for soccer matches are usually bright neon colors.", "An alternative referee uniform color is black and white stripes."], "decomposition": ["What are the typically colors of a referee's uniform?", "Is #1 easy to see from afar?"], "evidence": [[[["Referee (association football)-6"]], ["operation"]], [[["Referee (association football)-6"]], ["no_evidence"]], [[["Kit (association football)-12"]], [["Black-3"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the year 2000?\nFacts: The year 2000 was the first year that the year 2000 was the first year that the year 2000 was the first year that the year 2000 was the first year that the year 2000 was the first year that the year 2000 was the first year that the year 2000 was the first year that the year 2000", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is the referee at a soccer match highly visible against the field?\nFacts: Referees uniforms for soccer matches are usually bright neon colors. An alternative referee uniform color is black and white stripes.\nAnswer (True or False):", "ct_subject": "Referees uniforms for soccer matches are usually bright neon colors. An alternative referee uniform color is black and white stripes.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9232866"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9471454"], [" of", "1.0"], [" the", "1.0"], [" first", "0.99945194"], [" person", "0.99878305"], [" to", "1.0"], [" be", "0.9987136"], [" born", "0.9989146"], [" in", "1.0"], [" the", "1.0"], [" year", "0.20838524"], [" 2000", "0.99999845"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" year", "0.99994576"], [" 2000", "1.0"], [" was", "0.99985397"], [" the", "1.0"], [" first", "0.5258558"], [" year", "1.0"], [" that", "0.9999993"], [" the", "0.9992182"], [" year", "1.0"], [" 2000", "0.65944856"], [" was", "1.0"], [" the", "0.9999877"], [" first", "1.0"], [" year", "1.0"], [" that", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "1.0"], [" the", "1.0"], [" first", "1.0"], [" year", "1.0"], [" that", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "1.0"], [" the", "1.0"], [" first", "1.0"], [" year", "1.0"], [" that", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "1.0"], [" the", "1.0"], [" first", "1.0"], [" year", "1.0"], [" that", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "1.0"], [" the", "1.0"], [" first", "1.0"], [" year", "1.0"], [" that", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "1.0"], [" the", "1.0"], [" first", "1.0"], [" year", "1.0"], [" that", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"]]}, "625": {"qid": "497c4922b0552d7c5693", "term": "Argon", "description": "Chemical element with atomic number 18", "question": "Is Argon near Neon on the periodic table of elements?", "answer": true, "facts": ["Argon is a noble gas.", "Neon is a noble gas. ", "The noble gases are all clumped together on the periodic table of elements."], "decomposition": ["What group of the periodic table is argon in?", "What group of the periodic table is neon in?", "Is #1 the same as #2?"], "evidence": [[[["Noble gas-1"]], [["Noble gas-1"]], ["operation"]], [[["Argon-1"]], [["Noble gas-1", "Noble gas-2"]], ["operation"]], [[["Argon-1"]], [["Neon-21"]], [["Group (periodic table)-5"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the Earth's magnetic field a result of the Earth's rotation?\nFacts: The Earth's magnetic field is a result of the Earth's rotation. The Earth's magnetic field is a result of the Earth's rotation. The Earth's magnetic field is a result of the Earth's rotation. The Earth's magnetic field is a result of the Earth's rotation. The Earth's", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Argon near Neon on the periodic table of elements?\nFacts: Argon is a noble gas. Neon is a noble gas.  The noble gases are all clumped together on the periodic table of elements.\nAnswer (True or False):", "ct_subject": "Argon is a noble gas. Neon is a noble gas.  The noble gases are all clumped together on the periodic table of elements.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999999"], [" the", "0.9999906"], [" Earth", "0.92513156"], ["'s", "0.9547939"], [" magnetic", "0.99686396"], [" field", "1.0"], [" a", "0.41498768"], [" result", "0.99999475"], [" of", "1.0"], [" the", "1.0"], [" Earth", "1.0"], ["'s", "1.0"], [" rotation", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Earth", "1.0"], ["'s", "1.0"], [" magnetic", "1.0"], [" field", "1.0"], [" is", "1.0"], [" a", "0.9394203"], [" result", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Earth", "1.0"], ["'s", "1.0"], [" rotation", "1.0"], [".", "1.0"], [" The", "0.80614275"], [" Earth", "1.0"], ["'s", "1.0"], [" magnetic", "0.9999994"], [" field", "1.0"], [" is", "1.0"], [" a", "0.85839987"], [" result", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Earth", "1.0"], ["'s", "1.0"], [" rotation", "1.0"], [".", "1.0"], [" The", "0.99511707"], [" Earth", "1.0"], ["'s", "1.0"], [" magnetic", "1.0"], [" field", "1.0"], [" is", "1.0"], [" a", "1.0"], [" result", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Earth", "1.0"], ["'s", "1.0"], [" rotation", "1.0"], [".", "1.0"], [" The", "1.0"], [" Earth", "1.0"], ["'s", "1.0"], [" magnetic", "1.0"], [" field", "1.0"], [" is", "1.0"], [" a", "1.0"], [" result", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Earth", "1.0"], ["'s", "1.0"], [" rotation", "1.0"], [".", "1.0"], [" The", "1.0"], [" Earth", "1.0"], ["'s", "1.0"]]}, "626": {"qid": "a11f537f67260464d010", "term": "Mitsubishi", "description": "group of autonomous, Japanese multinational companies", "question": "Can someone in Uberlandia work for Mitsubishi?", "answer": true, "facts": ["Mitsubishi is a Japanese auto manufacturer", "Mitsubishi operates a plant in Catalao, Brazil", "Uberlandia is just under 70 miles from Catalao"], "decomposition": ["How far is Uberlandia from Catalao?", "Is #1 within reasonable distance to commute to work?", "Is there a Mitsubishi organization in Catalao?", "Are #2 and #3 positive?"], "evidence": [[[["Catal\u00e3o-1", "Uberl\u00e2ndia-1"], "no_evidence"], ["operation"], ["no_evidence"], ["operation"]], [[["Catal\u00e3o-4"]], ["operation"], [["Catal\u00e3o-1"]], ["operation"]], [[["Catal\u00e3o-1", "Uberl\u00e2ndia-1"], "no_evidence"], ["no_evidence", "operation"], [["Catal\u00e3o-1"], "operation"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the closest to the Amazon?\nFacts: The closest city to the Amazon is called Manaus, Brazil.\nAnswer (True or False): True\nQuestion: What is the name of the city in the United States that is the closest to the Amazon?\nFacts: The closest city to", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can someone in Uberlandia work for Mitsubishi?\nFacts: Mitsubishi is a Japanese auto manufacturer Mitsubishi operates a plant in Catalao, Brazil Uberlandia is just under 70 miles from Catalao\nAnswer (True or False):", "ct_subject": "Mitsubishi is a Japanese auto manufacturer Mitsubishi operates a plant in Catalao, Brazil Uberlandia is just under 70 miles from Catalao", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9969236"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999168"], [" of", "1.0"], [" the", "1.0"], [" city", "0.99997365"], [" in", "0.9999964"], [" the", "0.99706274"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "0.9807585"], [" the", "0.9999999"], [" closest", "1.0"], [" to", "1.0"], [" the", "0.99565035"], [" Amazon", "0.879619"], ["?", "0.9999974"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" closest", "1.0"], [" city", "1.0"], [" to", "1.0"], [" the", "1.0"], [" Amazon", "1.0"], [" is", "1.0"], [" called", "0.9968362"], [" Man", "0.99994457"], ["aus", "1.0"], [",", "0.99986446"], [" Brazil", "1.0"], [".", "0.118085966"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "0.9999999"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" closest", "1.0"], [" to", "1.0"], [" the", "1.0"], [" Amazon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" closest", "1.0"], [" city", "1.0"], [" to", "1.0"]]}, "627": {"qid": "18d50392cc241326112b", "term": "Ronda Rousey", "description": "American professional wrestler, actress, author, mixed martial artist and judoka", "question": "Does Ronda Rousey avoid BBQ restaraunts?", "answer": true, "facts": ["Ronda Rousey is a professional athlete in MMA.", "Ronda Rousey is a vegan.", "BBQ is a style of restaurant that predominantly serves cooked meat.", "Meat consumption is opposed and avoided by vegans.", "Vegans don't eat meat."], "decomposition": ["What kind of food is served at BBQ restaurants?", "What dietary restrictions does Ronda Rousey follow?", "Would #2 avoid #1?"], "evidence": [[[["Ribs (food)-4"]], [["Ronda Rousey-73"]], [["Veganism-24"]]], [[["Barbecue restaurant-10"]], [["Ronda Rousey-73"]], [["Paleolithic diet-11"], "operation"]], [[["Barbecue-3"]], [["Ronda Rousey-73"]], [["Veganism-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the year 2000?\nFacts: The year 2000 was the year of the first human birth.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the year", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Ronda Rousey avoid BBQ restaraunts?\nFacts: Ronda Rousey is a professional athlete in MMA. Ronda Rousey is a vegan. BBQ is a style of restaurant that predominantly serves cooked meat. Meat consumption is opposed and avoided by vegans. Vegans don't eat meat.\nAnswer (True or False):", "ct_subject": "Ronda Rousey is a professional athlete in MMA. Ronda Rousey is a vegan. BBQ is a style of restaurant that predominantly serves cooked meat. Meat consumption is opposed and avoided by vegans. Vegans don't eat meat.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.62633526"], [" is", "1.0"], [" the", "1.0"], [" name", "0.90626127"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9995647"], [" person", "0.9725938"], [" to", "1.0"], [" be", "0.90698075"], [" born", "0.99999523"], [" in", "1.0"], [" the", "1.0"], [" year", "0.9999976"], [" 2000", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" year", "0.9999999"], [" 2000", "1.0"], [" was", "0.9834473"], [" the", "1.0"], [" year", "0.9999435"], [" of", "0.9893128"], [" the", "1.0"], [" first", "0.9999546"], [" human", "0.9999999"], [" birth", "1.0"], [".", "1.0"], ["\n", "0.8349312"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"]]}, "628": {"qid": "dd3cb87f907f758229ec", "term": "Polymath", "description": "Individual whose knowledge spans a significant number of subjects", "question": "Would Tony Stark be considered a polymath?", "answer": true, "facts": ["A polymath is a person who has knowledge in a wide variety of subjects.", "Tony Stark is considered a genius in mathematics, engineering, computer science, and physics, as well as demonstrating skills in metalworking, engine design, and genetics."], "decomposition": ["What does one have to have to be considered a polymath?", "Does Tony Stark have #1?"], "evidence": [[[["Polymath-1"]], [["Iron Man-2"], "operation"]], [[["Polymath-1"]], [["Tony Stark (Marvel Cinematic Universe)-1"]]], [[["Polymath-1"]], [["Iron Man-71"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the planet that the Avengers are fighting on?\nFacts: The planet that the Avengers are fighting on is called Earth.\nAnswer (True or False): True\nQuestion: What is the name of the superhero team that the Avengers are fighting on?\nFacts", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Tony Stark be considered a polymath?\nFacts: A polymath is a person who has knowledge in a wide variety of subjects. Tony Stark is considered a genius in mathematics, engineering, computer science, and physics, as well as demonstrating skills in metalworking, engine design, and genetics.\nAnswer (True or False):", "ct_subject": "A polymath is a person who has knowledge in a wide variety of subjects. Tony Stark is considered a genius in mathematics, engineering, computer science, and physics, as well as demonstrating skills in metalworking, engine design, and genetics.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999999"], [" is", "1.0"], [" the", "1.0"], [" name", "0.92062247"], [" of", "1.0"], [" the", "1.0"], [" planet", "0.25954872"], [" that", "0.9995844"], [" the", "0.99999225"], [" Avengers", "0.9723307"], [" are", "0.99999905"], [" fighting", "0.9999908"], [" on", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" planet", "1.0"], [" that", "0.968691"], [" the", "1.0"], [" Avengers", "1.0"], [" are", "1.0"], [" fighting", "1.0"], [" on", "1.0"], [" is", "1.0"], [" called", "0.9999931"], [" Earth", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" superhero", "0.4692048"], [" team", "1.0"], [" that", "1.0"], [" the", "1.0"], [" Avengers", "1.0"], [" are", "1.0"], [" fighting", "1.0"], [" on", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"]]}, "629": {"qid": "2c9f3c00412a4018380f", "term": "Hair", "description": "protein filament that grows from follicles found in the dermis, or skin", "question": "Can furniture be made of hair?", "answer": true, "facts": ["Hair is a protein filament that grows from living bodies.", "Hair is durable when woven together. ", "Furniture cushions can be maid from horse hair. "], "decomposition": ["What is hair?", "Can #1 be woven together securely?"], "evidence": [[[["Hair-2"]], ["no_evidence", "operation"]], [[["Hair-2"]], [["Alpha-keratin-4"], "operation"]], [[["Hair-2"]], [["Braid-2", "Cushion-1"], "no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a man and a woman?\nFacts: A man is a male human being. A woman is a female human being.\nAnswer (True or False): True\nQuestion: What is the difference between a man and a woman?\nFacts: A man is a male human being. A woman is a female human being.\nAnswer (True or", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can furniture be made of hair?\nFacts: Hair is a protein filament that grows from living bodies. Hair is durable when woven together.  Furniture cushions can be maid from horse hair. \nAnswer (True or False):", "ct_subject": "Hair is a protein filament that grows from living bodies. Hair is durable when woven together.  Furniture cushions can be maid from horse hair. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.91827404"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.88847053"], [" between", "1.0"], [" a", "1.0"], [" man", "0.9858068"], [" and", "1.0"], [" a", "1.0"], [" woman", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" man", "1.0"], [" is", "1.0"], [" a", "1.0"], [" male", "0.9999496"], [" human", "0.9999907"], [" being", "1.0"], [".", "1.0"], [" A", "1.0"], [" woman", "1.0"], [" is", "1.0"], [" a", "1.0"], [" female", "1.0"], [" human", "1.0"], [" being", "1.0"], [".", "1.0"], ["\n", "0.9999994"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" man", "1.0"], [" and", "1.0"], [" a", "1.0"], [" woman", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" man", "1.0"], [" is", "1.0"], [" a", "1.0"], [" male", "1.0"], [" human", "1.0"], [" being", "1.0"], [".", "1.0"], [" A", "1.0"], [" woman", "1.0"], [" is", "1.0"], [" a", "1.0"], [" female", "1.0"], [" human", "1.0"], [" being", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"]]}, "630": {"qid": "3a343ae8735fc81f0377", "term": "March", "description": "third month in the Julian and Gregorian calendars", "question": "Is March named after Jupiter's son in Roman mythology?", "answer": true, "facts": ["March is named after the Roman god Mars.", "Mars was the son of the Roman gods Jupiter and Juno."], "decomposition": ["Who are the sons of Jupiter in Roman mythology?", "Who is the month of March named after?", "Is #2 included in #1?"], "evidence": [[[["Hercules-1", "Mars (mythology)-1", "Vulcan (mythology)-41"]], [["March-1"]], ["operation"]], [[["Jupiter (mythology)-106"]], [["Apollo-25"], "no_evidence"], ["operation"]], [[["Mars (mythology)-7"]], [["Martius (month)-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the month of May named after the Roman god of agriculture?\nFacts: May is named after the Roman god of agriculture. May was the son of the Roman god Jupiter and Juno.\nAnswer (True or False): True\nQuestion: Is the name of the month of June named after the Roman god of agriculture?\nFacts: June is named after the Roman god of agriculture. June", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is March named after Jupiter's son in Roman mythology?\nFacts: March is named after the Roman god Mars. Mars was the son of the Roman gods Jupiter and Juno.\nAnswer (True or False):", "ct_subject": "March is named after the Roman god Mars. Mars was the son of the Roman gods Jupiter and Juno.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.8081028"], [" the", "0.99166095"], [" name", "0.9983034"], [" of", "1.0"], [" the", "1.0"], [" month", "0.99725074"], [" of", "1.0"], [" May", "0.642751"], [" named", "0.99990606"], [" after", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" god", "0.9999924"], [" of", "0.9999993"], [" agriculture", "1.0"], ["?", "0.9999999"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" May", "0.9999267"], [" is", "1.0"], [" named", "1.0"], [" after", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" god", "1.0"], [" of", "1.0"], [" agriculture", "1.0"], [".", "0.99579155"], [" May", "0.9888825"], [" was", "0.6916373"], [" the", "1.0"], [" son", "0.9595195"], [" of", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" god", "0.9999994"], [" Jupiter", "1.0"], [" and", "1.0"], [" Juno", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" name", "0.9999987"], [" of", "1.0"], [" the", "1.0"], [" month", "1.0"], [" of", "1.0"], [" June", "0.9999989"], [" named", "1.0"], [" after", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" god", "1.0"], [" of", "1.0"], [" agriculture", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" June", "1.0"], [" is", "1.0"], [" named", "1.0"], [" after", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" god", "1.0"], [" of", "1.0"], [" agriculture", "1.0"], [".", "1.0"], [" June", "1.0"]]}, "631": {"qid": "2bfe7f37f939ee456600", "term": "Pig Latin", "description": "secret language game", "question": "Is it impossible for pigs to use pig latin?", "answer": true, "facts": ["Pig latin is a language game played by rearranging parts of words to disguise them", "Pigs are ungulates and incapable of speech using human languages"], "decomposition": ["What is referred to as pig latin?", "Which species are capable of using #1?", "Are pigs excluded from #2?"], "evidence": [[[["Pig Latin-1"]], [["English language-1", "Human-1", "Language-15"]], ["operation"]], [[["Pig Latin-1"]], [["Language-1"]], ["operation"]], [[["Pig Latin-1"]], [["Great ape language-1", "Language-1"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it possible for a man to be a woman?\nFacts: It is possible for a man to be a woman.\nAnswer (True or False): True\nQuestion: Is it possible for a man to be a woman?\nFacts: It is possible for a man to be a woman.\nAnswer (True or False): True\nQuestion: Is it", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is it impossible for pigs to use pig latin?\nFacts: Pig latin is a language game played by rearranging parts of words to disguise them Pigs are ungulates and incapable of speech using human languages\nAnswer (True or False):", "ct_subject": "Pig latin is a language game played by rearranging parts of words to disguise them Pigs are ungulates and incapable of speech using human languages", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999974"], [" it", "1.0"], [" possible", "1.0"], [" for", "0.9907121"], [" a", "1.0"], [" man", "0.97622275"], [" to", "1.0"], [" be", "0.9849135"], [" a", "0.99946135"], [" woman", "0.9995098"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" It", "0.4024665"], [" is", "1.0"], [" possible", "1.0"], [" for", "1.0"], [" a", "1.0"], [" man", "1.0"], [" to", "1.0"], [" be", "1.0"], [" a", "1.0"], [" woman", "1.0"], [".", "0.99999976"], ["\n", "0.98921025"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" possible", "1.0"], [" for", "1.0"], [" a", "1.0"], [" man", "1.0"], [" to", "1.0"], [" be", "1.0"], [" a", "1.0"], [" woman", "1.0"], ["?", "0.9982901"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" It", "1.0"], [" is", "1.0"], [" possible", "1.0"], [" for", "1.0"], [" a", "1.0"], [" man", "1.0"], [" to", "1.0"], [" be", "1.0"], [" a", "1.0"], [" woman", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"]]}, "632": {"qid": "a7c6f840fb6a8be77625", "term": "J. D. Salinger", "description": "American writer", "question": "Is J.D. Salinger's most successful work influential to killers?", "answer": true, "facts": ["J.D. Salinger's most popular work was Catcher in the Rye.", "John Hinckley Jr. tried to assassinate Ronald Reagan after reading Catcher in the Rye.", "Mark David Chapman had a copy of Catcher in the Rye when he assassinated John Lennon.", "Robert John Bardo carried a copy of Catcher in the Rye when he murdered actress Rebecca Schaeffer."], "decomposition": ["Which of J.D. Salinger's novels was most popular?", "has #1 been associated with inspiring murder?"], "evidence": [[[["J. D. Salinger-1"]], [["The Catcher in the Rye-26"]]], [[["J. D. Salinger-1"]], [["The Catcher in the Rye-26"]]], [[["J. D. Salinger-1"]], [["The Catcher in the Rye-26"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the word \"cuckold\" a modern invention?\nFacts: The word \"cuckold\" is a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is J.D. Salinger's most successful work influential to killers?\nFacts: J.D. Salinger's most popular work was Catcher in the Rye. John Hinckley Jr. tried to assassinate Ronald Reagan after reading Catcher in the Rye. Mark David Chapman had a copy of Catcher in the Rye when he assassinated John Lennon. Robert John Bardo carried a copy of Catcher in the Rye when he murdered actress Rebecca Schaeffer.\nAnswer (True or False):", "ct_subject": "J.D. Salinger's most popular work was Catcher in the Rye. John Hinckley Jr. tried to assassinate Ronald Reagan after reading Catcher in the Rye. Mark David Chapman had a copy of Catcher in the Rye when he assassinated John Lennon. Robert John Bardo carried a copy of Catcher in the Rye when he murdered actress Rebecca Schaeffer.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999995"], [" the", "0.9999496"], [" word", "0.82019424"], [" \"", "1.0"], ["c", "0.9976472"], ["uck", "0.97647583"], ["old", "1.0"], ["\"", "1.0"], [" a", "0.9999994"], [" modern", "0.60135925"], [" invention", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99984586"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["uck", "1.0"], ["old", "1.0"], ["\"", "1.0"], [" is", "0.99990237"], [" a", "0.9999999"]]}, "633": {"qid": "b7c897e34556ecccbc47", "term": "Ice", "description": "water frozen into the solid state", "question": "Did Ice make people rich?", "answer": true, "facts": ["Trading ice was common in the 1800s.", "People created industries harvesting and selling ice.", "Some ice sellers became extremely rich. "], "decomposition": ["In the 1800's, what item was commonly traded?", "Did some people become rich off of selling #1?"], "evidence": [[[["Ice trade-1"]], [["Ice trade-10"]]], [[["Ice-48"], "no_evidence"], [["Ice-49"], "operation"]], [[["Ice trade-1"]], [["Ice trade-2"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the moon land on the Earth?\nFacts: The moon was not made of solid rock. It was made of a mixture of rock and dust. The moon was not made of solid rock. It was made of a mixture of rock and dust.\nAnswer (True or False): True\nQuestion: Did the sun rise and set on the same day?\nFacts: The sun rises and sets on the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Ice make people rich?\nFacts: Trading ice was common in the 1800s. People created industries harvesting and selling ice. Some ice sellers became extremely rich. \nAnswer (True or False):", "ct_subject": "Trading ice was common in the 1800s. People created industries harvesting and selling ice. Some ice sellers became extremely rich. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999399"], [" the", "1.0"], [" moon", "0.982249"], [" land", "1.0"], [" on", "1.0"], [" the", "1.0"], [" Earth", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" moon", "1.0"], [" was", "0.6664164"], [" not", "0.9988681"], [" made", "0.5122947"], [" of", "1.0"], [" solid", "0.99891734"], [" rock", "1.0"], [".", "1.0"], [" It", "0.99999976"], [" was", "1.0"], [" made", "0.9999994"], [" of", "1.0"], [" a", "0.9996327"], [" mixture", "1.0"], [" of", "1.0"], [" rock", "0.9988595"], [" and", "0.99999857"], [" dust", "1.0"], [".", "1.0"], [" The", "0.99969316"], [" moon", "0.99999344"], [" was", "0.9999851"], [" not", "1.0"], [" made", "0.98100084"], [" of", "1.0"], [" solid", "1.0"], [" rock", "1.0"], [".", "1.0"], [" It", "1.0"], [" was", "1.0"], [" made", "1.0"], [" of", "1.0"], [" a", "1.0"], [" mixture", "1.0"], [" of", "1.0"], [" rock", "1.0"], [" and", "1.0"], [" dust", "1.0"], [".", "1.0"], ["\n", "0.9969477"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" sun", "0.9796896"], [" rise", "0.9999753"], [" and", "0.99996173"], [" set", "1.0"], [" on", "0.8376588"], [" the", "1.0"], [" same", "1.0"], [" day", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" sun", "1.0"], [" rises", "0.98649824"], [" and", "1.0"], [" sets", "1.0"], [" on", "1.0"], [" the", "0.9999999"]]}, "634": {"qid": "05cce7e36ecb95166a28", "term": "Justin Bieber", "description": "Canadian singer-songwriter and actor", "question": "Does Justin Bieber vote in October?", "answer": true, "facts": ["Justin Bieber is a Canadian citizen", "Canadian elections are held on the third Monday in October"], "decomposition": ["What country is Justin Bieber a citizen of?", "When does #1 hold its national elections?", "Is #2 October?"], "evidence": [[[["Justin Bieber-1", "Justin Bieber-51"]], [["Fixed election dates in Canada-13"]], ["operation"]], [[["Justin Bieber-1"]], [["Elections in Canada-23"]], ["operation"]], [[["Justin Bieber-1"]], [["Elections in Canada-13"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: The first person to be executed in the United States was John Brown, who was hanged on the first Monday in May, 1859.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: The first person to be executed in the United States was John Brown, who", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Justin Bieber vote in October?\nFacts: Justin Bieber is a Canadian citizen Canadian elections are held on the third Monday in October\nAnswer (True or False):", "ct_subject": "Justin Bieber is a Canadian citizen Canadian elections are held on the third Monday in October", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999607"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99999845"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9631609"], [" person", "0.8272871"], [" to", "1.0"], [" be", "0.93582875"], [" executed", "0.8206765"], [" in", "0.99999976"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.8948081"], [" first", "1.0"], [" person", "1.0"], [" to", "0.9969511"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" John", "0.9999969"], [" Brown", "0.9999589"], [",", "0.84287524"], [" who", "1.0"], [" was", "1.0"], [" hanged", "1.0"], [" on", "0.73903716"], [" the", "0.9151572"], [" first", "0.67902356"], [" Monday", "1.0"], [" in", "1.0"], [" May", "0.16976315"], [",", "1.0"], [" 18", "1.0"], ["59", "1.0"], [".", "0.999897"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999738"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" John", "1.0"], [" Brown", "1.0"], [",", "1.0"], [" who", "1.0"]]}, "635": {"qid": "a60c5a3a8bd811b26208", "term": "Flying fish", "description": "Family of marine fish that can make powerful, self-propelled leaps out of water", "question": "Do flying fish have good eyesight?", "answer": true, "facts": ["Flying fish  are commonly found in the epipelagic zone, the top layer of the ocean to a depth of about 200 m (656 ft). ", "The epipelagic zone is the illuminated zone at the surface of the sea where enough light is available for photosynthesis. ", "Good eyesight is a necessary survival trait for animals living in well-lit areas."], "decomposition": ["Which layer of the ocean are flying fish usually found?", "What are the lighting conditions characteristic of #1?", "Would good eyesight be necessary for organisms in #2 environment?"], "evidence": [[[["Flying fish-6"]], [["Photic zone-1"]], [["Photic zone-3"], "no_evidence"]], [[["Flying fish-6"]], [["Photic zone-1"]], [["Photic zone-1"], "operation"]], [[["Flying fish-6"]], [["Photic zone-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance a human being has ever traveled?\nFacts: The longest distance a human being has ever traveled is about 1,000 miles (1,600 km", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do flying fish have good eyesight?\nFacts: Flying fish are commonly found in the epipelagic zone, the top layer of the ocean to a depth of about 200 m (656 ft).  The epipelagic zone is the illuminated zone at the surface of the sea where enough light is available for photosynthesis.  Good eyesight is a necessary survival trait for animals living in well-lit areas.\nAnswer (True or False):", "ct_subject": "Flying fish are commonly found in the epipelagic zone, the top layer of the ocean to a depth of about 200 m (656 ft).  The epipelagic zone is the illuminated zone at the surface of the sea where enough light is available for photosynthesis.  Good eyesight is a necessary survival trait for animals living in well-lit areas.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9997459"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.9978933"], [" distance", "1.0"], [" a", "0.99996746"], [" human", "0.9466354"], [" being", "0.6190344"], [" has", "1.0"], [" ever", "0.9999999"], [" traveled", "1.0"], ["?", "0.99999857"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "1.0"], [" has", "1.0"], [" ever", "1.0"], [" traveled", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 1", "0.99886346"], [",", "0.99999404"], ["000", "0.9999974"], [" miles", "0.99951005"], [" (", "1.0"], ["1", "1.0"], [",", "1.0"], ["600", "0.9948131"], [" km", "1.0"]]}, "636": {"qid": "26aefc40b2d04ca6b78b", "term": "Xenophobia", "description": "dislike of that which is perceived to be foreign or strange", "question": "Is xenophobia hypothetically unimportant between Saladin and Ali Askari?", "answer": true, "facts": ["Xenophobia is the dislike of someone that is foreign or from a different background.", "Saladin was a Kurdish leader that became sultan of Egypt.", "Ali Askari was a Kurdish politician."], "decomposition": ["Which relation between two parties could lead bring about xenophobia?", "What was Saladin's ethnicity?", "What was Ali Askari's ethnicity?", "Does the relation between #2 and #3 fail to describe #1?"], "evidence": [[[["Xenophobia-1"]], [["Saladin-1"]], [["Ali Askari-3"]], ["operation"]], [[["Xenophobia-1"]], [["Saladin-1"]], [["Ali Askari-2"]], ["operation"]], [[["Xenophobia-1"]], [["Saladin-1"]], [["Ali Askari-3"]], [["In-group and out-group-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city of Jerusalem the same as the name of the city of Jerusalem in the Bible?\nFacts: Jerusalem is the name of the city in the Bible.\nAnswer (True or False): True\nQuestion: Is the name of the city of Jerusalem the same as the name of the city", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is xenophobia hypothetically unimportant between Saladin and Ali Askari?\nFacts: Xenophobia is the dislike of someone that is foreign or from a different background. Saladin was a Kurdish leader that became sultan of Egypt. Ali Askari was a Kurdish politician.\nAnswer (True or False):", "ct_subject": "Xenophobia is the dislike of someone that is foreign or from a different background. Saladin was a Kurdish leader that became sultan of Egypt. Ali Askari was a Kurdish politician.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99985945"], [" the", "0.8823949"], [" name", "0.7395487"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9999969"], [" of", "1.0"], [" Jerusalem", "0.63636047"], [" the", "0.9813338"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Jerusalem", "0.9686802"], [" in", "0.9981629"], [" the", "1.0"], [" Bible", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Jerusalem", "0.9639325"], [" is", "0.99999976"], [" the", "1.0"], [" name", "0.9998877"], [" of", "1.0"], [" the", "0.99999905"], [" city", "1.0"], [" in", "0.99999905"], [" the", "1.0"], [" Bible", "1.0"], [".", "1.0"], ["\n", "0.8829432"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Jerusalem", "1.0"], [" the", "1.0"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"]]}, "637": {"qid": "38a0f2619ec98ba7fbd5", "term": "The Young and the Restless", "description": "television series", "question": "Would a binge watch of entire Young and the Restless take longer than a leap year?", "answer": true, "facts": ["A leap year has 366 total days.", "As of March 19th, 2018, every episode of the Young and the Restless would take 467 days and 2 hours to watch."], "decomposition": ["How many days are in a leap year?", "How long would it take to watch every episode of the Young and the Restless?", "Is #2 greater than #1?"], "evidence": [[[["Leap year-2"]], [["The Young and the Restless-1", "The Young and the Restless-12"], "no_evidence"], ["operation"]], [[["Leap year-1", "Leap year-6"]], [["The Young and the Restless-1", "The Young and the Restless-12"], "no_evidence"], ["operation"]], [[["Leap year-2"]], [["The Young and the Restless-1"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest time a person can be in a coma?\nFacts: A coma is defined as a state of unconsciousness lasting more than 24 hours. A person can be in a coma for up to a year.\nAnswer (True or False): True\nQuestion: What is the longest time", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a binge watch of entire Young and the Restless take longer than a leap year?\nFacts: A leap year has 366 total days. As of March 19th, 2018, every episode of the Young and the Restless would take 467 days and 2 hours to watch.\nAnswer (True or False):", "ct_subject": "A leap year has 366 total days. As of March 19th, 2018, every episode of the Young and the Restless would take 467 days and 2 hours to watch.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999976"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.9999999"], [" time", "0.9572262"], [" a", "0.9699195"], [" person", "1.0"], [" can", "0.9996933"], [" be", "0.9991185"], [" in", "1.0"], [" a", "1.0"], [" coma", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.99929976"], [" coma", "0.99999344"], [" is", "0.99978167"], [" defined", "0.91736394"], [" as", "1.0"], [" a", "1.0"], [" state", "0.99999905"], [" of", "1.0"], [" unconscious", "1.0"], ["ness", "1.0"], [" lasting", "0.84577066"], [" more", "1.0"], [" than", "1.0"], [" 24", "1.0"], [" hours", "1.0"], [".", "1.0"], [" A", "0.9735665"], [" person", "1.0"], [" can", "1.0"], [" be", "1.0"], [" in", "1.0"], [" a", "1.0"], [" coma", "1.0"], [" for", "1.0"], [" up", "0.9422329"], [" to", "1.0"], [" a", "0.8261612"], [" year", "0.99985874"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" time", "1.0"]]}, "638": {"qid": "fb0aef4d6278a1623abc", "term": "Preventive healthcare", "description": "Prevent and minimize the occurrence of diseases", "question": "Do you need to schedule separate preventive healthcare and sickness visits? ", "answer": true, "facts": ["Preventive healthcare options are typically covered at no charge by health insurance.", "Sick visits to the doctor are billed separately from preventive healthcare visits.", "Sick visits and preventive healthcare visits are generally given different time allotments. "], "decomposition": ["How are preventive healthcare visits billed to insurance companies?", "How are sick visits to the doctor billed to insurance companies?", "Is #1 different from #2?"], "evidence": [[[["Preventive healthcare-60"], "no_evidence"], ["no_evidence"], ["operation"]], [[["Health care-11"], "no_evidence"], [["Health care-15"], "no_evidence"], ["operation"]], [[["Preventive healthcare-54"], "no_evidence"], [["Managed care-38"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Do you need to schedule separate preventive healthcare and sickness visits?\nFacts: Preventive healthcare options are typically covered at no charge by health insurance. Sick visits to the doctor are billed separately from preventive healthcare visits. Sick visits and preventive healthcare visits are generally given different time allotments. \nAnswer (", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do you need to schedule separate preventive healthcare and sickness visits? \nFacts: Preventive healthcare options are typically covered at no charge by health insurance. Sick visits to the doctor are billed separately from preventive healthcare visits. Sick visits and preventive healthcare visits are generally given different time allotments. \nAnswer (True or False):", "ct_subject": "Preventive healthcare options are typically covered at no charge by health insurance. Sick visits to the doctor are billed separately from preventive healthcare visits. Sick visits and preventive healthcare visits are generally given different time allotments. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Do", "0.9992094"], [" you", "1.0"], [" need", "1.0"], [" to", "1.0"], [" schedule", "1.0"], [" separate", "1.0"], [" preventive", "1.0"], [" healthcare", "1.0"], [" and", "1.0"], [" sickness", "1.0"], [" visits", "1.0"], ["?", "1.0"], ["\n", "0.99995875"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Prevent", "1.0"], ["ive", "1.0"], [" healthcare", "1.0"], [" options", "1.0"], [" are", "1.0"], [" typically", "1.0"], [" covered", "1.0"], [" at", "1.0"], [" no", "1.0"], [" charge", "1.0"], [" by", "1.0"], [" health", "1.0"], [" insurance", "1.0"], [".", "1.0"], [" Sick", "1.0"], [" visits", "1.0"], [" to", "1.0"], [" the", "1.0"], [" doctor", "1.0"], [" are", "1.0"], [" billed", "1.0"], [" separately", "1.0"], [" from", "1.0"], [" preventive", "1.0"], [" healthcare", "1.0"], [" visits", "1.0"], [".", "1.0"], [" Sick", "1.0"], [" visits", "1.0"], [" and", "1.0"], [" preventive", "1.0"], [" healthcare", "1.0"], [" visits", "1.0"], [" are", "1.0"], [" generally", "1.0"], [" given", "1.0"], [" different", "1.0"], [" time", "1.0"], [" allot", "1.0"], ["ments", "1.0"], [".", "1.0"], [" ", "0.9999739"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"]]}, "639": {"qid": "811f06c3d045430a5359", "term": "Black", "description": "The darkest shade, resulting from the absence or complete absorption of light. Like white and grey, it has no hue", "question": "Is Anakin Skywalker from Star Wars associated with the color black?", "answer": true, "facts": ["As a Jedi during the Clone Wars, Anakin Skywalker often wore black robes.", "After he was burned and transformed into the cyborg Darth Vader, he received a distinctive and famous all-black outfit including a black mask."], "decomposition": ["What is the color of most outfits worn by Star Wars' Anakin Skywalker?", "Is #1 the same as black?"], "evidence": [[[["Darth Vader-1", "Darth Vader-15"]], ["operation"]], [[["Darth Vader-15"]], ["operation"]], [[["Darth Vader-1", "Darth Vader-15"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the planet in Star Wars: Episode IV A New Hope \"Yavin 4\"?\nFacts: The planet Yavin 4 is named after the planet Yavin, the site of the first battle of the Clone Wars.\nAnswer (True or False): True\nQuestion:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Anakin Skywalker from Star Wars associated with the color black?\nFacts: As a Jedi during the Clone Wars, Anakin Skywalker often wore black robes. After he was burned and transformed into the cyborg Darth Vader, he received a distinctive and famous all-black outfit including a black mask.\nAnswer (True or False):", "ct_subject": "As a Jedi during the Clone Wars, Anakin Skywalker often wore black robes. After he was burned and transformed into the cyborg Darth Vader, he received a distinctive and famous all-black outfit including a black mask.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" name", "0.99997437"], [" of", "0.8958634"], [" the", "1.0"], [" planet", "1.0"], [" in", "0.98975414"], [" Star", "0.58918834"], [" Wars", "1.0"], [":", "0.9095697"], [" Episode", "1.0"], [" IV", "0.9999993"], [" A", "0.9999999"], [" New", "1.0"], [" Hope", "1.0"], [" \"", "0.9991309"], ["Y", "0.96576416"], ["avin", "1.0"], [" 4", "0.99999344"], ["\"?", "0.59097105"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.76385194"], [" planet", "1.0"], [" Y", "0.9998073"], ["avin", "1.0"], [" 4", "1.0"], [" is", "0.9999472"], [" named", "0.9982135"], [" after", "1.0"], [" the", "1.0"], [" planet", "1.0"], [" Y", "1.0"], ["avin", "1.0"], [",", "1.0"], [" the", "0.3924423"], [" site", "0.20565836"], [" of", "1.0"], [" the", "1.0"], [" first", "0.99945253"], [" battle", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Clone", "0.6752404"], [" Wars", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"]]}, "640": {"qid": "3848c0621fc2a9d1c79f", "term": "Viscosity", "description": "Resistance of a fluid to shear deformation", "question": "Do people with swallowing disorders need high viscosity drinks?", "answer": true, "facts": ["Swallowing disorders can make thin liquids like water dangerous to drink.", "Liquid thickeners are marketed towards people with difficulty drinking."], "decomposition": ["If a person has a swallowing disorder, what types of liquids are dangerous for them to drink?", "Are high viscosity drinks the opposite of #1?"], "evidence": [[[["Thickened fluids-1"]], [["Viscosity-1"], "operation"]], [[["Dysphagia-2"], "no_evidence"], [["Viscosity-1"], "operation"]], [[["Oropharyngeal dysphagia-2"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Do people with high blood pressure need to take blood thinners?\nFacts: High blood pressure is a risk factor for heart disease. People with high blood pressure should not take blood thinners.\nAnswer (True or False): True\nQuestion: Do people with high cholesterol need to take blood thinners?\nFacts: People with high cholesterol should not take blood thinners.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do people with swallowing disorders need high viscosity drinks?\nFacts: Swallowing disorders can make thin liquids like water dangerous to drink. Liquid thickeners are marketed towards people with difficulty drinking.\nAnswer (True or False):", "ct_subject": "Swallowing disorders can make thin liquids like water dangerous to drink. Liquid thickeners are marketed towards people with difficulty drinking.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Do", "0.9936912"], [" people", "0.99999666"], [" with", "1.0"], [" high", "0.68296224"], [" blood", "1.0"], [" pressure", "1.0"], [" need", "1.0"], [" to", "1.0"], [" take", "0.99994445"], [" blood", "0.99999547"], [" thin", "0.8077688"], ["ners", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" High", "0.2530635"], [" blood", "1.0"], [" pressure", "1.0"], [" is", "0.99999964"], [" a", "0.99999595"], [" risk", "0.9891234"], [" factor", "1.0"], [" for", "1.0"], [" heart", "0.99879515"], [" disease", "0.9999635"], [".", "1.0"], [" People", "0.9750761"], [" with", "1.0"], [" high", "1.0"], [" blood", "1.0"], [" pressure", "1.0"], [" should", "0.99055326"], [" not", "0.9969296"], [" take", "1.0"], [" blood", "1.0"], [" thin", "1.0"], ["ners", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Do", "1.0"], [" people", "1.0"], [" with", "1.0"], [" high", "0.9154269"], [" cholesterol", "1.0"], [" need", "1.0"], [" to", "1.0"], [" take", "1.0"], [" blood", "1.0"], [" thin", "1.0"], ["ners", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" People", "0.99999666"], [" with", "1.0"], [" high", "1.0"], [" cholesterol", "1.0"], [" should", "1.0"], [" not", "1.0"], [" take", "1.0"], [" blood", "1.0"], [" thin", "1.0"], ["ners", "1.0"], [".", "1.0"]]}, "641": {"qid": "83e854a290df701ecb8d", "term": "Mail carrier", "description": "employee of the post office or postal service, who delivers mail to residences and businesses", "question": "Was being a mail carrier considered one of the most dangerous jobs?", "answer": true, "facts": ["The Pony Express was one of the first mail carrier services.", "The Pony Express operated form 1860 to 1861", "Pony Express riders would have to travel hundreds of miles on horse back through extreme weather and terrain. ", "The Pony Express sought to hire young expert horse riders willing to risk death."], "decomposition": ["What was the name of the mail carrier service that operated from 1860 to 1861?", "What would riders on #1 have to endure?", "Is #2 considered very dangerous?"], "evidence": [[[["Pony Express-1"]], [["Pony Express-31"]], ["operation"]], [[["Pony Express-1"]], [["Pony Express-38", "Pony Express-40"], "no_evidence"], ["operation"]], [[["Pony Express-1"]], [["Pony Express-30", "Pony Express-31"]], [["Pony Express-30", "Pony Express-31"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first American president?\nFacts: George Washington was the first president of the United States.\nAnswer (True or False): True\nQuestion: What is the name of the first American president?\nF", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was being a mail carrier considered one of the most dangerous jobs?\nFacts: The Pony Express was one of the first mail carrier services. The Pony Express operated form 1860 to 1861 Pony Express riders would have to travel hundreds of miles on horse back through extreme weather and terrain.  The Pony Express sought to hire young expert horse riders willing to risk death.\nAnswer (True or False):", "ct_subject": "The Pony Express was one of the first mail carrier services. The Pony Express operated form 1860 to 1861 Pony Express riders would have to travel hundreds of miles on horse back through extreme weather and terrain.  The Pony Express sought to hire young expert horse riders willing to risk death.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99866915"], [" is", "1.0"], [" the", "1.0"], [" name", "0.24790065"], [" of", "1.0"], [" the", "1.0"], [" first", "0.98491144"], [" American", "0.2636137"], [" president", "0.89057016"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" George", "0.99988747"], [" Washington", "1.0"], [" was", "0.73701334"], [" the", "1.0"], [" first", "1.0"], [" president", "0.99999976"], [" of", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [".", "1.0"], ["\n", "0.999998"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" American", "1.0"], [" president", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"]]}, "642": {"qid": "2cb0bc060c5fb708a43f", "term": "Leopard seal", "description": "Species of mammal", "question": "Is Sea World hazardous to leopard seal's health?", "answer": true, "facts": ["Leopard seals have only one natural predator, the killer whale.", "Sea World is an aquatic show that involves many water animals.", "Killer Whales, such as Tilikum, are headliners at Sea World."], "decomposition": ["What is the leopard seals's predator?", "Would one find a #1 at Sea World?"], "evidence": [[[["Leopard seal-1"]], [["Kamogawa Sea World-15"]]], [[["Leopard seal-1"]], [["Shamu-1"], "operation"]], [[["Killer whale-7", "Leopard seal-1"]], [["SeaWorld-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city of New York City derived from the name of the city of New York?\nFacts: The name of the city of New York City is derived from the name of the city of New York.\nAnswer (True or False): True\nQuestion: Is the name of the city of", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Sea World hazardous to leopard seal's health?\nFacts: Leopard seals have only one natural predator, the killer whale. Sea World is an aquatic show that involves many water animals. Killer Whales, such as Tilikum, are headliners at Sea World.\nAnswer (True or False):", "ct_subject": "Leopard seals have only one natural predator, the killer whale. Sea World is an aquatic show that involves many water animals. Killer Whales, such as Tilikum, are headliners at Sea World.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99999964"], [" the", "0.9666604"], [" name", "0.94418716"], [" of", "0.9999573"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "0.9977239"], [" York", "1.0"], [" City", "1.0"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" name", "0.99999535"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999993"], [" name", "1.0"], [" of", "1.0"], [" the", "0.9537438"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" City", "0.8825733"], [" is", "0.9999857"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [".", "1.0"], ["\n", "0.26243404"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"]]}, "643": {"qid": "812774b35638d154d0d1", "term": "Compact disc", "description": "Optical disc for storage and playback of digital audio", "question": "Would a compact disc melt in magma?", "answer": true, "facts": ["Magma is the molten material beneath the earth's surface.", "The temperature of magma ranges between 1300F and 2400F degrees.", "A compact disc is made of polycarbonate plastic.", "Polycarbonate plastic melts at 311F degrees."], "decomposition": ["What material is a compact disc made of?", "At what temperature does #1 melt", "What is the typical temperature range of magma?", "Is #2 less than or within #3"], "evidence": [[[["Compact disc-23"]], [["Polycarbonate-25"], "no_evidence"], [["Magma-6"]], ["no_evidence", "operation"]], [[["CD-ROM-4"]], [["Plastic-19"]], [["Magma-6"]], ["operation"]], [[["CD-R-7"]], [["Polycarbonate-10"]], [["Magma-6"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human?\nFacts: The average human weighs about 100 pounds.\nAnswer (True or False): True\nQuestion: What is the average weight of a cow?\nFacts: The average cow weighs about 1,000 pounds.\nAnswer (True or", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a compact disc melt in magma?\nFacts: Magma is the molten material beneath the earth's surface. The temperature of magma ranges between 1300F and 2400F degrees. A compact disc is made of polycarbonate plastic. Polycarbonate plastic melts at 311F degrees.\nAnswer (True or False):", "ct_subject": "Magma is the molten material beneath the earth's surface. The temperature of magma ranges between 1300F and 2400F degrees. A compact disc is made of polycarbonate plastic. Polycarbonate plastic melts at 311F degrees.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9997415"], [" is", "1.0"], [" the", "1.0"], [" average", "0.95324636"], [" weight", "0.99996984"], [" of", "1.0"], [" a", "0.9999999"], [" human", "1.0"], ["?", "0.97883576"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.5700995"], [" weighs", "0.63255733"], [" about", "0.9997371"], [" 100", "0.76921386"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "0.9999999"], [" of", "1.0"], [" a", "1.0"], [" cow", "0.76659596"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" cow", "1.0"], [" weighs", "1.0"], [" about", "1.0"], [" 1", "0.99964833"], [",", "1.0"], ["000", "0.9999342"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"]]}, "644": {"qid": "3bba311f7abb8ad4be82", "term": "Monarch", "description": "Person at the head of a monarchy", "question": "Does Canada have a relationship with a monarch?", "answer": true, "facts": ["Canada is a constitutional monarchy.", "The head of the monarchy that rules Canada is Queen Elizabeth."], "decomposition": ["What system of government does Canada follow?", "Who is the head of #1?", "Is #2 a monarch?"], "evidence": [[[["Government of Canada-6"]], ["no_evidence"], [["Records of heads of state-7"]]], [[["By the Grace of God-10", "Constitutional monarchy-1"], "no_evidence"], [["Monarchy-1"]], ["operation"]], [[["Government of Canada-1"]], [["Government of Canada-1"]], [["Government of Canada-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the country that is the largest land mass in the world?\nFacts: The United States of America.\nAnswer (True or False): True\nQuestion: What is the name of the country that is the largest land mass in the world?\nFacts: The United States of America.\nAnswer (True or False): True\nQuestion: What is the name of the country that is the largest land mass", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Canada have a relationship with a monarch?\nFacts: Canada is a constitutional monarchy. The head of the monarchy that rules Canada is Queen Elizabeth.\nAnswer (True or False):", "ct_subject": "Canada is a constitutional monarchy. The head of the monarchy that rules Canada is Queen Elizabeth.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99743134"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" country", "0.99989414"], [" that", "1.0"], [" is", "0.99988925"], [" the", "1.0"], [" largest", "0.9998097"], [" land", "0.010794652"], [" mass", "1.0"], [" in", "0.9999825"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "0.99999535"], [" States", "1.0"], [" of", "0.9999988"], [" America", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" country", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" land", "1.0"], [" mass", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "1.0"], [" States", "1.0"], [" of", "1.0"], [" America", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" country", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" land", "1.0"], [" mass", "1.0"]]}, "645": {"qid": "9b00b215a02a7e4a8aa1", "term": "Edmund Hillary", "description": "New Zealand mountaineer", "question": "Would Mount Wycheproof be a breeze for Edmund Hillary?", "answer": true, "facts": ["Edmund Hillary was a mountaineer that climbed Mount Everest.", "Mount Everest reaches 29,029 feet in the air.", "Mount Wycheproof is the smallest mountain in the world.", "Mount Wycheproof rises a mere 486 feet above sea level."], "decomposition": ["Which famous mountain has Edmund Hillary climbed?", "How tall is #1?", "How tall is Mount Wycheproof?", "Is #3 several thousand feet smaller than #2?"], "evidence": [[[["Edmund Hillary-1"]], [["Mount Everest-2"]], [["Mount Wycheproof-1"]], ["operation"]], [[["Edmund Hillary-1"]], [["Mount Everest-2"]], [["Mount Wycheproof-1"]], ["operation"]], [[["Edmund Hillary-1"]], [["Mount Everest-2"]], [["Mount Wycheproof-3"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the tallest mountain in the world?\nFacts: Mount Everest is the tallest mountain in the world. Mount Kilimanjaro is the tallest mountain in Africa.\nAnswer (True or False): True\nQuestion: What is the tallest mountain in the world?", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Mount Wycheproof be a breeze for Edmund Hillary?\nFacts: Edmund Hillary was a mountaineer that climbed Mount Everest. Mount Everest reaches 29,029 feet in the air. Mount Wycheproof is the smallest mountain in the world. Mount Wycheproof rises a mere 486 feet above sea level.\nAnswer (True or False):", "ct_subject": "Edmund Hillary was a mountaineer that climbed Mount Everest. Mount Everest reaches 29,029 feet in the air. Mount Wycheproof is the smallest mountain in the world. Mount Wycheproof rises a mere 486 feet above sea level.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999974"], [" is", "1.0"], [" the", "1.0"], [" tallest", "0.9999875"], [" mountain", "0.9929623"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Mount", "0.9999641"], [" Everest", "1.0"], [" is", "1.0"], [" the", "1.0"], [" tallest", "1.0"], [" mountain", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [".", "1.0"], [" Mount", "0.9999982"], [" Kil", "0.99999917"], ["iman", "1.0"], ["jar", "1.0"], ["o", "1.0"], [" is", "1.0"], [" the", "1.0"], [" tallest", "0.9999784"], [" mountain", "1.0"], [" in", "1.0"], [" Africa", "1.0"], [".", "1.0"], ["\n", "0.9992372"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" tallest", "1.0"], [" mountain", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "0.9836986"], ["?", "1.0"]]}, "646": {"qid": "1ffe6c11a37f8e4e2542", "term": "Grey seal", "description": "species of seal", "question": "Can a grey seal swim in the same water as the subject of Moby Dick?", "answer": true, "facts": ["The range of gray seals is limited to parts of the northern hemisphere bordered by the Atlantic ocean", "The subject of Moby Dick was a sperm whale", "Sperm whales can be found in the north Atlantic, in addition to most other bodies of water on earth."], "decomposition": ["What kind of whale was Moby Dick?", "What is the range of #1?", "What is the range of gray seals?", "Is there an overlap between #2 and #3?"], "evidence": [[[["Moby-Dick-1"]], [["Sperm whale-2"], "no_evidence"], [["Grey seal-1"]], ["no_evidence", "operation"]], [[["Moby-Dick-1"]], [["Sperm whale-2"]], [["Grey seal-1"]], ["operation"]], [[["Moby-Dick-1"]], [["Sperm whale-2"]], [["Grey seal-7"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance between two points on the earth?\nFacts: The shortest distance between two points on the earth is the equator. The longest distance between two points on the earth is the equator. The shortest distance between", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can a grey seal swim in the same water as the subject of Moby Dick?\nFacts: The range of gray seals is limited to parts of the northern hemisphere bordered by the Atlantic ocean The subject of Moby Dick was a sperm whale Sperm whales can be found in the north Atlantic, in addition to most other bodies of water on earth.\nAnswer (True or False):", "ct_subject": "The range of gray seals is limited to parts of the northern hemisphere bordered by the Atlantic ocean The subject of Moby Dick was a sperm whale Sperm whales can be found in the north Atlantic, in addition to most other bodies of water on earth.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99775404"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.9994068"], [" distance", "0.9999958"], [" between", "0.8930459"], [" two", "1.0"], [" points", "1.0"], [" on", "0.9999981"], [" the", "0.50987875"], [" earth", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" shortest", "0.27809727"], [" distance", "1.0"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"], [" on", "1.0"], [" the", "1.0"], [" earth", "1.0"], [" is", "1.0"], [" the", "0.64473027"], [" equ", "0.99610424"], ["ator", "1.0"], [".", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" between", "0.9999951"], [" two", "1.0"], [" points", "1.0"], [" on", "1.0"], [" the", "1.0"], [" earth", "1.0"], [" is", "1.0"], [" the", "1.0"], [" equ", "1.0"], ["ator", "1.0"], [".", "1.0"], [" The", "0.9989868"], [" shortest", "1.0"], [" distance", "1.0"], [" between", "1.0"]]}, "647": {"qid": "7447fdc904d63a6f77bf", "term": "C-SPAN", "description": "American pay television network", "question": "Does the name C-SPAN refer to a form of telecommunications that utilizes outer space?", "answer": true, "facts": ["The S in C-SPAN refers to Satellite.", "Satellite communications require communicating with satellites that orbit the Earth in outer space."], "decomposition": ["What does C-SPAN's transmission equipment consist of, according to its full meaning?", "Is any of #1 located in outer space?"], "evidence": [[[["Cable television-2"]], [["Satellite-1"], "operation"]], [[["C-SPAN-14"]], [["Technology of television-4"]]], [[["C-SPAN-1"]], [["Satellite-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city of Rome?\nFacts: Rome is the name of the city in which the Roman Empire was founded.\nAnswer (True or False): True\nQuestion: What is the name of the city of Rome?\nFacts: Rome is the name of the city in which the Roman Empire was founded.\nAnswer (True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does the name C-SPAN refer to a form of telecommunications that utilizes outer space?\nFacts: The S in C-SPAN refers to Satellite. Satellite communications require communicating with satellites that orbit the Earth in outer space.\nAnswer (True or False):", "ct_subject": "The S in C-SPAN refers to Satellite. Satellite communications require communicating with satellites that orbit the Earth in outer space.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9993253"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "0.90414923"], [" of", "0.9882459"], [" Rome", "0.9783691"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "0.9995534"], [" is", "0.99997246"], [" the", "0.9999999"], [" name", "0.99330574"], [" of", "1.0"], [" the", "1.0"], [" city", "0.92795116"], [" in", "1.0"], [" which", "0.9999951"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [" was", "1.0"], [" founded", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "0.99964964"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" which", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [" was", "1.0"], [" founded", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"]]}, "648": {"qid": "0a84eaea4d26d46c7c30", "term": "Rainbow", "description": "meteorological phenomenon", "question": "Are flag of Gabon colors found in rainbow?", "answer": true, "facts": ["Rainbows contain the following colors:  red, orange, yellow, green, blue, indigo and violet.", "The flag of Gabon is green, yellow, and blue."], "decomposition": ["What colors are found in a rainbow?", "What colors are in the flag of the country Gabon?", "Are all the colors in #2 found in #1?"], "evidence": [[[["ROYGBIV-1"]], [["Flag of Gabon-3"]], ["operation"]], [[["ROYGBIV-1"]], [["Flag of Gabon-5"]], ["operation"]], [[["ROYGBIV-1"]], [["Flag of Gabon-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any other countries that have a flag that is the same as the flag of Gabon?\nFacts: Yes. The flag of Gabon is the same as the flag of the Republic of Gabon.\nAnswer (True or False): True\nQuestion: Are there any other countries that have a flag that is the same as the flag", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are flag of Gabon colors found in rainbow?\nFacts: Rainbows contain the following colors: red, orange, yellow, green, blue, indigo and violet. The flag of Gabon is green, yellow, and blue.\nAnswer (True or False):", "ct_subject": "Rainbows contain the following colors: red, orange, yellow, green, blue, indigo and violet. The flag of Gabon is green, yellow, and blue.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.9243697"], [" there", "0.89953583"], [" any", "0.62668836"], [" other", "0.9999777"], [" countries", "0.9537814"], [" that", "1.0"], [" have", "1.0"], [" a", "0.99999976"], [" flag", "1.0"], [" that", "0.93662727"], [" is", "1.0"], [" the", "0.99973613"], [" same", "1.0"], [" as", "0.99999475"], [" the", "1.0"], [" flag", "1.0"], [" of", "1.0"], [" Gab", "1.0"], ["on", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Yes", "0.64217556"], [".", "0.9271286"], [" The", "0.9998841"], [" flag", "1.0"], [" of", "1.0"], [" Gab", "1.0"], ["on", "1.0"], [" is", "1.0"], [" the", "1.0"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" flag", "1.0"], [" of", "1.0"], [" the", "0.9999999"], [" Republic", "1.0"], [" of", "1.0"], [" Gab", "0.99999917"], ["on", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.8093316"], [" there", "1.0"], [" any", "1.0"], [" other", "1.0"], [" countries", "0.99999475"], [" that", "1.0"], [" have", "1.0"], [" a", "1.0"], [" flag", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" flag", "1.0"]]}, "649": {"qid": "898ff9314804144d6f90", "term": "Johann Sebastian Bach", "description": "German composer", "question": "Did Johann Sebastian Bach influence heavy metal?", "answer": true, "facts": ["Johann Sebastian Bach was a classical German composer born in 1685.", "Lead singer of heavy metal band Skid Row, Sebastian Bach, took his name from German composer Johann Sebastian Bach.", "Heavy Metal band Metallica released a live album with the San Francisco Symphony.", "Deep Purple, n English hard rock/heavy metal band has cited classical musicians as their inspiration.", "Deep Purple's keyboard and guitar solos on \"Highway Star,\" have been called Bach-like in harmonic progression and virtuosic arpeggio figuration."], "decomposition": ["Who is the lead singer of \"Skid Row\"?", "Who did #1 name himself after?", "Which classic musician's work have Deep Purple's solo on \"Highway Star\" been compared with?", "Are #2 and #3 Johann Sebastian Bach and both bands heavy metal?"], "evidence": [[[["Sebastian Bach-1"]], [["Johann Sebastian Bach-1"], "no_evidence"], [["Highway Star (song)-4"]], [["Deep Purple-1", "Skid Row (American band) discography-2"], "operation"]], [[["Skid Row (American band)-1"]], [["Johann Sebastian Bach-1"]], [["Highway Star (song)-3"]], ["operation"]], [[["Sebastian Bach-1"]], [["Johann Sebastian Bach-1"]], [["Highway Star (song)-3"]], [["Deep Purple-1", "Skid Row (American band)-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Beatles", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Johann Sebastian Bach influence heavy metal?\nFacts: Johann Sebastian Bach was a classical German composer born in 1685. Lead singer of heavy metal band Skid Row, Sebastian Bach, took his name from German composer Johann Sebastian Bach. Heavy Metal band Metallica released a live album with the San Francisco Symphony. Deep Purple, n English hard rock/heavy metal band has cited classical musicians as their inspiration. Deep Purple's keyboard and guitar solos on \"Highway Star,\" have been called Bach-like in harmonic progression and virtuosic arpeggio figuration.\nAnswer (True or False):", "ct_subject": "Johann Sebastian Bach was a classical German composer born in 1685. Lead singer of heavy metal band Skid Row, Sebastian Bach, took his name from German composer Johann Sebastian Bach. Heavy Metal band Metallica released a live album with the San Francisco Symphony. Deep Purple, n English hard rock/heavy metal band has cited classical musicians as their inspiration. Deep Purple's keyboard and guitar solos on \"Highway Star,\" have been called Bach-like in harmonic progression and virtuosic arpeggio figuration.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9987998"], [" the", "1.0"], [" Beatles", "0.99999976"]]}, "650": {"qid": "5d0248445c0be4f1dc13", "term": "Sahara", "description": "desert in Africa", "question": "Can Poland Spring make money in the Sahara?", "answer": true, "facts": ["The Sahara is the largest hot desert", "Deserts are dry regions that receive little precipitation", "Poland Spring sells bottled water"], "decomposition": ["What does Poland Spring produce and sell?", "What is the weather condition in the Sahara?", "Would #2 make #1 highly desirable?"], "evidence": [[[["Poland Spring-1"]], [["Sahara-14", "Sahara-15"]], [["Thirst-1"]]], [[["Poland Spring-2"]], [["Sahara desert (ecoregion)-5"]], [["Sahara-12"]]], [[["Poland Spring-1"]], [["Sahara-1", "Sahara-20"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"C\" and a \"K\"?\nFacts: A \"C\" is a \"C\" is a \"C\" is a \"C\" is a \"C\" is a \"C\" is a \"C\" is a \"C\" is a \"C\" is a \"C\" is a \"C\" is a \"C\" is a \"C\" is a \"C", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can Poland Spring make money in the Sahara?\nFacts: The Sahara is the largest hot desert Deserts are dry regions that receive little precipitation Poland Spring sells bottled water\nAnswer (True or False):", "ct_subject": "The Sahara is the largest hot desert Deserts are dry regions that receive little precipitation Poland Spring sells bottled water", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9998834"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.81518316"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.9976234"], ["C", "0.9869061"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["K", "0.979292"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.8883371"], [" \"", "1.0"], ["C", "1.0"], ["\"", "1.0"], [" is", "0.99999857"], [" a", "0.9999995"], [" \"", "0.9999764"], ["C", "0.69835836"], ["\"", "1.0"], [" is", "0.9963561"], [" a", "1.0"], [" \"", "1.0"], ["C", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["C", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["C", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["C", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["C", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["C", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["C", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["C", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["C", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["C", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["C", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["C", "1.0"]]}, "651": {"qid": "c4b44208283d5d405c43", "term": "Ivan the Terrible", "description": "Grand Prince of Moscow and 1st Tsar of Russia", "question": "Was 847 Pope Leo same iteration of his name as Ivan the Terrible?", "answer": true, "facts": ["Pope Leo in 847 AD was the fourth Leo to have that name and was called Leo IV.", "Ivan the Terrible was the 4th Tsar to have the name Ivan and was known as Ivan IV Vasilyevich."], "decomposition": ["Which Pope Leo is associated with the year 847 AD?", "How many similarly named popes were before #1?", "What was Ivan the Terrible's title as a ruler?", "How many similarly named #3 ruled before him?", "Is #2 equal to #4?"], "evidence": [[[["Pope Leo IV-1"]], [["Pope Leo I-1", "Pope Leo II-1", "Pope Leo III-1"]], [["Ivan the Terrible-1"]], [["Ivan I of Moscow-1", "Ivan II of Moscow-1", "Ivan III of Russia-1"]], ["operation"]], [[["Pope Leo IV-1"]], ["operation"], [["Ivan the Terrible-1"]], ["operation"], ["operation"]], [[["Pope Leo IV-1"]], ["operation"], [["Ivan the Terrible-1"]], ["operation"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Was the first person to have a name that was the same as the name of a person who had a name that was the same as the name of a person who had a name that was the same as the name of a person who had a name that was the same", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was 847 Pope Leo same iteration of his name as Ivan the Terrible?\nFacts: Pope Leo in 847 AD was the fourth Leo to have that name and was called Leo IV. Ivan the Terrible was the 4th Tsar to have the name Ivan and was known as Ivan IV Vasilyevich.\nAnswer (True or False):", "ct_subject": "Pope Leo in 847 AD was the fourth Leo to have that name and was called Leo IV. Ivan the Terrible was the 4th Tsar to have the name Ivan and was known as Ivan IV Vasilyevich.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "0.98158294"], [" the", "1.0"], [" first", "0.99770737"], [" person", "0.92568564"], [" to", "1.0"], [" have", "0.2568925"], [" a", "0.9999937"], [" name", "1.0"], [" that", "0.9999999"], [" was", "0.7615212"], [" the", "0.9545698"], [" same", "1.0"], [" as", "1.0"], [" the", "0.41366687"], [" name", "0.99771106"], [" of", "1.0"], [" a", "0.99994457"], [" person", "0.99999845"], [" who", "0.986728"], [" had", "0.6076018"], [" a", "0.9977022"], [" name", "1.0"], [" that", "1.0"], [" was", "1.0"], [" the", "1.0"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" a", "1.0"], [" person", "1.0"], [" who", "1.0"], [" had", "1.0"], [" a", "1.0"], [" name", "1.0"], [" that", "1.0"], [" was", "1.0"], [" the", "1.0"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" a", "1.0"], [" person", "1.0"], [" who", "1.0"], [" had", "1.0"], [" a", "1.0"], [" name", "1.0"], [" that", "1.0"], [" was", "1.0"], [" the", "1.0"], [" same", "1.0"]]}, "652": {"qid": "06b9ed3f803e3d5796ed", "term": "The Powerpuff Girls", "description": "American animated television series", "question": "Could the Powepuff Girls make the background to the Azerbaijani flag?", "answer": true, "facts": ["The national flag of the Republic of Azerbaijan is a horizontal tricolour featuring three equally sized fesses of blue, red, and green", "Each of the Powerpuff Girls creates a trail of a different color when she flies: Bubbles makes blue, Blossom makes red, and Buttercup makes green."], "decomposition": ["What colors are present on the Azerbaijani flag?", "What colors are the Powerpuff Girls?", "Is #1 the same as #2?"], "evidence": [[[["Flag of Azerbaijan-1"]], [["The Powerpuff Girls-5"]], ["operation"]], [[["Flag of Azerbaijan-1"]], [["The Powerpuff Girls-9"]], ["operation"]], [[["Flag of Azerbaijan-1"]], [["The Powerpuff Girls-5"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the character in the movie \"The Princess Bride\" who is the only one who can see the future?\nFacts: The Princess Bride is based on the novel by William Goldman", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could the Powepuff Girls make the background to the Azerbaijani flag?\nFacts: The national flag of the Republic of Azerbaijan is a horizontal tricolour featuring three equally sized fesses of blue, red, and green Each of the Powerpuff Girls creates a trail of a different color when she flies: Bubbles makes blue, Blossom makes red, and Buttercup makes green.\nAnswer (True or False):", "ct_subject": "The national flag of the Republic of Azerbaijan is a horizontal tricolour featuring three equally sized fesses of blue, red, and green Each of the Powerpuff Girls creates a trail of a different color when she flies: Bubbles makes blue, Blossom makes red, and Buttercup makes green.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999595"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" character", "0.8323862"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "1.0"], ["The", "1.0"], [" Princess", "1.0"], [" Bride", "1.0"], ["\"", "0.99994683"], [" who", "1.0"], [" is", "0.9999857"], [" the", "0.99999976"], [" only", "0.8701857"], [" one", "0.9999807"], [" who", "1.0"], [" can", "0.99999976"], [" see", "0.99922884"], [" the", "1.0"], [" future", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Princess", "0.9830185"], [" Bride", "1.0"], [" is", "1.0"], [" based", "0.83076155"], [" on", "1.0"], [" the", "0.9998834"], [" novel", "0.9999391"], [" by", "1.0"], [" William", "1.0"], [" Goldman", "1.0"]]}, "653": {"qid": "ae25e990129848141330", "term": "Florence", "description": "Capital and most populous city of the Italian region of Tuscany", "question": "Is there a Harry Potter character named after Florence?", "answer": true, "facts": ["Firenze is the native Italian form of the name Florence.", "There is a centaur who appars as a minor character in the Harry Potter series named Firenze.", "Firenze appears in three of the Harry Potter books but only one movie."], "decomposition": ["What is the native Italian form for the name Florence?", "What is the name of the centaur who appears  in the Harry Potter series?", "Is #1 the same as #2?"], "evidence": [[[["Florence (given name)-5"]], [["Magical creatures in Harry Potter-65"]], [["Florence (given name)-5", "Magical creatures in Harry Potter-65"], "operation"]], [[["Florence-1"]], [["Magical creatures in Harry Potter-65"]], ["operation"]], [[["Florence-1"]], [["Magical creatures in Harry Potter-65"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is there a Harry Potter character named after the Roman god of fire?\nFacts: There is a centaur who appears in the Harry Potter series named Firenze. Firenze appears in three of the Harry Potter books but only one movie.\nAnswer (True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is there a Harry Potter character named after Florence?\nFacts: Firenze is the native Italian form of the name Florence. There is a centaur who appars as a minor character in the Harry Potter series named Firenze. Firenze appears in three of the Harry Potter books but only one movie.\nAnswer (True or False):", "ct_subject": "Firenze is the native Italian form of the name Florence. There is a centaur who appars as a minor character in the Harry Potter series named Firenze. Firenze appears in three of the Harry Potter books but only one movie.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999999"], [" there", "1.0"], [" a", "1.0"], [" Harry", "0.8349712"], [" Potter", "1.0"], [" character", "1.0"], [" named", "1.0"], [" after", "1.0"], [" the", "0.73641837"], [" Roman", "0.9972404"], [" god", "0.7171204"], [" of", "1.0"], [" fire", "0.6631179"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "0.99999774"], [" is", "1.0"], [" a", "1.0"], [" cent", "0.9902744"], ["aur", "1.0"], [" who", "1.0"], [" appears", "1.0"], [" in", "0.9793707"], [" the", "1.0"], [" Harry", "1.0"], [" Potter", "1.0"], [" series", "1.0"], [" named", "1.0"], [" Fire", "0.9991172"], ["n", "1.0"], ["ze", "1.0"], [".", "1.0"], [" Fire", "1.0"], ["n", "1.0"], ["ze", "1.0"], [" appears", "1.0"], [" in", "1.0"], [" three", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Harry", "1.0"], [" Potter", "1.0"], [" books", "1.0"], [" but", "1.0"], [" only", "1.0"], [" one", "1.0"], [" movie", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"]]}, "654": {"qid": "3ba4e4cd6db5e42e06b0", "term": "Ham", "description": "Pork from a leg cut that has been preserved by wet or dry curing, with or without smoking", "question": "Did Malcolm X avoid eating ham?", "answer": true, "facts": ["Malcolm X was a practicing Muslim", "Muslims are prohibited from eating foods derived from pigs"], "decomposition": ["What religion did Malcolm X practice?", "Does #1 forbid its believers eating pig products?"], "evidence": [[[["Malcolm X-1"]], ["operation"]], [[["Malcolm X-1"]], [["Islamic culture-45"]]], [[["Malcolm X-50"]], [["Islamic culture-45"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Beatles have a secret song?\nFacts: The Beatles were not a secret band. They were a popular band in the 1960s.\nAnswer (True or False): True\nQuestion: Did the Beatles have a secret song?\nFacts: The Beatles were not a secret band. They were a popular band in the 1960s.\nAnswer (True or False): True\nQuestion: Did the Beatles have a secret song?\nFacts:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Malcolm X avoid eating ham?\nFacts: Malcolm X was a practicing Muslim Muslims are prohibited from eating foods derived from pigs\nAnswer (True or False):", "ct_subject": "Malcolm X was a practicing Muslim Muslims are prohibited from eating foods derived from pigs", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99999976"], [" the", "1.0"], [" Beatles", "0.85442585"], [" have", "1.0"], [" a", "1.0"], [" secret", "0.9971078"], [" song", "1.0"], ["?", "0.99889565"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Beatles", "1.0"], [" were", "0.6262174"], [" not", "0.99971944"], [" a", "1.0"], [" secret", "1.0"], [" band", "1.0"], [".", "0.9999932"], [" They", "0.9783332"], [" were", "1.0"], [" a", "0.99611413"], [" popular", "0.9999999"], [" band", "0.99994373"], [" in", "0.86279243"], [" the", "1.0"], [" 1960", "0.99679834"], ["s", "0.99989784"], [".", "0.99292386"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" Beatles", "1.0"], [" have", "1.0"], [" a", "1.0"], [" secret", "1.0"], [" song", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Beatles", "1.0"], [" were", "1.0"], [" not", "1.0"], [" a", "1.0"], [" secret", "1.0"], [" band", "1.0"], [".", "1.0"], [" They", "1.0"], [" were", "1.0"], [" a", "1.0"], [" popular", "1.0"], [" band", "1.0"], [" in", "1.0"], [" the", "1.0"], [" 1960", "1.0"], ["s", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" Beatles", "1.0"], [" have", "1.0"], [" a", "1.0"], [" secret", "1.0"], [" song", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"]]}, "655": {"qid": "56ce10fe8bfede3f56ba", "term": "New Year's Eve", "description": "holiday celebrated on 31 December", "question": "Would New Year's Eve hypothetically be Bacchus's favorite holiday?", "answer": true, "facts": ["Bacchus was the Roman god of wine and revelry.", "One of the main New Year's Eve traditions is drinking a toast to the new year.", "New Year\u2019s Eve is the biggest day of the year for liquor stores in terms of sales."], "decomposition": ["What was Bacchus the Roman god of?", "Do people tend to celebrate with #1 on New Year's Eve?"], "evidence": [[[["Dionysus-1", "Dionysus-2"]], [["New Year's Eve-15"], "operation"]], [[["Dionysus-1", "Dionysus-2"]], [["New Year's Eve-136", "New Year's Eve-97"]]], [[["Dionysus-1"]], [["New Year's Eve-15"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: The first person to be executed in the United States was John Brown.\nAnswer (True or False): True\nQuestion: What is the name", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would New Year's Eve hypothetically be Bacchus's favorite holiday?\nFacts: Bacchus was the Roman god of wine and revelry. One of the main New Year's Eve traditions is drinking a toast to the new year. New Year\u2019s Eve is the biggest day of the year for liquor stores in terms of sales.\nAnswer (True or False):", "ct_subject": "Bacchus was the Roman god of wine and revelry. One of the main New Year's Eve traditions is drinking a toast to the new year. New Year\u2019s Eve is the biggest day of the year for liquor stores in terms of sales.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9995278"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9666332"], [" person", "0.9999405"], [" to", "1.0"], [" be", "0.9974215"], [" executed", "0.987225"], [" in", "0.99999785"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.8313293"], [" first", "1.0"], [" person", "1.0"], [" to", "0.80364615"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" John", "0.9999982"], [" Brown", "0.9996792"], [".", "0.88909924"], ["\n", "0.16358988"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"]]}, "656": {"qid": "502d4dae6f08e73a5569", "term": "Crucifixion", "description": "Method of capital punishment in which the victim is tied or nailed to a large wooden beam and left to hang until eventual death", "question": "Is Home Depot a one stop shop for crucifixion supplies?", "answer": true, "facts": ["A one stop shop is a store where multiple items are supplied.", "Crucifixion is a form of punishment in which a person is nailed to a wooden cross.", "Home Depot sells numerous supplies including: hammers, nails, and wood."], "decomposition": ["What is the definition of a one stop shop?", "What tools are necessary for Crucifixion?", "Is Home Depot a #1 for all of #2?"], "evidence": [[[["One stop shop-1"]], [["Crucifixion-1"]], ["operation"]], [[["One stop shop-1"]], [["Descriptions in antiquity of the execution cross-6"], "no_evidence"], [["The Home Depot-1"], "operation"]], [[["One stop shop-1"]], [["Crucifixion-1"]], [["The Home Depot-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city of Rome derived from the Latin word for \"crucify\" or \"crucify\"?\nFacts: The name of the city of Rome is derived from the Latin word for \"crucify\" or \"crucify\". The word", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Home Depot a one stop shop for crucifixion supplies?\nFacts: A one stop shop is a store where multiple items are supplied. Crucifixion is a form of punishment in which a person is nailed to a wooden cross. Home Depot sells numerous supplies including: hammers, nails, and wood.\nAnswer (True or False):", "ct_subject": "A one stop shop is a store where multiple items are supplied. Crucifixion is a form of punishment in which a person is nailed to a wooden cross. Home Depot sells numerous supplies including: hammers, nails, and wood.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9993581"], [" the", "0.99995697"], [" name", "0.96693647"], [" of", "0.9999896"], [" the", "1.0"], [" city", "0.99999905"], [" of", "0.9999999"], [" Rome", "0.9999958"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" Latin", "0.9859794"], [" word", "1.0"], [" for", "0.9999863"], [" \"", "1.0"], ["cru", "0.98774916"], ["c", "1.0"], ["ify", "0.9999192"], ["\"", "0.99808383"], [" or", "1.0"], [" \"", "0.97837055"], ["cru", "1.0"], ["c", "1.0"], ["ify", "1.0"], ["\"?", "0.98462754"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" name", "0.96210366"], [" of", "1.0"], [" the", "0.90445817"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], [" is", "1.0"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" Latin", "1.0"], [" word", "1.0"], [" for", "1.0"], [" \"", "1.0"], ["cru", "1.0"], ["c", "1.0"], ["ify", "1.0"], ["\"", "0.9878312"], [" or", "1.0"], [" \"", "1.0"], ["cru", "1.0"], ["c", "1.0"], ["ify", "1.0"], ["\".", "1.0"], [" The", "0.9795369"], [" word", "0.99999976"]]}, "657": {"qid": "ff848539d05ca985ed4f", "term": "Hyena", "description": "family of mammal", "question": "Would a human following a hyena diet be unwelcome at a vegan festival?", "answer": true, "facts": ["A hyena is a carnivorous mammal that feeds on the flesh of other animals.", "Vegans are people that stick to a strict diet that does not include animals or animal products."], "decomposition": ["What does the hyena diet consist mainly of?", "What do people on a vegan diet eat?", "Is there an overlap between #1 and #2?"], "evidence": [[[["Hyena-1", "Hyena-3"]], [["Veganism-1"]], ["operation"]], [[["Hyena-21"]], [["Veganism-1"]], ["operation"]], [[["Striped hyena-15"]], [["Veganism-31"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a vegetarian and a vegan?\nFacts: A vegetarian is a person that does not eat meat, fish, or poultry. A vegan is a person that does not eat any animal products.\nAnswer (True or False): True\nQuestion: What is the difference between a vegetarian and a vegan", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a human following a hyena diet be unwelcome at a vegan festival?\nFacts: A hyena is a carnivorous mammal that feeds on the flesh of other animals. Vegans are people that stick to a strict diet that does not include animals or animal products.\nAnswer (True or False):", "ct_subject": "A hyena is a carnivorous mammal that feeds on the flesh of other animals. Vegans are people that stick to a strict diet that does not include animals or animal products.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9993774"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9999999"], [" between", "1.0"], [" a", "1.0"], [" vegetarian", "0.30895695"], [" and", "1.0"], [" a", "1.0"], [" vegan", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.999931"], [" vegetarian", "1.0"], [" is", "1.0"], [" a", "0.99951375"], [" person", "1.0"], [" that", "0.86989087"], [" does", "0.993646"], [" not", "1.0"], [" eat", "1.0"], [" meat", "1.0"], [",", "0.99803966"], [" fish", "1.0"], [",", "1.0"], [" or", "0.9999627"], [" poultry", "0.9999988"], [".", "1.0"], [" A", "0.99988985"], [" vegan", "1.0"], [" is", "1.0"], [" a", "1.0"], [" person", "1.0"], [" that", "1.0"], [" does", "1.0"], [" not", "1.0"], [" eat", "1.0"], [" any", "1.0"], [" animal", "1.0"], [" products", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" vegetarian", "1.0"], [" and", "1.0"], [" a", "1.0"], [" vegan", "1.0"]]}, "658": {"qid": "63de2dfa4f52f3dbf28e", "term": "Snickers", "description": "brand name chocolate bar made by Mars, Incorporated", "question": "Would 2019 Natalie Portman avoid a Snickers bar due to her diet?", "answer": true, "facts": ["Actress Natalie Portman resumed her vegan diet in 2018 after giving birth, and has been vegan ever since.", "Vegans do not eat animal products.", "Snickers contains egg whites as an ingredient."], "decomposition": ["What foods has Natalie Portman avoided since 2018?", "What are the ingredients in a Snickers bar?", "Is anything from #2 also in #1?"], "evidence": [[[["Natalie Portman-30", "Veganism-1"]], [["Snickers-1"]], [["Nougat-1", "Types of chocolate-6"], "operation"]], [[["Natalie Portman-27", "Veganism-1"]], [["Snickers-1"]], ["operation"]], [[["Natalie Portman-27"], "no_evidence"], [["Snickers-7"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake cake\"?\nFacts: A snowflake is a single crystal of ice. A snowflake cake is a multi-crystal cake.\nAnswer (True or False): True\nQuestion: What is the difference between a \"", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would 2019 Natalie Portman avoid a Snickers bar due to her diet?\nFacts: Actress Natalie Portman resumed her vegan diet in 2018 after giving birth, and has been vegan ever since. Vegans do not eat animal products. Snickers contains egg whites as an ingredient.\nAnswer (True or False):", "ct_subject": "Actress Natalie Portman resumed her vegan diet in 2018 after giving birth, and has been vegan ever since. Vegans do not eat animal products. Snickers contains egg whites as an ingredient.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999774"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.20681204"], [" between", "1.0"], [" a", "0.99999905"], [" \"", "1.0"], ["s", "0.91201556"], ["now", "0.39060965"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.9999856"], ["now", "1.0"], ["flake", "1.0"], [" cake", "0.82841897"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.99999905"], [" snow", "0.56153417"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "0.756869"], [" crystal", "0.99056715"], [" of", "0.88965976"], [" ice", "1.0"], [".", "0.517779"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" cake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" multi", "0.8226081"], ["-", "1.0"], ["cry", "0.9999907"], ["stal", "0.9995875"], [" cake", "0.99992967"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"]]}, "659": {"qid": "ec77d443ba555a906316", "term": "Taco Bell", "description": "American fast-food chain", "question": "Does the Taco Bell kitchen contain cinnamon?", "answer": true, "facts": ["Taco Bell serves churros.", "Cinnamon is an ingredient in churros."], "decomposition": ["What dough pastry based snack does Taco Bell serve?", "Does #1 contain Cinnamon?"], "evidence": [[[["Taco Bell-1"]], ["no_evidence", "operation"]], [[["Taco Bell-21"]], [["Cinnabon-3"]]], [[["Taco Bell-21"]], [["Cinnabon-3"], "no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first man to walk on the moon?\nFacts: Neil Armstrong.\nAnswer (True or False): True\nQuestion: What is the name of the first man to walk on the moon?\nFacts: Neil Armstrong.\nAnswer (True or False): True\nQuestion: What is the name of the first man to walk on the moon?\nFacts: Neil Armstrong.\nAnswer (True or False):", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does the Taco Bell kitchen contain cinnamon?\nFacts: Taco Bell serves churros. Cinnamon is an ingredient in churros.\nAnswer (True or False):", "ct_subject": "Taco Bell serves churros. Cinnamon is an ingredient in churros.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99998295"], [" is", "1.0"], [" the", "1.0"], [" name", "0.999992"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9201509"], [" man", "0.049154304"], [" to", "0.99999905"], [" walk", "0.97657824"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Neil", "1.0"], [" Armstrong", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "0.9999895"], [" to", "1.0"], [" walk", "0.99999964"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Neil", "1.0"], [" Armstrong", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Neil", "1.0"], [" Armstrong", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"]]}, "660": {"qid": "2bec732d14cf2b289435", "term": "Santa Claus", "description": "Folkloric figure, said to deliver gifts to children on Christmas Eve", "question": "Are most mall Santa Claus actors white?", "answer": true, "facts": ["In 2016, a black man playing Santa Claus at the Mall of America made national headlines.", "There are map websites dedicated to locating black Santa Claus mall actors."], "decomposition": ["What is the ethnicity of the man who made headlines for playing Santa Claus at the Mall of America in 2016?", "Does #1 imply that black Santas are a rare occurrence?"], "evidence": [[["no_evidence"], ["no_evidence"]], [[["Santa Claus-2"], "no_evidence"], ["operation"]], [["no_evidence"], [["Santa Claus-50"], "no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are most people today related to the ancient Sumerians?\nFacts: The Sumerians were a people who lived in Mesopotamia, a region that is now Iraq, Iran, and Turkey. They were a civilization that existed from about 3,000 B.C. to about 3,000 B.C. They were a civilization that was very advanced in agriculture,", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are most mall Santa Claus actors white?\nFacts: In 2016, a black man playing Santa Claus at the Mall of America made national headlines. There are map websites dedicated to locating black Santa Claus mall actors.\nAnswer (True or False):", "ct_subject": "In 2016, a black man playing Santa Claus at the Mall of America made national headlines. There are map websites dedicated to locating black Santa Claus mall actors.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.9994837"], [" most", "0.23078702"], [" people", "1.0"], [" today", "0.32615834"], [" related", "1.0"], [" to", "1.0"], [" the", "0.9999933"], [" ancient", "0.7203497"], [" Sum", "0.8054097"], ["er", "1.0"], ["ians", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99998665"], [" Sum", "1.0"], ["er", "1.0"], ["ians", "1.0"], [" were", "1.0"], [" a", "0.99952865"], [" people", "0.6430775"], [" who", "0.9999999"], [" lived", "1.0"], [" in", "1.0"], [" Mes", "1.0"], ["opot", "1.0"], ["amia", "1.0"], [",", "0.99999976"], [" a", "0.9946714"], [" region", "1.0"], [" that", "0.6523802"], [" is", "0.999998"], [" now", "1.0"], [" Iraq", "0.9999999"], [",", "0.99999845"], [" Iran", "1.0"], [",", "1.0"], [" and", "0.99537426"], [" Turkey", "1.0"], [".", "1.0"], [" They", "0.9814511"], [" were", "1.0"], [" a", "0.9997063"], [" civilization", "0.99995875"], [" that", "1.0"], [" existed", "0.58174115"], [" from", "1.0"], [" about", "0.9999963"], [" 3", "0.98529935"], [",", "0.19564211"], ["000", "1.0"], [" B", "0.15695032"], [".", "1.0"], ["C", "1.0"], [".", "1.0"], [" to", "1.0"], [" about", "1.0"], [" 3", "0.98086435"], [",", "1.0"], ["000", "1.0"], [" B", "1.0"], [".", "1.0"], ["C", "1.0"], [".", "1.0"], [" They", "0.9998442"], [" were", "1.0"], [" a", "0.999995"], [" civilization", "0.99502796"], [" that", "1.0"], [" was", "0.9991702"], [" very", "0.99953616"], [" advanced", "1.0"], [" in", "0.8655558"], [" agriculture", "0.96434647"], [",", "0.9999994"]]}, "661": {"qid": "a253f98e01008297d4b3", "term": "Lip", "description": "Visible body part at the mouth", "question": "Does having lip piercings lead to more expensive dental bills?", "answer": true, "facts": ["Lip piercings can rub the enamel on your teeth and can cause tissue damage to the gums.", "Tooth enamel protects the teeth from decay."], "decomposition": ["What is the function of Tooth Enamel?", "Can Lip piercing cause damage to #1", "Will #2 cost you more expensive dental bills?"], "evidence": [[[["Tooth enamel-1"]], [["Tooth enamel-26"], "no_evidence"], ["no_evidence"]], [[["Tooth enamel-1"]], [["Lip piercing-5"], "no_evidence"], ["no_evidence", "operation"]], [[["Tooth enamel-21"]], [["Body piercing-42", "Lip piercing-4"]], [["Tooth enamel-39"], "no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it true that the first person to have a tattoo was a woman?\nFacts: The first person to have a tattoo was a man. The first person to have a tattoo was a woman.\nAnswer (True or False): True\nQuestion: Is it true that the first person to have a tattoo was a woman?\nFacts: The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does having lip piercings lead to more expensive dental bills?\nFacts: Lip piercings can rub the enamel on your teeth and can cause tissue damage to the gums. Tooth enamel protects the teeth from decay.\nAnswer (True or False):", "ct_subject": "Lip piercings can rub the enamel on your teeth and can cause tissue damage to the gums. Tooth enamel protects the teeth from decay.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.6642936"], [" it", "0.99999976"], [" true", "1.0"], [" that", "1.0"], [" the", "1.0"], [" first", "0.9861666"], [" person", "0.99010676"], [" to", "1.0"], [" have", "0.893645"], [" a", "0.99999964"], [" tattoo", "0.8247002"], [" was", "1.0"], [" a", "1.0"], [" woman", "0.99744403"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "0.9998586"], [" to", "1.0"], [" have", "1.0"], [" a", "1.0"], [" tattoo", "1.0"], [" was", "1.0"], [" a", "1.0"], [" man", "0.8151428"], [".", "1.0"], [" The", "0.9993949"], [" first", "1.0"], [" person", "0.7775982"], [" to", "1.0"], [" have", "1.0"], [" a", "1.0"], [" tattoo", "1.0"], [" was", "1.0"], [" a", "1.0"], [" woman", "0.9999999"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" true", "1.0"], [" that", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" have", "1.0"], [" a", "1.0"], [" tattoo", "1.0"], [" was", "1.0"], [" a", "1.0"], [" woman", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"]]}, "662": {"qid": "9384dfc0aabc73026925", "term": "Heart", "description": "organ for the circulation of blood in animal circulatory systems", "question": "Do anatomical and symbolic hearts look remarkably different?", "answer": true, "facts": ["Symbolic hearts are sharply pointed at the bottom and feature a sharp valley between the bumps at the top.", "Anatomical hearts are rounded, have numerous vascular tubes entering and exiting them, and do not feature sharp angles."], "decomposition": ["What are the dimensions of the heart symbol?", "Do anatomical hearts lack the dimensions of #1?"], "evidence": [[[["Heart symbol-1"]], [["Heart-6", "Heart-7"]]], [[["Heart symbol-1"], "no_evidence"], [["Heart-2"], "no_evidence", "operation"]], [[["Heart symbol-3"]], [["Heart-8"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"saddle\" and a \"saddleback\"?\nFacts: Saddlebacks are the back of a horse's back. Saddlebacks are not the same as saddlebacks. Saddlebacks are the back of a horse's back.\nAnswer (True or False): True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do anatomical and symbolic hearts look remarkably different?\nFacts: Symbolic hearts are sharply pointed at the bottom and feature a sharp valley between the bumps at the top. Anatomical hearts are rounded, have numerous vascular tubes entering and exiting them, and do not feature sharp angles.\nAnswer (True or False):", "ct_subject": "Symbolic hearts are sharply pointed at the bottom and feature a sharp valley between the bumps at the top. Anatomical hearts are rounded, have numerous vascular tubes entering and exiting them, and do not feature sharp angles.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9998605"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.84873813"], [" between", "1.0"], [" a", "0.9999753"], [" \"", "0.3323128"], ["s", "0.7229346"], ["addle", "0.74043113"], ["\"", "0.9997905"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.9999982"], ["addle", "1.0"], ["back", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" S", "0.3030394"], ["addle", "0.99977106"], ["backs", "0.999363"], [" are", "1.0"], [" the", "0.98157215"], [" back", "0.99998605"], [" of", "1.0"], [" a", "0.9975942"], [" horse", "1.0"], ["'s", "0.9995857"], [" back", "0.9978393"], [".", "0.9981806"], [" Sadd", "0.17367764"], ["le", "0.9153726"], ["backs", "1.0"], [" are", "1.0"], [" not", "1.0"], [" the", "1.0"], [" same", "0.9806841"], [" as", "1.0"], [" saddle", "0.95918256"], ["backs", "1.0"], [".", "1.0"], [" Sadd", "0.7834427"], ["le", "1.0"], ["backs", "1.0"], [" are", "1.0"], [" the", "1.0"], [" back", "1.0"], [" of", "1.0"], [" a", "1.0"], [" horse", "1.0"], ["'s", "1.0"], [" back", "1.0"], [".", "1.0"], ["\n", "0.99999976"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"]]}, "663": {"qid": "0e19518a97864d4babba", "term": "Gulf of Mexico", "description": "An Atlantic Ocean basin extending into southern North America", "question": "Are fossil fuels reducing jobs in the Gulf of Mexico?", "answer": true, "facts": ["An oil spill is still polluting the Gulf of Mexico", "Workers such as fishermen are out of work due to pollution"], "decomposition": ["What are the consequences of fossil fuel presence in the Gulf of Mexico?", "Is #1 putting some people out of job?"], "evidence": [[[["Deepwater Horizon oil spill-2", "Taylor oil spill-2"]], [["Deepwater Horizon oil spill-71", "Deepwater Horizon oil spill-72"], "operation"]], [[["Gulf of Mexico-38"]], [["Gulf of Mexico-38"], "no_evidence"]], [[["Gulf of Mexico-42"]], ["no_evidence"]], [[["Gulf of Mexico-42"]], [["Gulf of Mexico-36"], "no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there more people in the United States today than there were in the United States in the year 2000?\nFacts: There are more people in the United States today than there were in the United States in the year 2000.\nAnswer (True or False): True\nQuestion: Are there more people in the United States today than there were in the United States in the year 2000?\nFacts: There", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are fossil fuels reducing jobs in the Gulf of Mexico?\nFacts: An oil spill is still polluting the Gulf of Mexico Workers such as fishermen are out of work due to pollution\nAnswer (True or False):", "ct_subject": "An oil spill is still polluting the Gulf of Mexico Workers such as fishermen are out of work due to pollution", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.99982446"], [" there", "0.99996996"], [" more", "0.9852705"], [" people", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "0.9987865"], [" States", "1.0"], [" today", "1.0"], [" than", "1.0"], [" there", "0.9999981"], [" were", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "0.6555049"], [" States", "1.0"], [" in", "1.0"], [" the", "0.75090855"], [" year", "1.0"], [" 2000", "0.7493213"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "0.9999999"], [" are", "1.0"], [" more", "1.0"], [" people", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" today", "1.0"], [" than", "1.0"], [" there", "1.0"], [" were", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [".", "0.9999956"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "1.0"], [" more", "1.0"], [" people", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" today", "1.0"], [" than", "1.0"], [" there", "1.0"], [" were", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "1.0"]]}, "664": {"qid": "c6adebffcdabb5ba7063", "term": "Small Solar System body", "description": "object in the Solar System that is neither a planet, nor a dwarf planet, nor a satellite", "question": "Is the name of a mythical creature also the name of a Small Solar System body?", "answer": true, "facts": ["A centaur is a kind of Small Solar System body with characteristics of both asteroids and comets.", "A centaur is also a mythical creature that has the body of a horse and the head of a man."], "decomposition": ["What is a mythical creature with the body of a horse and the head of a man called?", "Is any Small Solar System named after #1?"], "evidence": [[[["Centaur-1"]], [["Centaur (small Solar System body)-1"], "operation"]], [[["Centaur-5"]], [["Centaurus-18"]]], [[["Centaur-1"]], [["Centaur (small Solar System body)-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of a mythical creature also the name of a Small Solar System body?\nFacts: A centaur is a kind of Small Solar System body with characteristics of both asteroids and comets. A centaur is also a mythical creature that has the body of a horse and", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is the name of a mythical creature also the name of a Small Solar System body?\nFacts: A centaur is a kind of Small Solar System body with characteristics of both asteroids and comets. A centaur is also a mythical creature that has the body of a horse and the head of a man.\nAnswer (True or False):", "ct_subject": "A centaur is a kind of Small Solar System body with characteristics of both asteroids and comets. A centaur is also a mythical creature that has the body of a horse and the head of a man.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99691725"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" a", "1.0"], [" mythical", "1.0"], [" creature", "1.0"], [" also", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" a", "1.0"], [" Small", "1.0"], [" Solar", "1.0"], [" System", "1.0"], [" body", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999995"], [" cent", "1.0"], ["aur", "1.0"], [" is", "1.0"], [" a", "1.0"], [" kind", "1.0"], [" of", "1.0"], [" Small", "1.0"], [" Solar", "1.0"], [" System", "1.0"], [" body", "1.0"], [" with", "1.0"], [" characteristics", "1.0"], [" of", "1.0"], [" both", "1.0"], [" asteroids", "1.0"], [" and", "1.0"], [" com", "1.0"], ["ets", "1.0"], [".", "1.0"], [" A", "1.0"], [" cent", "1.0"], ["aur", "1.0"], [" is", "1.0"], [" also", "1.0"], [" a", "1.0"], [" mythical", "1.0"], [" creature", "1.0"], [" that", "1.0"], [" has", "1.0"], [" the", "1.0"], [" body", "1.0"], [" of", "1.0"], [" a", "1.0"], [" horse", "1.0"], [" and", "1.0"]]}, "665": {"qid": "7c3759cc1da78e9fbd79", "term": "Crustacean", "description": "subphylum of arthropods", "question": "Could a Diwali celebration feature a crustacean?", "answer": true, "facts": ["Diwali is an important Hindu holiday which includes feasting.", "Crustaceans are shelled, mainly aquatic animals that include shrimp, lobster, and crabs.", "Hindus are allowed to eat any food besides beef, since cows are sacred.", "Hindus are allowed to eat "], "decomposition": ["What religion celebrates Diwali with feasting?", "What foods are on #1 s forbidden list?", "Is lobster part of #2?"], "evidence": [[[["Diwali-4"]], [["Buddhist cuisine-9"]], ["operation"]], [[["Diwali-1"]], [["Hinduism-94"]], [["Lobster-2"]]], [[["Diwali-15"]], [["Diet in Hinduism-27"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"Hindu\" and a \"Muslim\"?\nFacts: Hindus are followers of the Hindu religion. Muslims are followers of the Muslim religion.\nAnswer (True or False): True\nQuestion: What is the difference", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could a Diwali celebration feature a crustacean?\nFacts: Diwali is an important Hindu holiday which includes feasting. Crustaceans are shelled, mainly aquatic animals that include shrimp, lobster, and crabs. Hindus are allowed to eat any food besides beef, since cows are sacred. Hindus are allowed to eat \nAnswer (True or False):", "ct_subject": "Diwali is an important Hindu holiday which includes feasting. Crustaceans are shelled, mainly aquatic animals that include shrimp, lobster, and crabs. Hindus are allowed to eat any food besides beef, since cows are sacred. Hindus are allowed to eat ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9986362"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.14557743"], [" between", "1.0"], [" a", "0.99999166"], [" \"", "0.871095"], ["H", "0.98360485"], ["indu", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["Muslim", "1.0"], ["\"?", "0.9985468"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Hindus", "0.99583215"], [" are", "0.95727223"], [" followers", "0.9999988"], [" of", "1.0"], [" the", "0.9999995"], [" Hindu", "1.0"], [" religion", "1.0"], [".", "0.99997437"], [" Muslims", "1.0"], [" are", "1.0"], [" followers", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Muslim", "1.0"], [" religion", "1.0"], [".", "1.0"], ["\n", "0.99999356"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"]]}, "666": {"qid": "946d0b97a56bbe7acca2", "term": "Chives", "description": "edible species of plant", "question": "Are there any chives hypothetically good for battling vampires?", "answer": true, "facts": ["Vampires in folklore have a weakness to garlic.", "Chives, an edible plant species, come in a number of varieties.", "Garlic chives are a variant of chives first found in China thousands of years ago."], "decomposition": ["What items are used to ward off vampires according to folklore?", "What are the varieties of chives that exist?", "Is any of #1 included in #2?"], "evidence": [[[["Garlic-61"]], [["Garlic-1"]], [["Garlic-1"], "operation"]], [[["Vampire-16"]], [["Chives-1"]], ["operation"]], [[["Garlic-61"]], [["Allium-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake\" with a \"snowflake\" on top?\nFacts: A \"snowflake\" is a small, round, and shiny object. A \"snowflake\" with a \"snowflake", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are there any chives hypothetically good for battling vampires?\nFacts: Vampires in folklore have a weakness to garlic. Chives, an edible plant species, come in a number of varieties. Garlic chives are a variant of chives first found in China thousands of years ago.\nAnswer (True or False):", "ct_subject": "Vampires in folklore have a weakness to garlic. Chives, an edible plant species, come in a number of varieties. Garlic chives are a variant of chives first found in China thousands of years ago.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.94971097"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.6888699"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.9999995"], ["s", "0.23572658"], ["now", "0.9999751"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\"", "0.52149445"], [" with", "0.9999968"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\"", "0.9999906"], [" on", "0.99961334"], [" top", "0.99994195"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.98611856"], [" \"", "0.7729487"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" small", "0.04822285"], [",", "1.0"], [" round", "0.99994695"], [",", "1.0"], [" and", "0.976313"], [" shiny", "0.99999964"], [" object", "0.99952996"], [".", "0.9996455"], [" A", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\"", "1.0"], [" with", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"]]}, "667": {"qid": "e304827eeb573bb2612e", "term": "Golden Gate Bridge", "description": "suspension bridge on the San Francisco Bay", "question": "Can you make an MP3 from the Golden Gate Bridge?", "answer": true, "facts": ["MP3 is a file compression format for audio recordings", "The Golden Gate Bridge has been reported emitting sounds when the wind passes through its sidewalk railing slats"], "decomposition": ["What is an MP3 a compressed file of?", "Does the Golden Gate Bridge produce #1?"], "evidence": [[[["MP3-1"]], ["no_evidence", "operation"]], [[["MP3-1"]], ["no_evidence", "operation"]], [[["MP3-1"]], [["Golden Gate Bridge-12"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the capital of the state of California?\nFacts: San Francisco\nAnswer (True or False): False\nQuestion: What is the name of the city in the United States that is the capital of the state of California?\nFacts: San Francisco\nAnswer (True or False): San Francisco\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can you make an MP3 from the Golden Gate Bridge?\nFacts: MP3 is a file compression format for audio recordings The Golden Gate Bridge has been reported emitting sounds when the wind passes through its sidewalk railing slats\nAnswer (True or False):", "ct_subject": "MP3 is a file compression format for audio recordings The Golden Gate Bridge has been reported emitting sounds when the wind passes through its sidewalk railing slats", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999943"], [" is", "1.0"], [" the", "1.0"], [" name", "0.8620977"], [" of", "1.0"], [" the", "1.0"], [" city", "0.8426401"], [" in", "0.9948726"], [" the", "0.99999"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "0.9882746"], [" the", "1.0"], [" capital", "0.8558093"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" California", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" San", "0.8672791"], [" Francisco", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" False", "0.9497786"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" California", "0.9721016"], ["?", "1.0"], ["\n", "1.0"], ["F", "0.98844427"], ["acts", "1.0"], [":", "1.0"], [" San", "1.0"], [" Francisco", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" San", "0.57819885"], [" Francisco", "1.0"], ["\n", "1.0"]]}, "668": {"qid": "6b3601b4c251046cbef4", "term": "Saint Vincent and the Grenadines", "description": "Country in the Caribbean", "question": "Was Saint Vincent and the Grenadines named by an Italian explorer?", "answer": true, "facts": ["Christopher Columbus, an Italian explorer, was the first European to discover the islands.", "He named them after St. Vincent because he first saw the island on the saint's feast day, and the Spanish city of Granada."], "decomposition": ["Who discovered Saint Vincent and the Grenadines?", "Was #1 from Italy?"], "evidence": [[[["Saint Vincent and the Grenadines-7"]], [["Christopher Columbus-1"], "operation"]], [[["Saint Vincent and the Grenadines-7"]], [["Christopher Columbus-1"]]], [[["Saint Vincent and the Grenadines-7"]], [["Christopher Columbus-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city of New York?\nFacts: New York is the capital of the United States. It is located in New York City, New York.\nAnswer (True or False): True\nQuestion: What is the name of the city of Paris?\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Saint Vincent and the Grenadines named by an Italian explorer?\nFacts: Christopher Columbus, an Italian explorer, was the first European to discover the islands. He named them after St. Vincent because he first saw the island on the saint's feast day, and the Spanish city of Granada.\nAnswer (True or False):", "ct_subject": "Christopher Columbus, an Italian explorer, was the first European to discover the islands. He named them after St. Vincent because he first saw the island on the saint's feast day, and the Spanish city of Granada.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99694175"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999995"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9991974"], [" of", "0.99529207"], [" New", "0.96427286"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" New", "1.0"], [" York", "1.0"], [" is", "0.99933845"], [" the", "1.0"], [" capital", "0.99924976"], [" of", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [".", "0.77143747"], [" It", "0.99172115"], [" is", "0.9999999"], [" located", "0.99998486"], [" in", "1.0"], [" New", "0.34033445"], [" York", "1.0"], [" City", "0.9999895"], [",", "0.9977938"], [" New", "1.0"], [" York", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Paris", "0.45916402"], ["?", "1.0"], ["\n", "1.0"]]}, "669": {"qid": "289c2f15051024b8b27f", "term": "Alice in Wonderland (1951 film)", "description": "1951 American animated musical fantasy film produced by Walt Disney Productions", "question": "Was milliner in Alice in Wonderland (1951 film) likely in need of succimer?", "answer": true, "facts": ["A milliner is someone who makes hats.", "The character of the Mad Hatter was a milliner in the 1951 Alice in Wonderland film.", "The phrase, Mad as a Hatter, comes from the fact that hat makers used mercury to line their hats and often suffered mercury poisoning.", "Succimer is a chemical that is used to treat lead, mercury, and arsenic poisoning."], "decomposition": ["What does a milliner do?", "Which Alice in Wonderland (1951 film) character did #1?", "Which element did #2 use for work that could be harmful to their mental health?", "Is succimer useful for treatment of the effects of #3?"], "evidence": [[[["Hatmaking-1"]], [["Alice in Wonderland (1951 film)-7"]], [["Erethism-1"]], [["Dimercaptosuccinic acid-1"], "operation"]], [[["Hatmaking-1"]], [["Hatter (Alice's Adventures in Wonderland)-1"]], [["Hatter (Alice's Adventures in Wonderland)-5"]], [["Dimercaptosuccinic acid-1"]]], [[["Hatmaking-1"]], [["Hatter (Alice's Adventures in Wonderland)-1"]], [["Hatter (Alice's Adventures in Wonderland)-5"]], [["Dimercaptosuccinic acid-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Was the name of the character in the movie, \"The Wizard of Oz\" Dorothy?\nFacts:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was milliner in Alice in Wonderland (1951 film) likely in need of succimer?\nFacts: A milliner is someone who makes hats. The character of the Mad Hatter was a milliner in the 1951 Alice in Wonderland film. The phrase, Mad as a Hatter, comes from the fact that hat makers used mercury to line their hats and often suffered mercury poisoning. Succimer is a chemical that is used to treat lead, mercury, and arsenic poisoning.\nAnswer (True or False):", "ct_subject": "A milliner is someone who makes hats. The character of the Mad Hatter was a milliner in the 1951 Alice in Wonderland film. The phrase, Mad as a Hatter, comes from the fact that hat makers used mercury to line their hats and often suffered mercury poisoning. Succimer is a chemical that is used to treat lead, mercury, and arsenic poisoning.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "0.34894386"], [" the", "1.0"], [" name", "0.30024043"], [" of", "1.0"], [" the", "1.0"], [" character", "1.0"], [" in", "0.9998807"], [" the", "1.0"], [" movie", "0.99999475"], [",", "0.9990252"], [" \"", "0.7022717"], ["The", "1.0"], [" Wizard", "1.0"], [" of", "1.0"], [" Oz", "1.0"], ["\"", "1.0"], [" Dorothy", "0.8969233"], ["?", "0.99942005"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"]]}, "670": {"qid": "6c6d1853b7e97e66ef46", "term": "Red hair", "description": "Hair color", "question": "If you have black hair and want red hair, do you need bleach?", "answer": true, "facts": ["You cannot dye hair to be lighter than the starting color.", "To make hair a color lighter than the starting color, you need to bleach the hair."], "decomposition": ["Why would someone need bleach when dying their hair?", "Is red hair #1 than black hair?"], "evidence": [[[["Hair coloring-11"]], [["Hair coloring-11"]]], [[["Bleach-22"], "no_evidence"], [["Red hair-2"], "operation"]], [[["Hair coloring-26"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: If you have a black eye, do you need to see a doctor?\nFacts: You do not need to see a doctor if you have a black eye. If you have a black eye, you should see a doctor if you have any other eye problems.\nAnswer (True or False): True\nQuestion: If you have a black eye", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: If you have black hair and want red hair, do you need bleach?\nFacts: You cannot dye hair to be lighter than the starting color. To make hair a color lighter than the starting color, you need to bleach the hair.\nAnswer (True or False):", "ct_subject": "You cannot dye hair to be lighter than the starting color. To make hair a color lighter than the starting color, you need to bleach the hair.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" If", "0.99999154"], [" you", "1.0"], [" have", "1.0"], [" a", "0.99999857"], [" black", "0.9999888"], [" eye", "1.0"], [",", "0.9980323"], [" do", "0.87982434"], [" you", "1.0"], [" need", "1.0"], [" to", "0.99992526"], [" see", "0.8512547"], [" a", "1.0"], [" doctor", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" You", "0.34212148"], [" do", "0.9999994"], [" not", "1.0"], [" need", "1.0"], [" to", "1.0"], [" see", "1.0"], [" a", "1.0"], [" doctor", "1.0"], [" if", "0.99999857"], [" you", "0.99999964"], [" have", "1.0"], [" a", "1.0"], [" black", "1.0"], [" eye", "1.0"], [".", "1.0"], [" If", "0.96730274"], [" you", "1.0"], [" have", "1.0"], [" a", "1.0"], [" black", "1.0"], [" eye", "1.0"], [",", "0.99999917"], [" you", "1.0"], [" should", "0.8220243"], [" see", "1.0"], [" a", "1.0"], [" doctor", "1.0"], [" if", "0.97235334"], [" you", "1.0"], [" have", "1.0"], [" any", "0.15006775"], [" other", "0.9734336"], [" eye", "1.0"], [" problems", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" If", "1.0"], [" you", "1.0"], [" have", "1.0"], [" a", "1.0"], [" black", "1.0"], [" eye", "0.9999999"]]}, "671": {"qid": "141e0ea8af89ad9c91af", "term": "Management", "description": "Coordinating the efforts of people", "question": "In order to work in district management, does one need a car?", "answer": true, "facts": ["District managers are responsible for supervising many stores within an area.", "District managers must travel to the various stores they supervise to ensure peak performance."], "decomposition": ["What is the main responsibility of district managers?", "In order to do #1 efficiently, is a car needed? "], "evidence": [[["no_evidence"], ["no_evidence", "operation"]], [[["Account manager-1"], "no_evidence"], [["Account manager-9"], "operation"]], [[["District Programme Manager-1"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake with a beard\"?\nFacts: A snowflake is a small, fragile, and fragile snowflake. A snowflake with a beard is a large, solid, and solid snowflake.\nAnswer (True or False): True\nQuestion: What is the difference between a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: In order to work in district management, does one need a car?\nFacts: District managers are responsible for supervising many stores within an area. District managers must travel to the various stores they supervise to ensure peak performance.\nAnswer (True or False):", "ct_subject": "District managers are responsible for supervising many stores within an area. District managers must travel to the various stores they supervise to ensure peak performance.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99997914"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9999132"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.99369603"], ["s", "0.61518085"], ["now", "0.9999865"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], [" with", "0.66147393"], [" a", "0.99989164"], [" beard", "0.9934029"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999999"], [" snow", "0.9999981"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" small", "0.98677397"], [",", "1.0"], [" fragile", "0.58725387"], [",", "1.0"], [" and", "1.0"], [" fragile", "0.2604562"], [" snow", "0.935046"], ["flake", "1.0"], [".", "0.99999845"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" with", "1.0"], [" a", "1.0"], [" beard", "1.0"], [" is", "1.0"], [" a", "1.0"], [" large", "0.99999964"], [",", "1.0"], [" solid", "0.9995697"], [",", "1.0"], [" and", "1.0"], [" solid", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"]]}, "672": {"qid": "2e8e9e51f0f290e32d27", "term": "Celery", "description": "species of plant", "question": "Did any cultures associate celery with death?", "answer": true, "facts": ["Ancient Greeks used garlands of celery leafs to bury their dead.", "Ancient Greece was considered a culture. "], "decomposition": ["What are the various forms of cultural depictions of celery?", "Is any of #1 associated with death?"], "evidence": [[[["Celery-44"]], ["operation"]], [[["Celery-44"]], ["operation"]], [[["Celery-44"]], [["Celery-44"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Romans have a word for \"cabbage\"?\nFacts: The Romans called the plant \"cabbage\" and used it to make a soup. \nAnswer (True or False): True\nQuestion: Did the Romans have a word for \"cabbage\"?\nFacts: The Romans called the plant \"cabbage\" and used it to make a soup. \nAnswer (True or False):", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did any cultures associate celery with death?\nFacts: Ancient Greeks used garlands of celery leafs to bury their dead. Ancient Greece was considered a culture. \nAnswer (True or False):", "ct_subject": "Ancient Greeks used garlands of celery leafs to bury their dead. Ancient Greece was considered a culture. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9983443"], [" the", "0.99998045"], [" Romans", "0.99998796"], [" have", "0.90244395"], [" a", "1.0"], [" word", "0.99999917"], [" for", "1.0"], [" \"", "0.9999999"], ["c", "0.9966107"], ["abbage", "0.9999659"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Romans", "1.0"], [" called", "0.89807254"], [" the", "0.99963725"], [" plant", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["abbage", "1.0"], ["\"", "0.99999976"], [" and", "0.9997756"], [" used", "0.9956601"], [" it", "1.0"], [" to", "0.9999722"], [" make", "1.0"], [" a", "0.9999857"], [" soup", "0.9476563"], [".", "0.99999034"], [" ", "0.9989792"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999995"], [" the", "1.0"], [" Romans", "1.0"], [" have", "1.0"], [" a", "1.0"], [" word", "1.0"], [" for", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["abbage", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Romans", "1.0"], [" called", "1.0"], [" the", "1.0"], [" plant", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["abbage", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" used", "1.0"], [" it", "1.0"], [" to", "1.0"], [" make", "1.0"], [" a", "1.0"], [" soup", "1.0"], [".", "1.0"], [" ", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"]]}, "673": {"qid": "692119e8ebcbf634d224", "term": "Butter", "description": "dairy product", "question": "Would toast for a vegan have margarine instead of butter?", "answer": true, "facts": ["Margarine is typically made without the use of dairy ingredients.", "Vegans do not eat any animal products, including dairy and eggs."], "decomposition": ["Which products are avoided in vegan diet?", "Is margarine free of #1?"], "evidence": [[[["Veganism-1"]], [["Margarine-36"]]], [[["Veganism-1"]], [["Margarine-2"], "operation"]], [[["Veganism-1"]], [["Margarine-2"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"sausage\" and a \"chicken\" in the English language?\nFacts: A sausage is a meat product that is cooked and then sliced. A chicken is a meat product that is cooked and then cooked again.\nAnswer (True or False): True\nQuestion: What is the difference between a \"sausage\" and a \"", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would toast for a vegan have margarine instead of butter?\nFacts: Margarine is typically made without the use of dairy ingredients. Vegans do not eat any animal products, including dairy and eggs.\nAnswer (True or False):", "ct_subject": "Margarine is typically made without the use of dairy ingredients. Vegans do not eat any animal products, including dairy and eggs.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99914646"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9999813"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.99974805"], ["s", "0.65686846"], ["aus", "0.99050385"], ["age", "1.0"], ["\"", "0.99999607"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["ch", "0.9593875"], ["icken", "1.0"], ["\"", "0.99960774"], [" in", "1.0"], [" the", "0.99998796"], [" English", "1.0"], [" language", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.99167836"], [" sausage", "0.9974595"], [" is", "1.0"], [" a", "1.0"], [" meat", "0.999995"], [" product", "0.999998"], [" that", "0.99999547"], [" is", "0.94906497"], [" cooked", "1.0"], [" and", "0.99959046"], [" then", "0.999579"], [" sliced", "0.99282575"], [".", "0.99995875"], [" A", "1.0"], [" chicken", "1.0"], [" is", "1.0"], [" a", "1.0"], [" meat", "1.0"], [" product", "1.0"], [" that", "1.0"], [" is", "1.0"], [" cooked", "1.0"], [" and", "1.0"], [" then", "1.0"], [" cooked", "0.9998091"], [" again", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["aus", "1.0"], ["age", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"]]}, "674": {"qid": "b3c8f537cfb900ba92e5", "term": "Salsa (sauce)", "description": "Sauce", "question": "Would the chef at La Grenouille find salsa to be a strange request?", "answer": true, "facts": ["La Grenouille is a classic French cuisine restaurant in NYC.", "Salsa is a staple food in Mexican cuisine."], "decomposition": ["What type of cuisine does La Grenouille serve?", "Would you typically find salsa in #1?"], "evidence": [[[["La Grenouille (restaurant)-3"], "operation"], ["no_evidence"]], [[["La Grenouille (restaurant)-1"]], [["Mexican cuisine-28"]]], [[["La Grenouille (restaurant)-3"]], [["La Grenouille (restaurant)-1", "Salsa-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"salt and pepper\" and a \"salt and pepper shaker\"?\nFacts: The salt and pepper shaker is a small container that holds a small amount of salt and pepper. The salt and pepper shaker is used to make a small amount of salt and pepper.\nAnswer (True or False): True\nQuestion: What is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would the chef at La Grenouille find salsa to be a strange request?\nFacts: La Grenouille is a classic French cuisine restaurant in NYC. Salsa is a staple food in Mexican cuisine.\nAnswer (True or False):", "ct_subject": "La Grenouille is a classic French cuisine restaurant in NYC. Salsa is a staple food in Mexican cuisine.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999785"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.7650866"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.9931044"], ["alt", "0.98812973"], [" and", "0.99903584"], [" pepper", "1.0"], ["\"", "0.5672101"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["alt", "1.0"], [" and", "1.0"], [" pepper", "0.031354003"], [" sh", "0.99640226"], ["aker", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.50995874"], [" salt", "0.9996191"], [" and", "0.99999774"], [" pepper", "1.0"], [" sh", "1.0"], ["aker", "1.0"], [" is", "1.0"], [" a", "0.9998709"], [" small", "0.99654776"], [" container", "0.065032266"], [" that", "0.9999989"], [" holds", "0.9999776"], [" a", "1.0"], [" small", "1.0"], [" amount", "1.0"], [" of", "1.0"], [" salt", "1.0"], [" and", "1.0"], [" pepper", "0.9999783"], [".", "1.0"], [" The", "1.0"], [" salt", "1.0"], [" and", "1.0"], [" pepper", "1.0"], [" sh", "1.0"], ["aker", "1.0"], [" is", "1.0"], [" used", "1.0"], [" to", "1.0"], [" make", "0.5188527"], [" a", "1.0"], [" small", "0.9979899"], [" amount", "1.0"], [" of", "1.0"], [" salt", "0.8829432"], [" and", "1.0"], [" pepper", "1.0"], [".", "0.9982253"], ["\n", "0.9999999"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"]]}, "675": {"qid": "979b4b0fa0a8606bfcae", "term": "Lecturer", "description": "tenure-track or tenured position at a university or similar institution", "question": "Would Quiet from Metal Gear be a poor hypothetical choice for lecturer at Haub?", "answer": true, "facts": ["Quiet is an assassin from the Metal Gear video game series that does not speak.", "Haub is a school at Pace University that has annual lectures.", "Haub is a law school that has annual lectures on topics in the law field."], "decomposition": ["Who is Quiet?", "What is #1 unable to do?", "How does one convey information as a lecturer?", "Is #2 the same as #3?"], "evidence": [[[["Quiet (Metal Gear)-1"]], [["Quiet (Metal Gear)-5"]], [["Lecture-1"]], ["operation"]], [[["Quiet (Metal Gear)-1"]], [["Quiet (Metal Gear)-7"]], [["Lecture-1"]], ["operation"]], [[["Quiet (Metal Gear)-1"]], [["Quiet (Metal Gear)-7"]], [["Lecture-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the character in the movie \"The Matrix\" that is the main character?\nFacts: Neo.\nAnswer (True or False): True\nQuestion: What is the name of the character in the movie \"The Matrix\" that is the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Quiet from Metal Gear be a poor hypothetical choice for lecturer at Haub?\nFacts: Quiet is an assassin from the Metal Gear video game series that does not speak. Haub is a school at Pace University that has annual lectures. Haub is a law school that has annual lectures on topics in the law field.\nAnswer (True or False):", "ct_subject": "Quiet is an assassin from the Metal Gear video game series that does not speak. Haub is a school at Pace University that has annual lectures. Haub is a law school that has annual lectures on topics in the law field.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999404"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99999976"], [" of", "1.0"], [" the", "1.0"], [" character", "0.0104109775"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "1.0"], ["The", "1.0"], [" Matrix", "0.9906583"], ["\"", "0.95603395"], [" that", "0.86192"], [" is", "0.99999976"], [" the", "0.9999645"], [" main", "1.0"], [" character", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Neo", "1.0"], [".", "0.70003515"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" character", "1.0"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "1.0"], ["The", "1.0"], [" Matrix", "1.0"], ["\"", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"]]}, "676": {"qid": "1b6fbe4fe4254a8e6f6e", "term": "Ammonia", "description": "Chemical compound of nitrogen and hydrogen", "question": "Is an ammonia fighting cleaner good for pet owners?", "answer": true, "facts": ["Ammonia is a component in pet urine.", "Ammonia has a very pungent and unpleasant odor."], "decomposition": ["What unsanitary substances contain ammonia?", "Is animal waste included in #1?"], "evidence": [[[["Ammonia-32"]], ["operation"]], [[["Ammonia-32"]], [["Urination-3"]]], [[["Ammonia-32"]], [["Ammonia-32"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it true that the first person to use the word \"candy\" was a man named John Candy?\nFacts: John Candy was a candy maker. He was the first person to use the word \"candy\" in a commercial.\nAnswer (True or False): True\nQuestion: Is it true that the first person to use the word \"candy\" was a man named John Candy?", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is an ammonia fighting cleaner good for pet owners?\nFacts: Ammonia is a component in pet urine. Ammonia has a very pungent and unpleasant odor.\nAnswer (True or False):", "ct_subject": "Ammonia is a component in pet urine. Ammonia has a very pungent and unpleasant odor.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999664"], [" it", "0.9999082"], [" true", "1.0"], [" that", "1.0"], [" the", "1.0"], [" first", "0.99989116"], [" person", "0.99140656"], [" to", "1.0"], [" use", "0.9976338"], [" the", "0.9999896"], [" word", "1.0"], [" \"", "1.0"], ["c", "0.999476"], ["andy", "0.99999714"], ["\"", "1.0"], [" was", "1.0"], [" a", "1.0"], [" man", "0.9223593"], [" named", "1.0"], [" John", "0.99993074"], [" Candy", "0.99999404"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "0.99999857"], [" Candy", "1.0"], [" was", "1.0"], [" a", "0.999998"], [" candy", "0.9999988"], [" maker", "0.99999976"], [".", "0.99991035"], [" He", "0.3032802"], [" was", "1.0"], [" the", "0.0024595975"], [" first", "1.0"], [" person", "0.9996592"], [" to", "1.0"], [" use", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"", "0.9999999"], [" in", "0.9859292"], [" a", "0.9464463"], [" commercial", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" true", "1.0"], [" that", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" use", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"", "1.0"], [" was", "1.0"], [" a", "1.0"], [" man", "1.0"], [" named", "1.0"], [" John", "1.0"], [" Candy", "1.0"], ["?", "1.0"]]}, "677": {"qid": "38819f5a64ab543fbd90", "term": "Isaac", "description": "Biblical character", "question": "Did Isaac's father almost commit similar crime as Marvin Gay Sr.?", "answer": true, "facts": ["Filicide is the act of killing a son or a daughter.", "Marvin Gay Sr. committed filicide in 1984 when he shot his son, singer Marvin Gaye.", "Isaac's father Abraham, was commanded by God to sacrifice his son Isaac, but was spared by an angel."], "decomposition": ["What crime did Marvin Gay Sr commit?", "Was the Biblical Abraham going to commit #1?"], "evidence": [[[["Marvin Gaye-36"]], [["Binding of Isaac-3"]]], [[["Marvin Gay Sr.-1"]], [["Binding of Isaac-1"]]], [[["Marvin Gay Sr.-7"], "operation"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did Abraham's son Isaac have a son named Ishmael?\nFacts: Isaac had a son named Ishmael. Ishmael was the son of Abraham's son Isaac.\nAnswer (True or False): True\nQuestion", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Isaac's father almost commit similar crime as Marvin Gay Sr.?\nFacts: Filicide is the act of killing a son or a daughter. Marvin Gay Sr. committed filicide in 1984 when he shot his son, singer Marvin Gaye. Isaac's father Abraham, was commanded by God to sacrifice his son Isaac, but was spared by an angel.\nAnswer (True or False):", "ct_subject": "Filicide is the act of killing a son or a daughter. Marvin Gay Sr. committed filicide in 1984 when he shot his son, singer Marvin Gaye. Isaac's father Abraham, was commanded by God to sacrifice his son Isaac, but was spared by an angel.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99997723"], [" Abraham", "0.9999912"], ["'s", "1.0"], [" son", "1.0"], [" Isaac", "1.0"], [" have", "1.0"], [" a", "1.0"], [" son", "0.9990827"], [" named", "1.0"], [" Is", "0.9999969"], ["hma", "1.0"], ["el", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Isaac", "0.9934354"], [" had", "0.968345"], [" a", "0.9845383"], [" son", "1.0"], [" named", "1.0"], [" Is", "1.0"], ["hma", "1.0"], ["el", "1.0"], [".", "0.99989843"], [" Is", "1.0"], ["hma", "1.0"], ["el", "1.0"], [" was", "1.0"], [" the", "0.999969"], [" son", "1.0"], [" of", "1.0"], [" Abraham", "0.9524913"], ["'s", "0.99999917"], [" son", "0.99997103"], [" Isaac", "1.0"], [".", "1.0"], ["\n", "0.9998473"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"]]}, "678": {"qid": "3295844627a9bd1b9135", "term": "The Matrix", "description": "1999 science fiction action film directed by the Wachowskis", "question": "Was Harry Potter a better investment than The Matrix for Warner Bros.?", "answer": true, "facts": ["Warner Bros. distributes several movie franchises including The Matrix, Harry Potter, and The Dark Knight.", "The Matrix had 2 sequels.", "Harry Potter had 7 sequels and several spin-offs.", "Harry Potter and the Deathly Hallows \u2013 Part 2 is Warner Bros. highest grossing film worldwide with a box office gross of $1,342,932,398."], "decomposition": ["How much did the Harry Potter (film series) gross?", "How much did the The Matrix (franchise) gross?", "Is #1 greater than #2?"], "evidence": [[[["Harry Potter (film series)-4"]], [["The Matrix (franchise)-4"]], ["operation"]], [[["Harry Potter (film series)-4"]], [["The Matrix (franchise)-4"]], ["operation"]], [[["Harry Potter-3"], "no_evidence"], [["The Matrix-36"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the year 2000?\nFacts: The year 2000 was the first year that the year", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Harry Potter a better investment than The Matrix for Warner Bros.?\nFacts: Warner Bros. distributes several movie franchises including The Matrix, Harry Potter, and The Dark Knight. The Matrix had 2 sequels. Harry Potter had 7 sequels and several spin-offs. Harry Potter and the Deathly Hallows \u2013 Part 2 is Warner Bros. highest grossing film worldwide with a box office gross of $1,342,932,398.\nAnswer (True or False):", "ct_subject": "Warner Bros. distributes several movie franchises including The Matrix, Harry Potter, and The Dark Knight. The Matrix had 2 sequels. Harry Potter had 7 sequels and several spin-offs. Harry Potter and the Deathly Hallows \u2013 Part 2 is Warner Bros. highest grossing film worldwide with a box office gross of $1,342,932,398.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99821293"], [" is", "1.0"], [" the", "1.0"], [" name", "0.8103384"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9986595"], [" person", "0.9970745"], [" to", "1.0"], [" be", "0.91350853"], [" born", "0.9877579"], [" in", "1.0"], [" the", "1.0"], [" year", "0.99997795"], [" 2000", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" year", "0.72545415"], [" 2000", "1.0"], [" was", "0.99999964"], [" the", "1.0"], [" first", "0.9986957"], [" year", "1.0"], [" that", "0.9999908"], [" the", "0.9995963"], [" year", "1.0"]]}, "679": {"qid": "87acce77a8b6362f4f96", "term": "Toyota Supra", "description": "A sports car and grand tourer manufactured by Toyota Motor Corporation", "question": "Would 2020 Toyota Supra lag behind at a Nascar rally?", "answer": true, "facts": ["The 2020 Toyota Supra has a top speed of 155 MPH.", "Nascar stock cars routinely exceed 200 MPH."], "decomposition": ["What speeds do stock cars in a NASCAR race routinely attain?", "What is the top speed of a Toyota Supra?", "Is #2 less than #1?"], "evidence": [[[["Stock car racing-3"]], [["Toyota Supra-61"]], ["operation"]], [[["Stock car racing-2"]], [["Toyota Supra-77"]], [["Stock car racing-2", "Toyota Supra-77"]]], [[["Stock car racing-65"]], [["Toyota Supra-61"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human?\nFacts: The average human is about 6'2\" tall.\nAnswer (True or False): True\nQuestion: What is the average weight of a horse?\nFacts: The average horse is about 1,000 pounds.\nAnswer (True or False): True\nQuestion: What is the average weight of a cow?\nFacts", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would 2020 Toyota Supra lag behind at a Nascar rally?\nFacts: The 2020 Toyota Supra has a top speed of 155 MPH. Nascar stock cars routinely exceed 200 MPH.\nAnswer (True or False):", "ct_subject": "The 2020 Toyota Supra has a top speed of 155 MPH. Nascar stock cars routinely exceed 200 MPH.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99978954"], [" is", "1.0"], [" the", "1.0"], [" average", "0.99876714"], [" weight", "0.9988501"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], ["?", "0.9422383"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.9999685"], [" is", "0.59253204"], [" about", "0.9987276"], [" 6", "0.99336725"], ["'", "1.0"], ["2", "0.9999325"], ["\"", "1.0"], [" tall", "0.99999964"], [".", "0.99266464"], ["\n", "0.99999905"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "0.99993646"], [" of", "1.0"], [" a", "1.0"], [" horse", "0.55871326"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" horse", "0.9997235"], [" is", "0.9997979"], [" about", "1.0"], [" 1", "0.9862943"], [",", "1.0"], ["000", "0.9975545"], [" pounds", "0.9996536"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "0.99999833"], [" of", "1.0"], [" a", "1.0"], [" cow", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"]]}, "680": {"qid": "d717f5321d1e1d3d5e37", "term": "Attack on Pearl Harbor", "description": "Surprise attack by the Imperial Japanese Navy on the U.S. Pacific Fleet in Pearl Harbor in Hawaii", "question": "Did the Pearl Harbor attack occur during autumn?", "answer": true, "facts": ["Autumn runs from about September 20 to about December 20.", "Pearl Harbor was attacked on December 7, 1941."], "decomposition": ["When did the Pearl Harbor attack happen?", "Where is Pearl Harbor located?", "When is it autumn in #2?", "Does #1 fall within the range of #3?"], "evidence": [[[["Attack on Pearl Harbor-13"]], [["Pearl Harbor-1"]], [["Autumn-1"], "no_evidence"], ["operation"]], [[["Attack on Pearl Harbor-1"]], [["Pearl Harbor-1"]], [["Climate of Hawaii-11"], "no_evidence"], ["no_evidence", "operation"]], [[["Attack on Pearl Harbor-1"]], [["Pearl Harbor-1"]], [["Autumn-3"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city of Rome?\nFacts: Rome is the capital of the Roman Empire.\nAnswer (True or False): True\nQuestion: What is the name of the city of Rome?\nFacts: Rome is the capital of the Roman Empire.\nAnswer (True or False): True\nQuestion: What is the name of the city of Rome?\nFacts: Rome is the capital", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did the Pearl Harbor attack occur during autumn?\nFacts: Autumn runs from about September 20 to about December 20. Pearl Harbor was attacked on December 7, 1941.\nAnswer (True or False):", "ct_subject": "Autumn runs from about September 20 to about December 20. Pearl Harbor was attacked on December 7, 1941.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9842479"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999989"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9995907"], [" of", "0.9999987"], [" Rome", "0.9437886"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "1.0"], [" is", "0.9999994"], [" the", "0.99998844"], [" capital", "0.9999964"], [" of", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"]]}, "681": {"qid": "d8bed090b4755e2f7b67", "term": "Knight", "description": "An award of an honorary title for past or future service with its roots in chivalry in the Middle Ages", "question": "Are the knights in the Medieval Times show not authentic knights?", "answer": true, "facts": ["The Medieval Times show is popular in the United States.", "The United States does not actually bestow knighthood on its citizens.", "Authentic knights are citizens of certain countries, mainly the United Kingdom, who are given the honorary title by their country."], "decomposition": ["Which country is the Medieval Times show hosted in?", "Does #1 not confer knighthood on its citizen?"], "evidence": [[[["Medieval Times-2"]], ["no_evidence"]], [[["Medieval Times-2"]], [["Order of the British Empire-12", "Order of the British Empire-3"], "operation"]], [[["Medieval Times-1"]], [["Knight-1"], "no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake's\"?\nFacts: A snowflake is a small, round, and soft, often white, crystal. A snowflake's is a larger, round, and hard,", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are the knights in the Medieval Times show not authentic knights?\nFacts: The Medieval Times show is popular in the United States. The United States does not actually bestow knighthood on its citizens. Authentic knights are citizens of certain countries, mainly the United Kingdom, who are given the honorary title by their country.\nAnswer (True or False):", "ct_subject": "The Medieval Times show is popular in the United States. The United States does not actually bestow knighthood on its citizens. Authentic knights are citizens of certain countries, mainly the United Kingdom, who are given the honorary title by their country.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.43591025"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9548478"], [" between", "1.0"], [" a", "0.99933004"], [" \"", "0.9999994"], ["s", "0.1722148"], ["now", "0.9990393"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["'s", "0.50494343"], ["\"", "0.8859392"], ["?", "0.99999535"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.99998116"], [" snow", "0.15159409"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" small", "0.99936324"], [",", "1.0"], [" round", "0.99952185"], [",", "1.0"], [" and", "0.9979373"], [" soft", "0.35061237"], [",", "0.9708355"], [" often", "0.8636324"], [" white", "0.9999994"], [",", "1.0"], [" crystal", "0.9203511"], [".", "1.0"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], ["'s", "1.0"], [" is", "1.0"], [" a", "1.0"], [" larger", "0.99851054"], [",", "1.0"], [" round", "1.0"], [",", "1.0"], [" and", "1.0"], [" hard", "1.0"], [",", "1.0"]]}, "682": {"qid": "21dd2c3906362e8860d3", "term": "Macbeth", "description": "play by William Shakespeare", "question": "Would costumes with robes and pointy hats be helpful for Macbeth?", "answer": true, "facts": ["Macbeth features scenes with three witches throughout the play. ", "Witches are often displayed with pointy hats and long black robes."], "decomposition": ["What characters are in Macbeth?", "What characters wear pointy hats and robes?", "Would any of #1 wear #2?"], "evidence": [[[["Macbeth-2"], "no_evidence"], [["Cloak-10", "Pointed hat-5"]], ["operation"]], [[["Macbeth-5"]], ["no_evidence"], ["operation"]], [[["Macbeth-2"]], [["Witch hat-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the character in the movie \"The Wizard of Oz\"?\nFacts: The Wizard of Oz is a story about a boy who is transported to a land of magic. The boy is named Dorothy.\nAnswer (True or False): True\nQuestion: What is the name of the character in the movie \"The Wizard of Oz\"?\nFacts", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would costumes with robes and pointy hats be helpful for Macbeth?\nFacts: Macbeth features scenes with three witches throughout the play.  Witches are often displayed with pointy hats and long black robes.\nAnswer (True or False):", "ct_subject": "Macbeth features scenes with three witches throughout the play.  Witches are often displayed with pointy hats and long black robes.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999976"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999927"], [" of", "1.0"], [" the", "1.0"], [" character", "1.0"], [" in", "1.0"], [" the", "0.9999819"], [" movie", "0.99079365"], [" \"", "1.0"], ["The", "1.0"], [" Wizard", "1.0"], [" of", "1.0"], [" Oz", "1.0"], ["\"?", "0.5265634"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Wizard", "0.99999845"], [" of", "1.0"], [" Oz", "1.0"], [" is", "1.0"], [" a", "0.99999976"], [" story", "0.99999976"], [" about", "1.0"], [" a", "1.0"], [" boy", "0.9998977"], [" who", "0.9999988"], [" is", "0.9999975"], [" transported", "0.9896708"], [" to", "1.0"], [" a", "1.0"], [" land", "0.7025885"], [" of", "1.0"], [" magic", "0.99999976"], [".", "0.56100065"], [" The", "0.99985206"], [" boy", "0.99999285"], [" is", "0.9999815"], [" named", "1.0"], [" Dorothy", "0.9999912"], [".", "0.99999845"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" character", "1.0"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "1.0"], ["The", "1.0"], [" Wizard", "1.0"], [" of", "1.0"], [" Oz", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"]]}, "683": {"qid": "0e4f1dbccd54ac5b8c32", "term": "Lord Voldemort", "description": "Fictional character of Harry Potter series", "question": "Was Lord Voldemort taught by Professor Dumbledore?", "answer": true, "facts": ["Lord Voldemort grew up as the orphan boy Tom Riddle.", "Tom Riddle was brought to Hogwarts by Professor Dumbledore.", "Dumbledore was one of Riddle's teachers during his time as a student."], "decomposition": ["Who did Lord Voldemort grow up as?", "Where was #1 brought to by a professor as a child?", "Was Professor Dumbledore a teacher at #2 when Tom Riddle was there?"], "evidence": [[[["Lord Voldemort-23"]], [["Lord Voldemort-12"]], ["operation"]], [[["Lord Voldemort-5"]], [["Lord Voldemort-12"]], [["Lord Voldemort-12"]]], [[["Lord Voldemort-1"]], [["Lord Voldemort-12"]], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Death Eaters have a leader?\nFacts: The Death Eaters were led by Voldemort.\nAnswer (True or False): True\nQuestion: Did the Death Eaters have a leader?\nFacts: The Death Eaters were led by Voldemort.\nAnswer (True or False): True\nQuestion: Did the Death E", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Lord Voldemort taught by Professor Dumbledore?\nFacts: Lord Voldemort grew up as the orphan boy Tom Riddle. Tom Riddle was brought to Hogwarts by Professor Dumbledore. Dumbledore was one of Riddle's teachers during his time as a student.\nAnswer (True or False):", "ct_subject": "Lord Voldemort grew up as the orphan boy Tom Riddle. Tom Riddle was brought to Hogwarts by Professor Dumbledore. Dumbledore was one of Riddle's teachers during his time as a student.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.98902434"], [" the", "0.9999995"], [" Death", "0.6002178"], [" E", "1.0"], ["aters", "1.0"], [" have", "0.99978024"], [" a", "0.9999993"], [" leader", "0.99999213"], ["?", "0.9999726"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999999"], [" Death", "1.0"], [" E", "1.0"], ["aters", "1.0"], [" were", "0.99968135"], [" led", "0.9999993"], [" by", "1.0"], [" Voldemort", "0.99966145"], [".", "1.0"], ["\n", "0.63326854"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99998224"], [" the", "1.0"], [" Death", "1.0"], [" E", "1.0"], ["aters", "1.0"], [" have", "1.0"], [" a", "1.0"], [" leader", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Death", "1.0"], [" E", "1.0"], ["aters", "1.0"], [" were", "1.0"], [" led", "1.0"], [" by", "1.0"], [" Voldemort", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" Death", "1.0"], [" E", "1.0"]]}, "684": {"qid": "616e4f23506bcd9c6ef1", "term": "Satanism", "description": "group of ideological and philosophical beliefs based on Satan", "question": "Is Christianity better for global warming than Satanism?", "answer": true, "facts": ["Global warming happens because CO2 is released into the atmosphere and warms it up.", "Humans release two tons of carbon dioxide a year.", "Reducing the population will lead to a reduction of CO2 in the atmosphere.", "The Christian Crusades caused the deaths of nearly three million people.", "In 1980s and 1990s there were allegations of Satanic ritual deaths, though only a few cases were substantiated."], "decomposition": ["How much population reduction is needed for there to be an impact on carbon dioxide levels in the atmosphere?", "How many people have been killed in the name of Christianity?", "How many people have been killed in the name of Satanism?", "Is #2 closer to #1 than #3 is?"], "evidence": [[[["Carbon footprint-18"], "no_evidence"], [["Christianity and violence-25", "Crusades-1", "Persecution of Muslims-33"], "no_evidence"], [["Satanic ritual abuse-32"], "no_evidence"], ["operation"]], [[["Global warming-32", "Global warming-54"], "no_evidence"], [["Crusades-1"], "no_evidence"], [["Satanism-79"], "no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the Bible the only book that is true?\nFacts: The Bible is the only book that is true. The Bible is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Christianity better for global warming than Satanism?\nFacts: Global warming happens because CO2 is released into the atmosphere and warms it up. Humans release two tons of carbon dioxide a year. Reducing the population will lead to a reduction of CO2 in the atmosphere. The Christian Crusades caused the deaths of nearly three million people. In 1980s and 1990s there were allegations of Satanic ritual deaths, though only a few cases were substantiated.\nAnswer (True or False):", "ct_subject": "Global warming happens because CO2 is released into the atmosphere and warms it up. Humans release two tons of carbon dioxide a year. Reducing the population will lead to a reduction of CO2 in the atmosphere. The Christian Crusades caused the deaths of nearly three million people. In 1980s and 1990s there were allegations of Satanic ritual deaths, though only a few cases were substantiated.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "0.99999666"], [" Bible", "0.99563843"], [" the", "0.94937307"], [" only", "0.96834457"], [" book", "0.64616007"], [" that", "0.9999784"], [" is", "0.9998859"], [" true", "0.9999989"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Bible", "1.0"], [" is", "1.0"], [" the", "1.0"], [" only", "1.0"], [" book", "1.0"], [" that", "1.0"], [" is", "1.0"], [" true", "1.0"], [".", "1.0"], [" The", "0.66301787"], [" Bible", "1.0"], [" is", "1.0"]]}, "685": {"qid": "c72aa50ebf75e16dfac5", "term": "Purple", "description": "Range of colors with the hues between blue and red", "question": "Were mollusks an ingredient in the color purple?", "answer": true, "facts": ["Mollusks are a group of invertebrate animals including snails and slugs.", "Purple dye was used in the early 15th century BC to color clothing.", "The purple dye used in ancient Tyre was made from a liquid extracted from a gland in snails."], "decomposition": ["Which animal was used in making purple glands in ancient Tyre?", "Is #1 a mollusk?"], "evidence": [[[["Tyrian purple-1"]], [["Sea snail-1"], "operation"]], [[["Tyrian purple-1"]], [["Sea snail-1"]]], [[["Purple-5"]], [["Bolinus brandaris-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city of Rome?\nFacts: Rome was the capital of the Roman Empire. The city was founded by Julius Caesar in 43 BC.\nAnswer (True or False): True\nQuestion: What is the name", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Were mollusks an ingredient in the color purple?\nFacts: Mollusks are a group of invertebrate animals including snails and slugs. Purple dye was used in the early 15th century BC to color clothing. The purple dye used in ancient Tyre was made from a liquid extracted from a gland in snails.\nAnswer (True or False):", "ct_subject": "Mollusks are a group of invertebrate animals including snails and slugs. Purple dye was used in the early 15th century BC to color clothing. The purple dye used in ancient Tyre was made from a liquid extracted from a gland in snails.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9102496"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9914512"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9807"], [" of", "0.9999987"], [" Rome", "0.9999999"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "0.9997485"], [" was", "0.2545611"], [" the", "0.99999714"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [".", "0.9999597"], [" The", "0.99222344"], [" city", "0.81072474"], [" was", "1.0"], [" founded", "0.99999654"], [" by", "0.44438395"], [" Julius", "1.0"], [" Caesar", "1.0"], [" in", "1.0"], [" 43", "0.0610417"], [" BC", "1.0"], [".", "1.0"], ["\n", "0.99897647"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"]]}, "686": {"qid": "8ce466f6f1430a309edf", "term": "Yeti", "description": "Folkloric ape-like creature from Asia", "question": "Would a Yeti be likely to have prehensile limbs?", "answer": true, "facts": ["The animals that Yetis are said to look similar to are able to use their hands or toes to grasp items", "The ability to grasp with hands or other limbs is to be prehensile. "], "decomposition": ["What does it mean to be prehensile?", "What animals are Yetis said to look like?", "Would #2 be considered #1?"], "evidence": [[[["Prehensile feet-1"]], [["Yeti-28"]], ["operation"]], [[["Prehensile feet-1"]], [["Yeti-4"]], [["Yeti-4"]]], [[["Prehensility-1"]], [["Yeti-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance a human has ever traveled?\nFacts: The longest distance a human has ever traveled is about 1,000 miles.\nAnswer (True or False): True\nQuestion: What is the longest distance a human has ever traveled?\nFacts: The longest distance a human has ever traveled", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a Yeti be likely to have prehensile limbs?\nFacts: The animals that Yetis are said to look similar to are able to use their hands or toes to grasp items The ability to grasp with hands or other limbs is to be prehensile. \nAnswer (True or False):", "ct_subject": "The animals that Yetis are said to look similar to are able to use their hands or toes to grasp items The ability to grasp with hands or other limbs is to be prehensile. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999002"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.68065715"], [" distance", "0.99999917"], [" a", "0.9996928"], [" human", "0.80591583"], [" has", "0.036748864"], [" ever", "0.99946886"], [" traveled", "0.9986193"], ["?", "0.9999988"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" human", "1.0"], [" has", "1.0"], [" ever", "1.0"], [" traveled", "1.0"], [" is", "1.0"], [" about", "0.9999896"], [" 1", "0.9979504"], [",", "0.9999918"], ["000", "0.99999404"], [" miles", "1.0"], [".", "1.0"], ["\n", "0.9999391"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "0.9999999"], [" a", "1.0"], [" human", "1.0"], [" has", "1.0"], [" ever", "1.0"], [" traveled", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" human", "1.0"], [" has", "1.0"], [" ever", "1.0"], [" traveled", "1.0"]]}, "687": {"qid": "0d6eda5f7e8e806c4d78", "term": "Asteroid", "description": "Minor planet that is not a comet", "question": "Can an asteroid be linked with virginity?", "answer": true, "facts": ["An asteroid discovered in 1807 was named Vesta", "Vesta is the Roman virgin goddess of hearth, home and family"], "decomposition": ["What was the name of the asteroid that was discovered in 1807?", "What did #1 stand for as a Roman goddess?", "Is #2 related to virginity?"], "evidence": [[[["4 Vesta-1"]], [["Vesta (mythology)-1"]], [["Vesta (mythology)-8"]]], [[["4 Vesta-1"]], [["Vesta (mythology)-1"]], ["operation"]], [[["4 Vesta-1"]], [["Vesta (mythology)-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first man to be born in the United States?\nFacts: John Smith\nAnswer (True or False): True\nQuestion: What is the name of the first man to be born in the United States?\nFacts: John Smith\nAnswer (True or False): True\nQuestion: What is the name of the first man to be born in the United States?\nF", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can an asteroid be linked with virginity?\nFacts: An asteroid discovered in 1807 was named Vesta Vesta is the Roman virgin goddess of hearth, home and family\nAnswer (True or False):", "ct_subject": "An asteroid discovered in 1807 was named Vesta Vesta is the Roman virgin goddess of hearth, home and family", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.98859626"], [" is", "1.0"], [" the", "1.0"], [" name", "0.93218416"], [" of", "1.0"], [" the", "1.0"], [" first", "0.99974877"], [" man", "0.7515711"], [" to", "0.96437275"], [" be", "0.769549"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "0.99942446"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "0.99997973"], [" Smith", "1.0"], ["\n", "0.96956563"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "0.998982"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "1.0"], [" Smith", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"]]}, "688": {"qid": "f778fcf4fce27aa1f7ae", "term": "Common Era", "description": "alternative (and religiously neutral) naming of the traditional calendar era, Anno Domini", "question": "Is entire Common Era minuscule to lifespan of some trees?", "answer": true, "facts": ["The Common Era has lasted for over 2,000 years as of 2020.", "A tree named Methuselah, from California's White Mountains, is almost 5,000 years old."], "decomposition": ["How long has the Common Era lasted?", "How old is the oldest known tree?", "Is #2 greater than #1?"], "evidence": [[[["Common Era-1"]], [["Methuselah (tree)-3"]], [["Methuselah (tree)-3"], "operation"]], [[["21st century-1"]], [["Methuselah (tree)-1"]], ["operation"]], [[["Common Era-1"], "no_evidence"], [["Pinus longaeva-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the Earth flat?\nFacts: The Earth is round.\nAnswer (True or False): True\nQuestion: Is the Earth round?\nFacts: The Earth is flat.\nAnswer (True or False): True\nQuestion: Is the Earth flat?\nFacts: The Earth is round.\nAnswer", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is entire Common Era minuscule to lifespan of some trees?\nFacts: The Common Era has lasted for over 2,000 years as of 2020. A tree named Methuselah, from California's White Mountains, is almost 5,000 years old.\nAnswer (True or False):", "ct_subject": "The Common Era has lasted for over 2,000 years as of 2020. A tree named Methuselah, from California's White Mountains, is almost 5,000 years old.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999858"], [" the", "0.9772257"], [" Earth", "1.0"], [" flat", "0.9674164"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" round", "0.9913469"], [".", "1.0"], ["\n", "0.9999989"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" Earth", "1.0"], [" round", "0.9896908"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" flat", "0.998579"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" Earth", "1.0"], [" flat", "0.99999297"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" round", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"]]}, "689": {"qid": "1969318e30a31e49e5ca", "term": "Chinook salmon", "description": "species of fish", "question": "Could eating Chinook salmon help Ryan Reynolds?", "answer": true, "facts": ["Chinook salmon is high in omega-3 fatty acids.", "Omega-3 fatty acids can aid treatment of depression. ", "Ryan Reynolds has struggled with depression."], "decomposition": ["What mental disorder did Ryan Reynolds suffer from?", "What nutrient may be able to aid in treatment of #1?", "Is chinook salmon high in #2?"], "evidence": [[[["Ryan Reynolds-24"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Ryan Reynolds-24"]], [["Anxiety disorder-29", "Anxiety-58"], "no_evidence"], [["Chinook salmon-2"], "no_evidence"]], [[["Ryan Reynolds-24"]], [["Generalized anxiety disorder-20"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake's eye\"?\nFacts: A snowflake is a small, round, and shiny object. A snowflake's eye is a small, round, and shiny object.\nAnswer (True or False): True\nQuestion: What is the difference between a \"snowflake\" and a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could eating Chinook salmon help Ryan Reynolds?\nFacts: Chinook salmon is high in omega-3 fatty acids. Omega-3 fatty acids can aid treatment of depression.  Ryan Reynolds has struggled with depression.\nAnswer (True or False):", "ct_subject": "Chinook salmon is high in omega-3 fatty acids. Omega-3 fatty acids can aid treatment of depression.  Ryan Reynolds has struggled with depression.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99981946"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.18458195"], [" between", "1.0"], [" a", "0.99999857"], [" \"", "1.0"], ["s", "0.904464"], ["now", "0.9999957"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.99978703"], ["now", "1.0"], ["flake", "1.0"], ["'s", "0.10246781"], [" eye", "0.83163124"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.99961936"], [" snow", "0.9999999"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" small", "0.74085134"], [",", "1.0"], [" round", "0.99989915"], [",", "1.0"], [" and", "0.9999889"], [" shiny", "0.9995913"], [" object", "0.99996376"], [".", "0.9999999"], [" A", "0.99999905"], [" snow", "1.0"], ["flake", "1.0"], ["'s", "1.0"], [" eye", "1.0"], [" is", "1.0"], [" a", "1.0"], [" small", "0.9948348"], [",", "1.0"], [" round", "1.0"], [",", "1.0"], [" and", "1.0"], [" shiny", "1.0"], [" object", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"]]}, "690": {"qid": "35cc615c2a3356d8ba26", "term": "Depression (mood)", "description": "state of low mood and fatigue", "question": "In teenagers and young adults with depression, are SSRI medications less safe than they are for adults?", "answer": true, "facts": ["In teens, SSRI medications may increase the risk of suicidal thinking.", "In adults over 25, SSRI medications are regarded as generally safe."], "decomposition": ["What are potential side effects of SSRIs for adults?", "What are the potential side effects of SSRIs for teenagers?", "Are the hazards in #2 worse than the hazards in #1?"], "evidence": [[[["Selective serotonin reuptake inhibitor-37"]], [["Selective serotonin reuptake inhibitor-33"]], ["operation"]], [[["Development and discovery of SSRI drugs-10"], "no_evidence"], [["Development and discovery of SSRI drugs-10"], "no_evidence"], ["no_evidence"]], [[["Selective serotonin reuptake inhibitor-21"], "no_evidence"], [["Selective serotonin reuptake inhibitor-33"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average age of death for a person who has been buried in a cemetery?\nFacts: The average age of death for a person who has been buried in a cemetery is between 80 and 90 years old.\nAnswer (True or False): True\nQuestion: What is the average age of death for a person who has", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: In teenagers and young adults with depression, are SSRI medications less safe than they are for adults?\nFacts: In teens, SSRI medications may increase the risk of suicidal thinking. In adults over 25, SSRI medications are regarded as generally safe.\nAnswer (True or False):", "ct_subject": "In teens, SSRI medications may increase the risk of suicidal thinking. In adults over 25, SSRI medications are regarded as generally safe.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.89402574"], [" is", "1.0"], [" the", "1.0"], [" average", "0.9763549"], [" age", "0.99811804"], [" of", "1.0"], [" death", "0.9990996"], [" for", "0.9999975"], [" a", "0.9999999"], [" person", "1.0"], [" who", "1.0"], [" has", "0.7458541"], [" been", "0.99011374"], [" buried", "0.95132715"], [" in", "0.99997663"], [" a", "0.9948592"], [" cemetery", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" age", "1.0"], [" of", "1.0"], [" death", "1.0"], [" for", "1.0"], [" a", "1.0"], [" person", "1.0"], [" who", "0.99999976"], [" has", "1.0"], [" been", "1.0"], [" buried", "1.0"], [" in", "1.0"], [" a", "1.0"], [" cemetery", "1.0"], [" is", "1.0"], [" between", "0.9894154"], [" 80", "0.75275046"], [" and", "1.0"], [" 90", "0.9807842"], [" years", "1.0"], [" old", "0.9999994"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" age", "0.9986613"], [" of", "1.0"], [" death", "1.0"], [" for", "1.0"], [" a", "1.0"], [" person", "1.0"], [" who", "1.0"], [" has", "1.0"]]}, "691": {"qid": "566d1ac4f66f33b71aee", "term": "Anorexia nervosa", "description": "Eating disorder characterized by refusal to maintain a healthy body weight, and fear of gaining weight due to a distorted self image", "question": "Are red legs a sign of failing health in those with Anorexia Nervosa?", "answer": true, "facts": ["Heart failure or disease can lead to the legs becoming red or pink in color.", "Anorexia Nervosa can lead to heart failure and death."], "decomposition": ["What is a complication associated with Anorexia Nervosa that affects the heart?", "What happens to a person's legs when #1 occurs?", "Is #2 a sign of failing health?"], "evidence": [[[["Anorexia nervosa-58"], "no_evidence"], [["Rash-2"], "no_evidence"], ["operation"]], [[["Anorexia nervosa-1"], "no_evidence"], [["Cardiovascular disease-6"], "no_evidence"], ["no_evidence", "operation"]], [[["Anorexia nervosa-53"]], ["no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any other diseases that are caused by the sun?\nFacts: There are many diseases that are caused by the sun. The sun is the cause of many cancers, such as skin cancer, and eye cancer.\nAnswer (True or False): True\nQuestion: Are there any other diseases that are caused by the sun?", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are red legs a sign of failing health in those with Anorexia Nervosa?\nFacts: Heart failure or disease can lead to the legs becoming red or pink in color. Anorexia Nervosa can lead to heart failure and death.\nAnswer (True or False):", "ct_subject": "Heart failure or disease can lead to the legs becoming red or pink in color. Anorexia Nervosa can lead to heart failure and death.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.45781043"], [" there", "0.9999099"], [" any", "1.0"], [" other", "0.99999595"], [" diseases", "1.0"], [" that", "1.0"], [" are", "0.01394102"], [" caused", "0.99976474"], [" by", "1.0"], [" the", "0.8397215"], [" sun", "0.99844944"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "0.993454"], [" are", "1.0"], [" many", "1.0"], [" diseases", "1.0"], [" that", "0.9999995"], [" are", "0.99999964"], [" caused", "1.0"], [" by", "1.0"], [" the", "1.0"], [" sun", "1.0"], [".", "1.0"], [" The", "0.99938357"], [" sun", "0.9999993"], [" is", "0.999647"], [" the", "0.99999046"], [" cause", "0.67843753"], [" of", "1.0"], [" many", "0.99999404"], [" cancers", "0.60476357"], [",", "0.49519855"], [" such", "0.84417206"], [" as", "1.0"], [" skin", "1.0"], [" cancer", "1.0"], [",", "0.9999988"], [" and", "0.99952304"], [" eye", "1.0"], [" cancer", "1.0"], [".", "1.0"], ["\n", "0.99998844"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.99940073"], [" there", "1.0"], [" any", "1.0"], [" other", "0.99740136"], [" diseases", "1.0"], [" that", "1.0"], [" are", "1.0"], [" caused", "1.0"], [" by", "1.0"], [" the", "1.0"], [" sun", "1.0"], ["?", "1.0"]]}, "692": {"qid": "b039b0d6e263ee8fb81a", "term": "Jumping spider", "description": "family of arachnids", "question": "Would a jumping spider need over half a dozen contact lenses?", "answer": true, "facts": ["Jumping spiders have eight eyes.", "Half a dozen objects is equal to six objects.", "One contact lens is worn per eye."], "decomposition": ["How many eyes do jumping spiders have?", "How much is half a dozen?", "Is #1 more than #2?"], "evidence": [[[["Jumping spider-3"]], [["Dozen-3"]], [["Jumping spider-3"]]], [[["Jumping spider-1"]], [["Dozen-1"], "operation"], ["operation"]], [[["Jumping spider-1"]], [["Dozen-3", "One half-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human?\nFacts: The average weight of a human is about 100 pounds.\nAnswer (True or False): True\nQuestion: What is the average height of a human?\nFacts: The average height of a human is about 5 feet 10 inches.\nAnswer (True or False): True\nQuestion: What is the average weight of a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a jumping spider need over half a dozen contact lenses?\nFacts: Jumping spiders have eight eyes. Half a dozen objects is equal to six objects. One contact lens is worn per eye.\nAnswer (True or False):", "ct_subject": "Jumping spiders have eight eyes. Half a dozen objects is equal to six objects. One contact lens is worn per eye.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99941754"], [" is", "1.0"], [" the", "1.0"], [" average", "0.28709456"], [" weight", "0.99999964"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" weight", "0.94473"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" is", "1.0"], [" about", "0.99999976"], [" 100", "0.9723627"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" height", "0.97401345"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" height", "1.0"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 5", "0.9998561"], [" feet", "0.99999964"], [" 10", "0.868179"], [" inches", "1.0"], [".", "0.99999833"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "1.0"], [" of", "1.0"], [" a", "1.0"]]}, "693": {"qid": "45ff478a039792001dbc", "term": "Harvey Milk", "description": "American politician who became a martyr in the gay community", "question": "Would Harvey Milk have approved of Obama?", "answer": true, "facts": ["Obama awarded Harvey Milk a posthumous Medal of Freedom. ", "Obama was known for supporting marriage equality and LGBT rights. "], "decomposition": ["What was Harvey Milk known for?", "Did Obama support #1?", "Is #1 the same as #2?"], "evidence": [[[["Harvey Milk-1"]], [["Barack Obama-4"]], ["operation"]], [[["Harvey Milk-1"]], [["Barack Obama-4"]], ["operation"]], [[["Harvey Milk-4"]], [["Stuart Milk-4"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: The first person to be executed in the United States was John Brown.  Brown was executed in 1859. \nAnswer (True or False): True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: The first person to be executed in the United", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Harvey Milk have approved of Obama?\nFacts: Obama awarded Harvey Milk a posthumous Medal of Freedom.  Obama was known for supporting marriage equality and LGBT rights. \nAnswer (True or False):", "ct_subject": "Obama awarded Harvey Milk a posthumous Medal of Freedom.  Obama was known for supporting marriage equality and LGBT rights. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99488306"], [" is", "1.0"], [" the", "1.0"], [" name", "0.97427285"], [" of", "1.0"], [" the", "1.0"], [" first", "0.99885654"], [" person", "0.6538951"], [" to", "1.0"], [" be", "0.9999927"], [" executed", "0.44882643"], [" in", "0.99979657"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99997234"], [" first", "1.0"], [" person", "1.0"], [" to", "0.29821667"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" John", "0.9997236"], [" Brown", "1.0"], [".", "0.9999989"], [" ", "0.9960502"], [" Brown", "0.7522828"], [" was", "1.0"], [" executed", "0.9562285"], [" in", "0.35649216"], [" 18", "1.0"], ["59", "1.0"], [".", "0.9172679"], [" ", "0.99999917"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"]]}, "694": {"qid": "669b3c6a48f494a5d74e", "term": "Kurt Cobain", "description": "American singer, composer, and musician", "question": "Was Kurt Cobain's death indirectly caused by Daniel LeFever?", "answer": true, "facts": ["Kurt Cobain committed suicide with a shotgun.", "Daniel LeFever was the inventor of the American hammerless shotgun."], "decomposition": ["What object caused the death of Kurt Cobain?", "Was #1 invented by Daniel LeFever?"], "evidence": [[[["Suicide of Kurt Cobain-1"]], ["operation"]], [[["Kurt Cobain-3"]], [["Daniel Myron LeFever-1"]]], [[["Kurt Cobain-55"]], [["Shotgun-38"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Titanic sink because of the icebergs?\nFacts: The Titanic sank because of the icebergs.\nAnswer (True or False): True\nQuestion: Did the Titanic sink because of the icebergs?\nFacts: The Titanic sank because of the icebergs.\nAnswer (True or False): True\nQuestion: Did the Titanic sink because of the icebergs?\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Kurt Cobain's death indirectly caused by Daniel LeFever?\nFacts: Kurt Cobain committed suicide with a shotgun. Daniel LeFever was the inventor of the American hammerless shotgun.\nAnswer (True or False):", "ct_subject": "Kurt Cobain committed suicide with a shotgun. Daniel LeFever was the inventor of the American hammerless shotgun.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.7575238"], [" the", "1.0"], [" Titanic", "0.99955565"], [" sink", "1.0"], [" because", "1.0"], [" of", "0.9999999"], [" the", "0.9520774"], [" iceberg", "1.0"], ["s", "1.0"], ["?", "0.9999982"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Titanic", "0.9998505"], [" sank", "0.9982343"], [" because", "1.0"], [" of", "1.0"], [" the", "1.0"], [" iceberg", "1.0"], ["s", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9998016"], [" the", "1.0"], [" Titanic", "1.0"], [" sink", "1.0"], [" because", "1.0"], [" of", "1.0"], [" the", "1.0"], [" iceberg", "1.0"], ["s", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Titanic", "1.0"], [" sank", "1.0"], [" because", "1.0"], [" of", "1.0"], [" the", "1.0"], [" iceberg", "1.0"], ["s", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999999"], [" the", "1.0"], [" Titanic", "1.0"], [" sink", "1.0"], [" because", "1.0"], [" of", "1.0"], [" the", "1.0"], [" iceberg", "1.0"], ["s", "1.0"], ["?", "1.0"], ["\n", "1.0"]]}, "695": {"qid": "3836171bc71856136668", "term": "White", "description": "color", "question": "Can paresthesia be caused by a white pigment?", "answer": true, "facts": ["Tingling in the hands or feet is a type of paresthesia", "Lead white exposure can lead to lead poisoning", "Symptoms of lead poisoning include tingling in the hands and feet"], "decomposition": ["What kinds of white pigment have adverse health effects?", "What are the symptoms of paresthesia?", "Can any of #1 cause #2?"], "evidence": [[[["Lead paint-7"]], [["Paresthesia-1"]], [["Lead poisoning-1"], "operation"]], [["no_evidence"], [["Paresthesia-1"]], ["no_evidence", "operation"]], [[["Powder-11"], "no_evidence"], [["Paresthesia-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Can a person be born with a heart defect?\nFacts: A heart defect is a congenital abnormality that is present at birth. A heart defect is not caused by a person's genes.\nAnswer (True or False): True\nQuestion: Can a person be born with a heart defect?\nFacts: A", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can paresthesia be caused by a white pigment?\nFacts: Tingling in the hands or feet is a type of paresthesia Lead white exposure can lead to lead poisoning Symptoms of lead poisoning include tingling in the hands and feet\nAnswer (True or False):", "ct_subject": "Tingling in the hands or feet is a type of paresthesia Lead white exposure can lead to lead poisoning Symptoms of lead poisoning include tingling in the hands and feet", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Can", "0.6528946"], [" a", "0.99999535"], [" person", "1.0"], [" be", "0.9819519"], [" born", "0.98268837"], [" with", "1.0"], [" a", "1.0"], [" heart", "0.99999917"], [" defect", "0.7365283"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999404"], [" heart", "0.9999999"], [" defect", "1.0"], [" is", "1.0"], [" a", "1.0"], [" congen", "0.7109751"], ["ital", "1.0"], [" abnorm", "0.99989045"], ["ality", "1.0"], [" that", "0.99446195"], [" is", "0.73051655"], [" present", "1.0"], [" at", "1.0"], [" birth", "1.0"], [".", "1.0"], [" A", "0.90014714"], [" heart", "1.0"], [" defect", "1.0"], [" is", "0.78526485"], [" not", "1.0"], [" caused", "0.11941135"], [" by", "1.0"], [" a", "1.0"], [" person", "0.9997236"], ["'s", "1.0"], [" genes", "1.0"], [".", "0.99999964"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Can", "0.9999933"], [" a", "1.0"], [" person", "1.0"], [" be", "1.0"], [" born", "1.0"], [" with", "1.0"], [" a", "1.0"], [" heart", "0.9999999"], [" defect", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"]]}, "696": {"qid": "987260ffef60cb2c5439", "term": "Pearl Harbor", "description": "Harbor on the island of Oahu, Hawaii", "question": "Is Pearl Harbor the mythical home of a shark goddess?", "answer": true, "facts": ["The native Hawaiian people believed Pearl Harbor was the home of Ka\u02bbahupahau.", "Ka\u02bbahupahau is a shark goddess in Hawaiian legends. "], "decomposition": ["What did the native Hawaiian people believe Pearl Harbor was home to?", "What was #1?", "IS #2 the same as a shark goddess?"], "evidence": [[[["Pearl Harbor-2"]], [["Pearl Harbor-2"]], [["Pearl Harbor-2"]]], [[["Pearl Harbor-2"]], [["Pearl Harbor-2"]], ["operation"]], [[["Pearl Harbor-2"]], [["Pearl Harbor-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the moon the home of a giant turtle?\nFacts: The moon is the home of a giant turtle.\nAnswer (True or False): True\nQuestion: Is the sun the home of a giant turtle?\nFacts: The sun is the home of a giant turtle.\nAnswer (True or False): True\nQuestion:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Pearl Harbor the mythical home of a shark goddess?\nFacts: The native Hawaiian people believed Pearl Harbor was the home of Ka\u02bbahupahau. Ka\u02bbahupahau is a shark goddess in Hawaiian legends. \nAnswer (True or False):", "ct_subject": "The native Hawaiian people believed Pearl Harbor was the home of Ka\u02bbahupahau. Ka\u02bbahupahau is a shark goddess in Hawaiian legends. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999795"], [" the", "1.0"], [" moon", "0.7890801"], [" the", "0.99995804"], [" home", "0.17860693"], [" of", "1.0"], [" a", "0.99998856"], [" giant", "0.9999888"], [" turtle", "0.87610507"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" moon", "1.0"], [" is", "1.0"], [" the", "0.99996424"], [" home", "1.0"], [" of", "1.0"], [" a", "0.9940686"], [" giant", "0.9952207"], [" turtle", "1.0"], [".", "1.0"], ["\n", "0.9997373"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" sun", "0.9953719"], [" the", "1.0"], [" home", "1.0"], [" of", "1.0"], [" a", "1.0"], [" giant", "1.0"], [" turtle", "0.9895223"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" sun", "1.0"], [" is", "1.0"], [" the", "1.0"], [" home", "1.0"], [" of", "1.0"], [" a", "1.0"], [" giant", "1.0"], [" turtle", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"]]}, "697": {"qid": "557b6230413fc65f6a16", "term": "Ocean sunfish", "description": "species of fish", "question": "Would it be impossible to keep an ocean sunfish and a goldfish in the same tank?", "answer": true, "facts": ["Ocean sunfish live in salt water environments.", "Goldfish live in fresh water environments.", "Putting a fish into the wrong water type can cause them to die."], "decomposition": ["What kind of water habitat does the ocean sunfish live in?", "What kind of water habitat do goldfish live in?", "Is #1 interchangeable with #2"], "evidence": [[[["Ocean sunfish-21", "Ocean-2"]], [["Goldfish-1"]], ["operation"]], [[["Ocean sunfish-1"]], [["Goldfish-1"]], ["operation"]], [[["Ocean-11", "Saltwater fish-3"]], [["Freshwater fish-3", "Goldfish-1"]], [["Freshwater fish-3", "Saltwater fish-3"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"saltwater\" and a \"freshwater\" fish?\nFacts: Saltwater fish live in salt water environments. Freshwater fish live in fresh water environments.\nAnswer (True or False): True\nQuestion: What is the difference between a \"saltwater\" and a \"", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would it be impossible to keep an ocean sunfish and a goldfish in the same tank?\nFacts: Ocean sunfish live in salt water environments. Goldfish live in fresh water environments. Putting a fish into the wrong water type can cause them to die.\nAnswer (True or False):", "ct_subject": "Ocean sunfish live in salt water environments. Goldfish live in fresh water environments. Putting a fish into the wrong water type can cause them to die.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99990225"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.99940026"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.9998299"], ["s", "0.9952767"], ["alt", "0.99862707"], ["water", "0.93964726"], ["\"", "1.0"], [" and", "0.99320215"], [" a", "0.99900025"], [" \"", "1.0"], ["fresh", "1.0"], ["water", "1.0"], ["\"", "1.0"], [" fish", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Salt", "0.9999794"], ["water", "1.0"], [" fish", "1.0"], [" live", "0.99997556"], [" in", "1.0"], [" salt", "0.9878242"], [" water", "1.0"], [" environments", "0.99985576"], [".", "1.0"], [" Fresh", "1.0"], ["water", "1.0"], [" fish", "1.0"], [" live", "1.0"], [" in", "1.0"], [" fresh", "1.0"], [" water", "1.0"], [" environments", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["alt", "1.0"], ["water", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"]]}, "698": {"qid": "2294d2f030b7fd6f77e3", "term": "Maize", "description": "Cereal grain", "question": "Did Demi Lovato's ancestors help turn maize into popcorn?", "answer": true, "facts": ["Demi Lovato's father is Mexican.", "Maize is another word for corn.", "Corn was first domesticated in southern Mexico about 10,000 years ago.", "Popcorn is made from kernels of corn."], "decomposition": ["Where is popcorn from originally?", "Where is Demi Lovato's ancestors from?", "Are #1 and #2 the same?"], "evidence": [[[["Popcorn-5"]], [["Demi Lovato-5"]], ["operation"]], [[["Popcorn-5"]], [["Demi Lovato-5"]], ["operation"]], [[["Popcorn-5"]], [["Demi Lovato-5"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the first man to walk on the moon walk on the moon?\nFacts: The first man to walk on the moon was Neil Armstrong. He was the first man to walk on the moon.\nAnswer (True or False): True\nQuestion: Did the first man to walk on the moon walk", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Demi Lovato's ancestors help turn maize into popcorn?\nFacts: Demi Lovato's father is Mexican. Maize is another word for corn. Corn was first domesticated in southern Mexico about 10,000 years ago. Popcorn is made from kernels of corn.\nAnswer (True or False):", "ct_subject": "Demi Lovato's father is Mexican. Maize is another word for corn. Corn was first domesticated in southern Mexico about 10,000 years ago. Popcorn is made from kernels of corn.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999565"], [" the", "1.0"], [" first", "1.0"], [" man", "0.9329542"], [" to", "0.9778061"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" walk", "0.99976116"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" was", "1.0"], [" Neil", "1.0"], [" Armstrong", "1.0"], [".", "1.0"], [" He", "0.4563012"], [" was", "0.9999999"], [" the", "1.0"], [" first", "1.0"], [" man", "0.99999714"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [".", "0.99534035"], ["\n", "0.99952495"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" walk", "1.0"]]}, "699": {"qid": "4a0e604f174af36e3ace", "term": "Globalization", "description": "process of international integration arising from the interchange of world views, products, ideas, and other aspects of culture", "question": "Are System of a Down opposed to globalization?", "answer": true, "facts": ["In Boom!, System of a Down condemns globalization.", "The lead vocalist of the band System of a Down is outspoken against globalization. "], "decomposition": ["What is globalization?", "Is the lead vocalist of the band System of a Down against #1?"], "evidence": [[[["Globalization-1"]], [["Serj Tankian-38"], "no_evidence", "operation"]], [[["Globalization-1"]], [["Serj Tankian-1", "Serj Tankian-16"], "no_evidence", "operation"]], [[["Globalization-1"]], [["System of a Down-32"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any other bands that have influenced System of a Down?\nFacts: System of a Down has influenced many other bands. \nAnswer (True or False): True\nQuestion: Are there any other bands that have influenced System of a Down?\nFacts: System of a Down has influenced many other bands. \nAnswer (True or False): True\nQuestion: Are there", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are System of a Down opposed to globalization?\nFacts: In Boom!, System of a Down condemns globalization. The lead vocalist of the band System of a Down is outspoken against globalization. \nAnswer (True or False):", "ct_subject": "In Boom!, System of a Down condemns globalization. The lead vocalist of the band System of a Down is outspoken against globalization. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.99458164"], [" there", "0.9906178"], [" any", "1.0"], [" other", "0.99998844"], [" bands", "1.0"], [" that", "1.0"], [" have", "0.90219694"], [" influenced", "0.8710322"], [" System", "1.0"], [" of", "1.0"], [" a", "1.0"], [" Down", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" System", "1.0"], [" of", "1.0"], [" a", "1.0"], [" Down", "1.0"], [" has", "0.9988036"], [" influenced", "1.0"], [" many", "0.9999982"], [" other", "0.9976374"], [" bands", "1.0"], [".", "1.0"], [" ", "0.99889344"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.95508665"], [" there", "0.9999999"], [" any", "1.0"], [" other", "1.0"], [" bands", "1.0"], [" that", "1.0"], [" have", "1.0"], [" influenced", "1.0"], [" System", "1.0"], [" of", "1.0"], [" a", "1.0"], [" Down", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" System", "1.0"], [" of", "1.0"], [" a", "1.0"], [" Down", "1.0"], [" has", "1.0"], [" influenced", "1.0"], [" many", "1.0"], [" other", "1.0"], [" bands", "1.0"], [".", "1.0"], [" ", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "1.0"]]}, "700": {"qid": "c89862fb8a7e09c34f47", "term": "Scottish people", "description": "ethnic inhabitants of Scotland", "question": "Are Scottish people Albidosi?", "answer": true, "facts": ["The Scottish people emerged from an amalgamation of two Celtic-speaking peoples, the Picts and Gaels, who founded the Kingdom of Scotland.", "What the Picts called themselves is unknown. It has been proposed that they called themselves Albidosi.", "The Kingdom of Scotland is also known as the Kingdom of Alba."], "decomposition": ["Which tribes did the Scottish people emerge from?", "Have any of #1 been referred to as Albidosi?"], "evidence": [[[["Scottish people-1"]], [["Picts-6"]]], [[["Scottish people-1"]], [["Picts-6"]]], [[["Scottish people-1"]], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any people in the world who are not related to each other?\nFacts: There are many people who are not related to each other. The most common example is that of the human race. There are many", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are Scottish people Albidosi?\nFacts: The Scottish people emerged from an amalgamation of two Celtic-speaking peoples, the Picts and Gaels, who founded the Kingdom of Scotland. What the Picts called themselves is unknown. It has been proposed that they called themselves Albidosi. The Kingdom of Scotland is also known as the Kingdom of Alba.\nAnswer (True or False):", "ct_subject": "The Scottish people emerged from an amalgamation of two Celtic-speaking peoples, the Picts and Gaels, who founded the Kingdom of Scotland. What the Picts called themselves is unknown. It has been proposed that they called themselves Albidosi. The Kingdom of Scotland is also known as the Kingdom of Alba.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.99999356"], [" there", "0.7135189"], [" any", "0.9801682"], [" people", "0.9999969"], [" in", "0.99441373"], [" the", "1.0"], [" world", "1.0"], [" who", "0.94408363"], [" are", "0.4257206"], [" not", "1.0"], [" related", "1.0"], [" to", "1.0"], [" each", "0.99999964"], [" other", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "1.0"], [" are", "1.0"], [" many", "0.5296625"], [" people", "1.0"], [" who", "0.93747073"], [" are", "1.0"], [" not", "1.0"], [" related", "1.0"], [" to", "1.0"], [" each", "1.0"], [" other", "1.0"], [".", "1.0"], [" The", "0.91549367"], [" most", "0.99999976"], [" common", "0.4095713"], [" example", "0.9098966"], [" is", "1.0"], [" that", "0.11500218"], [" of", "1.0"], [" the", "1.0"], [" human", "0.9992519"], [" race", "0.991017"], [".", "1.0"], [" There", "0.99995494"], [" are", "1.0"], [" many", "1.0"]]}, "701": {"qid": "24b3b5e476a4c7b4824e", "term": "Aldi", "description": "Germany-based supermarket chain", "question": "Would you spend less on your food at Aldi than at Whole Foods?", "answer": true, "facts": ["Whole Foods is known for costing 10-20% more than other stores.", "Aldi is known for having deeply discounted food and home supplies."], "decomposition": ["What is Aldi mainly known for?", "Compared to other stores, how do Whole Foods prices compare?", "Would #1 have goods that cost less than #2?"], "evidence": [[[["Aldi-1"]], [["Whole Foods Market-2"], "no_evidence"], ["no_evidence", "operation"]], [[["Aldi-1"]], [["Whole Foods Market-1", "Whole Foods Market-24"], "no_evidence"], [["Discount store-1"], "operation"]], [[["Aldi-1"]], [["Wild Oats Markets-11"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"sausage roll\" and a \"pork roll\"?\nFacts: A \"sausage roll\" is a roll of bread that has been cut into strips and then fried. A \"pork roll\" is a roll of bread that has been cut into strips and then fried.\nAnswer (True or False):", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would you spend less on your food at Aldi than at Whole Foods?\nFacts: Whole Foods is known for costing 10-20% more than other stores. Aldi is known for having deeply discounted food and home supplies.\nAnswer (True or False):", "ct_subject": "Whole Foods is known for costing 10-20% more than other stores. Aldi is known for having deeply discounted food and home supplies.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9475672"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.52035964"], [" between", "1.0"], [" a", "0.9999999"], [" \"", "1.0"], ["s", "0.5556008"], ["aus", "0.9957118"], ["age", "1.0"], [" roll", "0.85618395"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["p", "0.95708764"], ["ork", "0.70746785"], [" roll", "0.9999939"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" \"", "0.99357045"], ["s", "1.0"], ["aus", "1.0"], ["age", "1.0"], [" roll", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" roll", "0.88488126"], [" of", "1.0"], [" bread", "1.0"], [" that", "0.95598036"], [" has", "0.9971955"], [" been", "1.0"], [" cut", "0.8677222"], [" into", "1.0"], [" strips", "0.8408117"], [" and", "0.17275329"], [" then", "0.99999356"], [" fried", "0.9382471"], [".", "1.0"], [" A", "1.0"], [" \"", "1.0"], ["p", "1.0"], ["ork", "1.0"], [" roll", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" roll", "1.0"], [" of", "1.0"], [" bread", "1.0"], [" that", "1.0"], [" has", "1.0"], [" been", "1.0"], [" cut", "1.0"], [" into", "1.0"], [" strips", "1.0"], [" and", "1.0"], [" then", "1.0"], [" fried", "0.99964666"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"]]}, "702": {"qid": "7459eacff621fe539afc", "term": "Los Angeles Memorial Sports Arena", "description": "Former arena in California, United States", "question": "Was Los Angeles Memorial Sports Arena hypothetically inadequate for hosting Coachella?", "answer": true, "facts": ["The Los Angeles Memorial Sports Arena had a capacity of 16,740 people.", "Coachella has had attendance numbers in excess of 99.000 people.", "Coachella relies on an outdoor set up to accommodate the massive crowds."], "decomposition": ["How many people can the Los Angeles Memorial Sports Arena hold?", "How many people usually attend Coachella?", "Is #2 greater than #1?"], "evidence": [[[["Los Angeles Memorial Sports Arena-15"]], [["Coachella Valley Music and Arts Festival-3"]], [["Coachella Valley Music and Arts Festival-3", "Los Angeles Memorial Sports Arena-15"], "operation"]], [[["Los Angeles Memorial Sports Arena-15"]], [["Coachella Valley Music and Arts Festival-3"]], ["operation"]], [[["Los Angeles Memorial Sports Arena-15"]], [["Coachella Valley Music and Arts Festival-20"]], [["Coachella Valley Music and Arts Festival-20"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the band that performed at Coachella?\nFacts: The band that performed at Coachella was The Killers.\nAnswer (True or False): True\nQuestion: What is the name of the band that performed at Coachella?", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Los Angeles Memorial Sports Arena hypothetically inadequate for hosting Coachella?\nFacts: The Los Angeles Memorial Sports Arena had a capacity of 16,740 people. Coachella has had attendance numbers in excess of 99.000 people. Coachella relies on an outdoor set up to accommodate the massive crowds.\nAnswer (True or False):", "ct_subject": "The Los Angeles Memorial Sports Arena had a capacity of 16,740 people. Coachella has had attendance numbers in excess of 99.000 people. Coachella relies on an outdoor set up to accommodate the massive crowds.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9881021"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999894"], [" of", "1.0"], [" the", "1.0"], [" band", "0.8772639"], [" that", "1.0"], [" performed", "0.94132507"], [" at", "1.0"], [" Co", "0.99999976"], ["ache", "1.0"], ["lla", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" band", "1.0"], [" that", "0.9107323"], [" performed", "1.0"], [" at", "1.0"], [" Co", "1.0"], ["ache", "1.0"], ["lla", "1.0"], [" was", "0.996671"], [" The", "0.27623484"], [" Kill", "0.99968505"], ["ers", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" band", "1.0"], [" that", "1.0"], [" performed", "1.0"], [" at", "1.0"], [" Co", "1.0"], ["ache", "1.0"], ["lla", "1.0"], ["?", "1.0"]]}, "703": {"qid": "27e9e6d368cab11438c4", "term": "NATO", "description": "Intergovernmental military alliance of Western states", "question": "NATO doesn't recognize double triangle flag countries?", "answer": true, "facts": ["NATO is a members only alliance of several countries.", "Nepal has a double triangle flag.", "Nepal has not been recognized as a member of NATO."], "decomposition": ["What country has a flag with double triangles on it?", "Which countries are part of NATO?", "Is #1 not included in #2?"], "evidence": [[[["Flag of Nepal-5"]], [["Member states of NATO-3"]], ["operation"]], [[["Flag of Nepal-5"]], [["Member states of NATO-2"]], ["operation"]], [[["Flag of Nepal-1"]], [["NATO-30"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"double triangle\" flag and a \"triangle\" flag?\nFacts: A double triangle flag is a flag that has three triangles on it. A triangle flag is a flag that has three equal sides.\nAnswer (True or False): True\nQuestion: What is the difference between a \"triangle\" flag and a \"triangle\" flag with", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: NATO doesn't recognize double triangle flag countries?\nFacts: NATO is a members only alliance of several countries. Nepal has a double triangle flag. Nepal has not been recognized as a member of NATO.\nAnswer (True or False):", "ct_subject": "NATO is a members only alliance of several countries. Nepal has a double triangle flag. Nepal has not been recognized as a member of NATO.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.977866"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.91720426"], [" between", "1.0"], [" a", "0.9999937"], [" \"", "0.99405414"], ["double", "0.9996376"], [" triangle", "1.0"], ["\"", "0.99951506"], [" flag", "0.9999993"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["tri", "1.0"], ["angle", "0.9999999"], ["\"", "0.9878748"], [" flag", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.99947256"], [" double", "0.99811125"], [" triangle", "1.0"], [" flag", "1.0"], [" is", "0.99999726"], [" a", "1.0"], [" flag", "1.0"], [" that", "0.9951782"], [" has", "1.0"], [" three", "0.9999485"], [" triangles", "0.52966344"], [" on", "0.9999422"], [" it", "1.0"], [".", "1.0"], [" A", "1.0"], [" triangle", "1.0"], [" flag", "0.9999993"], [" is", "1.0"], [" a", "1.0"], [" flag", "1.0"], [" that", "1.0"], [" has", "1.0"], [" three", "0.99999964"], [" equal", "0.9998534"], [" sides", "0.99999046"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999988"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["tri", "1.0"], ["angle", "1.0"], ["\"", "1.0"], [" flag", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["tri", "1.0"], ["angle", "1.0"], ["\"", "1.0"], [" flag", "1.0"], [" with", "1.0"]]}, "704": {"qid": "da6af47f5a3bc2afb4fb", "term": "Saltwater crocodile", "description": "species of reptile", "question": "Is the saltwater crocodile less endangered than the European otter?", "answer": true, "facts": ["The saltwater crocodile is listed as \"least concern\" on the International Union for the Conservation of Nature Red List.", "The European otter is listed as \"near threatened\" on the International Union for the Conservation of Nature Red List.", "The International Union for the Conservation of Nature Red List starts with \"least concern\", then \"near threatened\", \"vulnerable\", \"endangered\", \"critically endangered\", \"extinct in the wild\", and \"extinct\"."], "decomposition": ["What is the saltwater crocodile's conservation status on the IUCN red list?", "What is the European otter's conservation status on the IUCN red list?", "Is #1 less severe than #2?"], "evidence": [[[["Saltwater crocodile-1"]], [["Eurasian otter-8"]], [["Least-concern species-1"], "operation"]], [[["Saltwater crocodile-1"]], [["Eurasian otter-8"]], [["IUCN Red List-11"], "no_evidence", "operation"]], [[["Saltwater crocodile-1"]], [["Otter-12"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the American bald eagle more endangered", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is the saltwater crocodile less endangered than the European otter?\nFacts: The saltwater crocodile is listed as \"least concern\" on the International Union for the Conservation of Nature Red List. The European otter is listed as \"near threatened\" on the International Union for the Conservation of Nature Red List. The International Union for the Conservation of Nature Red List starts with \"least concern\", then \"near threatened\", \"vulnerable\", \"endangered\", \"critically endangered\", \"extinct in the wild\", and \"extinct\".\nAnswer (True or False):", "ct_subject": "The saltwater crocodile is listed as \"least concern\" on the International Union for the Conservation of Nature Red List. The European otter is listed as \"near threatened\" on the International Union for the Conservation of Nature Red List. The International Union for the Conservation of Nature Red List starts with \"least concern\", then \"near threatened\", \"vulnerable\", \"endangered\", \"critically endangered\", \"extinct in the wild\", and \"extinct\".", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99519354"], [" the", "1.0"], [" American", "0.9999318"], [" bald", "0.9999999"], [" eagle", "1.0"], [" more", "1.0"], [" endangered", "1.0"]]}, "705": {"qid": "ca93c9ed9f7914eaa963", "term": "Muslim world", "description": "Muslim-majority countries, states, districts, or towns", "question": "Is the Muslim world hostile to Israel?", "answer": true, "facts": ["Israel, a small Middle Eastern nation which is considered the Jewish holy land, contains the ancient city of Jerusalem and other ancient holy sites.", "Jerusalem is the third most holy site for Muslims, after Mecca and Medina.", "Prior to English occupation in the 18th-20th centuries, the Muslim-based Ottoman Empire controlled Jerusalem.", "The Muslims want to reclaim Jerusalem and the surrounding holy lands."], "decomposition": ["What is the religious significance of Israel's historic cities to Muslims?", "Are the Muslims presently in control of Israel?", "Considering #1, do the Muslims wish to change the situation of #2?"], "evidence": [[[["Jerusalem-3"], "no_evidence"], [["Israel-68"], "no_evidence"], ["no_evidence", "operation"]], [[["Religious significance of Jerusalem-15"]], [["Islam in Israel-1"]], [["Muslim supporters of Israel-23"]]], [[["Holy Land-1"]], [["Israel-33"], "no_evidence"], [["2006 Lebanon War-1"], "no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the Muslim world hostile to the United States?\nFacts: The Muslim world is hostile to the United States. The United States is the largest Muslim nation", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is the Muslim world hostile to Israel?\nFacts: Israel, a small Middle Eastern nation which is considered the Jewish holy land, contains the ancient city of Jerusalem and other ancient holy sites. Jerusalem is the third most holy site for Muslims, after Mecca and Medina. Prior to English occupation in the 18th-20th centuries, the Muslim-based Ottoman Empire controlled Jerusalem. The Muslims want to reclaim Jerusalem and the surrounding holy lands.\nAnswer (True or False):", "ct_subject": "Israel, a small Middle Eastern nation which is considered the Jewish holy land, contains the ancient city of Jerusalem and other ancient holy sites. Jerusalem is the third most holy site for Muslims, after Mecca and Medina. Prior to English occupation in the 18th-20th centuries, the Muslim-based Ottoman Empire controlled Jerusalem. The Muslims want to reclaim Jerusalem and the surrounding holy lands.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" Muslim", "0.9999877"], [" world", "1.0"], [" hostile", "1.0"], [" to", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Muslim", "0.9967315"], [" world", "1.0"], [" is", "1.0"], [" hostile", "0.904683"], [" to", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [".", "0.9965102"], [" The", "1.0"], [" United", "0.9862786"], [" States", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "0.73058236"], [" Muslim", "0.9999994"], [" nation", "0.9999999"]]}, "706": {"qid": "c7d6bfa777382af6c5bb", "term": "Table tennis", "description": "Racket sport", "question": "Does table tennis use prime numbers?", "answer": true, "facts": ["A table tennis game is won by the player first scoring 11 points.", "11 is a prime number."], "decomposition": ["What are the scores that can be awarded in a game of table tennis?", "Is any of #1 a prime number?"], "evidence": [[[["Table tennis-9"]], [["Prime number-13"], "operation"]], [[["Table tennis-24"]], ["operation"]], [[["Table tennis-24"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest word in the English language?\nFacts: The longest word in the English language is \"the\".\nAnswer (True or False): True\nQuestion: What is the longest word in the English language?\nFacts: The longest word in the English language is \"the\".\nAnswer (True or False): True\nQuestion: What is the longest word in the English language?\nFacts: The longest word", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does table tennis use prime numbers?\nFacts: A table tennis game is won by the player first scoring 11 points. 11 is a prime number.\nAnswer (True or False):", "ct_subject": "A table tennis game is won by the player first scoring 11 points. 11 is a prime number.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9981844"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.70637745"], [" word", "0.9999995"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], [" is", "1.0"], [" \"", "1.0"], ["the", "0.9963169"], ["\".", "0.9951558"], ["\n", "0.99999607"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.9999999"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], [" is", "1.0"], [" \"", "1.0"], ["the", "1.0"], ["\".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" word", "1.0"]]}, "707": {"qid": "594aac879ebd2a4123cb", "term": "Surveillance", "description": "monitoring of behavior, activities, or other changing information", "question": "Can you conduct surveillance from a teddy bear?", "answer": true, "facts": ["Surveillance is the act of monitoring or observation", "Nanny cams are used for surveillance of behavior when a family leaves their home and/or children in the care of a third party", "Nanny cams are often placed in common household objects like teddy bears"], "decomposition": ["In what kind of context/environment are nanny cams used for surveillance?", "Would a teddy bear accommodate a nanny can and be commonly found in #1?"], "evidence": [[[["Hidden camera-8"]], [["Hidden camera-8"]]], [[["Hidden camera-1"], "no_evidence"], [["Teddy bear-1"], "no_evidence", "operation"]], [[["Hidden camera-8"]], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"toy\" and a \"toy gun\"?\nFacts: A toy gun is a firearm that is designed to be fired from a distance. A toy gun is not a firearm. A toy gun is a toy that is designed to be", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can you conduct surveillance from a teddy bear?\nFacts: Surveillance is the act of monitoring or observation Nanny cams are used for surveillance of behavior when a family leaves their home and/or children in the care of a third party Nanny cams are often placed in common household objects like teddy bears\nAnswer (True or False):", "ct_subject": "Surveillance is the act of monitoring or observation Nanny cams are used for surveillance of behavior when a family leaves their home and/or children in the care of a third party Nanny cams are often placed in common household objects like teddy bears", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999858"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.3002445"], ["t", "0.9989398"], ["oy", "0.9665285"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["t", "1.0"], ["oy", "1.0"], [" gun", "0.9116945"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9698462"], [" toy", "1.0"], [" gun", "1.0"], [" is", "1.0"], [" a", "1.0"], [" firearm", "0.7393709"], [" that", "1.0"], [" is", "1.0"], [" designed", "0.9999995"], [" to", "1.0"], [" be", "0.76478183"], [" fired", "0.9882464"], [" from", "0.9999975"], [" a", "0.99983346"], [" distance", "0.9999999"], [".", "0.9868427"], [" A", "1.0"], [" toy", "1.0"], [" gun", "1.0"], [" is", "1.0"], [" not", "1.0"], [" a", "0.7260434"], [" firearm", "1.0"], [".", "0.99889684"], [" A", "0.9997955"], [" toy", "1.0"], [" gun", "1.0"], [" is", "1.0"], [" a", "0.72488207"], [" toy", "1.0"], [" that", "0.9996582"], [" is", "1.0"], [" designed", "0.99999964"], [" to", "1.0"], [" be", "1.0"]]}, "708": {"qid": "f4a336168f5165de7f0d", "term": "Northern fur seal", "description": "The largest fur seal in the northern hemisphere", "question": "Is a northern fur seal needing emergency surgery in July likely a safe anesthesia candidate?", "answer": true, "facts": ["Northern fur seals fast throughout the mating season", "It is recommended that patients, including animals, fast for a time before surgery that requires anesthesia ", "Peak mating season for northern fur seals occurs in June and July"], "decomposition": ["What is recommended for patients needing anesthesia?", "What do northern fur seals do in July?", "Does #2 include #1?"], "evidence": [[[["Anesthesia-12"]], [["Northern fur seal-17"]], ["operation"]], [[["Anesthesia-7"]], [["Northern fur seal-17"]], ["operation"]], [[["Anesthesia-34"], "no_evidence"], [["Northern fur seal-17"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is a northern fur seal likely to die from a heart attack?\nFacts: Northern fur seals are not known to have a heart attack\nAnswer (True or False): True\nQuestion: Is a northern fur seal likely to die from a heart attack?\nFacts: Northern fur seals are not", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is a northern fur seal needing emergency surgery in July likely a safe anesthesia candidate?\nFacts: Northern fur seals fast throughout the mating season It is recommended that patients, including animals, fast for a time before surgery that requires anesthesia  Peak mating season for northern fur seals occurs in June and July\nAnswer (True or False):", "ct_subject": "Northern fur seals fast throughout the mating season It is recommended that patients, including animals, fast for a time before surgery that requires anesthesia  Peak mating season for northern fur seals occurs in June and July", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9746251"], [" a", "0.980699"], [" northern", "0.9943711"], [" fur", "1.0"], [" seal", "1.0"], [" likely", "1.0"], [" to", "1.0"], [" die", "0.96131736"], [" from", "0.99999726"], [" a", "0.9998399"], [" heart", "0.99758613"], [" attack", "1.0"], ["?", "0.97245795"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Northern", "0.9999999"], [" fur", "1.0"], [" seals", "1.0"], [" are", "0.9999969"], [" not", "0.9997873"], [" known", "1.0"], [" to", "1.0"], [" have", "0.99999857"], [" a", "1.0"], [" heart", "1.0"], [" attack", "1.0"], ["\n", "0.94608164"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" a", "1.0"], [" northern", "1.0"], [" fur", "1.0"], [" seal", "1.0"], [" likely", "1.0"], [" to", "1.0"], [" die", "1.0"], [" from", "1.0"], [" a", "1.0"], [" heart", "0.98508066"], [" attack", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Northern", "1.0"], [" fur", "1.0"], [" seals", "1.0"], [" are", "1.0"], [" not", "1.0"]]}, "709": {"qid": "3fc6ba608677b8314b77", "term": "Red Sea", "description": "Arm of the Indian Ocean between Arabia and Africa", "question": "Would it be very difficult for Nuno Gomes to dive to the Red Sea's deepest point?", "answer": true, "facts": ["The Red Sea has a maximum depth of 3,040 m (9,970 ft).", "Nuno Gomes' deepest dive in the Red Sea to date is 318 metres (1,043 ft)."], "decomposition": ["How deep is the Red Sea's maximum depth?", "What is the deepest Nuno Gomes can dive?", "Is #1 greater than #2?"], "evidence": [[[["Red Sea-2"]], [["Nuno Gomes (diver)-2", "Nuno Gomes (diver)-4"]], ["operation"]], [[["Red Sea-2"]], [["Nuno Gomes (diver)-4"]], ["operation"]], [[["Red Sea-2"], "no_evidence"], [["Nuno Gomes (diver)-2"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance a person can travel in a day?\nFacts: The longest distance a person can travel in a day is about 1,000 km (621 miles).\nAnswer (True or False): True\nQuestion: What is the longest distance a person", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would it be very difficult for Nuno Gomes to dive to the Red Sea's deepest point?\nFacts: The Red Sea has a maximum depth of 3,040 m (9,970 ft). Nuno Gomes' deepest dive in the Red Sea to date is 318 metres (1,043 ft).\nAnswer (True or False):", "ct_subject": "The Red Sea has a maximum depth of 3,040 m (9,970 ft). Nuno Gomes' deepest dive in the Red Sea to date is 318 metres (1,043 ft).", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999715"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.99999774"], [" distance", "0.9991742"], [" a", "0.94626874"], [" person", "0.99852365"], [" can", "0.9998734"], [" travel", "0.92093277"], [" in", "0.99974734"], [" a", "0.8820696"], [" day", "0.91705567"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "0.9999999"], [" distance", "1.0"], [" a", "1.0"], [" person", "1.0"], [" can", "1.0"], [" travel", "1.0"], [" in", "1.0"], [" a", "1.0"], [" day", "1.0"], [" is", "1.0"], [" about", "0.99999726"], [" 1", "0.9443926"], [",", "1.0"], ["000", "0.99999905"], [" km", "1.0"], [" (", "1.0"], ["6", "0.99678314"], ["21", "1.0"], [" miles", "0.9999987"], [").", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" person", "1.0"]]}, "710": {"qid": "f7b1cc5b3fab95aa1be4", "term": "Breast cancer", "description": "cancer that originates in the mammary gland", "question": "Is breast cancer associated with a ribbon?", "answer": true, "facts": ["Breast cancer is one of many diseases associated with a specific color of ribbon.", "Breast cancer's ribbon is pink."], "decomposition": ["Which diseases are associated with a (certain color of) ribbon?", "Is breast cancer included in #1?"], "evidence": [[[["Awareness ribbon-3"]], ["operation"]], [[["Awareness ribbon-3"]], ["operation"]], [[["Awareness ribbon-15"]], [["Awareness ribbon-15"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the number of people who have died from the flu this year greater than the number of people who have died from the plague?\nFacts: The number of people who have died from the flu this year is greater than the number of people who have died from the plague.\nAnswer (True or False): True\nQuestion: Is the number of people who have died from the flu this year greater than the number of people", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is breast cancer associated with a ribbon?\nFacts: Breast cancer is one of many diseases associated with a specific color of ribbon. Breast cancer's ribbon is pink.\nAnswer (True or False):", "ct_subject": "Breast cancer is one of many diseases associated with a specific color of ribbon. Breast cancer's ribbon is pink.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9988237"], [" the", "0.932753"], [" number", "0.5194805"], [" of", "1.0"], [" people", "0.9999765"], [" who", "0.9988727"], [" have", "1.0"], [" died", "0.99992883"], [" from", "0.999997"], [" the", "0.15474299"], [" flu", "1.0"], [" this", "0.99999356"], [" year", "1.0"], [" greater", "0.99996746"], [" than", "1.0"], [" the", "1.0"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" who", "1.0"], [" have", "0.9999993"], [" died", "1.0"], [" from", "1.0"], [" the", "1.0"], [" plague", "0.9999999"], ["?", "0.99947983"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" number", "0.42843518"], [" of", "1.0"], [" people", "1.0"], [" who", "1.0"], [" have", "1.0"], [" died", "1.0"], [" from", "1.0"], [" the", "1.0"], [" flu", "1.0"], [" this", "1.0"], [" year", "1.0"], [" is", "1.0"], [" greater", "1.0"], [" than", "1.0"], [" the", "1.0"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" who", "1.0"], [" have", "1.0"], [" died", "1.0"], [" from", "1.0"], [" the", "1.0"], [" plague", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "0.9999999"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" who", "1.0"], [" have", "1.0"], [" died", "1.0"], [" from", "1.0"], [" the", "1.0"], [" flu", "1.0"], [" this", "1.0"], [" year", "1.0"], [" greater", "1.0"], [" than", "1.0"], [" the", "1.0"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"]]}, "711": {"qid": "c4a4c8fb77384ecaa472", "term": "Common warthog", "description": "Wild member of the pig family", "question": "Could common warthog be useful for scrimshaw?", "answer": true, "facts": ["Scrimshaw is the process of carving designs or symbols into materials such as ivory, whalebone, and tusks.", "The common warthog has two sets of long tusks.", "The common warthog has large teeth that are harnessed for ivory.", "The common warthog is not an endangered species."], "decomposition": ["What materials can be used in scrimshaw?", "Do warthogs have any of the things in #1?"], "evidence": [[[["Scrimshaw-1"]], [["Phacochoerus-2"], "operation"]], [[["Scrimshaw-1"]], [["Phacochoerus-2"]]], [[["Scrimshaw-1"]], [["Common warthog-3"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the largest animal in the world?\nFacts: The largest animal in the world is the blue whale. The blue whale is the largest animal in the world.\nAnswer (True or", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could common warthog be useful for scrimshaw?\nFacts: Scrimshaw is the process of carving designs or symbols into materials such as ivory, whalebone, and tusks. The common warthog has two sets of long tusks. The common warthog has large teeth that are harnessed for ivory. The common warthog is not an endangered species.\nAnswer (True or False):", "ct_subject": "Scrimshaw is the process of carving designs or symbols into materials such as ivory, whalebone, and tusks. The common warthog has two sets of long tusks. The common warthog has large teeth that are harnessed for ivory. The common warthog is not an endangered species.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9755888"], [" is", "1.0"], [" the", "1.0"], [" largest", "0.9364929"], [" animal", "0.98138005"], [" in", "0.9999807"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" largest", "1.0"], [" animal", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" is", "1.0"], [" the", "1.0"], [" blue", "1.0"], [" whale", "1.0"], [".", "1.0"], [" The", "1.0"], [" blue", "1.0"], [" whale", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" animal", "1.0"], [" in", "0.9996828"], [" the", "1.0"], [" world", "1.0"], [".", "1.0"], ["\n", "0.9966408"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"]]}, "712": {"qid": "ebbf0b9ffffd08d812e5", "term": "Spice Girls", "description": "British girl group", "question": "Tata Hexa can accomodate every Spice Girl?", "answer": true, "facts": ["The Spice Girls is a five woman musical group from Britain.", "The Tata Hexa is a car with 6 and 7 seat capacities."], "decomposition": ["How many women are in the Spice Girls group?", "How many people can the Tata Hexa seat?", "Is #2 greater than or equal to #1?"], "evidence": [[[["Spice Girls-1"]], [["Tata Aria-2", "Tata Aria-5"], "no_evidence"], ["operation"]], [[["Spice Girls-1"]], [["Tata Hexa-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Spice Girls-1"]], ["no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first man to fly across the Atlantic?\nFacts: The first man to fly across the Atlantic was Charles Lindbergh.\nAnswer (True or False): True\nQuestion: What is the name of the first man to fly across the Atlantic?\nFacts: The first man to fly across the Atlantic was Charles Lindbergh.\nAnswer (", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Tata Hexa can accomodate every Spice Girl?\nFacts: The Spice Girls is a five woman musical group from Britain. The Tata Hexa is a car with 6 and 7 seat capacities.\nAnswer (True or False):", "ct_subject": "The Spice Girls is a five woman musical group from Britain. The Tata Hexa is a car with 6 and 7 seat capacities.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999552"], [" is", "1.0"], [" the", "1.0"], [" name", "0.8280539"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9493807"], [" man", "0.85398775"], [" to", "0.9999994"], [" fly", "0.9829289"], [" across", "0.9999397"], [" the", "1.0"], [" Atlantic", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" fly", "1.0"], [" across", "1.0"], [" the", "1.0"], [" Atlantic", "1.0"], [" was", "1.0"], [" Charles", "0.98134446"], [" Lind", "1.0"], ["ber", "1.0"], ["gh", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" fly", "1.0"], [" across", "1.0"], [" the", "1.0"], [" Atlantic", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" fly", "1.0"], [" across", "1.0"], [" the", "1.0"], [" Atlantic", "1.0"], [" was", "1.0"], [" Charles", "1.0"], [" Lind", "1.0"], ["ber", "1.0"], ["gh", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"]]}, "713": {"qid": "5fe8b705fc7bb1783a61", "term": "Foreign and Commonwealth Office", "description": "Ministry of Foreign Affairs of the United Kingdom", "question": "Is the Foreign and Commonwealth Office a European political agency?", "answer": true, "facts": ["The Office is part of the government of the United Kingdom.", "The United Kingdom is in Europe."], "decomposition": ["What country is the Foreign and Commonwealth Office part of?", "Is #1 located in Europe?"], "evidence": [[[["Foreign and Commonwealth Office-1"]], [["Outline of the United Kingdom-1"], "operation"]], [[["Foreign and Commonwealth Office-4"]], [["Turkey\u2013United Kingdom relations-13"]]], [[["Foreign and Commonwealth Office-1"]], [["Outline of the United Kingdom-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the United Kingdom a country?\nFacts: The United Kingdom is a political union of England, Scotland, Wales, and Northern Ireland.\nAnswer (True or False): True\nQuestion: Is the United Kingdom a country?\nFacts: The United Kingdom is a political union of England, Scotland, Wales, and Northern Ireland.\nAnswer (True or False): True\nQuestion: Is the United Kingdom a country", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is the Foreign and Commonwealth Office a European political agency?\nFacts: The Office is part of the government of the United Kingdom. The United Kingdom is in Europe.\nAnswer (True or False):", "ct_subject": "The Office is part of the government of the United Kingdom. The United Kingdom is in Europe.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99964356"], [" the", "1.0"], [" United", "0.9999975"], [" Kingdom", "0.033988155"], [" a", "1.0"], [" country", "0.97696817"], ["?", "0.99999976"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "1.0"], [" Kingdom", "1.0"], [" is", "1.0"], [" a", "1.0"], [" political", "0.9993705"], [" union", "0.99863964"], [" of", "1.0"], [" England", "0.9954396"], [",", "1.0"], [" Scotland", "1.0"], [",", "1.0"], [" Wales", "1.0"], [",", "0.9671175"], [" and", "1.0"], [" Northern", "1.0"], [" Ireland", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999999"], [" the", "1.0"], [" United", "1.0"], [" Kingdom", "0.993522"], [" a", "1.0"], [" country", "0.9996253"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "1.0"], [" Kingdom", "1.0"], [" is", "1.0"], [" a", "1.0"], [" political", "1.0"], [" union", "1.0"], [" of", "1.0"], [" England", "1.0"], [",", "1.0"], [" Scotland", "1.0"], [",", "1.0"], [" Wales", "1.0"], [",", "1.0"], [" and", "1.0"], [" Northern", "1.0"], [" Ireland", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" United", "1.0"], [" Kingdom", "1.0"], [" a", "1.0"], [" country", "1.0"]]}, "714": {"qid": "7f4effbc97ab2b5fd4a7", "term": "British cuisine", "description": "culinary traditions of the United Kingdom", "question": "Could an American confuse breakfast in British cuisine for dinner?", "answer": true, "facts": ["In British Cuisine, baked beans are served with toast for breakfast.", "In the US, baked beans are served alongside barbecue dinners.", "British 'Full breakfasts' include grilled vegetables like mushrooms and whole cherry tomatoes.", "Grilled mushrooms and tomatoes are used more often in evening dishes in the US."], "decomposition": ["What foods are part of a traditional British breakfast?", "What foods are part of a traditional American dinner?", "Is there overlap between #1 and #2?"], "evidence": [[[["Breakfast-63"]], [["Meal-16"]], ["operation"]], [[["Breakfast-63"]], [["Burger King breakfast sandwiches-6"]], ["operation"]], [[["Full breakfast-1"]], [["Mushroom-28", "Tomato-85"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a 'pig' and a 'piggy'?\nFacts: A pig is a large, domesticated animal. A piggy is a small, wild animal.\nAnswer (True or", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could an American confuse breakfast in British cuisine for dinner?\nFacts: In British Cuisine, baked beans are served with toast for breakfast. In the US, baked beans are served alongside barbecue dinners. British 'Full breakfasts' include grilled vegetables like mushrooms and whole cherry tomatoes. Grilled mushrooms and tomatoes are used more often in evening dishes in the US.\nAnswer (True or False):", "ct_subject": "In British Cuisine, baked beans are served with toast for breakfast. In the US, baked beans are served alongside barbecue dinners. British 'Full breakfasts' include grilled vegetables like mushrooms and whole cherry tomatoes. Grilled mushrooms and tomatoes are used more often in evening dishes in the US.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99962366"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.999961"], [" between", "1.0"], [" a", "0.9999999"], [" '", "1.0"], ["p", "0.83989847"], ["ig", "0.9982868"], ["'", "1.0"], [" and", "1.0"], [" a", "1.0"], [" '", "1.0"], ["p", "1.0"], ["ig", "1.0"], ["gy", "0.99999654"], ["'?", "0.96946454"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.99999595"], [" pig", "0.99999464"], [" is", "1.0"], [" a", "1.0"], [" large", "1.0"], [",", "0.9865059"], [" domest", "0.76787585"], ["icated", "1.0"], [" animal", "1.0"], [".", "0.9969892"], [" A", "1.0"], [" pig", "1.0"], ["gy", "1.0"], [" is", "1.0"], [" a", "1.0"], [" small", "1.0"], [",", "1.0"], [" wild", "1.0"], [" animal", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"]]}, "715": {"qid": "9a1bf03a1359fa94eb7c", "term": "Cheeseburger", "description": "hamburger topped with cheese", "question": "Is a krabby patty similar to a cheeseburger?", "answer": true, "facts": ["A krabby patty is a fictional sandwich featuring a patty on a split bun with toppings like lettuce, onion, and tomato.", "A hamburger is typically served on a bun and offers toppings like lettuce, onion, and tomato."], "decomposition": ["What are the ingredients of a Krabby Patty?", "What are the ingredients of a cheeseburger?", "Is there significant overlap between #1 and #2?"], "evidence": [[[["Krabby Patty-4"]], [["Cheeseburger-10"]], ["operation"]], [[["Krabby Patty-4"]], [["Cheeseburger-1"]], ["operation"]], [[["Krabby Patty-4"]], [["Cheeseburger-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a kibbles and a kibbles and a kibbles and a kibbles?\nFacts: A kibbles is a fictional sandwich featuring a patty on a split bun with toppings like lettuce, onion", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is a krabby patty similar to a cheeseburger?\nFacts: A krabby patty is a fictional sandwich featuring a patty on a split bun with toppings like lettuce, onion, and tomato. A hamburger is typically served on a bun and offers toppings like lettuce, onion, and tomato.\nAnswer (True or False):", "ct_subject": "A krabby patty is a fictional sandwich featuring a patty on a split bun with toppings like lettuce, onion, and tomato. A hamburger is typically served on a bun and offers toppings like lettuce, onion, and tomato.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.6658111"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9998447"], [" between", "1.0"], [" a", "1.0"], [" k", "0.9999826"], ["ib", "0.9999987"], ["bles", "0.9851201"], [" and", "1.0"], [" a", "1.0"], [" k", "1.0"], ["ib", "0.9995689"], ["bles", "1.0"], [" and", "1.0"], [" a", "0.9999999"], [" k", "1.0"], ["ib", "1.0"], ["bles", "1.0"], [" and", "1.0"], [" a", "1.0"], [" k", "1.0"], ["ib", "1.0"], ["bles", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" k", "1.0"], ["ib", "1.0"], ["bles", "1.0"], [" is", "1.0"], [" a", "1.0"], [" fictional", "1.0"], [" sandwich", "1.0"], [" featuring", "1.0"], [" a", "1.0"], [" pat", "1.0"], ["ty", "1.0"], [" on", "1.0"], [" a", "1.0"], [" split", "1.0"], [" bun", "1.0"], [" with", "1.0"], [" topp", "1.0"], ["ings", "1.0"], [" like", "1.0"], [" lettuce", "1.0"], [",", "1.0"], [" onion", "1.0"]]}, "716": {"qid": "7a0e419ffb6009156828", "term": "Apollo 13", "description": "A failed crewed mission to land on the Moon", "question": "Was ship that recovered Apollo 13 named after a World War II battle?", "answer": true, "facts": ["Apollo 13 was recovered by the USS Iwo Jima.", "Iwo Jima was captured from the Imperial Japanese Army during World War II by the US in a conflict called the Battle of Iwo Jima."], "decomposition": ["Which ship recovered Apollo 13 crew?", "What was #1 named for?", "Did #2 occur during World War II?"], "evidence": [[[["USS Iwo Jima (LPH-2)-13"]], [["USS Iwo Jima (LPH-2)-1"]], [["Battle of Iwo Jima-1"]]], [[["Apollo 13-55"]], [["USS Iwo Jima (LPH-2)-1"]], ["operation"]], [[["Apollo 13-55"]], [["Iwo Jima-3"]], [["Iwo Jima-19"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the capital of the state of Texas?\nFacts: The city of Austin is the capital of Texas.\nAnswer (True or False): True\nQuestion: What is the name of the city in the United States that is the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was ship that recovered Apollo 13 named after a World War II battle?\nFacts: Apollo 13 was recovered by the USS Iwo Jima. Iwo Jima was captured from the Imperial Japanese Army during World War II by the US in a conflict called the Battle of Iwo Jima.\nAnswer (True or False):", "ct_subject": "Apollo 13 was recovered by the USS Iwo Jima. Iwo Jima was captured from the Imperial Japanese Army during World War II by the US in a conflict called the Battle of Iwo Jima.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9801713"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9415325"], [" in", "0.9998511"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "0.99978787"], [" the", "0.99975675"], [" capital", "0.99999785"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" Texas", "0.761003"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99618775"], [" city", "0.96299475"], [" of", "1.0"], [" Austin", "0.9999999"], [" is", "0.99985754"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" Texas", "0.33920485"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"]]}, "717": {"qid": "668a4e03534608476faf", "term": "Ringo Starr", "description": "British musician, drummer of the Beatles", "question": "Would Ringo Starr avoid the pot roast at a restaurant?", "answer": true, "facts": ["Ringo Starr is a vegetarian.", "Vegetarianism is the practice of abstaining from the consumption of meat.", "Pot roast is a braised beef dish made by browning a roast-sized piece of beef before slow cooking the meat in a covered dish, sometimes with vegetables, in or over liquid."], "decomposition": ["What dietary system does Ringo Starr follow?", "What type of foods are not allowed to be eaten by someone following #1?", "What is pot roast made of?", "Is #3 part of #2?"], "evidence": [[[["Ringo Starr-71"]], [["Vegetarianism-1"]], [["Pot roast-1"]], ["operation"]], [[["Ringo Starr-71"]], [["Vegetarianism-1"]], [["Pot roast-1"]], [["Beef-1"], "operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"sausage\" and a \"pork chop\"?\nFacts: A \"sausage\" is a sausage made from pork. A \"pork chop\" is a pork", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Ringo Starr avoid the pot roast at a restaurant?\nFacts: Ringo Starr is a vegetarian. Vegetarianism is the practice of abstaining from the consumption of meat. Pot roast is a braised beef dish made by browning a roast-sized piece of beef before slow cooking the meat in a covered dish, sometimes with vegetables, in or over liquid.\nAnswer (True or False):", "ct_subject": "Ringo Starr is a vegetarian. Vegetarianism is the practice of abstaining from the consumption of meat. Pot roast is a braised beef dish made by browning a roast-sized piece of beef before slow cooking the meat in a covered dish, sometimes with vegetables, in or over liquid.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999964"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.48024035"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.9999994"], ["s", "0.95178235"], ["aus", "0.87844586"], ["age", "1.0"], ["\"", "0.9858753"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["p", "0.9952583"], ["ork", "1.0"], [" chop", "0.99760455"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999938"], [" \"", "0.9908251"], ["s", "1.0"], ["aus", "1.0"], ["age", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" sausage", "0.99994886"], [" made", "1.0"], [" from", "0.98984575"], [" pork", "0.9999422"], [".", "0.99998665"], [" A", "1.0"], [" \"", "1.0"], ["p", "1.0"], ["ork", "1.0"], [" chop", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" pork", "1.0"]]}, "718": {"qid": "2549ab4ce062ef762c4c", "term": "Bern", "description": "Place in Switzerland", "question": "Are Citizens of Bern Switzerland are descendants of Genghis Khan?", "answer": true, "facts": ["Genghis Khan had sixteen children.", "1 in 200 men are direct descendants of Genghis Khan.", "Switzerland has a large Asian immigration population which was around 19,000 in 2018."], "decomposition": ["What ethnic groups contain much of Genghis Khan's descendants?", "Is there a large population of any of #1 in Bern?"], "evidence": [[[["Descent from Genghis Khan-2"], "no_evidence"], [["Bern-39"], "no_evidence"]], [[["Descent from Genghis Khan-2", "Descent from Genghis Khan-22"]], ["no_evidence"]], [[["Genghis Khan-2"], "no_evidence"], [["Bern-39"], "no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any people in the world today who are descendants of Genghis Khan?\nFacts: Genghis Khan had sixteen children. 1 in 200 men are direct descendants of Genghis Khan. There are no people in the world today who are descendants of Genghis Khan.\nAnswer (True or False):", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are Citizens of Bern Switzerland are descendants of Genghis Khan?\nFacts: Genghis Khan had sixteen children. 1 in 200 men are direct descendants of Genghis Khan. Switzerland has a large Asian immigration population which was around 19,000 in 2018.\nAnswer (True or False):", "ct_subject": "Genghis Khan had sixteen children. 1 in 200 men are direct descendants of Genghis Khan. Switzerland has a large Asian immigration population which was around 19,000 in 2018.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "0.99661887"], [" any", "0.9775164"], [" people", "0.98158276"], [" in", "0.9984779"], [" the", "1.0"], [" world", "1.0"], [" today", "0.99991083"], [" who", "0.7056171"], [" are", "0.99998057"], [" descendants", "0.90889376"], [" of", "1.0"], [" Gen", "1.0"], ["gh", "1.0"], ["is", "1.0"], [" Khan", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Gen", "0.9999999"], ["gh", "1.0"], ["is", "1.0"], [" Khan", "1.0"], [" had", "1.0"], [" sixteen", "1.0"], [" children", "1.0"], [".", "1.0"], [" 1", "1.0"], [" in", "1.0"], [" 200", "1.0"], [" men", "1.0"], [" are", "1.0"], [" direct", "1.0"], [" descendants", "1.0"], [" of", "1.0"], [" Gen", "1.0"], ["gh", "1.0"], ["is", "1.0"], [" Khan", "1.0"], [".", "1.0"], [" There", "0.9489694"], [" are", "1.0"], [" no", "0.7013055"], [" people", "1.0"], [" in", "0.9995184"], [" the", "1.0"], [" world", "1.0"], [" today", "1.0"], [" who", "1.0"], [" are", "1.0"], [" descendants", "0.51839"], [" of", "1.0"], [" Gen", "1.0"], ["gh", "1.0"], ["is", "1.0"], [" Khan", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"]]}, "719": {"qid": "8c3ca12decfff256eddb", "term": "Lord Voldemort", "description": "Fictional character of Harry Potter series", "question": "Is Lord Voldemort associated with a staff member of Durmstrang?", "answer": true, "facts": ["Igor Karkaroff is the headmaster of Durmstrang school.", "Karkaroff is a former Death Eater.", "The Death Eaters were Voldemort's minions."], "decomposition": ["Who is the headmaster of Durmstrang school?", "What did #1 part of in the past?", "Is #2 related to Lord Voldemort?"], "evidence": [[[["Places in Harry Potter-31"], "no_evidence"], [["Places in Harry Potter-32"]], ["no_evidence"]], [[["Death Eater-30"]], [["Death Eater-31"]], [["Death Eater-1"]]], [[["Places in Harry Potter-31"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the wizarding school Hogwarts derived from the word \"Hogwarts\" or from the word \"Hogwarts\" itself?\nFacts: The name of the school is derived from the word \"Hogwarts\" itself.\nAnswer (True or False): True\nQuestion: Is the name of the wizard", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Lord Voldemort associated with a staff member of Durmstrang?\nFacts: Igor Karkaroff is the headmaster of Durmstrang school. Karkaroff is a former Death Eater. The Death Eaters were Voldemort's minions.\nAnswer (True or False):", "ct_subject": "Igor Karkaroff is the headmaster of Durmstrang school. Karkaroff is a former Death Eater. The Death Eaters were Voldemort's minions.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" name", "0.99866474"], [" of", "0.99999964"], [" the", "1.0"], [" wizard", "0.9999639"], ["ing", "1.0"], [" school", "0.99999785"], [" Hogwarts", "0.99292916"], [" derived", "0.9999931"], [" from", "1.0"], [" the", "1.0"], [" word", "0.090659976"], [" \"", "1.0"], ["H", "1.0"], ["og", "1.0"], ["warts", "1.0"], ["\"", "0.9999175"], [" or", "0.99999964"], [" from", "0.8194874"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["H", "1.0"], ["og", "1.0"], ["warts", "0.99991524"], ["\"", "0.27199098"], [" itself", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.6864193"], [" name", "0.92590904"], [" of", "0.99999964"], [" the", "0.9959804"], [" school", "1.0"], [" is", "0.9999862"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["H", "1.0"], ["og", "1.0"], ["warts", "1.0"], ["\"", "0.9739523"], [" itself", "0.99697113"], [".", "1.0"], ["\n", "0.7009876"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" wizard", "0.99997604"]]}, "720": {"qid": "4c72798de7aca9e0954e", "term": "Metallica", "description": "American heavy metal band", "question": "Is Metallica protective over their music?", "answer": true, "facts": ["Napster was a P2P music sharing service.", "Metallica sued Napster in order to remove their songs from the program, as they were not getting profit from it."], "decomposition": ["What did Metallica do in response to Napster hosting their songs?", "Did #1 involve legal action?"], "evidence": [[[["Metallica v. Napster, Inc.-1"]], [["Lawsuit-1"], "operation"]], [[["Metallica-3"]], ["operation"]], [[["Metallica-3"]], [["Metallica-29"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the word \"cunt\" a racial slur?\nFacts: The word \"cunt\" is a racial slur.\nAnswer (True or False): True\nQuestion: Is the word \"cunt\" a racial slur?\nFacts: The word \"cunt\" is a racial slur.\nAnswer (True or False): True\nQuestion", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Metallica protective over their music?\nFacts: Napster was a P2P music sharing service. Metallica sued Napster in order to remove their songs from the program, as they were not getting profit from it.\nAnswer (True or False):", "ct_subject": "Napster was a P2P music sharing service. Metallica sued Napster in order to remove their songs from the program, as they were not getting profit from it.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9355041"], [" the", "0.95510775"], [" word", "0.54604757"], [" \"", "1.0"], ["c", "0.8671388"], ["unt", "0.9675504"], ["\"", "1.0"], [" a", "0.99999905"], [" racial", "0.8235809"], [" slur", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99998796"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["unt", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "0.990855"], [" racial", "1.0"], [" slur", "1.0"], [".", "1.0"], ["\n", "0.9996606"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["unt", "1.0"], ["\"", "1.0"], [" a", "1.0"], [" racial", "1.0"], [" slur", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["unt", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" racial", "1.0"], [" slur", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"]]}, "721": {"qid": "33858996585a4d2ab95e", "term": "Ludacris", "description": "American rapper and actor", "question": "Does Ludacris have Greek heritage?", "answer": true, "facts": ["Ludacris's real name is Christopher Brian Bridges", "Christopher is a name derived from Greek origins"], "decomposition": ["What is Ludacris's real name?", "Where is #1 derived from?"], "evidence": [[[["Ludacris-1"]], ["no_evidence"]], [[["Ludacris-1"]], [["Christopher-1"]]], [[["Ludacris-1"]], [["Ludacris-3"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city of Rome derived from the Roman god of war?\nFacts: The name of the city of Rome is derived from the Roman god of war, Mars\nAnswer (True or False): True\nQuestion: Is the name of the city of Rome derived from the Roman god of war?\nFacts: The name of the city of Rome is derived from the Roman god of war, Mars\nAnswer (True or", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Ludacris have Greek heritage?\nFacts: Ludacris's real name is Christopher Brian Bridges Christopher is a name derived from Greek origins\nAnswer (True or False):", "ct_subject": "Ludacris's real name is Christopher Brian Bridges Christopher is a name derived from Greek origins", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99360174"], [" the", "0.9999043"], [" name", "0.99999595"], [" of", "0.96968913"], [" the", "1.0"], [" city", "0.9999819"], [" of", "1.0"], [" Rome", "0.9999629"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" Roman", "0.9669977"], [" god", "0.9999938"], [" of", "0.9999999"], [" war", "0.999998"], ["?", "0.9999614"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" name", "0.9999994"], [" of", "1.0"], [" the", "0.9334753"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], [" is", "1.0"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" god", "1.0"], [" of", "1.0"], [" war", "1.0"], [",", "0.9976051"], [" Mars", "1.0"], ["\n", "0.9999502"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" god", "1.0"], [" of", "1.0"], [" war", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], [" is", "1.0"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" god", "1.0"], [" of", "1.0"], [" war", "1.0"], [",", "1.0"], [" Mars", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"]]}, "722": {"qid": "01b8f2656a604fd71549", "term": "Spinach", "description": "species of plant", "question": "Has spinach been a source of power in a comic movie?", "answer": true, "facts": ["The comic character Popeye uses spinach as a source of power.", "A movie was made about Popeye.", "Popeye consumes spinach as a source of power in the movie."], "decomposition": ["Which movie was made for comic character Popeye?", "What was Popeye's source of power in #1", "Is #2 spinach?"], "evidence": [[[["Popeye-53"]], [["Popeye-68"]], [["Popeye-68"], "operation"]], [[["Popeye (film)-1"]], [["Popeye-6"]], ["operation"]], [[["Popeye the Sailor (film)-1"]], [["Popeye the Sailor (film)-4"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the character in the movie \"The Princess Bride\" who is the most powerful man in the world?\nFacts: The character in the movie \"The Princess Bride\" is named Vizzini. Vizzini is the most powerful man in the world.\nAnswer (True or False): True\nQuestion: What is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Has spinach been a source of power in a comic movie?\nFacts: The comic character Popeye uses spinach as a source of power. A movie was made about Popeye. Popeye consumes spinach as a source of power in the movie.\nAnswer (True or False):", "ct_subject": "The comic character Popeye uses spinach as a source of power. A movie was made about Popeye. Popeye consumes spinach as a source of power in the movie.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9976221"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99999964"], [" of", "1.0"], [" the", "1.0"], [" character", "0.22502436"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "1.0"], ["The", "1.0"], [" Princess", "0.9817085"], [" Bride", "1.0"], ["\"", "0.995773"], [" who", "1.0"], [" is", "0.9999995"], [" the", "0.99999344"], [" most", "0.5509928"], [" powerful", "0.9795436"], [" man", "0.98552424"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" character", "0.99999976"], [" in", "0.93003213"], [" the", "1.0"], [" movie", "1.0"], [" \"", "0.9999552"], ["The", "1.0"], [" Princess", "1.0"], [" Bride", "1.0"], ["\"", "1.0"], [" is", "0.99123234"], [" named", "0.9999819"], [" V", "0.99999964"], ["izz", "1.0"], ["ini", "1.0"], [".", "1.0"], [" V", "0.9847948"], ["izz", "1.0"], ["ini", "1.0"], [" is", "1.0"], [" the", "0.99999833"], [" most", "1.0"], [" powerful", "1.0"], [" man", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"]]}, "723": {"qid": "9608fbb33f01b799a816", "term": "Chlorophyll", "description": "group of chemical compounds", "question": "For Hostas to look their best, do they need lots of chlorophyll?", "answer": true, "facts": ["Hostas are characterized by large green striped leaves.", "The green color in plants is attributed to chlorophyll. "], "decomposition": ["What color is a visually appealing hosta?", "Do the get #1 from chlorophyll?"], "evidence": [[[["Hosta-2"]], [["Chlorophyll-2"], "operation"]], [[["Hosta-6"], "no_evidence"], [["Chloroplast-1", "Hosta-2"], "operation"]], [[["Hosta-2"]], [["Chlorophyll-2"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a hosta and a bromeliad?\nFacts: Hostas are plants that have a single flower. Bromeliads have many flowers.\nAnswer (True or False): True\nQuestion: What is the difference between a hosta and a bromeliad?\nFacts: Hostas are plants that have a single flower.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: For Hostas to look their best, do they need lots of chlorophyll?\nFacts: Hostas are characterized by large green striped leaves. The green color in plants is attributed to chlorophyll. \nAnswer (True or False):", "ct_subject": "Hostas are characterized by large green striped leaves. The green color in plants is attributed to chlorophyll. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9976767"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9999888"], [" between", "1.0"], [" a", "1.0"], [" host", "0.9999995"], ["a", "1.0"], [" and", "1.0"], [" a", "1.0"], [" b", "0.31344515"], ["rom", "1.0"], ["eli", "1.0"], ["ad", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Host", "1.0"], ["as", "1.0"], [" are", "1.0"], [" plants", "0.99999535"], [" that", "1.0"], [" have", "0.9951866"], [" a", "0.99999607"], [" single", "0.986814"], [" flower", "1.0"], [".", "1.0"], [" Brom", "1.0"], ["eli", "0.99999666"], ["ads", "1.0"], [" have", "0.9924942"], [" many", "0.69313186"], [" flowers", "1.0"], [".", "0.99999976"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" host", "1.0"], ["a", "1.0"], [" and", "1.0"], [" a", "1.0"], [" b", "1.0"], ["rom", "1.0"], ["eli", "1.0"], ["ad", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Host", "1.0"], ["as", "1.0"], [" are", "1.0"], [" plants", "1.0"], [" that", "1.0"], [" have", "1.0"], [" a", "1.0"], [" single", "1.0"], [" flower", "1.0"], [".", "1.0"]]}, "724": {"qid": "192bea4bc6d8b65a513a", "term": "QWERTY", "description": "keyboard layout where the first line is \"QWERTYUIOP\"", "question": "Can the majority of vowels be typed on the first line of a QWERTY keyboard?", "answer": true, "facts": ["In English the vowels consist of A, E, I, O, U, and sometimes Y.", "The first line of the QWERTY keyboard contains the vowels E, I, O, U, and Y.", "A majority means more than half of the total."], "decomposition": ["What letters are vowels in the English language?", "What are the letters on the first line of a Qwerty keyboard?", "Is more than half of the letters listed in #1 also listed in #2?"], "evidence": [[[["English alphabet-20"]], [["QWERTY-1", "Ray Tomlinson-5"], "no_evidence"], ["operation"]], [[["Vowel-49"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Vowel-49"]], [["QWERTY-1"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest word in the English language?\nFacts: The longest word in the English language is \"the\" which is 5,928 letters long. The longest word in the English language is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can the majority of vowels be typed on the first line of a QWERTY keyboard?\nFacts: In English the vowels consist of A, E, I, O, U, and sometimes Y. The first line of the QWERTY keyboard contains the vowels E, I, O, U, and Y. A majority means more than half of the total.\nAnswer (True or False):", "ct_subject": "In English the vowels consist of A, E, I, O, U, and sometimes Y. The first line of the QWERTY keyboard contains the vowels E, I, O, U, and Y. A majority means more than half of the total.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99925643"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.9999168"], [" word", "0.99993086"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], [" is", "1.0"], [" \"", "1.0"], ["the", "0.9999037"], ["\"", "0.98539865"], [" which", "0.999858"], [" is", "1.0"], [" 5", "0.90818894"], [",", "1.0"], ["9", "0.82759297"], ["28", "0.80479187"], [" letters", "0.9972247"], [" long", "1.0"], [".", "1.0"], [" The", "0.33877748"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], [" is", "1.0"]]}, "725": {"qid": "bcd77b200a2b90ca2ecc", "term": "Jackson Pollock", "description": "American painter", "question": "Is it understandable to compare a blood spatter pattern to a Jackson Pollock piece?", "answer": true, "facts": ["Jackson Pollock is well known for a style of art formed through splashing liquids on canvas.", "Blood spatter patterns are caused by a splash of blood onto a surface or multiple surfaces."], "decomposition": ["What kinds of work pieces is Jackson Pollock well known for?", "How does he form #1", "How is a blood splatter formed?", "Is #2 comparable to #3?"], "evidence": [[[["Jackson Pollock-1"]], [["Jackson Pollock-2"]], [["Bloodstain pattern analysis-5"]], ["operation"]], [[["Jackson Pollock-1"]], [["Jackson Pollock-2"]], [["Bloodstain pattern analysis-4"]], ["operation"]], [[["Jackson Pollock-10"]], [["Jackson Pollock-2"]], [["Blood squirt-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it possible to have a blood spatter pattern on a piece of paper?\nFacts: Yes, it is possible to have a blood spatter pattern on a piece of paper.\nAnswer (True or False): True\nQuestion: Is it possible to have a blood spatter pattern on a piece", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is it understandable to compare a blood spatter pattern to a Jackson Pollock piece?\nFacts: Jackson Pollock is well known for a style of art formed through splashing liquids on canvas. Blood spatter patterns are caused by a splash of blood onto a surface or multiple surfaces.\nAnswer (True or False):", "ct_subject": "Jackson Pollock is well known for a style of art formed through splashing liquids on canvas. Blood spatter patterns are caused by a splash of blood onto a surface or multiple surfaces.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999049"], [" it", "1.0"], [" possible", "1.0"], [" to", "1.0"], [" have", "0.49048135"], [" a", "1.0"], [" blood", "0.9923253"], [" sp", "1.0"], ["atter", "1.0"], [" pattern", "1.0"], [" on", "0.9992267"], [" a", "1.0"], [" piece", "0.8001923"], [" of", "1.0"], [" paper", "1.0"], ["?", "0.9999975"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Yes", "0.9999939"], [",", "0.92990625"], [" it", "1.0"], [" is", "1.0"], [" possible", "1.0"], [" to", "0.8860173"], [" have", "1.0"], [" a", "1.0"], [" blood", "1.0"], [" sp", "1.0"], ["atter", "1.0"], [" pattern", "1.0"], [" on", "1.0"], [" a", "1.0"], [" piece", "1.0"], [" of", "1.0"], [" paper", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" possible", "1.0"], [" to", "1.0"], [" have", "1.0"], [" a", "1.0"], [" blood", "1.0"], [" sp", "1.0"], ["atter", "1.0"], [" pattern", "1.0"], [" on", "1.0"], [" a", "1.0"], [" piece", "1.0"]]}, "726": {"qid": "b92b31f0e7124066eb48", "term": "Porch", "description": "a room or gallery at the front entrance of a building forming a low front", "question": "In Hey Arnold, did any characters stay on a porch all the time?", "answer": true, "facts": ["Hey Arnold was an animated children's series.", "Hey Arnold featured 'Stoop Kid', a character who never left the front stoop of his home.", "A stoop is the city equivalent of a porch."], "decomposition": ["Where is 'Stoop Kid' in Hey Arnold known to never leave?", "Is #1 in the series equivalent to a porch in real life?"], "evidence": [[[["Hey Arnold!-7"], "no_evidence"], [["Porch-1", "Stoop (architecture)-1"], "no_evidence", "operation"]], [["no_evidence"], [["Stoop (architecture)-2"]]], [[["Hey Arnold!-1"], "no_evidence"], [["Stoop (architecture)-2"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the character in the movie The Princess Bride who is always wearing a hat?\nFacts: The character in the movie The Princess Bride is named 'Vizzini'. Vizzini is a name of a character in the movie The Princess Bride.\nAnswer (True or", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: In Hey Arnold, did any characters stay on a porch all the time?\nFacts: Hey Arnold was an animated children's series. Hey Arnold featured 'Stoop Kid', a character who never left the front stoop of his home. A stoop is the city equivalent of a porch.\nAnswer (True or False):", "ct_subject": "Hey Arnold was an animated children's series. Hey Arnold featured 'Stoop Kid', a character who never left the front stoop of his home. A stoop is the city equivalent of a porch.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.90808946"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999937"], [" of", "1.0"], [" the", "1.0"], [" character", "1.0"], [" in", "0.99981326"], [" the", "0.9999999"], [" movie", "1.0"], [" The", "0.077090845"], [" Princess", "0.9999546"], [" Bride", "1.0"], [" who", "0.7102516"], [" is", "0.9998447"], [" always", "0.9774419"], [" wearing", "0.58849823"], [" a", "1.0"], [" hat", "0.9888864"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" character", "1.0"], [" in", "0.99999964"], [" the", "0.78549886"], [" movie", "1.0"], [" The", "1.0"], [" Princess", "1.0"], [" Bride", "1.0"], [" is", "0.9999416"], [" named", "1.0"], [" '", "0.89155585"], ["V", "0.99912065"], ["izz", "1.0"], ["ini", "1.0"], ["'.", "0.9999995"], [" V", "0.99995995"], ["izz", "1.0"], ["ini", "1.0"], [" is", "0.9999999"], [" a", "0.99917537"], [" name", "0.9467578"], [" of", "0.99990666"], [" a", "0.9962723"], [" character", "1.0"], [" in", "1.0"], [" the", "1.0"], [" movie", "0.9052556"], [" The", "1.0"], [" Princess", "1.0"], [" Bride", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"]]}, "727": {"qid": "e43424acbaf3f64feefd", "term": "Brazilian Navy", "description": "Naval warfare branch of Brazil's military forces", "question": "Are some Brazilian Navy ships built in Britian?", "answer": true, "facts": ["The Brazilian Navy stated in 2018 that they had purchased the helicopter carrier ship HMS Ocean.", "HMS stands for \"His/Her Majesty's Ship\", which is emblazoned on ships of the British Royal Navy. ", "Some of the ships in the Brazilian Navy are guided missile frigates built in Britian."], "decomposition": ["Which helicopter carrier ship did the Brazilian Navy announce that they had acquired in 2018?", "Was #1 built in Britain?"], "evidence": [[[["Brazilian Navy-62"]], [["HMS Ocean (L12)-1"]]], [[["HMS Ocean (L12)-2"]], [["HMS Ocean-1"]]], [[["Aircraft carrier-43"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any other countries that have a population of over 1 billion?\nFacts: There are over 1.3 billion people in the world today.\nAnswer (True or False): True\nQuestion: Are there any", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are some Brazilian Navy ships built in Britian?\nFacts: The Brazilian Navy stated in 2018 that they had purchased the helicopter carrier ship HMS Ocean. HMS stands for \"His/Her Majesty's Ship\", which is emblazoned on ships of the British Royal Navy.  Some of the ships in the Brazilian Navy are guided missile frigates built in Britian.\nAnswer (True or False):", "ct_subject": "The Brazilian Navy stated in 2018 that they had purchased the helicopter carrier ship HMS Ocean. HMS stands for \"His/Her Majesty's Ship\", which is emblazoned on ships of the British Royal Navy.  Some of the ships in the Brazilian Navy are guided missile frigates built in Britian.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.9860721"], [" there", "0.9995372"], [" any", "0.9999789"], [" other", "0.15629461"], [" countries", "0.99999857"], [" that", "1.0"], [" have", "1.0"], [" a", "1.0"], [" population", "0.9575547"], [" of", "0.99994314"], [" over", "0.99974686"], [" 1", "0.99960417"], [" billion", "1.0"], ["?", "0.9999616"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "0.99989235"], [" are", "1.0"], [" over", "0.77394277"], [" 1", "1.0"], [".", "0.27943254"], ["3", "0.9997248"], [" billion", "1.0"], [" people", "1.0"], [" in", "0.99999535"], [" the", "1.0"], [" world", "1.0"], [" today", "0.84739244"], [".", "1.0"], ["\n", "0.9999999"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.99993753"], [" there", "1.0"], [" any", "1.0"]]}, "728": {"qid": "80aa769f55b14c1e4d8d", "term": "Sable", "description": "Species of marten", "question": "Are sables related to wolverines?", "answer": true, "facts": ["The sable is a species of marten, which make up the genus Martes.", "Wolverines are from the genus Gulo.", "Both the Martes and the Gulo are from the family Mustelidae."], "decomposition": ["What species is a sable?", "What genus is #1 from?", "What genus are wolverines from?", "Are #2 and #3 from the same family?"], "evidence": [[[["Sable-1"]], [["Marten-1"]], [["Gulo-1"]], [["Gulo-1", "Marten-1"]]], [[["Sable-1"]], [["Marten-1"]], [["Gulo-1", "Wolverine-1"]], ["operation"]], [[["Sable-1"]], [["Marten-1"]], [["Gulo-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are the stars in the sky related to the stars in the sky?\nFacts: The stars in the sky are not related to the stars in the sky. The stars in the sky are not related to the stars in the sky. The stars in the sky are not related to the stars in the sky. The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are sables related to wolverines?\nFacts: The sable is a species of marten, which make up the genus Martes. Wolverines are from the genus Gulo. Both the Martes and the Gulo are from the family Mustelidae.\nAnswer (True or False):", "ct_subject": "The sable is a species of marten, which make up the genus Martes. Wolverines are from the genus Gulo. Both the Martes and the Gulo are from the family Mustelidae.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" the", "0.9510087"], [" stars", "0.99860686"], [" in", "1.0"], [" the", "1.0"], [" sky", "0.581388"], [" related", "1.0"], [" to", "1.0"], [" the", "1.0"], [" stars", "0.75514185"], [" in", "1.0"], [" the", "1.0"], [" sky", "0.99999964"], ["?", "0.99999976"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" stars", "1.0"], [" in", "1.0"], [" the", "1.0"], [" sky", "1.0"], [" are", "1.0"], [" not", "0.999982"], [" related", "1.0"], [" to", "1.0"], [" the", "0.99999964"], [" stars", "1.0"], [" in", "1.0"], [" the", "1.0"], [" sky", "1.0"], [".", "1.0"], [" The", "0.9999962"], [" stars", "1.0"], [" in", "1.0"], [" the", "1.0"], [" sky", "1.0"], [" are", "1.0"], [" not", "0.9986249"], [" related", "1.0"], [" to", "1.0"], [" the", "1.0"], [" stars", "1.0"], [" in", "1.0"], [" the", "1.0"], [" sky", "1.0"], [".", "1.0"], [" The", "1.0"], [" stars", "1.0"], [" in", "1.0"], [" the", "1.0"], [" sky", "1.0"], [" are", "1.0"], [" not", "1.0"], [" related", "1.0"], [" to", "1.0"], [" the", "1.0"], [" stars", "1.0"], [" in", "1.0"], [" the", "1.0"], [" sky", "1.0"], [".", "1.0"], [" The", "1.0"]]}, "729": {"qid": "d3bfac8218a421be63fe", "term": "Noah", "description": "Biblical figure", "question": "Was Noah concerned with buoyancy?", "answer": true, "facts": ["Buoyancy is the property of an object related to its ability to float in liquid", "Noah was tasked with building a boat to house many animals and survive a catastrophic flood", "Boats must be properly buoyant or they will sink"], "decomposition": ["What was Noah famous for building?", "Did #1 have to be buoyant to work?"], "evidence": [[[["Noah-2"]], [["Buoyancy-1", "Buoyancy-2"], "operation"]], [[["Noah's Ark-1"]], [["Ark (river boat)-4"]]], [[["Noah's Ark-1"]], [["Buoyancy-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Was Noah concerned with the number of animals on the Ark?\nFacts: Noah was concerned with the number of animals on the Ark. Noah was not concerned with the number of animals on the Ark. Noah was concerned with the number of animals on the Ark.\nAnswer (True or False): True\nQuestion: Was", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Noah concerned with buoyancy?\nFacts: Buoyancy is the property of an object related to its ability to float in liquid Noah was tasked with building a boat to house many animals and survive a catastrophic flood Boats must be properly buoyant or they will sink\nAnswer (True or False):", "ct_subject": "Buoyancy is the property of an object related to its ability to float in liquid Noah was tasked with building a boat to house many animals and survive a catastrophic flood Boats must be properly buoyant or they will sink", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "0.79468113"], [" Noah", "0.9930922"], [" concerned", "1.0"], [" with", "1.0"], [" the", "1.0"], [" number", "0.61093396"], [" of", "1.0"], [" animals", "1.0"], [" on", "0.91101414"], [" the", "1.0"], [" Ark", "0.99997926"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Noah", "1.0"], [" was", "1.0"], [" concerned", "0.99999106"], [" with", "1.0"], [" the", "1.0"], [" number", "1.0"], [" of", "1.0"], [" animals", "1.0"], [" on", "1.0"], [" the", "1.0"], [" Ark", "1.0"], [".", "0.8083463"], [" Noah", "0.81975836"], [" was", "0.9981487"], [" not", "0.99956614"], [" concerned", "1.0"], [" with", "1.0"], [" the", "1.0"], [" number", "1.0"], [" of", "1.0"], [" animals", "0.99965155"], [" on", "1.0"], [" the", "1.0"], [" Ark", "1.0"], [".", "1.0"], [" Noah", "0.9996927"], [" was", "1.0"], [" concerned", "1.0"], [" with", "1.0"], [" the", "1.0"], [" number", "1.0"], [" of", "1.0"], [" animals", "1.0"], [" on", "1.0"], [" the", "1.0"], [" Ark", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "0.99972516"]]}, "730": {"qid": "740b4c542a9c9512c3e3", "term": "Giraffe", "description": "Tall African ungulate", "question": "Is it foolish to stand on giraffe's head to see over Eiffel Tower?", "answer": true, "facts": ["The neck of a giraffe can be up to 7 feet in length.", "Including their necks, giraffes can be as tall as 20 feet.", "The Eiffel Tower is 1,063 feet tall."], "decomposition": ["How tall is a giraffe?", "How tall is the Eiffel Tower?", "Is #1 greater than #2?"], "evidence": [[[["Giraffe-16"]], [["Eiffel Tower-3"]], ["operation"]], [[["Giraffe-16"]], [["Eiffel Tower-3"]], ["operation"]], [[["Giraffe-16"]], [["Eiffel Tower-3"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it foolish to stand on giraffe's head to see over Eiffel Tower?\nFacts: The neck of a giraffe can be up to 7 feet in length. Including their necks, giraffes can be as tall as 20 feet. The Eiff", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is it foolish to stand on giraffe's head to see over Eiffel Tower?\nFacts: The neck of a giraffe can be up to 7 feet in length. Including their necks, giraffes can be as tall as 20 feet. The Eiffel Tower is 1,063 feet tall.\nAnswer (True or False):", "ct_subject": "The neck of a giraffe can be up to 7 feet in length. Including their necks, giraffes can be as tall as 20 feet. The Eiffel Tower is 1,063 feet tall.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.8829386"], [" it", "1.0"], [" foolish", "0.672698"], [" to", "1.0"], [" stand", "1.0"], [" on", "1.0"], [" gir", "0.99824405"], ["affe", "1.0"], ["'s", "1.0"], [" head", "1.0"], [" to", "1.0"], [" see", "1.0"], [" over", "1.0"], [" E", "1.0"], ["iff", "1.0"], ["el", "1.0"], [" Tower", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" neck", "1.0"], [" of", "1.0"], [" a", "1.0"], [" gir", "1.0"], ["affe", "1.0"], [" can", "1.0"], [" be", "1.0"], [" up", "1.0"], [" to", "1.0"], [" 7", "1.0"], [" feet", "1.0"], [" in", "1.0"], [" length", "1.0"], [".", "1.0"], [" Including", "1.0"], [" their", "1.0"], [" necks", "1.0"], [",", "1.0"], [" gir", "1.0"], ["aff", "1.0"], ["es", "1.0"], [" can", "1.0"], [" be", "1.0"], [" as", "1.0"], [" tall", "1.0"], [" as", "1.0"], [" 20", "1.0"], [" feet", "1.0"], [".", "1.0"], [" The", "1.0"], [" E", "1.0"], ["iff", "1.0"]]}, "731": {"qid": "704003c5c9786ae43746", "term": "Nicole Kidman", "description": "Australian-American actress and film producer", "question": "Does Nicole Kidman know any Scientologists?", "answer": true, "facts": ["Nicole Kidman was married to Tom Cruise.", "Tom Cruise is a Scientologist. "], "decomposition": ["Who has Nicole Kidman been married to?", "Have any of #1 practiced Scientology?"], "evidence": [[[["Nicole Kidman-4"]], [["Tom Cruise-36"], "operation"]], [[["Nicole Kidman-32"]], [["Tom Cruise-4"]]], [[["Nicole Kidman-4"]], [["Tom Cruise-4"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: The first person to be executed in the United States was John Brown. John Brown was executed in 1859.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: The first person to be executed in the United States was John Brown. John Brown was", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Nicole Kidman know any Scientologists?\nFacts: Nicole Kidman was married to Tom Cruise. Tom Cruise is a Scientologist. \nAnswer (True or False):", "ct_subject": "Nicole Kidman was married to Tom Cruise. Tom Cruise is a Scientologist. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9983279"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "0.998604"], [" person", "0.946762"], [" to", "1.0"], [" be", "0.99991596"], [" executed", "0.99999416"], [" in", "0.9741697"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999356"], [" first", "1.0"], [" person", "1.0"], [" to", "0.9786872"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" John", "0.9994252"], [" Brown", "0.99996436"], [".", "0.9999988"], [" John", "0.9999907"], [" Brown", "1.0"], [" was", "1.0"], [" executed", "0.3208537"], [" in", "0.9999565"], [" 18", "1.0"], ["59", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" John", "1.0"], [" Brown", "1.0"], [".", "1.0"], [" John", "1.0"], [" Brown", "1.0"], [" was", "1.0"]]}, "732": {"qid": "2533aef219d77a6860ef", "term": "Cream", "description": "Dairy product", "question": "Would Kylee Jenner ask for no cream in her coffee?", "answer": true, "facts": ["Kylee Jenner is lactose intolerant.", "Lactose intolerance makes it uncomfortable for people to digest foods containing lactose.", "Cream is a dairy product and is rich in lactose."], "decomposition": ["What dietary condition does Kylee (Kylie) Jenner suffer from?", "What do people who have #1 have to avoid?", "Does cream have #2 in it?"], "evidence": [[["no_evidence"], [["Lactose intolerance-1"]], [["Cream-1"], "operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence"]], [[["Kylie Jenner-1"]], [["Lactose intolerance-1"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: The first person executed in the United States was John Brown. John Brown was hanged in 1859.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be executed in the United States?", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Kylee Jenner ask for no cream in her coffee?\nFacts: Kylee Jenner is lactose intolerant. Lactose intolerance makes it uncomfortable for people to digest foods containing lactose. Cream is a dairy product and is rich in lactose.\nAnswer (True or False):", "ct_subject": "Kylee Jenner is lactose intolerant. Lactose intolerance makes it uncomfortable for people to digest foods containing lactose. Cream is a dairy product and is rich in lactose.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999931"], [" is", "1.0"], [" the", "1.0"], [" name", "0.972868"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9988558"], [" person", "0.9999857"], [" to", "1.0"], [" be", "0.9980205"], [" executed", "0.12627093"], [" in", "0.99999464"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9995028"], [" first", "1.0"], [" person", "1.0"], [" executed", "0.9874943"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" John", "0.99859077"], [" Brown", "1.0"], [".", "0.99839324"], [" John", "0.99986875"], [" Brown", "1.0"], [" was", "1.0"], [" hanged", "0.9359207"], [" in", "0.998917"], [" 18", "1.0"], ["59", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"]]}, "733": {"qid": "3496d185679a8f55b799", "term": "JPEG", "description": "Lossy compression method for reducing the size of digital images", "question": "Could the leader of Heaven's Gate save images in JPEG format?", "answer": true, "facts": ["Marshall Applewhite, Jr. was the leader of the Heaven's Gate cult", "Marshall Applewhite, Jr. died in 1997", "JPEG format was released in 1992"], "decomposition": ["Who led the Heaven's Gate cult?", "During what span of years was #1 alive?", "What year was JPEG released?", "Is #2 inclusive of #3?"], "evidence": [[[["Marshall Applewhite-1"]], [["Marshall Applewhite-1"]], [["JPEG-2"]], ["operation"]], [[["Heaven's Gate (religious group)-20"]], [["Marshall Applewhite-1"]], [["JPEG-15"]], ["operation"]], [[["Heaven's Gate (religious group)-1"]], [["Heaven's Gate (religious group)-1"]], [["JPEG-5"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake-shaped\" snowflake?\nFacts: A snowflake is a single crystal of ice. A snowflake-shaped snowflake is a cluster of crystals.\nAnswer (True or False): True\nQuestion: What is the difference between a \"snowflake\" and", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could the leader of Heaven's Gate save images in JPEG format?\nFacts: Marshall Applewhite, Jr. was the leader of the Heaven's Gate cult Marshall Applewhite, Jr. died in 1997 JPEG format was released in 1992\nAnswer (True or False):", "ct_subject": "Marshall Applewhite, Jr. was the leader of the Heaven's Gate cult Marshall Applewhite, Jr. died in 1997 JPEG format was released in 1992", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9998838"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.080723554"], [" between", "1.0"], [" a", "0.99997044"], [" \"", "1.0"], ["s", "0.24084902"], ["now", "0.89777315"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.9999987"], ["now", "1.0"], ["flake", "1.0"], ["-", "0.97107744"], ["shaped", "0.99997115"], ["\"", "1.0"], [" snow", "1.0"], ["flake", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999194"], [" snow", "0.99996054"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "0.99962294"], [" crystal", "1.0"], [" of", "0.8527438"], [" ice", "0.99999964"], [".", "0.3462166"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], ["-", "1.0"], ["shaped", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" cluster", "0.9970149"], [" of", "1.0"], [" crystals", "1.0"], [".", "0.97709286"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"]]}, "734": {"qid": "673be9f6d35f74ae8e91", "term": "Vitamin C", "description": "nutrient found in citrus fruits and other foods", "question": "Did pirates who had scurvy need more Vitamin C?", "answer": true, "facts": ["Pirates were known for having poor teeth and deteriorated gums.", "Gum deterioration and tooth decay is a symptom of scurvy.", "Scurvy is caused by a lack of dietary vitamin C."], "decomposition": ["What causes scurvy?", "Is #1 the same as insufficient vitamin C intake?"], "evidence": [[[["Scurvy-1"]], ["operation"]], [[["Scurvy-1"]], ["operation"]], [[["Scurvy-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Romans have a word for \"candy\"?\nFacts: The Romans had a word for \"candy\" which was \"carnis\". The word \"carnis\" was derived from the Latin word \"carne\" which means \"to eat\".\nAnswer (True or False): True\nQuestion:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did pirates who had scurvy need more Vitamin C?\nFacts: Pirates were known for having poor teeth and deteriorated gums. Gum deterioration and tooth decay is a symptom of scurvy. Scurvy is caused by a lack of dietary vitamin C.\nAnswer (True or False):", "ct_subject": "Pirates were known for having poor teeth and deteriorated gums. Gum deterioration and tooth decay is a symptom of scurvy. Scurvy is caused by a lack of dietary vitamin C.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999999"], [" the", "1.0"], [" Romans", "0.98644906"], [" have", "1.0"], [" a", "1.0"], [" word", "0.29298824"], [" for", "1.0"], [" \"", "0.99999976"], ["c", "0.6984339"], ["andy", "0.99812764"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Romans", "0.9999906"], [" had", "0.98833615"], [" a", "1.0"], [" word", "1.0"], [" for", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"", "1.0"], [" which", "0.87275213"], [" was", "1.0"], [" \"", "1.0"], ["c", "0.99990845"], ["arn", "0.77401656"], ["is", "0.99956053"], ["\".", "0.9999999"], [" The", "0.9567506"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["arn", "1.0"], ["is", "1.0"], ["\"", "1.0"], [" was", "0.13464251"], [" derived", "0.9994671"], [" from", "1.0"], [" the", "1.0"], [" Latin", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["car", "0.30379635"], ["ne", "0.999879"], ["\"", "0.87306243"], [" which", "1.0"], [" means", "1.0"], [" \"", "1.0"], ["to", "1.0"], [" eat", "1.0"], ["\".", "1.0"], ["\n", "0.9999999"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"]]}, "735": {"qid": "0d528ce1fdd22c9d8aa0", "term": "Easy Rider", "description": "1969 film by Dennis Hopper", "question": "Did producer of Easy Rider ever star in a movie with Dean Cain's Princeton girlfriend?", "answer": true, "facts": ["Easy Rider was produced by Peter Fonda.", "Dean Cain dated Brooke Shields while at Princeton.", "Brooke Shields and Peter Fonda star in the movie Wanda Nevada."], "decomposition": ["Who produced Easy Rider?", "Who did Dean Cain date while at Princeton?", "What movies did #1 star in?", "What movies did #2 star in?", "Is at least one element of #3 also found in #4?"], "evidence": [[[["Easy Rider-1"]], [["Dean Cain-16"]], [["Peter Fonda-48"], "no_evidence"], [["Brooke Shields-1"], "no_evidence"], [["Wanda Nevada-1"], "no_evidence", "operation"]], [[["Easy Rider-1"]], [["Dean Cain-3"]], [["Wanda Nevada-1"]], [["Wanda Nevada-1"]], ["operation"]], [[["Easy Rider-40"], "operation"], [["Dean Cain-16"], "operation"], ["no_evidence"], ["no_evidence"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Beatles ever have a song called \"I Want to Hold Your Hand\"?\nFacts: The Beatles did not write the song. The song was written by the band The Beach Boys.\nAnswer (True or False): True\nQuestion: Did the Beatles ever have a song called \"I Want to Hold Your Hand\"?\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did producer of Easy Rider ever star in a movie with Dean Cain's Princeton girlfriend?\nFacts: Easy Rider was produced by Peter Fonda. Dean Cain dated Brooke Shields while at Princeton. Brooke Shields and Peter Fonda star in the movie Wanda Nevada.\nAnswer (True or False):", "ct_subject": "Easy Rider was produced by Peter Fonda. Dean Cain dated Brooke Shields while at Princeton. Brooke Shields and Peter Fonda star in the movie Wanda Nevada.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99999523"], [" the", "1.0"], [" Beatles", "0.36214927"], [" ever", "0.9999645"], [" have", "0.9812556"], [" a", "1.0"], [" song", "1.0"], [" called", "1.0"], [" \"", "1.0"], ["I", "0.99999964"], [" Want", "0.99931896"], [" to", "1.0"], [" Hold", "1.0"], [" Your", "1.0"], [" Hand", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Beatles", "1.0"], [" did", "0.99978787"], [" not", "1.0"], [" write", "0.99999213"], [" the", "0.9814559"], [" song", "1.0"], [".", "0.8892467"], [" The", "0.68258035"], [" song", "1.0"], [" was", "1.0"], [" written", "1.0"], [" by", "1.0"], [" the", "0.97405404"], [" band", "0.9669307"], [" The", "1.0"], [" Beach", "0.9996598"], [" Boys", "1.0"], [".", "1.0"], ["\n", "0.9997819"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" Beatles", "1.0"], [" ever", "1.0"], [" have", "1.0"], [" a", "1.0"], [" song", "1.0"], [" called", "1.0"], [" \"", "1.0"], ["I", "1.0"], [" Want", "1.0"], [" to", "1.0"], [" Hold", "1.0"], [" Your", "1.0"], [" Hand", "1.0"], ["\"?", "1.0"], ["\n", "1.0"]]}, "736": {"qid": "f416f193c9cba1e27d4f", "term": "Laziness", "description": "disinclination to activity or exertion", "question": "Can depression be mistaken for laziness?", "answer": true, "facts": ["Symptoms of depression include low energy, inability to get out of bed, and low motivation.", "Low energy and low motivation can cause someone to seem like they are slacking on responsibility."], "decomposition": ["What are some common symptoms of depression?", "Does any of #1 share characteristics with laziness?"], "evidence": [[[["Depression (mood)-1"]], ["operation"]], [[["Symptom-11"]], [["Laziness-1"]]], [[["Depression (mood)-4"]], [["Laziness-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"good\" and a \"bad\" person?\nFacts: A \"good\" person is someone who is kind, helpful, and helpful to others. A \"bad\" person is someone who is mean, selfish, and selfish to others.\nAnswer (True or False): True\nQuestion: What is the difference between", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can depression be mistaken for laziness?\nFacts: Symptoms of depression include low energy, inability to get out of bed, and low motivation. Low energy and low motivation can cause someone to seem like they are slacking on responsibility.\nAnswer (True or False):", "ct_subject": "Symptoms of depression include low energy, inability to get out of bed, and low motivation. Low energy and low motivation can cause someone to seem like they are slacking on responsibility.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9828851"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9999862"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.9999995"], ["good", "0.9752033"], ["\"", "1.0"], [" and", "1.0"], [" a", "0.99838674"], [" \"", "1.0"], ["bad", "1.0"], ["\"", "1.0"], [" person", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" \"", "0.9999119"], ["good", "1.0"], ["\"", "1.0"], [" person", "1.0"], [" is", "1.0"], [" someone", "1.0"], [" who", "1.0"], [" is", "0.9997973"], [" kind", "0.9999701"], [",", "1.0"], [" helpful", "0.9998745"], [",", "1.0"], [" and", "1.0"], [" helpful", "0.9999999"], [" to", "1.0"], [" others", "1.0"], [".", "1.0"], [" A", "1.0"], [" \"", "1.0"], ["bad", "1.0"], ["\"", "1.0"], [" person", "1.0"], [" is", "1.0"], [" someone", "1.0"], [" who", "1.0"], [" is", "1.0"], [" mean", "0.99999547"], [",", "1.0"], [" selfish", "1.0"], [",", "1.0"], [" and", "1.0"], [" selfish", "1.0"], [" to", "1.0"], [" others", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"]]}, "737": {"qid": "acb7edac010554509519", "term": "Chlorophyll", "description": "group of chemical compounds", "question": "Would human race go extinct without chlorophyll?", "answer": true, "facts": ["Chlorophyll is a pigment in plants responsible for photosynthesis.", "Photosynthesis is the process by which plants release oxygen into the atmosphere.", "Humans need oxygen to live."], "decomposition": ["What is Chlorophyll responsible for in plants?", "What does #1 release into the air?", "Do humans need #2 in order to survive?"], "evidence": [[[["Chlorophyll-1"]], [["Photosynthesis-1"]], ["operation"]], [[["Chlorophyll-6"]], [["Photosynthesis-1"]], [["Breathing-2"]]], [[["Chlorophyll-1"]], [["Photosynthesis-1"]], [["Breathing-2"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a human and a chimpanzee?\nFacts: Humans are the only species on earth that can use tools. Chimpanzees are the only species on earth that can use tools.\nAnswer (True or False): True\nQuestion: What is the difference between a human and a gorilla?\nFacts: Humans are the only", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would human race go extinct without chlorophyll?\nFacts: Chlorophyll is a pigment in plants responsible for photosynthesis. Photosynthesis is the process by which plants release oxygen into the atmosphere. Humans need oxygen to live.\nAnswer (True or False):", "ct_subject": "Chlorophyll is a pigment in plants responsible for photosynthesis. Photosynthesis is the process by which plants release oxygen into the atmosphere. Humans need oxygen to live.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.988528"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.95632035"], [" between", "1.0"], [" a", "0.9999999"], [" human", "0.9999995"], [" and", "1.0"], [" a", "1.0"], [" chimpan", "0.99614394"], ["zee", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Humans", "0.9999999"], [" are", "0.9934483"], [" the", "0.99958616"], [" only", "1.0"], [" species", "1.0"], [" on", "0.60947454"], [" earth", "0.99441236"], [" that", "1.0"], [" can", "0.9994252"], [" use", "0.9969036"], [" tools", "1.0"], [".", "0.99997747"], [" Chim", "1.0"], ["pan", "1.0"], ["zees", "1.0"], [" are", "1.0"], [" the", "1.0"], [" only", "1.0"], [" species", "1.0"], [" on", "1.0"], [" earth", "1.0"], [" that", "1.0"], [" can", "1.0"], [" use", "0.9991622"], [" tools", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" human", "1.0"], [" and", "1.0"], [" a", "1.0"], [" gorilla", "0.9996872"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Humans", "1.0"], [" are", "1.0"], [" the", "1.0"], [" only", "1.0"]]}, "738": {"qid": "c06605d435462122d1de", "term": "Walt Disney", "description": "American entrepreneur, animator, voice actor and film producer", "question": "Walt Disney dominated his amusement park peers at Academy Awards?", "answer": true, "facts": ["Walt Disney won a total of 26 Academy Awards.", "The founder of Six Flags, Angus G Wynne, had 0 academy awards.", "The founder of Knott's Berry Farm, Walter Knott, had 0 academy awards."], "decomposition": ["At the Academy Awards, how many awards did Walt Disney win?", "At the Academy Awards, how many awards did Angus G Wynne win?", "At the Academy Awards, how many awards did Walter Knott win?", "Is #1 more than #2 and #3?"], "evidence": [[[["Walt Disney-1"]], [["Angus G. Wynne-1"], "no_evidence"], [["Walter Knott-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Walt Disney-1"]], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Walt Disney-1"]], [["Angus G. Wynne-1"]], [["Walter Knott-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: The first person to be executed in the United States was John Brown?\nFacts: John Brown was executed in 1859. The first person to be executed in the United States was John Brown.\nAnswer (True or False): True\nQuestion: The first person to be executed in the United States was", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Walt Disney dominated his amusement park peers at Academy Awards?\nFacts: Walt Disney won a total of 26 Academy Awards. The founder of Six Flags, Angus G Wynne, had 0 academy awards. The founder of Knott's Berry Farm, Walter Knott, had 0 academy awards.\nAnswer (True or False):", "ct_subject": "Walt Disney won a total of 26 Academy Awards. The founder of Six Flags, Angus G Wynne, had 0 academy awards. The founder of Knott's Berry Farm, Walter Knott, had 0 academy awards.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" The", "0.99994147"], [" first", "0.9999999"], [" person", "0.9993212"], [" to", "1.0"], [" be", "0.9990951"], [" executed", "0.99132055"], [" in", "0.9983529"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" John", "0.98339117"], [" Brown", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "1.0"], [" Brown", "1.0"], [" was", "1.0"], [" executed", "0.12474608"], [" in", "1.0"], [" 18", "1.0"], ["59", "1.0"], [".", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "0.99996364"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" John", "0.99999976"], [" Brown", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"]]}, "739": {"qid": "d4d98ff0a195302735a5", "term": "Toyota Hilux", "description": "Series of light commercial vehicles produced by the Japanese car-manufacturer Toyota.", "question": "Can a human heart last from NYC to Raleigh NC by Toyota Hiux?", "answer": true, "facts": ["Human hearts can last up to six hours outside the body.", "The distance from NYC to Raleigh, NC is 505 miles.", "The top speed of a Toyota Hilux is 105 MPH."], "decomposition": ["How many hours can a human heart last outside of the human body?", "What is the distance between NYC to Raleigh, NC in miles?", "What is the top speed of a Toyota Hilux in MPH?", "Is #1 times #3 more than #2?"], "evidence": [[[["Organ transplantation-3"]], [["New York City-1", "Raleigh, North Carolina-1"], "operation"], [["Toyota Hilux-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Organ donation-8"], "no_evidence"], ["no_evidence"], [["Toyota Hilux-9"]], ["operation"]], [["no_evidence"], ["no_evidence"], [["Toyota Hilux-2"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance a human can walk in one day?\nFacts: The longest distance a human can walk in one day is about 1,000 miles.\nAnswer (True or False): True\nQuestion: What is the longest distance a human can walk in one year?\nFacts: The longest distance", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can a human heart last from NYC to Raleigh NC by Toyota Hiux?\nFacts: Human hearts can last up to six hours outside the body. The distance from NYC to Raleigh, NC is 505 miles. The top speed of a Toyota Hilux is 105 MPH.\nAnswer (True or False):", "ct_subject": "Human hearts can last up to six hours outside the body. The distance from NYC to Raleigh, NC is 505 miles. The top speed of a Toyota Hilux is 105 MPH.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.98926103"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.74043536"], [" distance", "1.0"], [" a", "0.9955141"], [" human", "0.9999987"], [" can", "0.92483413"], [" walk", "0.57212526"], [" in", "0.99994373"], [" one", "0.99996316"], [" day", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99999976"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" human", "1.0"], [" can", "1.0"], [" walk", "1.0"], [" in", "1.0"], [" one", "1.0"], [" day", "1.0"], [" is", "1.0"], [" about", "0.9999894"], [" 1", "0.58806926"], [",", "1.0"], ["000", "0.9999995"], [" miles", "1.0"], [".", "1.0"], ["\n", "0.99581957"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" human", "1.0"], [" can", "1.0"], [" walk", "0.9999988"], [" in", "1.0"], [" one", "1.0"], [" year", "0.9999993"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" distance", "1.0"]]}, "740": {"qid": "f4ac300578dfab653d3f", "term": "Pompey", "description": "1st/2nd-century BC Roman general", "question": "Has type of political association Pompey had with Caesar influenced reality TV?", "answer": true, "facts": ["Pompey, Julius Caesar, and Marcus Licinius Crassus formed a political association called a triumvirate.", "A triumvirate spits rule between three powerful people that get to make decisions.", "Reality show The Challenge: Total Madness appoints three weekly winners to make decisions for the group, known as the Tribunal.", "Reality show American Idol has had three judges making decisions about which contestants advance."], "decomposition": ["Which political association did Pompey form with Julius Caesar and Marcus Licinius Crassus?", "How many people does #1 typically involve?", "How many judges are on reality show American Idol?", "Is #2 equal to #3?"], "evidence": [[[["First Triumvirate-1"]], [["Triumvirate-1"]], [["American Idol-10", "American Idol-9"]], ["operation"]], [[["Pompey-2"]], [["First Triumvirate-1"]], [["American Idol-3"]], ["operation"]], [[["Triumvirate-5"]], [["Triumvirate-3"]], [["American Idol-3"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Has type of political association Pompey had with Caesar influenced reality TV?\nFacts: Pompey, Julius Caesar, and Marcus Licinius Crassus formed a political association called a triumvirate. A triumvirate spits rule between three powerful people that get to make decisions. Reality show The Challenge: Total Madness appoints three weekly winners to make decisions for the group, known as the Tribunal. Reality show American Idol has had three judges making decisions about which contestants advance.\nAnswer (True or False):", "ct_subject": "Pompey, Julius Caesar, and Marcus Licinius Crassus formed a political association called a triumvirate. A triumvirate spits rule between three powerful people that get to make decisions. Reality show The Challenge: Total Madness appoints three weekly winners to make decisions for the group, known as the Tribunal. Reality show American Idol has had three judges making decisions about which contestants advance.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99997306"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9998568"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9545759"], [" person", "0.99998856"], [" to", "1.0"], [" be", "0.9999814"], [" executed", "0.9424267"], [" in", "0.99004"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"]]}, "741": {"qid": "3eb2a8cfa2616c47672e", "term": "Nissan", "description": "Japanese automobile manufacturer", "question": "Is CEO of Nissan an internationally wanted fugitive?", "answer": true, "facts": ["Carlos Ghosn was CEO of Nissan. ", "With help from an American private-security contractor, Carlos Ghosn fled from Japan to Lebanon on 30 December, breaking his bail conditions", "On 2 January 2020, Interpol issued a red notice to Lebanon seeking Carlos Ghosn's arrest."], "decomposition": ["Which of Nissan's former CEOs have been the subject of corporate unrest?", "Is #1 presently a fugitive on the run?"], "evidence": [[[["Carlos Ghosn-13"]], [["Carlos Ghosn-4"], "operation"]], [[["Carlos Ghosn-29"]], [["Carlos Ghosn-1"], "operation"]], [[["Carlos Ghosn-4"]], [["Carlos Ghosn-4"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city in the United States of America, New York, the same as the name of the city in China?\nFacts: The name of the city in the United States of America, New York, is the same", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is CEO of Nissan an internationally wanted fugitive?\nFacts: Carlos Ghosn was CEO of Nissan.  With help from an American private-security contractor, Carlos Ghosn fled from Japan to Lebanon on 30 December, breaking his bail conditions On 2 January 2020, Interpol issued a red notice to Lebanon seeking Carlos Ghosn's arrest.\nAnswer (True or False):", "ct_subject": "Carlos Ghosn was CEO of Nissan.  With help from an American private-security contractor, Carlos Ghosn fled from Japan to Lebanon on 30 December, breaking his bail conditions On 2 January 2020, Interpol issued a red notice to Lebanon seeking Carlos Ghosn's arrest.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99999976"], [" the", "0.9999981"], [" name", "0.9997788"], [" of", "1.0"], [" the", "1.0"], [" city", "0.99963224"], [" in", "0.79311347"], [" the", "0.9995254"], [" United", "1.0"], [" States", "1.0"], [" of", "0.65471137"], [" America", "1.0"], [",", "0.99989426"], [" New", "1.0"], [" York", "1.0"], [",", "0.9972741"], [" the", "0.062406663"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" China", "0.9999455"], ["?", "0.7439332"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.8169364"], [" name", "1.0"], [" of", "1.0"], [" the", "0.9999999"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" of", "1.0"], [" America", "1.0"], [",", "0.9999989"], [" New", "1.0"], [" York", "1.0"], [",", "1.0"], [" is", "1.0"], [" the", "1.0"], [" same", "1.0"]]}, "742": {"qid": "2acddc0a893af847a21b", "term": "Palaeography", "description": "Study of ancient handwriting", "question": "Paleography hypothetically helps to understand Cthulhu?", "answer": true, "facts": ["Palaeography involves the study of ancient writings.", "Paleography has helped decode Anatolian hieroglyphics from all the way back as the first millenia BC.", "Author H.P. Lovecraft's Cthulhu is an ancient mystical being from eons ago."], "decomposition": ["Palaeography is the study of what?", "Cthulhu is from which age?", "Will a language from #2 be a subject of focus of #1?"], "evidence": [[[["Palaeography-1"]], [["Cthulhu-9"]], ["operation"]], [[["Palaeography-1"]], [["Cthulhu-9"]], ["operation"]], [[["Palaeography-1"]], [["Cthulhu-1", "Cthulhu-9"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first man to walk on the moon?\nFacts: Neil Armstrong.\nAnswer (True or False): True\nQuestion: What is the name of the first man to walk on the moon?\nFacts: Neil Armstrong.\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Paleography hypothetically helps to understand Cthulhu?\nFacts: Palaeography involves the study of ancient writings. Paleography has helped decode Anatolian hieroglyphics from all the way back as the first millenia BC. Author H.P. Lovecraft's Cthulhu is an ancient mystical being from eons ago.\nAnswer (True or False):", "ct_subject": "Palaeography involves the study of ancient writings. Paleography has helped decode Anatolian hieroglyphics from all the way back as the first millenia BC. Author H.P. Lovecraft's Cthulhu is an ancient mystical being from eons ago.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9981223"], [" of", "1.0"], [" the", "1.0"], [" first", "0.76431787"], [" man", "0.95920527"], [" to", "0.99985504"], [" walk", "0.99709356"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Neil", "1.0"], [" Armstrong", "1.0"], [".", "0.9725009"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Neil", "1.0"], [" Armstrong", "1.0"], [".", "1.0"], ["\n", "1.0"]]}, "743": {"qid": "c69a3eeea9743e782831", "term": "Supreme Court of the United States", "description": "Highest court in the United States", "question": "Do members of the Supreme Court of the United States have longer terms than most senators?", "answer": true, "facts": ["Senators, on average, serve for 10 years.", "Supreme Court Justices serve for their entire life.", "The average term for a Supreme court justice is 16 years."], "decomposition": ["How many years is in a term for a U.S. Senator?", "What is the term for a Supreme Court justice?", "Is #1 a shorter term than #2?"], "evidence": [[[["United States Senate-2"]], [["Supreme Court of the United States-31"]], ["operation"]], [[["United States Senate-16"]], [["Supreme Court of the United States-2"]], ["operation"]], [[["Member of Congress-3"]], [["Supreme Court of the United States-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average length of a human life?\nFacts: The average human life span is about 100 years.\nAnswer (True or False): True\nQuestion: What is the average length of a human life?\nFacts: The average human life span is about 100 years.\nAnswer (True or False): True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do members of the Supreme Court of the United States have longer terms than most senators?\nFacts: Senators, on average, serve for 10 years. Supreme Court Justices serve for their entire life. The average term for a Supreme court justice is 16 years.\nAnswer (True or False):", "ct_subject": "Senators, on average, serve for 10 years. Supreme Court Justices serve for their entire life. The average term for a Supreme court justice is 16 years.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99946314"], [" is", "1.0"], [" the", "1.0"], [" average", "0.84262025"], [" length", "0.99976224"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" life", "0.9998512"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.99340725"], [" life", "1.0"], [" span", "0.9999435"], [" is", "1.0"], [" about", "0.9995491"], [" 100", "0.999948"], [" years", "1.0"], [".", "1.0"], ["\n", "0.9999999"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" length", "0.9999926"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" life", "0.994789"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "1.0"], [" life", "1.0"], [" span", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 100", "1.0"], [" years", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"]]}, "744": {"qid": "e1aee6fefd776d661c33", "term": "1999", "description": "Year", "question": "Were some people afraid of New Years Day coming in 1999?", "answer": true, "facts": ["It was believed that computers might not know how to change from 1999 to 2000 on New Years Day.", "People were concerned that human services and utilities that were computerized might crash due to the Y2K bug.", "People believed that the year 2000 would cause computers to crash due to the 'Y2K' bug."], "decomposition": ["Which New Year's Day followed 1999?", "What concerns were there about computers during the transition from 1999 to #1?", "Was #2 a cause of fear?"], "evidence": [[[["January 1-1"], "no_evidence"], [["Year 2000 problem-1"]], ["operation"]], [[["Year 2000 problem-3"]], [["Year 2000 problem-1"]], ["operation"]], [[["New Year's Day-1", "Year 2000 problem-15"]], [["Year 2000 problem-1"]], [["2000-3", "Year 2000 problem-15"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a 'C' and a 'K'?\nFacts: A 'C' is a 'Class A' and a 'K' is a 'Class K'.\nAnswer", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Were some people afraid of New Years Day coming in 1999?\nFacts: It was believed that computers might not know how to change from 1999 to 2000 on New Years Day. People were concerned that human services and utilities that were computerized might crash due to the Y2K bug. People believed that the year 2000 would cause computers to crash due to the 'Y2K' bug.\nAnswer (True or False):", "ct_subject": "It was believed that computers might not know how to change from 1999 to 2000 on New Years Day. People were concerned that human services and utilities that were computerized might crash due to the Y2K bug. People believed that the year 2000 would cause computers to crash due to the 'Y2K' bug.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9477457"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.5852456"], [" between", "1.0"], [" a", "0.9999956"], [" '", "0.9976146"], ["C", "0.5835273"], ["'", "1.0"], [" and", "1.0"], [" a", "1.0"], [" '", "1.0"], ["K", "0.9999243"], ["'?", "0.9999707"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.79013586"], [" '", "1.0"], ["C", "1.0"], ["'", "1.0"], [" is", "0.9999132"], [" a", "0.9999541"], [" '", "0.9975011"], ["Class", "0.092768654"], [" A", "0.6077909"], ["'", "1.0"], [" and", "0.97291845"], [" a", "1.0"], [" '", "1.0"], ["K", "1.0"], ["'", "1.0"], [" is", "1.0"], [" a", "1.0"], [" '", "1.0"], ["Class", "1.0"], [" K", "1.0"], ["'.", "1.0"], ["\n", "1.0"], ["Answer", "1.0"]]}, "745": {"qid": "018306d2af9359be2d8c", "term": "Chiropractic", "description": "form of alternative medicine", "question": "Are some chiropractic manipulations dangerous?", "answer": true, "facts": ["Manipulations of the neck can lead to complications such as stroke or paralysis.", "Manipulation of the lower back can lead to herniated disks."], "decomposition": ["What body parts do chiropractors manipulate?", "Are any of #1 prone to damage if mishandled?"], "evidence": [[[["Chiropractic-18"]], [["Chiropractic controversy and criticism-34"]]], [[["Chiropractic-1"]], ["operation"]], [[["Chiropractic-1"]], [["Chiropractic-2"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any benefits to using a homeopathic remedy?\nFacts: Homeopathic remedies are not effective.\nAnswer (True or False): True\nQuestion: Are there any benefits to using a vitamin C supplement?\nFacts: Vitamin C is not a vitamin.\nAnswer (True or False): True\nQuestion: Are there any benefits to using a vitamin E supplement?\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are some chiropractic manipulations dangerous?\nFacts: Manipulations of the neck can lead to complications such as stroke or paralysis. Manipulation of the lower back can lead to herniated disks.\nAnswer (True or False):", "ct_subject": "Manipulations of the neck can lead to complications such as stroke or paralysis. Manipulation of the lower back can lead to herniated disks.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.98606956"], [" there", "0.9999"], [" any", "1.0"], [" benefits", "0.9980275"], [" to", "1.0"], [" using", "0.08731348"], [" a", "1.0"], [" home", "1.0"], ["opathic", "1.0"], [" remedy", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Home", "1.0"], ["opathic", "0.90409094"], [" remedies", "1.0"], [" are", "1.0"], [" not", "0.9999881"], [" effective", "1.0"], [".", "0.5277913"], ["\n", "0.8682116"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.98389465"], [" there", "1.0"], [" any", "1.0"], [" benefits", "1.0"], [" to", "1.0"], [" using", "1.0"], [" a", "1.0"], [" vitamin", "0.9983071"], [" C", "1.0"], [" supplement", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Vitamin", "1.0"], [" C", "1.0"], [" is", "1.0"], [" not", "0.9999995"], [" a", "0.9387097"], [" vitamin", "1.0"], [".", "1.0"], ["\n", "0.8669008"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "1.0"], [" any", "1.0"], [" benefits", "1.0"], [" to", "1.0"], [" using", "1.0"], [" a", "1.0"], [" vitamin", "1.0"], [" E", "0.9999721"], [" supplement", "1.0"], ["?", "1.0"], ["\n", "1.0"]]}, "746": {"qid": "86791c3eb5f380b56439", "term": "Bulk carrier", "description": "merchant ship specially designed to transport unpackaged bulk cargo", "question": "Would eliminating competition in the Japanese bulk carrier market be profitable for a steel company?", "answer": true, "facts": ["62% of bulk carriers are built in Japan", "Bulk carrier hulls are made of steel"], "decomposition": ["Where are most bulk carriers built?", "What materials would #1 use in making bulk carriers?", "Is steel a major component of #2?"], "evidence": [[[["Bulk carrier-2"]], [["Bulk carrier-48"]], [["Bulk carrier-48"]]], [[["Bulk carrier-20", "Bulk carrier-22"], "no_evidence"], [["Shipbuilding-45"]], ["operation"]], [[["Malaysian Bulk Carriers-1"], "no_evidence"], ["no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average age of a person who has been a member of the United States Congress?\nFacts: The average age of a member of the United States Congress is 78.\nAnswer (True or False): True\nQuestion: What is the average age of a person who has been a member of the United States Senate?\nFacts: The average age of a member of the United States Senate is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would eliminating competition in the Japanese bulk carrier market be profitable for a steel company?\nFacts: 62% of bulk carriers are built in Japan Bulk carrier hulls are made of steel\nAnswer (True or False):", "ct_subject": "62% of bulk carriers are built in Japan Bulk carrier hulls are made of steel", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9988243"], [" is", "1.0"], [" the", "1.0"], [" average", "0.99803084"], [" age", "0.83523005"], [" of", "1.0"], [" a", "1.0"], [" person", "0.9999894"], [" who", "0.7641197"], [" has", "1.0"], [" been", "1.0"], [" a", "0.8069192"], [" member", "0.99999976"], [" of", "1.0"], [" the", "1.0"], [" United", "0.9998677"], [" States", "1.0"], [" Congress", "0.999767"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" age", "1.0"], [" of", "1.0"], [" a", "1.0"], [" member", "1.0"], [" of", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" Congress", "1.0"], [" is", "1.0"], [" 78", "0.99372214"], [".", "0.86905617"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" age", "1.0"], [" of", "1.0"], [" a", "1.0"], [" person", "1.0"], [" who", "1.0"], [" has", "1.0"], [" been", "1.0"], [" a", "1.0"], [" member", "1.0"], [" of", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" Senate", "0.9987803"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" age", "1.0"], [" of", "1.0"], [" a", "1.0"], [" member", "1.0"], [" of", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" Senate", "1.0"], [" is", "1.0"]]}, "747": {"qid": "038d2f23ebc149069a74", "term": "Isaac Newton", "description": "Influential British physicist and mathematician", "question": "Is Issac Newton often associated with a red fruit?", "answer": true, "facts": ["Issac Newton claimed to have contemplated gravity for the first time after seeing an apple fall.", "In most illustrations of Issac Newton discovering gravity, the apple shown falling is red."], "decomposition": ["Which of Isaac Newton's famous discoveries featured a fruit?", "Is #1 colored red in popular depictions?"], "evidence": [[[["Isaac Newton-84"]], ["no_evidence"]], [[["Isaac Newton-84"]], [["Apple-8"], "operation"]], [[["Isaac Newton-88"]], [["Gala (apple)-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the first woman in space, Valentina Tereshkova, a reference to the Russian word for \"Valentine\"?\nFacts: Valentina Tereshkova was the first woman in space.\nAnswer (True or False): True\nQuestion: Is the name of the first man in space, Yuri Gag", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Issac Newton often associated with a red fruit?\nFacts: Issac Newton claimed to have contemplated gravity for the first time after seeing an apple fall. In most illustrations of Issac Newton discovering gravity, the apple shown falling is red.\nAnswer (True or False):", "ct_subject": "Issac Newton claimed to have contemplated gravity for the first time after seeing an apple fall. In most illustrations of Issac Newton discovering gravity, the apple shown falling is red.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999038"], [" the", "0.99998915"], [" name", "0.40437514"], [" of", "0.98352736"], [" the", "1.0"], [" first", "0.99546945"], [" woman", "0.4731318"], [" in", "0.9850639"], [" space", "0.93785685"], [",", "1.0"], [" Valent", "1.0"], ["ina", "1.0"], [" Te", "1.0"], ["resh", "1.0"], ["k", "1.0"], ["ova", "1.0"], [",", "1.0"], [" a", "1.0"], [" reference", "0.9994018"], [" to", "1.0"], [" the", "1.0"], [" Russian", "0.99666363"], [" word", "0.99986935"], [" for", "0.9999999"], [" \"", "1.0"], ["Val", "0.9824547"], ["ent", "0.99999833"], ["ine", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Valent", "0.99997807"], ["ina", "1.0"], [" Te", "1.0"], ["resh", "1.0"], ["k", "1.0"], ["ova", "1.0"], [" was", "0.9996911"], [" the", "1.0"], [" first", "1.0"], [" woman", "1.0"], [" in", "1.0"], [" space", "1.0"], [".", "1.0"], ["\n", "0.26355842"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99999595"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "0.005738511"], [" in", "1.0"], [" space", "1.0"], [",", "1.0"], [" Yuri", "1.0"], [" G", "1.0"], ["ag", "1.0"]]}, "748": {"qid": "0edaff8446b0f53033c3", "term": "Alcatraz Island", "description": "United States historic place", "question": "Would it be possible to fit a football field in Alcatraz Island?", "answer": true, "facts": ["Alcatraz is 511 meters by 180 meters", "A football field is 91 meters by 48 meters"], "decomposition": ["What is the land area of a football field?", "What is the land area of the Alcatraz Island?", "s #1 less than or equal to #2?"], "evidence": [[[["Comparison of American football and rugby union-24"]], [["Alcatraz Island-4"]], [["Alcatraz Island-4", "Comparison of American football and rugby union-24"]]], [[["System of measurement-25"]], [["Alcatraz Island-4"]], ["operation"]], [[["American football field-2"]], [["Alcatraz Island-4"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance between two points on Earth?\nFacts: The distance between the North Pole and the South Pole is about 24,000 km.\nAnswer (True or False): True\nQuestion: What is the longest distance between two points on the Moon?\nFacts: The distance between the North Pole and the South Pole is about 24,000 km.\nAnswer (True or False", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would it be possible to fit a football field in Alcatraz Island?\nFacts: Alcatraz is 511 meters by 180 meters A football field is 91 meters by 48 meters\nAnswer (True or False):", "ct_subject": "Alcatraz is 511 meters by 180 meters A football field is 91 meters by 48 meters", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999995"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.98720306"], [" distance", "0.99999964"], [" between", "0.9999995"], [" two", "1.0"], [" points", "1.0"], [" on", "0.9977209"], [" Earth", "0.99999857"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" distance", "0.9995937"], [" between", "1.0"], [" the", "1.0"], [" North", "0.99935466"], [" Pole", "1.0"], [" and", "1.0"], [" the", "1.0"], [" South", "1.0"], [" Pole", "1.0"], [" is", "1.0"], [" about", "0.9999695"], [" 24", "0.9627588"], [",", "1.0"], ["000", "1.0"], [" km", "0.5150405"], [".", "1.0"], ["\n", "0.8778814"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"], [" on", "1.0"], [" the", "1.0"], [" Moon", "0.9991233"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" distance", "1.0"], [" between", "1.0"], [" the", "1.0"], [" North", "0.99987364"], [" Pole", "0.99548644"], [" and", "1.0"], [" the", "1.0"], [" South", "1.0"], [" Pole", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 24", "0.99999666"], [",", "1.0"], ["000", "1.0"], [" km", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"]]}, "749": {"qid": "fabdc0199bd6444eb78c", "term": "Mongoose", "description": "family of mammals", "question": "Did mongoose come from later period than rhinos?", "answer": true, "facts": ["The mongoose originated  in the Neogene geological period.", "Rhinos are from the Paleogene geological period.", "The Paleogene period spans 43 million years from the end of the Cretaceous Period 66 million years ago to the beginning of the Neogene Period."], "decomposition": ["During which period did the mongoose originate?", "Which period did Rhinos originate from?", "Is #2 before #1?"], "evidence": [[[["Mongoose-2"]], [["Rhinoceros-5"]], [["Mongoose-2", "Rhinoceros-5"], "operation"]], [[["Mongoose-1", "Mongoose-2"]], [["Rhinoceros-5"]], ["operation"]], [[["Mongoose-2"]], [["Rhinoceros-5"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the first man come from Africa?\nFacts: The first man came from Africa. The first man came from Africa.\nAnswer (True or False): True\nQuestion: Did the first man come from the Middle East?\nFacts: The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did mongoose come from later period than rhinos?\nFacts: The mongoose originated in the Neogene geological period. Rhinos are from the Paleogene geological period. The Paleogene period spans 43 million years from the end of the Cretaceous Period 66 million years ago to the beginning of the Neogene Period.\nAnswer (True or False):", "ct_subject": "The mongoose originated in the Neogene geological period. Rhinos are from the Paleogene geological period. The Paleogene period spans 43 million years from the end of the Cretaceous Period 66 million years ago to the beginning of the Neogene Period.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.8333404"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" come", "0.9999995"], [" from", "1.0"], [" Africa", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" man", "1.0"], [" came", "0.99999857"], [" from", "1.0"], [" Africa", "1.0"], [".", "1.0"], [" The", "0.79518384"], [" first", "1.0"], [" man", "0.44800738"], [" came", "0.63219416"], [" from", "1.0"], [" Africa", "0.9999999"], [".", "1.0"], ["\n", "0.6277159"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999999"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" come", "1.0"], [" from", "1.0"], [" the", "0.9981078"], [" Middle", "0.99999726"], [" East", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"]]}, "750": {"qid": "bd7193627e010af0ecba", "term": "Penny", "description": "unit of currency in various countries", "question": "Would 1943-S penny be good for making silverware?", "answer": true, "facts": ["Modern pennies are made of zinc and copper.", "The 1943-S penny was made of 99% steel and 1% zinc.", "Modern silverware is made from stainless steel."], "decomposition": ["What are 1943-S pennies made out of?", "What is typically modern silverware made out of?", "Is there any overlap between #1 and #2?"], "evidence": [[[["Penny (United States coin)-5"]], [["Cutlery-5", "Cutlery-6"]], ["operation"]], [[["1943 steel cent-1"]], [["Spoon-17", "Tableware-3"]], ["operation"]], [[["1943 steel cent-1"]], [["Cutlery-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"penny\" and a \"dime\"?\nFacts: A penny is a coin that is made of copper. A dime is a coin that is made of zinc.\nAnswer (True or False): True\nQuestion: What is the difference between a \"dime\" and a \"cent\"?\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would 1943-S penny be good for making silverware?\nFacts: Modern pennies are made of zinc and copper. The 1943-S penny was made of 99% steel and 1% zinc. Modern silverware is made from stainless steel.\nAnswer (True or False):", "ct_subject": "Modern pennies are made of zinc and copper. The 1943-S penny was made of 99% steel and 1% zinc. Modern silverware is made from stainless steel.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999994"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9999943"], [" between", "1.0"], [" a", "0.99999976"], [" \"", "1.0"], ["p", "0.31925896"], ["enny", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["d", "0.9988317"], ["ime", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.99955267"], [" penny", "0.99998975"], [" is", "1.0"], [" a", "1.0"], [" coin", "0.99999976"], [" that", "0.99993896"], [" is", "1.0"], [" made", "0.9989857"], [" of", "1.0"], [" copper", "1.0"], [".", "0.46524283"], [" A", "1.0"], [" dime", "1.0"], [" is", "1.0"], [" a", "1.0"], [" coin", "1.0"], [" that", "1.0"], [" is", "1.0"], [" made", "1.0"], [" of", "1.0"], [" zinc", "0.9999989"], [".", "0.9999995"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["d", "1.0"], ["ime", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["cent", "0.99999976"], ["\"?", "1.0"], ["\n", "1.0"]]}, "751": {"qid": "09a493a0d04b4bed378b", "term": "Red hair", "description": "Hair color", "question": "Does a Disney princess on Broadway have red hair?", "answer": true, "facts": ["Ariel, the princess from Disney's the Little Mermaid, has red hair.", "The Little Mermaid is one of several Disney animated classics that was adapted for the stage and performed on Broadway."], "decomposition": ["What is the name of the princess with red hair?", "What is the name of the animated classic in which #1 is the main star of?", "Has #2 been adapted for Broadway?"], "evidence": [[[["Merida (Brave)-9"]], [["Merida (Brave)-10"]], [["Disney Princess-36"]]], [[["Ariel (The Little Mermaid)-7", "Merida (Brave)-9"]], [["Ariel (The Little Mermaid)-1", "Merida (Brave)-1"]], [["Ariel (The Little Mermaid)-33"]]], [[["Ariel (The Little Mermaid)-7"]], [["Ariel (The Little Mermaid)-49"]], [["The Little Mermaid (musical)-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the capital of the state of Texas?\nFacts: The capital of Texas is Austin.\nAnswer (True or False): True\nQuestion: What is the name of the city in the United States that is the capital of the state of California?\nFacts: The capital", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does a Disney princess on Broadway have red hair?\nFacts: Ariel, the princess from Disney's the Little Mermaid, has red hair. The Little Mermaid is one of several Disney animated classics that was adapted for the stage and performed on Broadway.\nAnswer (True or False):", "ct_subject": "Ariel, the princess from Disney's the Little Mermaid, has red hair. The Little Mermaid is one of several Disney animated classics that was adapted for the stage and performed on Broadway.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9742234"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999527"], [" of", "1.0"], [" the", "1.0"], [" city", "0.11556104"], [" in", "0.9999641"], [" the", "0.9998629"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "0.9999076"], [" the", "1.0"], [" capital", "0.9999976"], [" of", "1.0"], [" the", "1.0"], [" state", "0.9875908"], [" of", "1.0"], [" Texas", "0.81163603"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99041116"], [" capital", "0.9999988"], [" of", "1.0"], [" Texas", "0.99213"], [" is", "1.0"], [" Austin", "1.0"], [".", "0.9881147"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" California", "0.9999989"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" capital", "1.0"]]}, "752": {"qid": "ab1cd501d3590b46c009", "term": "Bob Marley", "description": "Jamaican singer-songwriter", "question": "Can you find Bob Marley's face in most smoke shops?", "answer": true, "facts": ["Bob Marley's face is on the packaging of a popular brand of rolling papers.", "Bob Marley is a popular graphic to print on t-shirts for sale to smokers."], "decomposition": ["On what items is Bob Marley's face commonly found?", "Are some of #1 sold in most smoke shops?"], "evidence": [[["no_evidence"], [["Tobacconist-1"]]], [["no_evidence"], ["no_evidence", "operation"]], [[["Bob Marley-1"]], [["Head shop-1", "Tobacconist-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: The first person to be executed in the United States was John Brown. John Brown was hanged in 1859.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be executed in the United States", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can you find Bob Marley's face in most smoke shops?\nFacts: Bob Marley's face is on the packaging of a popular brand of rolling papers. Bob Marley is a popular graphic to print on t-shirts for sale to smokers.\nAnswer (True or False):", "ct_subject": "Bob Marley's face is on the packaging of a popular brand of rolling papers. Bob Marley is a popular graphic to print on t-shirts for sale to smokers.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.992733"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99956316"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9979007"], [" person", "0.9972722"], [" to", "1.0"], [" be", "0.9995647"], [" executed", "0.9740665"], [" in", "0.99998736"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999999"], [" first", "1.0"], [" person", "1.0"], [" to", "0.94203025"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" John", "0.9280265"], [" Brown", "1.0"], [".", "0.9618727"], [" John", "0.91408604"], [" Brown", "1.0"], [" was", "1.0"], [" hanged", "0.9766494"], [" in", "0.9869511"], [" 18", "1.0"], ["59", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"]]}, "753": {"qid": "cbc489c97e6797962787", "term": "Martyr", "description": "person who suffers persecution and death for advocating, refusing to renounce, and/or refusing to advocate a belief or cause, usually a religious one", "question": "Can a martyr saint have been excommunicated?", "answer": true, "facts": ["Joan of Arc was excommunicated by the Catholic Church in 1431.", "Joan of Arc was declared a martyr in 1456 after an investigation ordered by King Charles VII.", "Joan of Arc was canonized a Saint by the Catholic Church on May 16, 1920."], "decomposition": ["Is Joan of Arc considered a matyr?", "Was she initially excommunicated by the Catholic Church?", "Is #1 or #2 negative?"], "evidence": [[[["Canonization of Joan of Arc-1"]], [["Canonization of Joan of Arc-2"]], ["operation"]], [[["Joan of Arc-3"]], [["Heresy-3", "Joan of Arc-37"], "no_evidence"], ["operation"]], [[["Joan of Arc-3"]], [["Canonization of Joan of Arc-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a Catholic and a Protestant?\nFacts: Catholics believe in the Trinity, while Protestants believe in the Trinity of God. Catholics believe that Jesus Christ is the Son of God, while Protestants believe that Jesus Christ is the Son of the Father.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can a martyr saint have been excommunicated?\nFacts: Joan of Arc was excommunicated by the Catholic Church in 1431. Joan of Arc was declared a martyr in 1456 after an investigation ordered by King Charles VII. Joan of Arc was canonized a Saint by the Catholic Church on May 16, 1920.\nAnswer (True or False):", "ct_subject": "Joan of Arc was excommunicated by the Catholic Church in 1431. Joan of Arc was declared a martyr in 1456 after an investigation ordered by King Charles VII. Joan of Arc was canonized a Saint by the Catholic Church on May 16, 1920.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9613611"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9999999"], [" between", "1.0"], [" a", "0.9999596"], [" Catholic", "0.99249893"], [" and", "1.0"], [" a", "0.96065515"], [" Protestant", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Catholics", "0.9999999"], [" believe", "0.7876399"], [" in", "0.9999999"], [" the", "1.0"], [" Trinity", "0.113766804"], [",", "0.9999994"], [" while", "0.954684"], [" Protestants", "1.0"], [" believe", "1.0"], [" in", "1.0"], [" the", "1.0"], [" Trinity", "1.0"], [" of", "0.2554912"], [" God", "0.9999995"], [".", "1.0"], [" Catholics", "0.999741"], [" believe", "1.0"], [" that", "0.9445481"], [" Jesus", "1.0"], [" Christ", "0.9274545"], [" is", "0.99999595"], [" the", "0.99952185"], [" Son", "0.9999999"], [" of", "1.0"], [" God", "1.0"], [",", "1.0"], [" while", "1.0"], [" Protestants", "1.0"], [" believe", "1.0"], [" that", "1.0"], [" Jesus", "1.0"], [" Christ", "1.0"], [" is", "1.0"], [" the", "1.0"], [" Son", "1.0"], [" of", "1.0"], [" the", "0.9999838"], [" Father", "0.9866831"], [".", "1.0"]]}, "754": {"qid": "c1b017ede9f23c442166", "term": "Homelessness", "description": "circumstance when people desire a permanent dwelling but do not have one", "question": "Could Toyota stadium house people suffering homelessness in Michigan?", "answer": true, "facts": ["As of 2019 there were an estimated 8,575 people experiencing homelessness in Michigan.", "Toyota stadium has a capacity of 45,000 people."], "decomposition": ["How many people are experiencing homelessness in Michigan?", "What is the capacity of the Toyota stadium?", "Is #1 less than #2?"], "evidence": [[[["Homelessness in the United States by state-127"]], [["Toyota Stadium (Texas)-1", "Toyota Stadium-1"]], ["operation"]], [[["Michigan-1"], "no_evidence"], [["Toyota Stadium-1"]], ["no_evidence", "operation"]], [[["Homelessness-58"], "no_evidence"], [["Toyota Stadium (Texas)-1"], "operation"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Could the Titanic have been sunk by a meteor?\nFacts: The Titanic was not sunk by a meteor. The Titanic was sunk by a combination of the icebergs that were in the area and the icebergs that were not in the area.\nAnswer (True or False): True\nQuestion: Could the Titanic have been sunk by a meteor?\nFacts: The Titanic was not", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could Toyota stadium house people suffering homelessness in Michigan?\nFacts: As of 2019 there were an estimated 8,575 people experiencing homelessness in Michigan. Toyota stadium has a capacity of 45,000 people.\nAnswer (True or False):", "ct_subject": "As of 2019 there were an estimated 8,575 people experiencing homelessness in Michigan. Toyota stadium has a capacity of 45,000 people.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Could", "0.988279"], [" the", "0.9984188"], [" Titanic", "0.66584074"], [" have", "1.0"], [" been", "0.9999993"], [" sunk", "0.9603958"], [" by", "1.0"], [" a", "1.0"], [" meteor", "0.93116814"], ["?", "0.999992"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Titanic", "1.0"], [" was", "0.9999888"], [" not", "0.8559576"], [" sunk", "1.0"], [" by", "1.0"], [" a", "1.0"], [" meteor", "1.0"], [".", "1.0"], [" The", "1.0"], [" Titanic", "1.0"], [" was", "1.0"], [" sunk", "1.0"], [" by", "1.0"], [" a", "0.99999297"], [" combination", "1.0"], [" of", "1.0"], [" the", "0.4916714"], [" iceberg", "1.0"], ["s", "0.9999566"], [" that", "0.2835957"], [" were", "1.0"], [" in", "1.0"], [" the", "1.0"], [" area", "1.0"], [" and", "0.98504806"], [" the", "1.0"], [" iceberg", "0.99999857"], ["s", "1.0"], [" that", "1.0"], [" were", "1.0"], [" not", "0.79531056"], [" in", "1.0"], [" the", "1.0"], [" area", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Could", "1.0"], [" the", "1.0"], [" Titanic", "1.0"], [" have", "1.0"], [" been", "1.0"], [" sunk", "1.0"], [" by", "1.0"], [" a", "1.0"], [" meteor", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Titanic", "1.0"], [" was", "1.0"], [" not", "1.0"]]}, "755": {"qid": "4936ddf51863958ade85", "term": "Desperate Housewives", "description": "American comedy-drama TV series", "question": "Did Teri Hatcher last twice as many episodes on Desperate Housewives as her Superman show?", "answer": true, "facts": ["Actress Teri Hatcher completed a total of 180 episodes of Desperate Housewives.", "Teri Hatcher starred in Lois & Clark: The New Adventures of Superman based on the Superman comic.", "Teri Hatcher was in 87 episodes of Lois & Clark: The New Adventures of Superman."], "decomposition": ["How many episodes of Desperate Housewives did Teri Hatcher appear in?", "What show did Teri Hatcher appear in that is based on a Superman comic?", "How many episodes of #2 did Teri Hatcher appear in?", "What is #3 multiplied by 2?", "Is #1 greater than or equal to #4?"], "evidence": [[[["Desperate Housewives-1", "Susan Mayer-1"]], [["Lois & Clark: The New Adventures of Superman-1"]], [["The Booth at the End-21"]], ["operation"], ["operation"]], [[["Teri Hatcher-1"]], [["Lois & Clark: The New Adventures of Superman-1"]], [["Teri Hatcher-1"], "no_evidence"], ["operation"], ["operation"]], [[["Desperate Housewives-1"], "no_evidence"], [["Teri Hatcher-9"]], [["Lois & Clark: The New Adventures of Superman-1"], "no_evidence"], ["operation"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the first person to be born in space, Yuri Gagarin, have a name that was not his given name?\nFacts: Yuri Gagarin was born in the Soviet Union", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Teri Hatcher last twice as many episodes on Desperate Housewives as her Superman show?\nFacts: Actress Teri Hatcher completed a total of 180 episodes of Desperate Housewives. Teri Hatcher starred in Lois & Clark: The New Adventures of Superman based on the Superman comic. Teri Hatcher was in 87 episodes of Lois & Clark: The New Adventures of Superman.\nAnswer (True or False):", "ct_subject": "Actress Teri Hatcher completed a total of 180 episodes of Desperate Housewives. Teri Hatcher starred in Lois & Clark: The New Adventures of Superman based on the Superman comic. Teri Hatcher was in 87 episodes of Lois & Clark: The New Adventures of Superman.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9912697"], [" the", "1.0"], [" first", "0.9998691"], [" person", "0.9999857"], [" to", "1.0"], [" be", "0.5725828"], [" born", "0.9945068"], [" in", "0.9998884"], [" space", "0.95255274"], [",", "0.9999964"], [" Yuri", "1.0"], [" G", "1.0"], ["ag", "1.0"], ["arin", "1.0"], [",", "1.0"], [" have", "1.0"], [" a", "1.0"], [" name", "0.5098085"], [" that", "1.0"], [" was", "1.0"], [" not", "0.9992472"], [" his", "1.0"], [" given", "0.99998987"], [" name", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Yuri", "1.0"], [" G", "1.0"], ["ag", "1.0"], ["arin", "1.0"], [" was", "1.0"], [" born", "1.0"], [" in", "0.19128755"], [" the", "0.9925208"], [" Soviet", "1.0"], [" Union", "1.0"]]}, "756": {"qid": "d19a209a6cddeca38a94", "term": "National Hockey League", "description": "North American professional ice hockey league", "question": "Do American teams in National Hockey League outnumber Canadian teams?", "answer": true, "facts": ["The National Hockey League is the premiere North American hockey league.", "The National Hockey League has 7 Canadian teams.", "The National Hockey League has 24 teams from the United States."], "decomposition": ["How many Canadian teams are in the The National Hockey League?", "How many American teams are in the The National Hockey League?", "Is #2 greater than #1?"], "evidence": [[[["National Hockey League-1"]], [["National Hockey League-1"]], ["operation"]], [[["Ice hockey in the United States-5"]], [["Ice hockey in the United States-5"]], ["operation"]], [[["National Hockey League-1"]], [["National Hockey League-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the capital of the state of New York?\nFacts: The city of New York is the capital of the state of New York.\nAnswer (True or False): True\nQuestion: What is the name of the city in the United States that is the capital of the state of", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do American teams in National Hockey League outnumber Canadian teams?\nFacts: The National Hockey League is the premiere North American hockey league. The National Hockey League has 7 Canadian teams. The National Hockey League has 24 teams from the United States.\nAnswer (True or False):", "ct_subject": "The National Hockey League is the premiere North American hockey league. The National Hockey League has 7 Canadian teams. The National Hockey League has 24 teams from the United States.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99995184"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99993026"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9790469"], [" in", "0.99996233"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "0.99984896"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" New", "0.61875063"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999999"], [" city", "0.99916816"], [" of", "0.9975017"], [" New", "1.0"], [" York", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"]]}, "757": {"qid": "9eec85fab510de606494", "term": "Elon Musk", "description": "American industrialist and investor", "question": "Has Elon Musk's hairline changed?", "answer": true, "facts": ["When Elon Musk was much younger, he was clearly balding.", "Elon Musk does not show any signs of balding as of 2020."], "decomposition": ["What feature of Elon Musk's hair was notable when he was younger?", "Is #1 no longer observable in present times?"], "evidence": [[[["Elon Musk-5"], "no_evidence"], ["no_evidence", "operation"]], [[["Elon Musk-1"], "no_evidence"], [["Hair loss-4"], "no_evidence", "operation"]], [["no_evidence"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the moon made of cheese?\nFacts: The moon is made of cheese.\nAnswer (True or False): True\nQuestion: Is the moon made of cheese?\nFacts: The moon is made of cheese.\nAnswer (True or False): True\nQuestion: Is the moon made of cheese?\nFacts: The moon is made of cheese.\nAnswer (True or", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Has Elon Musk's hairline changed?\nFacts: When Elon Musk was much younger, he was clearly balding. Elon Musk does not show any signs of balding as of 2020.\nAnswer (True or False):", "ct_subject": "When Elon Musk was much younger, he was clearly balding. Elon Musk does not show any signs of balding as of 2020.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.933315"], [" the", "0.9990707"], [" moon", "0.9982681"], [" made", "0.9999995"], [" of", "1.0"], [" cheese", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" moon", "1.0"], [" is", "1.0"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99998784"], [" the", "1.0"], [" moon", "0.99987495"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" moon", "1.0"], [" is", "1.0"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" moon", "1.0"], [" is", "1.0"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"]]}, "758": {"qid": "cda85328c8825e86d3f0", "term": "Astronomer", "description": "Scientist who studies celestial bodies", "question": "Does Nintendo's link ever see an astronomer?", "answer": true, "facts": ["Link is the main character of the Nintendo franchise 'Zelda\".", "In \"Legend of Zelda: Majora's Mask\" Link meets an astronomer in an observatory."], "decomposition": ["Which game is Link from?", "In #1, did link meet an astronomer?"], "evidence": [[[["The Legend of Zelda-24"]], [["Universe of The Legend of Zelda-60"], "no_evidence", "operation"]], [[["Link (The Legend of Zelda)-1"]], ["no_evidence"]], [[["Link (The Legend of Zelda)-1"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to ever be born in the year 2000?\nFacts: The first person to ever be born in the year 2000 was born in the year 2000.\nAnswer (True or False): True\nQuestion: What is the name of the first person to ever be born in the year 2000?\nFacts: The first person", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Nintendo's link ever see an astronomer?\nFacts: Link is the main character of the Nintendo franchise 'Zelda\". In \"Legend of Zelda: Majora's Mask\" Link meets an astronomer in an observatory.\nAnswer (True or False):", "ct_subject": "Link is the main character of the Nintendo franchise 'Zelda\". In \"Legend of Zelda: Majora's Mask\" Link meets an astronomer in an observatory.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99730444"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "0.99593484"], [" person", "0.9999298"], [" to", "1.0"], [" ever", "0.81528467"], [" be", "0.99980754"], [" born", "1.0"], [" in", "0.99999976"], [" the", "1.0"], [" year", "0.99999917"], [" 2000", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "0.89986354"], [" person", "1.0"], [" to", "1.0"], [" ever", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "0.9999902"], [" born", "0.99606186"], [" in", "0.9986105"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" ever", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "0.99999964"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"]]}, "759": {"qid": "f6e1ccbadcebd21aadf9", "term": "Swastika", "description": "a geometrical figure and an ancient religious icon in the cultures of Eurasia and 20th-century symbol of Nazism", "question": "Does the swastika have positive uses?", "answer": true, "facts": ["The swastika is used in the Hindu religion to represent the sun.", "People practicing Hindu believe the swastika represents prosperity and good luck."], "decomposition": ["What does the swastika represent in Hinduism?", "What beliefs do Hindu worshippers associate with #1?", "Are #2 positive?"], "evidence": [[[["Swastika-34"]], [["Swastika-34"]], [["Swastika-34"]]], [[["Swastika-1", "Swastika-3"]], [["Swastika-3"]], ["operation"]], [[["Swastika-3"]], [["Swastika-3"]], [["Luck-24", "Prosperity-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"soul\" and a \"spirit\"?\nFacts: A soul is a part of a person that is separate from the body. A spirit is a part of a person that is not separate from the body.\nAnswer (True or False): True\nQuestion: What is the difference between a \"soul\" and a \"spirit\"?\nFacts: A", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does the swastika have positive uses?\nFacts: The swastika is used in the Hindu religion to represent the sun. People practicing Hindu believe the swastika represents prosperity and good luck.\nAnswer (True or False):", "ct_subject": "The swastika is used in the Hindu religion to represent the sun. People practicing Hindu believe the swastika represents prosperity and good luck.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9905689"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9886154"], [" between", "1.0"], [" a", "0.9992988"], [" \"", "0.9999713"], ["s", "0.9770198"], ["oul", "0.86885774"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["spirit", "0.45261914"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.52359927"], [" soul", "1.0"], [" is", "1.0"], [" a", "0.9999964"], [" part", "1.0"], [" of", "1.0"], [" a", "0.98848957"], [" person", "1.0"], [" that", "0.9962256"], [" is", "1.0"], [" separate", "0.9988072"], [" from", "1.0"], [" the", "1.0"], [" body", "1.0"], [".", "1.0"], [" A", "1.0"], [" spirit", "1.0"], [" is", "1.0"], [" a", "1.0"], [" part", "1.0"], [" of", "1.0"], [" a", "0.9999999"], [" person", "1.0"], [" that", "1.0"], [" is", "1.0"], [" not", "0.999496"], [" separate", "1.0"], [" from", "1.0"], [" the", "1.0"], [" body", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["oul", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["spirit", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"]]}, "760": {"qid": "a202af46315d9970d768", "term": "University of Pittsburgh", "description": "American state-related research university located in Pittsburgh, Pennsylvania", "question": "Did University of Pittsburgh founder have great deal in common with Judith Sheindlin?", "answer": true, "facts": ["Hugh Henry Brackenridge founded University of Pittsburgh in 1787.", "Judith Sheindlin is a judge, lawyer, and author.", "Hugh Henry Brackenridge was a writer, lawyer, judge, and Justice of the Supreme Court of Pennsylvania."], "decomposition": ["Who was the founder of University of Pittsburgh?", "What are the major things #1 is known for?", "What are the major things Judith Sheindlin is known for?", "Is there an overlap between #2 and #3?"], "evidence": [[[["History of the University of Pittsburgh-2"]], [["Hugh Henry Brackenridge-4"]], [["Judy Sheindlin-1"]], [["Judge-5"], "operation"]], [[["University of Pittsburgh-1"]], [["Hugh Henry Brackenridge-1"]], [["Judy Sheindlin-1"]], ["operation"]], [[["History of the University of Pittsburgh-2"]], [["Hugh Henry Brackenridge-1"]], [["Judy Sheindlin-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the first person to be born in the United States have a name that was not his father's?\nFacts: The first person to be born in the United States was a boy named John Smith. He was born in 1731. His", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did University of Pittsburgh founder have great deal in common with Judith Sheindlin?\nFacts: Hugh Henry Brackenridge founded University of Pittsburgh in 1787. Judith Sheindlin is a judge, lawyer, and author. Hugh Henry Brackenridge was a writer, lawyer, judge, and Justice of the Supreme Court of Pennsylvania.\nAnswer (True or False):", "ct_subject": "Hugh Henry Brackenridge founded University of Pittsburgh in 1787. Judith Sheindlin is a judge, lawyer, and author. Hugh Henry Brackenridge was a writer, lawyer, judge, and Justice of the Supreme Court of Pennsylvania.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9994486"], [" the", "1.0"], [" first", "0.985358"], [" person", "0.8697638"], [" to", "1.0"], [" be", "0.99837995"], [" born", "0.98279405"], [" in", "1.0"], [" the", "0.99999976"], [" United", "0.999974"], [" States", "1.0"], [" have", "0.9999782"], [" a", "1.0"], [" name", "1.0"], [" that", "1.0"], [" was", "0.9999976"], [" not", "0.9991714"], [" his", "0.9979132"], [" father", "0.98329645"], ["'s", "1.0"], ["?", "0.78705794"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "0.99999964"], [" a", "0.998389"], [" boy", "0.77235043"], [" named", "1.0"], [" John", "1.0"], [" Smith", "1.0"], [".", "1.0"], [" He", "0.9764099"], [" was", "1.0"], [" born", "1.0"], [" in", "1.0"], [" 17", "0.9994747"], ["31", "0.9998528"], [".", "0.9999994"], [" His", "1.0"]]}, "761": {"qid": "1a4eb85c31188af515b0", "term": "Los Angeles County, California", "description": "County in California, United States", "question": "Is Disney associated with Los Angeles County?", "answer": true, "facts": ["Disney Concert Hall and Disney Studio are located in Los Angeles.", "The city of Los Angeles is located in Los Angeles County."], "decomposition": ["Where are Disney Concert Hall and Disney Studio located?", "Is #1 located in Los Angeles County?"], "evidence": [[[["Walt Disney Animation Studios-1", "Walt Disney Concert Hall-1"]], [["Burbank, California-1", "Central Los Angeles-1", "Downtown Los Angeles-1"], "operation"]], [[["Walt Disney Concert Hall-1", "Walt Disney Studios (division)-1"]], [["Burbank, California-1", "Los Angeles County, California-1"]]], [[["Walt Disney Studios (Burbank)-4"]], [["Burbank, California-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city of Los Angeles derived from the name of the city of Venice?\nFacts: The name of the city of Venice is derived from the name of the city of Venice.\nAnswer (True or False): True\nQuestion: Is the name of the city of Los Angeles derived from the name of the city of Venice?\nFacts: The name of the city of Venice is derived", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Disney associated with Los Angeles County?\nFacts: Disney Concert Hall and Disney Studio are located in Los Angeles. The city of Los Angeles is located in Los Angeles County.\nAnswer (True or False):", "ct_subject": "Disney Concert Hall and Disney Studio are located in Los Angeles. The city of Los Angeles is located in Los Angeles County.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999937"], [" the", "0.9998932"], [" name", "0.9999999"], [" of", "0.99999726"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Los", "1.0"], [" Angeles", "1.0"], [" derived", "0.9999603"], [" from", "1.0"], [" the", "1.0"], [" name", "0.9999999"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Venice", "0.9993292"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999999"], [" name", "0.9999949"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Venice", "0.8375743"], [" is", "1.0"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Venice", "1.0"], [".", "1.0"], ["\n", "0.9999974"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Los", "0.99999964"], [" Angeles", "1.0"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Venice", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Venice", "1.0"], [" is", "1.0"], [" derived", "1.0"]]}, "762": {"qid": "9738b3c0a4db11d5f8bc", "term": "Cerebral palsy", "description": "A group of disorders affecting the development of movement and posture, often accompanied by disturbances of sensation, perception, cognition, and behavior. It results from damage to the fetal or infant brain.", "question": "Is a slime mold safe from cerebral palsy?", "answer": true, "facts": ["Cerebral palsy is a disorder caused by damage to fetal or infant brains.", "Slime molds are simple organisms that are similar to fungi.", "Slime molds do not possess a brain."], "decomposition": ["Damage to what structure can cause cerebral palsy?", "What structures do slime molds have?", "Is #1 listed in #2?"], "evidence": [[[["Cerebral palsy-2"]], [["Plasmodium (life cycle)-3"]], ["operation"]], [[["Cerebral palsy-2"]], [["Slime mold-18"]], ["operation"]], [[["Cerebral palsy-2"]], [["Slime mold-18"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is a slime mold safe from cerebral palsy?\nFacts: Cerebral palsy is a disorder caused by damage to fetal or infant brains. Slime molds are simple organisms that are similar to fungi. Slime molds do not possess a brain.\nAnswer (True or False): True\nQuestion: Is a slime mold safe from cerebral", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is a slime mold safe from cerebral palsy?\nFacts: Cerebral palsy is a disorder caused by damage to fetal or infant brains. Slime molds are simple organisms that are similar to fungi. Slime molds do not possess a brain.\nAnswer (True or False):", "ct_subject": "Cerebral palsy is a disorder caused by damage to fetal or infant brains. Slime molds are simple organisms that are similar to fungi. Slime molds do not possess a brain.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.999998"], [" a", "0.9933391"], [" slime", "0.93841815"], [" mold", "1.0"], [" safe", "1.0"], [" from", "1.0"], [" cerebral", "0.9999999"], [" pals", "1.0"], ["y", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Cere", "0.99999976"], ["bral", "1.0"], [" pals", "1.0"], ["y", "1.0"], [" is", "1.0"], [" a", "1.0"], [" disorder", "1.0"], [" caused", "1.0"], [" by", "1.0"], [" damage", "1.0"], [" to", "1.0"], [" fetal", "1.0"], [" or", "1.0"], [" infant", "1.0"], [" brains", "1.0"], [".", "1.0"], [" Slime", "1.0"], [" m", "1.0"], ["olds", "1.0"], [" are", "1.0"], [" simple", "1.0"], [" organisms", "1.0"], [" that", "1.0"], [" are", "1.0"], [" similar", "1.0"], [" to", "1.0"], [" fungi", "1.0"], [".", "1.0"], [" Slime", "1.0"], [" m", "1.0"], ["olds", "1.0"], [" do", "1.0"], [" not", "1.0"], [" possess", "1.0"], [" a", "1.0"], [" brain", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" a", "1.0"], [" slime", "1.0"], [" mold", "1.0"], [" safe", "1.0"], [" from", "1.0"], [" cerebral", "1.0"]]}, "763": {"qid": "2fca3349a211a68b41f2", "term": "Barley", "description": "Species of plant", "question": "Would the owners of the company Peter Griffin works for need barley?", "answer": true, "facts": ["Peter Griffin works for Pawtucket Brewery.", "Pawtucket Brewery produces beer.", "Barley is the preferred grain for making beer."], "decomposition": ["What kind of company is Peter Griffin?", "What does #1 produce?", "Does producing #2 require barley?"], "evidence": [[[["Peter Griffin-2"], "no_evidence"], [["Brewery-1"]], [["Brewery-27"], "operation"]], [[["Peter Griffin-2"]], [["Brewery-1"]], [["Beer-1"], "operation"]], [[["Peter Griffin-2"]], [["Brewery-1"]], [["Barley-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the capital of Rhode Island?\nFacts: The capital of Rhode Island is Providence.\nAnswer (True or False): True\nQuestion: What is the name of the city in the United States that is the capital of Massachusetts?\nFacts: The capital of Massachusetts is Boston.\nAnswer (True or False", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would the owners of the company Peter Griffin works for need barley?\nFacts: Peter Griffin works for Pawtucket Brewery. Pawtucket Brewery produces beer. Barley is the preferred grain for making beer.\nAnswer (True or False):", "ct_subject": "Peter Griffin works for Pawtucket Brewery. Pawtucket Brewery produces beer. Barley is the preferred grain for making beer.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.999992"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "0.99819934"], [" in", "0.9999945"], [" the", "0.9999999"], [" United", "0.99999964"], [" States", "1.0"], [" that", "1.0"], [" is", "0.9989427"], [" the", "1.0"], [" capital", "0.9999902"], [" of", "1.0"], [" Rhode", "0.8858569"], [" Island", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.97708184"], [" capital", "0.4742964"], [" of", "1.0"], [" Rhode", "1.0"], [" Island", "1.0"], [" is", "1.0"], [" Providence", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" Massachusetts", "0.87054914"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" Massachusetts", "1.0"], [" is", "1.0"], [" Boston", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"]]}, "764": {"qid": "47e4f407f7186ba6b86f", "term": "Chipmunk", "description": "Tribe of mammals (rodent (marmot))", "question": "Is an Eastern chipmunk likely to die before seeing two leap years?", "answer": true, "facts": ["A leap year happens every four years.", "The Eastern chipmunk has an average lifespan of three years."], "decomposition": ["What is the average lifespan of an Eastern chipmunk?", "How often does a leap year occur?", "Is #2 greater than #1?"], "evidence": [[[["Eastern chipmunk-7"]], [["Leap year-6"]], ["operation"]], [[["Chipmunk-11"]], [["Leap year-2"]], ["operation"]], [[["Chipmunk-11"]], [["Leap year-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is a person more likely to be born in the same year as a person who has a birthday on the same day?\nFacts: A person is more likely to be born on the same day as a person who has a birthday on the same day.\nAnswer (True or False): True\nQuestion: Is a person more likely to be born on the same day as a person who has", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is an Eastern chipmunk likely to die before seeing two leap years?\nFacts: A leap year happens every four years. The Eastern chipmunk has an average lifespan of three years.\nAnswer (True or False):", "ct_subject": "A leap year happens every four years. The Eastern chipmunk has an average lifespan of three years.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999753"], [" a", "0.9989164"], [" person", "0.877235"], [" more", "0.99699956"], [" likely", "1.0"], [" to", "1.0"], [" be", "0.99997675"], [" born", "0.99971324"], [" in", "1.0"], [" the", "0.99954236"], [" same", "0.9999845"], [" year", "1.0"], [" as", "0.99999964"], [" a", "0.9991636"], [" person", "1.0"], [" who", "1.0"], [" has", "0.5266815"], [" a", "0.248459"], [" birthday", "0.99886984"], [" on", "0.9334793"], [" the", "0.99999726"], [" same", "1.0"], [" day", "1.0"], ["?", "0.9999981"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" person", "1.0"], [" is", "1.0"], [" more", "1.0"], [" likely", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" on", "0.9927417"], [" the", "1.0"], [" same", "1.0"], [" day", "1.0"], [" as", "1.0"], [" a", "1.0"], [" person", "1.0"], [" who", "1.0"], [" has", "1.0"], [" a", "1.0"], [" birthday", "1.0"], [" on", "1.0"], [" the", "1.0"], [" same", "1.0"], [" day", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" a", "0.9999982"], [" person", "1.0"], [" more", "1.0"], [" likely", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" on", "0.9999832"], [" the", "1.0"], [" same", "1.0"], [" day", "1.0"], [" as", "1.0"], [" a", "1.0"], [" person", "1.0"], [" who", "1.0"], [" has", "1.0"]]}, "765": {"qid": "12802f22d30bcaf903a9", "term": "Holy Land", "description": "Term used by Jews, Christians, and Muslims to describe the Land of Israel and Palestine", "question": "Did Holy Land belong to Adamu's tribe?", "answer": true, "facts": ["The Holy Land is a place that Jews, Muslims, and Christians revere.", "Adamu was an early king of Assyria.", "The Assyrians were in regions of the Holy Land as far back as 2600 BC.", "The predecessors to the Assyrians were in regions of the Holy Land as far back as 3500 BC."], "decomposition": ["Which place is referred to as the Holy Land?", "Which tribe was Adamu a leader of?", "Did #2 occupy #1?"], "evidence": [[[["Holy Land-1"]], [["Adamu (Assyrian king)-1"]], [["Adamu (Assyrian king)-4", "Assyria-1", "Mesopotamia-1"]]], [[["Holy Land-1"]], [["Adamu (Assyrian king)-1", "Assyrian people-1", "Assyrian people-2", "Assyrian people-51"]], ["no_evidence", "operation"]], [[["Holy place-8"], "operation"], [["Adamu Adamu-2"], "no_evidence"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Holy Land belong to the tribe of Judah?\nFacts: The Holy Land was a place that Jews, Muslims, and Christians revere. Judah was an early king of Judah. The ancestors of", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Holy Land belong to Adamu's tribe?\nFacts: The Holy Land is a place that Jews, Muslims, and Christians revere. Adamu was an early king of Assyria. The Assyrians were in regions of the Holy Land as far back as 2600 BC. The predecessors to the Assyrians were in regions of the Holy Land as far back as 3500 BC.\nAnswer (True or False):", "ct_subject": "The Holy Land is a place that Jews, Muslims, and Christians revere. Adamu was an early king of Assyria. The Assyrians were in regions of the Holy Land as far back as 2600 BC. The predecessors to the Assyrians were in regions of the Holy Land as far back as 3500 BC.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999999"], [" the", "0.97660863"], [" Holy", "0.9175727"], [" Land", "1.0"], [" belong", "1.0"], [" to", "1.0"], [" the", "0.99999905"], [" tribe", "0.081313245"], [" of", "1.0"], [" Judah", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Holy", "1.0"], [" Land", "1.0"], [" was", "0.9967757"], [" a", "0.99527776"], [" place", "0.9999467"], [" that", "1.0"], [" Jews", "1.0"], [",", "1.0"], [" Muslims", "1.0"], [",", "1.0"], [" and", "1.0"], [" Christians", "1.0"], [" reve", "1.0"], ["re", "1.0"], [".", "1.0"], [" Judah", "0.80886585"], [" was", "1.0"], [" an", "0.6919105"], [" early", "1.0"], [" king", "1.0"], [" of", "1.0"], [" Judah", "1.0"], [".", "1.0"], [" The", "1.0"], [" ancestors", "0.9989536"], [" of", "1.0"]]}, "766": {"qid": "7f435c65e98ee7f07b85", "term": "Cornwall", "description": "County of England", "question": "Was John George Bice's birthplace near Cornwall?", "answer": true, "facts": ["Politician John George Bice was born in Callington.", "Cornwall is a place located in South West England.", "Callington is a small town in South East Cornwall."], "decomposition": ["Where was John George Bice born?", "Is #1 located close to Cornwall?"], "evidence": [[[["John George Bice-2"]], ["operation"]], [[["John George Bice-2"]], [["John George Bice-2"]]], [[["John George Bice-2"]], [["Callington-1"]]], [[["John George Bice-2"]], [["John George Bice-2"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Was the name of the town of Callington, Cornwall, named after John George Bice?\nFacts: The town of Callington, Cornwall, is named after John George Bice.\nAnswer (True or False): True\nQuestion: Was the town of Callington, Cornwall, named after John George Bice?\nFacts: The town of", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was John George Bice's birthplace near Cornwall?\nFacts: Politician John George Bice was born in Callington. Cornwall is a place located in South West England. Callington is a small town in South East Cornwall.\nAnswer (True or False):", "ct_subject": "Politician John George Bice was born in Callington. Cornwall is a place located in South West England. Callington is a small town in South East Cornwall.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "0.9997081"], [" the", "1.0"], [" name", "0.99999833"], [" of", "1.0"], [" the", "1.0"], [" town", "0.9975132"], [" of", "1.0"], [" Call", "1.0"], ["ington", "1.0"], [",", "0.99976355"], [" Cornwall", "0.98635244"], [",", "1.0"], [" named", "0.99847037"], [" after", "1.0"], [" John", "1.0"], [" George", "1.0"], [" B", "1.0"], ["ice", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999995"], [" town", "1.0"], [" of", "1.0"], [" Call", "1.0"], ["ington", "1.0"], [",", "0.99776196"], [" Cornwall", "1.0"], [",", "1.0"], [" is", "0.909907"], [" named", "1.0"], [" after", "1.0"], [" John", "1.0"], [" George", "1.0"], [" B", "1.0"], ["ice", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "0.9999999"], [" the", "0.9991904"], [" town", "0.99671495"], [" of", "1.0"], [" Call", "1.0"], ["ington", "1.0"], [",", "1.0"], [" Cornwall", "1.0"], [",", "1.0"], [" named", "1.0"], [" after", "1.0"], [" John", "1.0"], [" George", "1.0"], [" B", "1.0"], ["ice", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999713"], [" town", "1.0"], [" of", "1.0"]]}, "767": {"qid": "ae0891799990442fbb0e", "term": "Urban planner", "description": "professional who works on city planning", "question": "Would Paul Bunyan hypothetically be a poor choice for an urban planner?", "answer": true, "facts": ["Paul Bunyan was a legendary giant from tall tale stories.", "Urban planners need to design buildings and ceilings in cities.", "Paul Bunyan lived in the country side with a giant blue ox.", "New York city apartment ceilings average around 8 feet in height.", "Paul Bunyan was over seven feet tall."], "decomposition": ["What environments are urban planners experts of?", "Did Paul Bunyan live or work in #1?"], "evidence": [[[["Urban planner-1", "Urban planner-2"]], [["Paul Bunyan-1"], "operation"]], [[["Urban planner-2"]], [["Paul Bunyan-1"], "operation"]], [[["Urban planner-2"], "no_evidence"], [["Lumberjack-5", "Paul Bunyan-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average height of a man in the United States?\nFacts: The average height of a man in the United States is 5'10\".\nAnswer (True or False): True\nQuestion:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Paul Bunyan hypothetically be a poor choice for an urban planner?\nFacts: Paul Bunyan was a legendary giant from tall tale stories. Urban planners need to design buildings and ceilings in cities. Paul Bunyan lived in the country side with a giant blue ox. New York city apartment ceilings average around 8 feet in height. Paul Bunyan was over seven feet tall.\nAnswer (True or False):", "ct_subject": "Paul Bunyan was a legendary giant from tall tale stories. Urban planners need to design buildings and ceilings in cities. Paul Bunyan lived in the country side with a giant blue ox. New York city apartment ceilings average around 8 feet in height. Paul Bunyan was over seven feet tall.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999883"], [" is", "1.0"], [" the", "1.0"], [" average", "0.9785856"], [" height", "0.45615992"], [" of", "1.0"], [" a", "1.0"], [" man", "0.9999963"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "0.99999976"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" height", "1.0"], [" of", "1.0"], [" a", "1.0"], [" man", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "1.0"], [" 5", "0.9999999"], ["'", "0.958105"], ["10", "0.4993487"], ["\".", "0.99987864"], ["\n", "0.9999988"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"]]}, "768": {"qid": "55b8d3c7ae5fbb58ddce", "term": "Leafhopper", "description": "family of insects", "question": "Do Leafhoppers compete with Log Cabin syrup producers for resources?", "answer": true, "facts": ["Leafhoppers are insects that feed on sap from trees.", "Log Cabin is a  company that originated in Minnesota and makes several varieties of maple syrup.", "Sap is an ingredient in maple syrup."], "decomposition": ["What does the leafhopper diet consist of?", "What kind of syrup is produced by Log Cabin?", "What are the ingredients in #2?", "Is any substance listed in #1 also found in #3?"], "evidence": [[[["Leafhopper-6"], "no_evidence"], [["Log Cabin syrup-1", "Syrup-2"], "no_evidence"], ["operation"], ["operation"]], [[["Leafhopper-1"]], [["Log Cabin syrup-1"], "no_evidence"], [["Maple syrup-1"]], ["operation"]], [[["Leafhopper-6"]], [["Log Cabin syrup-4"]], [["Maple syrup-6"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: The first person to be executed in the United States was John Brown, who was hanged in 1859.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be executed in the United", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do Leafhoppers compete with Log Cabin syrup producers for resources?\nFacts: Leafhoppers are insects that feed on sap from trees. Log Cabin is a company that originated in Minnesota and makes several varieties of maple syrup. Sap is an ingredient in maple syrup.\nAnswer (True or False):", "ct_subject": "Leafhoppers are insects that feed on sap from trees. Log Cabin is a company that originated in Minnesota and makes several varieties of maple syrup. Sap is an ingredient in maple syrup.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9997743"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9852051"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9939091"], [" person", "0.97873497"], [" to", "1.0"], [" be", "0.999954"], [" executed", "0.9999554"], [" in", "0.99999976"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.53000087"], [" first", "1.0"], [" person", "1.0"], [" to", "0.9933315"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" John", "0.9993168"], [" Brown", "0.9999999"], [",", "0.63354844"], [" who", "1.0"], [" was", "1.0"], [" hanged", "1.0"], [" in", "0.99999976"], [" 18", "0.99999964"], ["59", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"]]}, "769": {"qid": "2127e3455127e099982b", "term": "Scottish people", "description": "ethnic inhabitants of Scotland", "question": "Does the Pixar film Brave feature Scottish people?", "answer": true, "facts": ["The movie Brave is set in the Scottish highlands.", "Merida, the main character of Brave, is a Princess of Medieval Scotland "], "decomposition": ["Who are the main characters of the Pixar film Brave?", "Are any of #1 from Scotland?"], "evidence": [[[["Brave (2012 film)-4"]], ["operation"]], [[["Brave (2012 film)-4"]], [["Brave (2012 film)-4"], "operation"]], [[["Brave (2012 film)-1", "Brave (2012 film)-4"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city of New York City derived from the name of the city of New York?\nFacts: The name of the city of New York is derived from the name of the city of New York. The name of the city of New York is derived from the name of the city of New York.\nAnswer (True or False): True\nQuestion: Is the name of", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does the Pixar film Brave feature Scottish people?\nFacts: The movie Brave is set in the Scottish highlands. Merida, the main character of Brave, is a Princess of Medieval Scotland \nAnswer (True or False):", "ct_subject": "The movie Brave is set in the Scottish highlands. Merida, the main character of Brave, is a Princess of Medieval Scotland ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9914522"], [" the", "0.9999995"], [" name", "0.99997354"], [" of", "0.9999982"], [" the", "1.0"], [" city", "0.9999919"], [" of", "1.0"], [" New", "0.997494"], [" York", "1.0"], [" City", "1.0"], [" derived", "0.78713775"], [" from", "1.0"], [" the", "1.0"], [" name", "0.99999297"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "0.96796656"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" is", "0.9198306"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [".", "1.0"], [" The", "0.99999666"], [" name", "0.99967206"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" is", "0.9998381"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [".", "1.0"], ["\n", "0.47697553"], ["Answer", "0.9999987"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99999976"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"]]}, "770": {"qid": "665a7a698ff08a8aa399", "term": "Chick-fil-A", "description": "American fast food chain", "question": "Would a vegetarian be able to eat something at Chick-fil-A?", "answer": true, "facts": ["Most people who follow a vegetarian diet don't eat meat, fish or poultry. ", "While Chick-fil-A sells chicken, they also offer other items. ", "Items that are meat free include: hash browns, waffle fries, and superfood sides."], "decomposition": ["What foods must a vegetarian avoid?", "What foods are on the menu of Chick-fil-A?", "Are any items in #2 free of #1?"], "evidence": [[[["Vegetarianism-1"]], [["Chick-fil-A-39"]], [["Crinkle-cutting-4", "Potato-1"], "operation"]], [[["Vegetarianism-1"]], [["Chick-fil-A-39"]], ["operation"]], [[["Vegetarianism-1"]], [["Chick-fil-A-39"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the most popular sport in the United States?\nFacts: The most popular sport in the United States is football.  The most popular sport in the world is soccer.  The most popular sport in the world is basketball.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a vegetarian be able to eat something at Chick-fil-A?\nFacts: Most people who follow a vegetarian diet don't eat meat, fish or poultry.  While Chick-fil-A sells chicken, they also offer other items.  Items that are meat free include: hash browns, waffle fries, and superfood sides.\nAnswer (True or False):", "ct_subject": "Most people who follow a vegetarian diet don't eat meat, fish or poultry.  While Chick-fil-A sells chicken, they also offer other items.  Items that are meat free include: hash browns, waffle fries, and superfood sides.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999917"], [" is", "1.0"], [" the", "1.0"], [" most", "0.16288374"], [" popular", "0.9986387"], [" sport", "0.7764266"], [" in", "1.0"], [" the", "1.0"], [" United", "0.38763604"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.83819276"], [" most", "1.0"], [" popular", "1.0"], [" sport", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "1.0"], [" football", "0.999997"], [".", "1.0"], [" ", "1.0"], [" The", "0.9999869"], [" most", "1.0"], [" popular", "1.0"], [" sport", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "0.9999788"], [" is", "1.0"], [" soccer", "1.0"], [".", "1.0"], [" ", "0.99170226"], [" The", "1.0"], [" most", "1.0"], [" popular", "1.0"], [" sport", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" is", "1.0"], [" basketball", "0.92689955"], [".", "1.0"]]}, "771": {"qid": "2be83f11f8b6602afe87", "term": "Psychotherapy", "description": "clinically applied psychology for desired behavior modification", "question": "Do some psychotherapy patients have no mental illness?", "answer": true, "facts": ["Psychotherapy is useful for couples navigating relationship issues.", "Grief is a common reason that people seek psychotherapy. "], "decomposition": ["What are some common issues that make people seek psychotherapy?", "Does #1 not always involve mental illness?"], "evidence": [[[["Psychotherapy-1"]], ["operation"]], [[["Psychotherapy-1"]], [["Psychotherapy-4"], "operation"]], [[["Psychotherapy-1"]], [["Psychotherapy-32"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"soul mate\" and a \"soul mate for life\"?\nFacts: A soul mate is someone who is compatible with you. A soul mate for life is someone who is compatible with you for the rest of your life.\nAnswer (True or False): True\nQuestion: What is the difference between a \"soul mate\" and a \"soul mate for", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do some psychotherapy patients have no mental illness?\nFacts: Psychotherapy is useful for couples navigating relationship issues. Grief is a common reason that people seek psychotherapy. \nAnswer (True or False):", "ct_subject": "Psychotherapy is useful for couples navigating relationship issues. Grief is a common reason that people seek psychotherapy. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999422"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9997303"], [" between", "1.0"], [" a", "0.99999905"], [" \"", "1.0"], ["s", "0.974288"], ["oul", "1.0"], [" mate", "0.99999917"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["oul", "1.0"], [" mate", "1.0"], [" for", "1.0"], [" life", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" soul", "1.0"], [" mate", "1.0"], [" is", "1.0"], [" someone", "0.9999194"], [" who", "0.9986909"], [" is", "0.9968924"], [" compatible", "1.0"], [" with", "1.0"], [" you", "1.0"], [".", "0.989691"], [" A", "1.0"], [" soul", "1.0"], [" mate", "1.0"], [" for", "1.0"], [" life", "1.0"], [" is", "1.0"], [" someone", "1.0"], [" who", "1.0"], [" is", "1.0"], [" compatible", "1.0"], [" with", "1.0"], [" you", "1.0"], [" for", "1.0"], [" the", "1.0"], [" rest", "1.0"], [" of", "1.0"], [" your", "1.0"], [" life", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["oul", "1.0"], [" mate", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["oul", "1.0"], [" mate", "1.0"], [" for", "1.0"]]}, "772": {"qid": "33f7f8c55b4acedb061a", "term": "Lolcat", "description": "image combining a photograph of a cat with text intended to contribute humour", "question": "Is purchasing food for a Lolcat unnecessary?", "answer": true, "facts": ["An image macro is a piece of digital media featuring a picture, or artwork, superimposed with some form of text.", "Food is any substance consumed to provide nutritional support for an organism.", "An organism is any individual entity that embodies the properties of life.", "Digital media does not embody the properties of life."], "decomposition": ["Which kind of entities require food?", "Is a lolcat excluded from #1?"], "evidence": [[[["Eating-1"]], [["Lolcat-2"]]], [[["Food-1", "Organism-1", "Organism-2"]], [["Image macro-1", "Lolcat-1", "Media (communication)-1"]]], [[["Food-1"]], [["Lolcat-2"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it possible to be a good person and a good parent?\nFacts: A person is a living being that is capable of experiencing pleasure and pain. A parent is a living being that is capable of experiencing pleasure and pain.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is purchasing food for a Lolcat unnecessary?\nFacts: An image macro is a piece of digital media featuring a picture, or artwork, superimposed with some form of text. Food is any substance consumed to provide nutritional support for an organism. An organism is any individual entity that embodies the properties of life. Digital media does not embody the properties of life.\nAnswer (True or False):", "ct_subject": "An image macro is a piece of digital media featuring a picture, or artwork, superimposed with some form of text. Food is any substance consumed to provide nutritional support for an organism. An organism is any individual entity that embodies the properties of life. Digital media does not embody the properties of life.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.8838013"], [" it", "0.99180484"], [" possible", "0.9461553"], [" to", "1.0"], [" be", "0.7659975"], [" a", "1.0"], [" good", "0.9995536"], [" person", "0.99999964"], [" and", "0.9996717"], [" a", "0.9999999"], [" good", "0.9916181"], [" parent", "0.99931157"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.93622345"], [" person", "0.99976426"], [" is", "0.9916294"], [" a", "1.0"], [" living", "0.5206298"], [" being", "0.9263368"], [" that", "0.99999964"], [" is", "0.99874413"], [" capable", "1.0"], [" of", "1.0"], [" experiencing", "1.0"], [" pleasure", "0.9946696"], [" and", "1.0"], [" pain", "1.0"], [".", "1.0"], [" A", "1.0"], [" parent", "1.0"], [" is", "1.0"], [" a", "1.0"], [" living", "0.9999999"], [" being", "1.0"], [" that", "1.0"], [" is", "1.0"], [" capable", "1.0"], [" of", "1.0"], [" experiencing", "1.0"], [" pleasure", "1.0"], [" and", "1.0"], [" pain", "1.0"], [".", "1.0"]]}, "773": {"qid": "7f5bde421ffbd43d7cdd", "term": "Sirius", "description": "Brightest star in the night sky", "question": "Is Sirius part of a constellation of an animal?", "answer": true, "facts": ["Sirius is the brightest star in the constellation Canis Major.", "Canis Major represents a large dog."], "decomposition": ["What constellation is Sirius a part of?", "What does #1 represent?", "Is #2 an animal?"], "evidence": [[[["Canis Major-2"]], [["Canis Major-1"]], ["operation"]], [[["Sirius-4"]], [["Canis Major-1"]], ["operation"]], [[["Sirius-4"]], [["Canis Major-4"]], [["Animal-4"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the moon a planet?\nFacts: The moon is a satellite of the earth.\nAnswer (True or False): True\nQuestion: Is the sun a planet?\nFacts: The sun is a satellite of the earth.\nAnswer (True or False): True\nQuestion: Is the earth flat?\nFacts: The earth is round.\nAnswer (True or False): True\nQuestion: Is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Sirius part of a constellation of an animal?\nFacts: Sirius is the brightest star in the constellation Canis Major. Canis Major represents a large dog.\nAnswer (True or False):", "ct_subject": "Sirius is the brightest star in the constellation Canis Major. Canis Major represents a large dog.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99996924"], [" the", "0.99980754"], [" moon", "0.98054737"], [" a", "0.99992955"], [" planet", "0.9998142"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" moon", "1.0"], [" is", "1.0"], [" a", "1.0"], [" satellite", "1.0"], [" of", "1.0"], [" the", "1.0"], [" earth", "0.99969435"], [".", "1.0"], ["\n", "0.9999995"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" sun", "0.9991198"], [" a", "1.0"], [" planet", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" sun", "1.0"], [" is", "1.0"], [" a", "1.0"], [" satellite", "0.9720191"], [" of", "1.0"], [" the", "1.0"], [" earth", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" earth", "0.99994135"], [" flat", "0.94505876"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" earth", "1.0"], [" is", "1.0"], [" round", "0.9999999"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"]]}, "774": {"qid": "10d718862227bef4a6ed", "term": "Chives", "description": "edible species of plant", "question": "Could chives be mistaken for grass?", "answer": true, "facts": ["Chives grow upwards in thin green cylindrical shoots. ", "Grass grows upwards in thin green flat shoots."], "decomposition": ["What is the shape and color of Chives?", "What is the shape and color of grass?", "Is #1 the same as #2?"], "evidence": [[[["Chives-4"]], ["no_evidence"], ["operation"]], [[["Chives-4"], "no_evidence"], [["Poaceae-42"], "no_evidence"], ["operation"]], [[["Chives-6"]], [["Poaceae-15"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a cat and a dog?\nFacts: A cat is a mammal. A dog is a carnivore.\nAnswer (True or False): True\nQuestion: What is the difference between a cat and a dog?\nFacts: A cat is a mammal. A dog is a carnivore.\nAnswer (True or False): True\nQuestion: What is the difference between a cat and", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could chives be mistaken for grass?\nFacts: Chives grow upwards in thin green cylindrical shoots.  Grass grows upwards in thin green flat shoots.\nAnswer (True or False):", "ct_subject": "Chives grow upwards in thin green cylindrical shoots.  Grass grows upwards in thin green flat shoots.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99998224"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.99806565"], [" between", "1.0"], [" a", "1.0"], [" cat", "0.50620925"], [" and", "1.0"], [" a", "1.0"], [" dog", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" cat", "1.0"], [" is", "1.0"], [" a", "1.0"], [" mammal", "0.93730646"], [".", "0.99999964"], [" A", "1.0"], [" dog", "1.0"], [" is", "1.0"], [" a", "1.0"], [" carniv", "0.99940574"], ["ore", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" cat", "0.99981326"], [" and", "1.0"], [" a", "1.0"], [" dog", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" cat", "1.0"], [" is", "1.0"], [" a", "1.0"], [" mammal", "1.0"], [".", "1.0"], [" A", "1.0"], [" dog", "1.0"], [" is", "1.0"], [" a", "1.0"], [" carniv", "1.0"], ["ore", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" cat", "1.0"], [" and", "1.0"]]}, "775": {"qid": "5694613473f2c2c557f7", "term": "New York Harbor", "description": "harbor in the New York City, U.S.A. metropolitan area", "question": "Does a giant green lady stand in New York Harbor?", "answer": true, "facts": ["New York Harbor is a body of water between south Manhattan and the Atlantic Ocean", "The Statue of Liberty stands in New York Harbor", "The Statue of Liberty is a very large, green statue of a woman"], "decomposition": ["Where is The Statue of Liberty located?", "Is #1 in New York Harbor?"], "evidence": [[[["Statue of Liberty-1"]], [["Statue of Liberty-1"]]], [[["Statue of Liberty-1"]], [["Statue of Liberty-1"]]], [[["Statue of Liberty-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city of New York?\nFacts: New York City is the largest city in the United States of America The city of New York is located in New York State The city of New York is located in New York State The city of New York is located in New York State The city of New York is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does a giant green lady stand in New York Harbor?\nFacts: New York Harbor is a body of water between south Manhattan and the Atlantic Ocean The Statue of Liberty stands in New York Harbor The Statue of Liberty is a very large, green statue of a woman\nAnswer (True or False):", "ct_subject": "New York Harbor is a body of water between south Manhattan and the Atlantic Ocean The Statue of Liberty stands in New York Harbor The Statue of Liberty is a very large, green statue of a woman", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99075365"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9999685"], [" of", "0.99789834"], [" New", "0.99490017"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" New", "1.0"], [" York", "1.0"], [" City", "1.0"], [" is", "1.0"], [" the", "0.9999465"], [" largest", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" of", "0.99193734"], [" America", "1.0"], [" The", "0.99999976"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" is", "1.0"], [" located", "0.9999995"], [" in", "1.0"], [" New", "0.99590176"], [" York", "1.0"], [" State", "1.0"], [" The", "0.99996126"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" is", "1.0"], [" located", "0.85804707"], [" in", "1.0"], [" New", "1.0"], [" York", "1.0"], [" State", "0.98175645"], [" The", "0.99999964"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" is", "1.0"], [" located", "1.0"], [" in", "1.0"], [" New", "1.0"], [" York", "1.0"], [" State", "1.0"], [" The", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" is", "1.0"]]}, "776": {"qid": "48da75d87c66754ccc2e", "term": "Attack on Pearl Harbor", "description": "Surprise attack by the Imperial Japanese Navy on the U.S. Pacific Fleet in Pearl Harbor in Hawaii", "question": "Was only woman to serve as U.S. Speaker of the House alive during the attack on Pearl Harbor?", "answer": true, "facts": ["Nancy Pelosi is the only woman to ever serve as Speaker of the United States House of Representatives.", "Nancy Pelosi was born on Mar 26, 1940", "The attach on Pearl Harbor occurred on December 7, 1941"], "decomposition": ["Who is the only woman to ever serve as Speaker of the United States House of Representatives?", "When was #1 born?", "When did the attack on Pearl Harbor occur?", "Is #2 before #3?"], "evidence": [[[["Speaker of the United States House of Representatives-3"]], [["Nancy Pelosi-1"]], [["Attack on Pearl Harbor-1"]], ["operation"]], [[["Nancy Pelosi-1"]], [["Nancy Pelosi-1"]], [["Attack on Pearl Harbor-1"]], ["operation"]], [[["Speaker of the United States House of Representatives-3"]], [["Nancy Pelosi-1"]], [["Attack on Pearl Harbor-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Was the first person to be executed in the United States for murder in the United States?\nFacts: The first person to be executed in the United States for murder was John Brown, who was hanged on May 10, 1859.\nAnswer (True or False):", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was only woman to serve as U.S. Speaker of the House alive during the attack on Pearl Harbor?\nFacts: Nancy Pelosi is the only woman to ever serve as Speaker of the United States House of Representatives. Nancy Pelosi was born on Mar 26, 1940 The attach on Pearl Harbor occurred on December 7, 1941\nAnswer (True or False):", "ct_subject": "Nancy Pelosi is the only woman to ever serve as Speaker of the United States House of Representatives. Nancy Pelosi was born on Mar 26, 1940 The attach on Pearl Harbor occurred on December 7, 1941", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "0.9875501"], [" the", "0.9999819"], [" first", "0.9999994"], [" person", "0.8575889"], [" to", "1.0"], [" be", "0.9907765"], [" executed", "0.9345999"], [" in", "0.9998317"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" for", "0.9972475"], [" murder", "0.8612307"], [" in", "1.0"], [" the", "1.0"], [" United", "0.5957076"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9997315"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" for", "1.0"], [" murder", "1.0"], [" was", "1.0"], [" John", "0.9991708"], [" Brown", "1.0"], [",", "0.8407079"], [" who", "0.9987638"], [" was", "1.0"], [" hanged", "1.0"], [" on", "0.9753475"], [" May", "0.9987754"], [" 10", "0.96147114"], [",", "1.0"], [" 18", "1.0"], ["59", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"]]}, "777": {"qid": "a61bbe3edc4de5272c0c", "term": "Honey", "description": "Sweet food made by bees mostly using nectar from flowers", "question": "Is honey associated with queens?", "answer": true, "facts": ["Honey is made by bees.", "Each bee hive is led by a queen bee."], "decomposition": ["What produces honey?", "Do #1 have queens?"], "evidence": [[[["Honey-1"]], [["Honey bee-53"], "operation"]], [[["Honey-1"]], [["Honey bee-53"], "operation"]], [[["Honey-1"]], [["Queen bee-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the moon made of cheese?\nFacts: The moon is made of cheese.\nAnswer (True or False): True\nQuestion: Is the sun made of cheese?\nFacts: The sun is made of cheese.\nAnswer (True or False): True\nQuestion: Is the sun made of cheese?\nFacts: The sun is made of cheese.\nAnswer (True or False): True\nQuestion: Is the sun made of cheese?", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is honey associated with queens?\nFacts: Honey is made by bees. Each bee hive is led by a queen bee.\nAnswer (True or False):", "ct_subject": "Honey is made by bees. Each bee hive is led by a queen bee.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999069"], [" the", "0.99646384"], [" moon", "0.101364285"], [" made", "0.9999629"], [" of", "1.0"], [" cheese", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" moon", "1.0"], [" is", "1.0"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" sun", "0.9987375"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" sun", "1.0"], [" is", "1.0"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" sun", "0.99994373"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" sun", "1.0"], [" is", "1.0"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" sun", "1.0"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], ["?", "1.0"]]}, "778": {"qid": "fc712fb2286006222e50", "term": "Basel", "description": "Place in Basel-Stadt, Switzerland", "question": "Is it dark is Basel during the day in Los Angeles?", "answer": true, "facts": ["Basel is located in the CEST time zone.", "Los Angeles is located in the PDT time zone."], "decomposition": ["What time zone is Basel in?", "What time zone is Los Angeles in?", "What is the time difference in hours between #1 and #2?", "Is #3 at least equal to 8?"], "evidence": [[[["Basel-1", "Central European Summer Time-1", "Central European Summer Time-6"]], [["Pacific Time Zone-1", "Pacific Time Zone-3"]], [["Central European Summer Time-1", "Pacific Time Zone-1"], "operation"], ["operation"]], [[["Basel-1", "UTC+00:30-2"]], [["Pacific Time Zone-9"]], ["no_evidence", "operation"], ["operation"]], [[["Basel-1", "UTC+01:00-1"]], [["Los Angeles-1", "UTC\u221208:00-1"]], ["operation"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it dark is Basel during the day in Los Angeles?\nFacts: Basel is located in the CEST time zone. Los Angeles is located in the PDT time zone.\nAnswer (True or False): True\nQuestion: Is it dark is Basel during the day in Los Angeles?\nFacts: Basel is located in the CEST time zone. Los Angeles is located in", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is it dark is Basel during the day in Los Angeles?\nFacts: Basel is located in the CEST time zone. Los Angeles is located in the PDT time zone.\nAnswer (True or False):", "ct_subject": "Basel is located in the CEST time zone. Los Angeles is located in the PDT time zone.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9996512"], [" it", "1.0"], [" dark", "0.9541273"], [" is", "1.0"], [" Bas", "1.0"], ["el", "1.0"], [" during", "1.0"], [" the", "1.0"], [" day", "1.0"], [" in", "1.0"], [" Los", "0.99999726"], [" Angeles", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Bas", "1.0"], ["el", "1.0"], [" is", "1.0"], [" located", "1.0"], [" in", "1.0"], [" the", "1.0"], [" C", "1.0"], ["EST", "1.0"], [" time", "1.0"], [" zone", "1.0"], [".", "1.0"], [" Los", "1.0"], [" Angeles", "1.0"], [" is", "1.0"], [" located", "1.0"], [" in", "1.0"], [" the", "1.0"], [" PDT", "1.0"], [" time", "1.0"], [" zone", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" dark", "1.0"], [" is", "1.0"], [" Bas", "1.0"], ["el", "1.0"], [" during", "1.0"], [" the", "1.0"], [" day", "1.0"], [" in", "1.0"], [" Los", "1.0"], [" Angeles", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Bas", "1.0"], ["el", "1.0"], [" is", "1.0"], [" located", "1.0"], [" in", "1.0"], [" the", "1.0"], [" C", "1.0"], ["EST", "1.0"], [" time", "1.0"], [" zone", "1.0"], [".", "1.0"], [" Los", "1.0"], [" Angeles", "1.0"], [" is", "1.0"], [" located", "1.0"], [" in", "1.0"]]}, "779": {"qid": "f43533225534420816d6", "term": "Giant squid", "description": "Deep-ocean dwelling squid in the family Architeuthidae", "question": "Can you house a giant squid at Soldier Field?", "answer": true, "facts": ["Soldier Field is a football stadium", "Football fields are 120 yards long, or 360 feet", "The maximum length of a giant squid is 43 feet"], "decomposition": ["How long are giant squid?", "What type of field is Soldier Field?", "How long are #2?", "Is #3 equal to or greater than #1?"], "evidence": [[[["Giant squid-1"]], [["Soldier Field-1"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Giant squid-1"]], [["Soldier Field-1"]], [["American football-36"]], ["operation"]], [[["Giant squid-1"]], [["Soldier Field-1"]], [["Gridiron football-7"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the largest animal in the world?\nFacts: The largest animal in the world is the blue whale The blue whale is the largest animal in the world The blue whale is the largest animal in the world The blue whale is the largest animal in the world The blue whale is the largest animal in the world The blue whale is the largest animal in the world The blue whale is the largest animal", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can you house a giant squid at Soldier Field?\nFacts: Soldier Field is a football stadium Football fields are 120 yards long, or 360 feet The maximum length of a giant squid is 43 feet\nAnswer (True or False):", "ct_subject": "Soldier Field is a football stadium Football fields are 120 yards long, or 360 feet The maximum length of a giant squid is 43 feet", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999938"], [" is", "1.0"], [" the", "1.0"], [" largest", "0.05279741"], [" animal", "0.97168046"], [" in", "0.99886537"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" largest", "1.0"], [" animal", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" is", "1.0"], [" the", "1.0"], [" blue", "1.0"], [" whale", "1.0"], [" The", "0.9991153"], [" blue", "0.9997423"], [" whale", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" animal", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" The", "0.9998436"], [" blue", "0.9994667"], [" whale", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" animal", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" The", "1.0"], [" blue", "1.0"], [" whale", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" animal", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" The", "1.0"], [" blue", "1.0"], [" whale", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" animal", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" The", "1.0"], [" blue", "1.0"], [" whale", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" animal", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" The", "1.0"], [" blue", "1.0"], [" whale", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" animal", "1.0"]]}, "780": {"qid": "cb08864902f07be76e77", "term": "Earth's magnetic field", "description": "Magnetic field that extends from the Earth\u2019s inner core to where it meets the solar wind", "question": "Do Flat Earthers doubt the existence of Earth's magnetic field?", "answer": true, "facts": ["Theories about the Earth's magnetic field depend on the globe model of the Earth.", "Flat Earthers are skeptical of most science related to the Earth and space, believing it to be part of a conspiracy coverup."], "decomposition": ["Which theory about the earth do the Flat-Earthers believe?", "Which earth theory supports the existence of the earth's magnetic field?", "Does #1 contradict #2?"], "evidence": [[[["Flat Earth-1"]], [["Earth's magnetic field-1"]], ["operation"]], [[["Modern flat Earth societies-1"]], [["Modern flat Earth societies-8"], "no_evidence"], ["no_evidence", "operation"]], [[["Flat Earth-58"]], [["History of geomagnetism-12"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Do Flat Earthers believe that the Earth is flat?\nFacts: Flat Earthers believe that the Earth is flat, but they do not believe that the Earth is flat. Flat Earthers believe that the Earth is round, but they do not believe that the Earth is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do Flat Earthers doubt the existence of Earth's magnetic field?\nFacts: Theories about the Earth's magnetic field depend on the globe model of the Earth. Flat Earthers are skeptical of most science related to the Earth and space, believing it to be part of a conspiracy coverup.\nAnswer (True or False):", "ct_subject": "Theories about the Earth's magnetic field depend on the globe model of the Earth. Flat Earthers are skeptical of most science related to the Earth and space, believing it to be part of a conspiracy coverup.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Do", "1.0"], [" Flat", "1.0"], [" E", "1.0"], ["art", "1.0"], ["hers", "1.0"], [" believe", "0.97169983"], [" that", "0.9999999"], [" the", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" flat", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Flat", "1.0"], [" E", "1.0"], ["art", "1.0"], ["hers", "1.0"], [" believe", "1.0"], [" that", "1.0"], [" the", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" flat", "0.9947914"], [",", "0.94082457"], [" but", "1.0"], [" they", "1.0"], [" do", "0.99999976"], [" not", "1.0"], [" believe", "1.0"], [" that", "1.0"], [" the", "0.9422582"], [" Earth", "1.0"], [" is", "1.0"], [" flat", "1.0"], [".", "1.0"], [" Flat", "0.9982773"], [" E", "1.0"], ["art", "1.0"], ["hers", "1.0"], [" believe", "1.0"], [" that", "1.0"], [" the", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" round", "0.99884254"], [",", "0.99999726"], [" but", "1.0"], [" they", "1.0"], [" do", "1.0"], [" not", "1.0"], [" believe", "1.0"], [" that", "1.0"], [" the", "1.0"], [" Earth", "1.0"], [" is", "1.0"]]}, "781": {"qid": "60aa0b975babf5475cf1", "term": "Myth", "description": "Type of traditional narrative", "question": "Was story of Jesus inspired by Egyptian myth?", "answer": true, "facts": ["Jesus was a biblical character that walked on water, was born of a virgin, and was killed beside two thieves.", "Horus was a character in ancient Egyptian myth that walked on water, had a virgin mother, and was executed beside two thieves."], "decomposition": ["What are the main characteristics of the Horus story?", "What are the main characteristics of the Jesus story?", "Is there evidence people believed #1 before #2?", "Is there significant overlap between #1 and #2?", "Are #4 and #3 both \"Yes\"?"], "evidence": [[[["Osiris myth-1"]], [["Jesus-3"]], [["Osiris myth-3"]], [["Jesus-1"]], ["operation"]], [[["Horus-8"], "no_evidence"], [["Jesus-1"], "no_evidence"], [["Ancient Egypt-1"]], ["no_evidence", "operation"], ["operation"]], [[["Horus-11"]], [["Jesus-11"]], [["Jesus-7"], "no_evidence"], [["Horus-1"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Was the story of Jesus inspired by the story of Moses?\nFacts: Jesus was a biblical character that walked on water, was born of a virgin, and was killed beside two thieves. Moses was a character in ancient Egyptian myth that walked on water, had a virgin mother, and was", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was story of Jesus inspired by Egyptian myth?\nFacts: Jesus was a biblical character that walked on water, was born of a virgin, and was killed beside two thieves. Horus was a character in ancient Egyptian myth that walked on water, had a virgin mother, and was executed beside two thieves.\nAnswer (True or False):", "ct_subject": "Jesus was a biblical character that walked on water, was born of a virgin, and was killed beside two thieves. Horus was a character in ancient Egyptian myth that walked on water, had a virgin mother, and was executed beside two thieves.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "0.99926883"], [" the", "1.0"], [" story", "1.0"], [" of", "1.0"], [" Jesus", "0.99999905"], [" inspired", "1.0"], [" by", "1.0"], [" the", "0.9999988"], [" story", "1.0"], [" of", "1.0"], [" Moses", "0.40207553"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Jesus", "0.5480613"], [" was", "1.0"], [" a", "1.0"], [" biblical", "1.0"], [" character", "1.0"], [" that", "1.0"], [" walked", "1.0"], [" on", "1.0"], [" water", "1.0"], [",", "1.0"], [" was", "1.0"], [" born", "1.0"], [" of", "1.0"], [" a", "1.0"], [" virgin", "1.0"], [",", "1.0"], [" and", "1.0"], [" was", "1.0"], [" killed", "1.0"], [" beside", "1.0"], [" two", "1.0"], [" thieves", "1.0"], [".", "1.0"], [" Moses", "1.0"], [" was", "1.0"], [" a", "1.0"], [" character", "1.0"], [" in", "1.0"], [" ancient", "1.0"], [" Egyptian", "1.0"], [" myth", "1.0"], [" that", "1.0"], [" walked", "1.0"], [" on", "1.0"], [" water", "1.0"], [",", "1.0"], [" had", "1.0"], [" a", "1.0"], [" virgin", "1.0"], [" mother", "1.0"], [",", "1.0"], [" and", "1.0"], [" was", "1.0"]]}, "782": {"qid": "0f70628653f7b064a50d", "term": "Harbor seal", "description": "Species of mammal", "question": "Can you see live harbor seals in Washington DC?", "answer": true, "facts": ["The Smithsonian National Zoo is in Washington DC.", "There is a harbor seal exhibit at the Smithsonian National Zoo. "], "decomposition": ["Is there a zoo in Washington DC?", "Is there a harbor seal exhibit at #1?"], "evidence": [[[["National Zoological Park (United States)-2"]], [["National Zoological Park (United States)-26"]]], [[["National Zoological Park (United States)-2"]], [["National Zoological Park (United States)-26"]]], [[["National Zoological Park (United States)-2"]], [["National Zoological Park (United States)-26"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city of Rome?\nFacts: Rome is the capital of the Roman Empire. \nAnswer (True or False): True\nQuestion: What is the name of the city of Rome?\nFacts: Rome is the capital of the Roman Empire. \nAnswer (True or False): True\nQuestion: What is the name of the city of Rome?\nFacts:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can you see live harbor seals in Washington DC?\nFacts: The Smithsonian National Zoo is in Washington DC. There is a harbor seal exhibit at the Smithsonian National Zoo. \nAnswer (True or False):", "ct_subject": "The Smithsonian National Zoo is in Washington DC. There is a harbor seal exhibit at the Smithsonian National Zoo. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99998796"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9997998"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9999831"], [" of", "0.3617176"], [" Rome", "0.55798966"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "0.9999995"], [" is", "0.9995633"], [" the", "0.9999999"], [" capital", "0.99981195"], [" of", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [".", "1.0"], [" ", "0.92875326"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [".", "1.0"], [" ", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"]]}, "783": {"qid": "97ffbaa790ff9df718ec", "term": "Atmosphere of Mars", "description": "atmosphere", "question": "Are all the elements plants need for photosynthesis present in atmosphere of Mars?", "answer": true, "facts": ["Plants need three elements for photosynthesis: Hydrogen, Oxygen, and Carbon.", "The atmosphere of Mars is composed of carbon dioxide, nitrogen, argon, and trace levels of water vapor, oxygen, carbon monoxide, hydrogen and other noble gases."], "decomposition": ["What are the elements needed by plants need for photosynthesis?", "Which elements are found in the atmosphere?", "Are #1 included in #2?"], "evidence": [[[["Photosynthesis-11"]], [["Atmosphere of Mars-11"]], ["operation"]], [[["Photosynthesis-72"], "no_evidence", "operation"], [["Atmosphere-14"], "operation"], ["no_evidence"]], [[["Photosynthesis-10"]], [["Atmosphere-14"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the Earth's atmosphere composed of water vapor?\nFacts: The atmosphere of the Earth is composed of carbon dioxide, nitrogen, argon, and trace levels of water vapor, oxygen, carbon monoxide, hydrogen and other noble gases.\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are all the elements plants need for photosynthesis present in atmosphere of Mars?\nFacts: Plants need three elements for photosynthesis: Hydrogen, Oxygen, and Carbon. The atmosphere of Mars is composed of carbon dioxide, nitrogen, argon, and trace levels of water vapor, oxygen, carbon monoxide, hydrogen and other noble gases.\nAnswer (True or False):", "ct_subject": "Plants need three elements for photosynthesis: Hydrogen, Oxygen, and Carbon. The atmosphere of Mars is composed of carbon dioxide, nitrogen, argon, and trace levels of water vapor, oxygen, carbon monoxide, hydrogen and other noble gases.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.68850213"], [" the", "0.9635823"], [" Earth", "1.0"], ["'s", "1.0"], [" atmosphere", "0.74333686"], [" composed", "0.999997"], [" of", "1.0"], [" water", "0.9995987"], [" vapor", "0.99998"], ["?", "0.99999976"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" atmosphere", "0.99477285"], [" of", "1.0"], [" the", "0.9999981"], [" Earth", "1.0"], [" is", "1.0"], [" composed", "1.0"], [" of", "1.0"], [" carbon", "0.99638087"], [" dioxide", "1.0"], [",", "1.0"], [" nitrogen", "1.0"], [",", "1.0"], [" arg", "1.0"], ["on", "1.0"], [",", "1.0"], [" and", "1.0"], [" trace", "1.0"], [" levels", "1.0"], [" of", "1.0"], [" water", "1.0"], [" vapor", "1.0"], [",", "0.3628422"], [" oxygen", "1.0"], [",", "1.0"], [" carbon", "1.0"], [" mon", "1.0"], ["oxide", "1.0"], [",", "1.0"], [" hydrogen", "1.0"], [" and", "1.0"], [" other", "1.0"], [" noble", "1.0"], [" gases", "1.0"], [".", "1.0"], ["\n", "1.0"]]}, "784": {"qid": "38ef97eb7cdd4200fd00", "term": "Latino", "description": "A group of people in the United States with ties to Latin America", "question": "Is blonde hair green eyed Sara Paxton considered a Latino?", "answer": true, "facts": ["Sara Paxton is an American actress.", "Latino's are people with ancestral ties to Latin America.", "Sara Paxton was born to an Irish/English father and a Mexican/Spanish/Chilean mother.", "Mexico is a country that is part of Latin America."], "decomposition": ["Latinos are people with which nationality?", "Which countries are Sara Paxton's parents from?", "Is any of #2 included in #1?"], "evidence": [[[["Latino (demonym)-1", "Latino (demonym)-2"]], [["Sara Paxton-3"]], ["operation"]], [[["Latin America-12", "Latino (demonym)-1", "Mexico-1"]], [["Sara Paxton-3"]], ["operation"]], [[["Latino (demonym)-18"]], [["Sara Paxton-3"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the word \"gay\" a derogatory term?\nFacts: The word \"gay\" is a term that has been used to describe a variety of sexual orientations. The word \"gay\" is not a derogatory term.\nAnswer (True or False", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is blonde hair green eyed Sara Paxton considered a Latino?\nFacts: Sara Paxton is an American actress. Latino's are people with ancestral ties to Latin America. Sara Paxton was born to an Irish/English father and a Mexican/Spanish/Chilean mother. Mexico is a country that is part of Latin America.\nAnswer (True or False):", "ct_subject": "Sara Paxton is an American actress. Latino's are people with ancestral ties to Latin America. Sara Paxton was born to an Irish/English father and a Mexican/Spanish/Chilean mother. Mexico is a country that is part of Latin America.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999913"], [" the", "0.88265234"], [" word", "0.37087888"], [" \"", "1.0"], ["gay", "0.9999982"], ["\"", "1.0"], [" a", "0.99999714"], [" derogatory", "0.9999777"], [" term", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["gay", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "0.7519357"], [" term", "0.99401265"], [" that", "0.98090357"], [" has", "0.7987324"], [" been", "1.0"], [" used", "1.0"], [" to", "0.99998164"], [" describe", "1.0"], [" a", "0.96835226"], [" variety", "0.9999293"], [" of", "1.0"], [" sexual", "0.9389654"], [" orient", "1.0"], ["ations", "1.0"], [".", "0.999966"], [" The", "0.99850583"], [" word", "0.99298894"], [" \"", "1.0"], ["gay", "1.0"], ["\"", "1.0"], [" is", "0.9997534"], [" not", "1.0"], [" a", "1.0"], [" derogatory", "1.0"], [" term", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"]]}, "785": {"qid": "3cc870b5665bc1bc2a8e", "term": "Bee", "description": "Clade of insects", "question": "Does the human stomach destroy a bee if ingested?", "answer": true, "facts": ["Bees have an outer protective exoskeleton made of chitin, a polymer of glucose.", "The human stomach releases harsh acids that break down materials.", "The human stomach breaks down glucose in about 33 hours."], "decomposition": ["What material protects a bee?", "What is #1 made of?", "Can the human stomach digest #2?"], "evidence": [[[["Exoskeleton-3"], "no_evidence"], [["Chitin-1"]], [["Glucose-1"], "operation"]], [[["Bee-40"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Bee-28", "Bee-65"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human?\nFacts: The average human is about 6'2\" tall and weighs about 180 pounds.\nAnswer (True or False): True\nQuestion: What is the average weight of a horse?\nFacts: The average horse is about 8'2\" tall and weighs", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does the human stomach destroy a bee if ingested?\nFacts: Bees have an outer protective exoskeleton made of chitin, a polymer of glucose. The human stomach releases harsh acids that break down materials. The human stomach breaks down glucose in about 33 hours.\nAnswer (True or False):", "ct_subject": "Bees have an outer protective exoskeleton made of chitin, a polymer of glucose. The human stomach releases harsh acids that break down materials. The human stomach breaks down glucose in about 33 hours.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9950854"], [" is", "1.0"], [" the", "1.0"], [" average", "0.9863253"], [" weight", "0.99999976"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], ["?", "0.99999607"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.99831283"], [" is", "0.98928905"], [" about", "1.0"], [" 6", "0.81259197"], ["'", "0.5429163"], ["2", "0.99995065"], ["\"", "1.0"], [" tall", "1.0"], [" and", "0.7051922"], [" weighs", "1.0"], [" about", "1.0"], [" 180", "0.9998777"], [" pounds", "0.99999654"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "0.9998685"], [" of", "1.0"], [" a", "1.0"], [" horse", "0.9951066"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" horse", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 8", "0.7778196"], ["'", "1.0"], ["2", "0.9959336"], ["\"", "1.0"], [" tall", "1.0"], [" and", "1.0"], [" weighs", "1.0"]]}, "786": {"qid": "84a83f07a543ae3024c9", "term": "Yellow pages", "description": "Telephone directory of businesses by category", "question": "Would it be uncommon for a high schooler to use the yellow pages?", "answer": true, "facts": ["High school students are between 14-19 years old. ", "Teenagers now are considered 'digital natives'. ", "'Digital natives' are individuals who have grown up during the computer age, being exposed to technology from early childhood."], "decomposition": ["Which age range do most high school students fall within?", "What is the age range of children who are considered to have grown during the computer age?", "Is #1 similar to #2?"], "evidence": [[[["High school (North America)-1"]], [["Information Age-1"]], ["operation"]], [[["Adolescence-1"]], [["Digital native-1"]], ["operation"]], [["no_evidence"], [["Yellow pages-2"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human?\nFacts: The average weight of a human is about 200 pounds.  The average weight of a human is about 200 pounds.  The average weight of a human is about 200 pounds.  The average weight of a human is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would it be uncommon for a high schooler to use the yellow pages?\nFacts: High school students are between 14-19 years old.  Teenagers now are considered 'digital natives'.  'Digital natives' are individuals who have grown up during the computer age, being exposed to technology from early childhood.\nAnswer (True or False):", "ct_subject": "High school students are between 14-19 years old.  Teenagers now are considered 'digital natives'.  'Digital natives' are individuals who have grown up during the computer age, being exposed to technology from early childhood.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999988"], [" is", "1.0"], [" the", "1.0"], [" average", "0.9987349"], [" weight", "0.95766115"], [" of", "1.0"], [" a", "0.99944204"], [" human", "0.9866527"], ["?", "0.9953296"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" weight", "0.99256164"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" is", "1.0"], [" about", "0.71796834"], [" 200", "0.6573867"], [" pounds", "1.0"], [".", "1.0"], [" ", "0.9999653"], [" The", "1.0"], [" average", "1.0"], [" weight", "1.0"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 200", "1.0"], [" pounds", "1.0"], [".", "1.0"], [" ", "1.0"], [" The", "0.9999999"], [" average", "1.0"], [" weight", "1.0"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 200", "1.0"], [" pounds", "1.0"], [".", "1.0"], [" ", "1.0"], [" The", "1.0"], [" average", "1.0"], [" weight", "1.0"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" is", "1.0"]]}, "787": {"qid": "d5cd4b71ec956f0e1272", "term": "5", "description": "Natural number", "question": "Is pi in excess of square root of 5?", "answer": true, "facts": ["Pi is a mathematical number approximately equal to 3.14", "The square root of a number are the two numbers multiplied together that equal that number.", "The square root of 5 is around 2.23."], "decomposition": ["What is the square root of 5?", "What is the value of pi?", "Is #2 greater than #1?"], "evidence": [[[["Square root-1"], "no_evidence", "operation"], [["Pi-1"]], ["no_evidence", "operation"]], [[["Square root of 5-1"]], [["Pi-1"]], ["operation"]], [[["Square root of 5-3"]], [["Pi-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the Earth round?\nFacts: The Earth is not round. The Earth is a sphere. The Earth is not a perfect sphere. The Earth is not a perfect sphere. The Earth is not a perfect sphere. The Earth is not a perfect sphere. The Earth is not a perfect sphere. The Earth is not a perfect", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is pi in excess of square root of 5?\nFacts: Pi is a mathematical number approximately equal to 3.14 The square root of a number are the two numbers multiplied together that equal that number. The square root of 5 is around 2.23.\nAnswer (True or False):", "ct_subject": "Pi is a mathematical number approximately equal to 3.14 The square root of a number are the two numbers multiplied together that equal that number. The square root of 5 is around 2.23.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99996746"], [" the", "0.9674146"], [" Earth", "0.51717114"], [" round", "0.9999856"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" not", "0.86265105"], [" round", "1.0"], [".", "1.0"], [" The", "0.9994616"], [" Earth", "1.0"], [" is", "1.0"], [" a", "1.0"], [" sphere", "1.0"], [".", "1.0"], [" The", "0.47716588"], [" Earth", "1.0"], [" is", "1.0"], [" not", "0.9999999"], [" a", "0.91254705"], [" perfect", "0.9999999"], [" sphere", "1.0"], [".", "1.0"], [" The", "0.9999682"], [" Earth", "1.0"], [" is", "1.0"], [" not", "1.0"], [" a", "0.9956995"], [" perfect", "1.0"], [" sphere", "1.0"], [".", "1.0"], [" The", "0.9763807"], [" Earth", "1.0"], [" is", "1.0"], [" not", "1.0"], [" a", "1.0"], [" perfect", "1.0"], [" sphere", "1.0"], [".", "1.0"], [" The", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" not", "1.0"], [" a", "1.0"], [" perfect", "1.0"], [" sphere", "1.0"], [".", "1.0"], [" The", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" not", "1.0"], [" a", "1.0"], [" perfect", "1.0"], [" sphere", "1.0"], [".", "1.0"], [" The", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" not", "1.0"], [" a", "1.0"], [" perfect", "1.0"]]}, "788": {"qid": "ac8bab79a8f573de8ec5", "term": "Doctor Strange", "description": "Superhero appearing in Marvel Comics publications and related media", "question": "Would Doctor Strange like the Pittsburgh Steelers logo?", "answer": true, "facts": ["The Pittsburgh Steelers logo features patches of red, gold and blue", "Doctor Strange's costume uses the colors red, gold and blue"], "decomposition": ["What colors are in Doctor Strange's costume?", "What are the colors in the logo of the Pittsburgh Steelers?", "Are #1 and #2 the same?"], "evidence": [[[["Doctor Strange-39"]], [["Pittsburgh Steelers-32"]], ["operation"]], [[["Doctor Strange-39"]], [["Logos and uniforms of the Pittsburgh Steelers-5"]], ["operation"]], [[["Doctor Strange-39"], "no_evidence"], [["Pittsburgh Steelers-32"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the character in the movie \"The Avengers\"?\nFacts: The character in the movie \"The Avengers\" is named \"Tony Stark\"\nAnswer (True or False): True\nQuestion: What is the name of the character in the movie \"The Avengers\"?\nFacts: The character in the movie \"The Avengers\" is named \"Tony Stark\"\nAnswer (True or False", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Doctor Strange like the Pittsburgh Steelers logo?\nFacts: The Pittsburgh Steelers logo features patches of red, gold and blue Doctor Strange's costume uses the colors red, gold and blue\nAnswer (True or False):", "ct_subject": "The Pittsburgh Steelers logo features patches of red, gold and blue Doctor Strange's costume uses the colors red, gold and blue", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999906"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" character", "0.9985436"], [" in", "0.9999994"], [" the", "1.0"], [" movie", "0.99760747"], [" \"", "1.0"], ["The", "1.0"], [" Avengers", "1.0"], ["\"?", "0.9458044"], ["\n", "1.0"], ["F", "0.99999976"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" character", "0.9999995"], [" in", "0.9999999"], [" the", "1.0"], [" movie", "1.0"], [" \"", "0.9999989"], ["The", "1.0"], [" Avengers", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" named", "0.9999976"], [" \"", "0.9999982"], ["Tony", "0.73138773"], [" Stark", "1.0"], ["\"", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" character", "1.0"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "1.0"], ["The", "0.9999999"], [" Avengers", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" character", "1.0"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "1.0"], ["The", "1.0"], [" Avengers", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" named", "1.0"], [" \"", "1.0"], ["Tony", "1.0"], [" Stark", "1.0"], ["\"", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"]]}, "789": {"qid": "b6cebe14a6b32222ad1b", "term": "Torah", "description": "First five books of the Hebrew Bible", "question": "Can you give at least one word from the Torah to all residents of Bunkie Louisiana?", "answer": true, "facts": ["Torah scrolls must be duplicated precisely by a trained scribe.", "The Torah has a total of 8,674 words.", "The population of Bunkie Louisiana is 3,939 people according to a 2018 census."], "decomposition": ["How many words are in the Torah?", "How many residents does Bunkie, Louisiana have?", "Is #1 greater than #2?"], "evidence": [[[["Torah-45"], "no_evidence"], [["Bunkie, Louisiana-1"]], ["no_evidence", "operation"]], [[["Torah-1"], "no_evidence"], [["Bunkie, Louisiana-1"]], ["no_evidence", "operation"]], [[["Sefer Torah-7"], "no_evidence"], [["Bunkie, Louisiana-1"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city of Bunkie Louisiana?\nFacts: Bunkie Louisiana is located in the state of Louisiana.\nAnswer (True or False): True\nQuestion: What is the name of the city of Bunkie Louisiana?\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can you give at least one word from the Torah to all residents of Bunkie Louisiana?\nFacts: Torah scrolls must be duplicated precisely by a trained scribe. The Torah has a total of 8,674 words. The population of Bunkie Louisiana is 3,939 people according to a 2018 census.\nAnswer (True or False):", "ct_subject": "Torah scrolls must be duplicated precisely by a trained scribe. The Torah has a total of 8,674 words. The population of Bunkie Louisiana is 3,939 people according to a 2018 census.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999918"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999943"], [" of", "1.0"], [" the", "1.0"], [" city", "0.99999976"], [" of", "0.99539775"], [" B", "1.0"], ["unk", "1.0"], ["ie", "1.0"], [" Louisiana", "0.9999758"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" B", "1.0"], ["unk", "1.0"], ["ie", "1.0"], [" Louisiana", "0.99796605"], [" is", "1.0"], [" located", "0.9769896"], [" in", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" Louisiana", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9999789"], [" of", "1.0"], [" B", "1.0"], ["unk", "1.0"], ["ie", "1.0"], [" Louisiana", "1.0"], ["?", "1.0"], ["\n", "1.0"]]}, "790": {"qid": "0e3b14f806bfea19faf2", "term": "Snow White", "description": "fairy tale", "question": "Can all of Snow White's dwarfs play a game of 7 Wonders simultaneously?", "answer": true, "facts": ["The fairy tale character Snow White was friends with seven dwarfs.", "The board game 7 Wonders is for 2 to 7 players."], "decomposition": ["How many players can participate in a game of 7 Wonders?", "How many dwarfs are in the story of Snow White?", "Is #2 less than or equal to #1?"], "evidence": [[[["7 Wonders (board game)-1"], "no_evidence"], [["Snow White and the Seven Dwarfs (1937 film)-1"]], ["no_evidence", "operation"]], [[["7 Wonders (board game)-14"], "no_evidence"], [["Snow White and the Seven Dwarfs (1937 film)-7"]], ["operation"]], [[["7 Wonders (board game)-21"]], [["Snow White-3"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to ever be born in the year 2000?\nFacts: The year 2000 was the first year that the year zero was not zero.\nAnswer (True or False): True\nQuestion: What is the name of the first person to ever be born in the year 2000?\nFacts: The year 2000 was the first year", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can all of Snow White's dwarfs play a game of 7 Wonders simultaneously?\nFacts: The fairy tale character Snow White was friends with seven dwarfs. The board game 7 Wonders is for 2 to 7 players.\nAnswer (True or False):", "ct_subject": "The fairy tale character Snow White was friends with seven dwarfs. The board game 7 Wonders is for 2 to 7 players.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9998031"], [" is", "1.0"], [" the", "1.0"], [" name", "0.6323886"], [" of", "1.0"], [" the", "1.0"], [" first", "0.998072"], [" person", "0.968978"], [" to", "1.0"], [" ever", "0.8676032"], [" be", "0.99999833"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "0.9999999"], [" 2000", "0.9999881"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "0.99986756"], [" the", "1.0"], [" first", "0.999905"], [" year", "1.0"], [" that", "0.99998033"], [" the", "0.6717566"], [" year", "1.0"], [" zero", "1.0"], [" was", "1.0"], [" not", "0.4881505"], [" zero", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" ever", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "0.99999523"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "1.0"], [" the", "1.0"], [" first", "1.0"], [" year", "1.0"]]}, "791": {"qid": "eb6aa79b52637aabc477", "term": "Subway (restaurant)", "description": "American fast food chain", "question": "Did Subway have a sex offender as a spokesperson?", "answer": true, "facts": ["Jared Fogle was a national spokesman for the company in the US starting in January 2000.", "Jared Scott Fogle is a convicted sex offender. "], "decomposition": ["Who was the spokesman for Subway in January 2000?", "Has #1 ever been convicted of a sex crime?"], "evidence": [[[["Jared Fogle-2"]], [["Jared Fogle-19"]]], [[["Jared Fogle-1", "Jared Fogle-8"]], [["Jared Fogle-3"], "operation"]], [[["Jared Fogle-1"]], [["Jared Fogle-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Titanic sink because of a faulty boiler?\nFacts: The Titanic sank because of a faulty boiler. \nAnswer (True or False): True\nQuestion: Did the Titanic sink because of a faulty boiler?\nFacts: The Titanic sank because of a faulty boiler. \nAnswer (True or False): True\nQuestion: Did the Titanic sink because of a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Subway have a sex offender as a spokesperson?\nFacts: Jared Fogle was a national spokesman for the company in the US starting in January 2000. Jared Scott Fogle is a convicted sex offender. \nAnswer (True or False):", "ct_subject": "Jared Fogle was a national spokesman for the company in the US starting in January 2000. Jared Scott Fogle is a convicted sex offender. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.17577447"], [" the", "1.0"], [" Titanic", "0.6328246"], [" sink", "0.9161882"], [" because", "1.0"], [" of", "1.0"], [" a", "0.9999101"], [" faulty", "0.9607972"], [" boiler", "0.9623877"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Titanic", "1.0"], [" sank", "0.9998566"], [" because", "1.0"], [" of", "1.0"], [" a", "1.0"], [" faulty", "1.0"], [" boiler", "1.0"], [".", "1.0"], [" ", "0.2743457"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" Titanic", "1.0"], [" sink", "1.0"], [" because", "1.0"], [" of", "1.0"], [" a", "1.0"], [" faulty", "1.0"], [" boiler", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Titanic", "1.0"], [" sank", "1.0"], [" because", "1.0"], [" of", "1.0"], [" a", "1.0"], [" faulty", "1.0"], [" boiler", "1.0"], [".", "1.0"], [" ", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" Titanic", "1.0"], [" sink", "1.0"], [" because", "1.0"], [" of", "1.0"], [" a", "1.0"]]}, "792": {"qid": "1c0ccde98d448ffeedac", "term": "Yeti", "description": "Folkloric ape-like creature from Asia", "question": "Is there a Yeti associated with Disney theme parks?", "answer": true, "facts": ["In the 1960s, an attraction called Matterhorn featuring a cartoon version of the Yeti opened in Disneyland.", "Later in 2005, Expedition Everest opened at Animal Kingdom in Disney World, featuring a much scarier version of the Yeti."], "decomposition": ["What 1960s attraction featured a cartoon version of the Yeti?", "Is #1 part of a Disney park?"], "evidence": [[[["Matterhorn Bobsleds-1", "Matterhorn Bobsleds-8"]], ["operation"]], [[["Expedition Everest-12"]], [["Disney's Animal Kingdom-1"]]], [[["Matterhorn Bobsleds-23"]], [["Disney's Animal Kingdom-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is there a Yeti in the Himalayas?\nFacts: There is a Yeti in the Himalayas.\nAnswer (True or False): True\nQuestion: Is there a Yeti in the Amazon?\nFacts: There is a Yeti in the Amazon", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is there a Yeti associated with Disney theme parks?\nFacts: In the 1960s, an attraction called Matterhorn featuring a cartoon version of the Yeti opened in Disneyland. Later in 2005, Expedition Everest opened at Animal Kingdom in Disney World, featuring a much scarier version of the Yeti.\nAnswer (True or False):", "ct_subject": "In the 1960s, an attraction called Matterhorn featuring a cartoon version of the Yeti opened in Disneyland. Later in 2005, Expedition Everest opened at Animal Kingdom in Disney World, featuring a much scarier version of the Yeti.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99999964"], [" there", "1.0"], [" a", "1.0"], [" Yet", "1.0"], ["i", "1.0"], [" in", "1.0"], [" the", "1.0"], [" Himal", "1.0"], ["ay", "1.0"], ["as", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "1.0"], [" is", "0.9998963"], [" a", "1.0"], [" Yet", "1.0"], ["i", "1.0"], [" in", "1.0"], [" the", "1.0"], [" Himal", "1.0"], ["ay", "1.0"], ["as", "1.0"], [".", "0.7613417"], ["\n", "0.9549181"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" there", "1.0"], [" a", "1.0"], [" Yet", "1.0"], ["i", "1.0"], [" in", "1.0"], [" the", "1.0"], [" Amazon", "0.9921341"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "1.0"], [" is", "1.0"], [" a", "1.0"], [" Yet", "1.0"], ["i", "1.0"], [" in", "1.0"], [" the", "1.0"], [" Amazon", "1.0"]]}, "793": {"qid": "918f19d38c16251343fc", "term": "Subway (restaurant)", "description": "American fast food chain", "question": "Was Subway involved in a pedophilia scandal?", "answer": true, "facts": ["In 2000, Jared Fogle became a national spokesman for Subway after he lost a lot of weight eating only Subway sandwiches.", "In 2015, Fogle was found guilty of child molestation and possession of child pornography, and Subway terminated its relationship with him."], "decomposition": ["Who was the famous spokesman for Subway?", "What crimes did #1 commit?", "Are #2 directly related to pedophilia?"], "evidence": [[[["Subway (restaurant)-30"]], [["Jared Fogle-3"]], [["Pedophilia-12"], "operation"]], [[["Jared Fogle-1"]], [["Jared Fogle-3"]], [["Child sex tourism-1", "Pedophilia-1"]]], [[["Jared Fogle-6"]], [["Jared Fogle-25"]], [["Jared Fogle-25"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Titanic sink because of a faulty boiler?\nFacts: The Titanic was built in 1912 and was the largest ship ever built. The boiler was designed to last for at least 100 years. The Titanic was built to last for at least 100 years.\nAnswer (True or", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Subway involved in a pedophilia scandal?\nFacts: In 2000, Jared Fogle became a national spokesman for Subway after he lost a lot of weight eating only Subway sandwiches. In 2015, Fogle was found guilty of child molestation and possession of child pornography, and Subway terminated its relationship with him.\nAnswer (True or False):", "ct_subject": "In 2000, Jared Fogle became a national spokesman for Subway after he lost a lot of weight eating only Subway sandwiches. In 2015, Fogle was found guilty of child molestation and possession of child pornography, and Subway terminated its relationship with him.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.6750079"], [" the", "1.0"], [" Titanic", "0.99208856"], [" sink", "1.0"], [" because", "1.0"], [" of", "1.0"], [" a", "0.999845"], [" faulty", "0.9048047"], [" boiler", "0.18941793"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Titanic", "1.0"], [" was", "0.49831775"], [" built", "0.9999963"], [" in", "0.9998215"], [" 1912", "0.8456898"], [" and", "0.9928423"], [" was", "0.9999999"], [" the", "0.8622901"], [" largest", "0.99997723"], [" ship", "0.99481493"], [" ever", "0.9998441"], [" built", "1.0"], [".", "0.9063764"], [" The", "0.010289699"], [" boiler", "0.9944312"], [" was", "0.99999976"], [" designed", "0.99999905"], [" to", "1.0"], [" last", "1.0"], [" for", "1.0"], [" at", "0.97902095"], [" least", "1.0"], [" 100", "0.99999917"], [" years", "1.0"], [".", "1.0"], [" The", "0.9995635"], [" Titanic", "1.0"], [" was", "0.99632096"], [" built", "0.9979371"], [" to", "0.6504336"], [" last", "0.9995602"], [" for", "1.0"], [" at", "1.0"], [" least", "1.0"], [" 100", "1.0"], [" years", "1.0"], [".", "1.0"], ["\n", "0.56108326"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"]]}, "794": {"qid": "703738f478e1ed466b59", "term": "Banana", "description": "edible fruit", "question": "Can a banana get a virus?", "answer": true, "facts": ["A virus is a disease that is caused by infectious agents.", "A banana comes from a banana plant.", "Blight is a disease that is caused by infections on plants.", "The Banana bunchy top virus (BBTV) is a plant virus of the genus Babuvirus,that causes diseased streaks."], "decomposition": ["What are the various diseases that affect banana plant?", "Are any of #1 caused by virus?"], "evidence": [[[["Banana bunchy top virus-1"]], [["Nanoviridae-1"], "operation"]], [[["Banana-54", "Banana-55"]], [["Banana bunchy top virus-6"]]], [[["Banana-50"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a banana and a banana peel?\nFacts: A banana is a fruit. A banana peel is a piece of the fruit.\nAnswer (True or False): True\nQuestion: What is the difference between a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can a banana get a virus?\nFacts: A virus is a disease that is caused by infectious agents. A banana comes from a banana plant. Blight is a disease that is caused by infections on plants. The Banana bunchy top virus (BBTV) is a plant virus of the genus Babuvirus,that causes diseased streaks.\nAnswer (True or False):", "ct_subject": "A virus is a disease that is caused by infectious agents. A banana comes from a banana plant. Blight is a disease that is caused by infections on plants. The Banana bunchy top virus (BBTV) is a plant virus of the genus Babuvirus,that causes diseased streaks.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999244"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.999998"], [" between", "1.0"], [" a", "1.0"], [" banana", "1.0"], [" and", "1.0"], [" a", "1.0"], [" banana", "1.0"], [" peel", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.99999666"], [" banana", "1.0"], [" is", "1.0"], [" a", "1.0"], [" fruit", "1.0"], [".", "0.99907327"], [" A", "1.0"], [" banana", "1.0"], [" peel", "1.0"], [" is", "1.0"], [" a", "0.99998236"], [" piece", "0.9995338"], [" of", "1.0"], [" the", "1.0"], [" fruit", "0.915035"], [".", "0.9997547"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"]]}, "795": {"qid": "ccf67586446bcc4a86b3", "term": "E.T. the Extra-Terrestrial", "description": "1982 American science fiction film directed by Steven Spielberg", "question": "Would E.T. the Extra-Terrestrial alien hypothetically love Friendly's?", "answer": true, "facts": ["E.T., the main alien from E.T. the Extra-Terrestrial, loved Reese's Pieces candy.", "Friendly's is a restaurant that serves dinner entrees and ice cream dishes.", "Friendly's has several desserts with Reese's candy including the Reese's Peanut Butter Cup Sundae, and Reese's Pieces Sundae."], "decomposition": ["What is E.T. the Extra-Terrestrial's favorite food?", "Does Friendly's serve dishes made with #1?"], "evidence": [[[["E.T. the Extra-Terrestrial-6"]], [["Friendly's-4", "Reese's Pieces-1"]]], [[["E.T. the Extra-Terrestrial-6"]], [["Friendly's-1"], "no_evidence", "operation"]], [[["E.T. the Extra-Terrestrial-6"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the song that plays when you open the door to the Friendly's restaurant?\nFacts: \"Friendly's\" is the name of", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would E.T. the Extra-Terrestrial alien hypothetically love Friendly's?\nFacts: E.T., the main alien from E.T. the Extra-Terrestrial, loved Reese's Pieces candy. Friendly's is a restaurant that serves dinner entrees and ice cream dishes. Friendly's has several desserts with Reese's candy including the Reese's Peanut Butter Cup Sundae, and Reese's Pieces Sundae.\nAnswer (True or False):", "ct_subject": "E.T., the main alien from E.T. the Extra-Terrestrial, loved Reese's Pieces candy. Friendly's is a restaurant that serves dinner entrees and ice cream dishes. Friendly's has several desserts with Reese's candy including the Reese's Peanut Butter Cup Sundae, and Reese's Pieces Sundae.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9906685"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" song", "0.97278386"], [" that", "0.99999785"], [" plays", "1.0"], [" when", "0.73874515"], [" you", "0.99999547"], [" open", "0.050023247"], [" the", "1.0"], [" door", "0.99999905"], [" to", "1.0"], [" the", "0.8837449"], [" Friendly", "1.0"], ["'s", "1.0"], [" restaurant", "0.9999976"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" \"", "0.9435308"], ["Friend", "0.9999927"], ["ly", "1.0"], ["'s", "1.0"], ["\"", "0.9999722"], [" is", "1.0"], [" the", "0.9986526"], [" name", "0.9999993"], [" of", "1.0"]]}, "796": {"qid": "5e2d9519c7c521411817", "term": "Pizza", "description": "Usually savory dish of flattened bread and toppings", "question": "Would a TMNT coloring book have pizza in it?", "answer": true, "facts": ["TMNT is an abbreviation for 'Teenage Mutant Ninja Turtles'.", "The Teenage Mutant Ninja Turtles canonically only ever ate pizza in the animated series. "], "decomposition": ["What cartoon does TMNT stand for?", "In the animated series, did #1 canonically eat only pizza?"], "evidence": [[[["Teenage Mutant Ninja Turtles-1"]], [["Teenage Mutant Ninja Turtles-9"], "operation"]], [[["Teenage Mutant Ninja Turtles-1"]], [["Teenage Mutant Ninja Turtles-19"], "operation"]], [[["Teenage Mutant Ninja Turtles-1"]], [["Teenage Mutant Ninja Turtles-19"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the character in the movie \"The Matrix\"?\nFacts: The character in the movie \"The Matrix\" is Neo. Neo is the name of the main character in the movie.\nAnswer (True or False): True\nQuestion: What is the name of the character in the movie \"The Matrix\"?\nFacts: The character", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a TMNT coloring book have pizza in it?\nFacts: TMNT is an abbreviation for 'Teenage Mutant Ninja Turtles'. The Teenage Mutant Ninja Turtles canonically only ever ate pizza in the animated series. \nAnswer (True or False):", "ct_subject": "TMNT is an abbreviation for 'Teenage Mutant Ninja Turtles'. The Teenage Mutant Ninja Turtles canonically only ever ate pizza in the animated series. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999951"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9992761"], [" of", "1.0"], [" the", "1.0"], [" character", "0.9979431"], [" in", "0.9879209"], [" the", "1.0"], [" movie", "1.0"], [" \"", "0.9999999"], ["The", "1.0"], [" Matrix", "0.9561142"], ["\"?", "0.9914278"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.52410924"], [" character", "0.9999982"], [" in", "0.99998105"], [" the", "1.0"], [" movie", "1.0"], [" \"", "1.0"], ["The", "1.0"], [" Matrix", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" Neo", "1.0"], [".", "1.0"], [" Neo", "1.0"], [" is", "1.0"], [" the", "0.99372804"], [" name", "0.99997807"], [" of", "1.0"], [" the", "1.0"], [" main", "0.95301884"], [" character", "1.0"], [" in", "1.0"], [" the", "1.0"], [" movie", "0.99965036"], [".", "0.8517236"], ["\n", "0.9999714"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" character", "1.0"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "1.0"], ["The", "1.0"], [" Matrix", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" character", "1.0"]]}, "797": {"qid": "a6f593e93ca87c53a0a1", "term": "Crucifixion", "description": "Method of capital punishment in which the victim is tied or nailed to a large wooden beam and left to hang until eventual death", "question": "Does crucifixion violate US eighth amendment?", "answer": true, "facts": ["The eighth amendment prohibits cruel and unusual punishment.", "Crucifixion was particularly barbaric as people do not die instantly and live for several days."], "decomposition": ["What does the Eighth Amendment say about punishment measures?", "What are the features of crucifixion as a method of punishment?", "Is #1 contradicted by #2?"], "evidence": [[[["Eighth Amendment to the United States Constitution-1"]], [["Crucifixion-1"]], ["operation"]], [[["Eighth Amendment to the United States Constitution-1"]], [["Crucifixion-1"]], [["Crucifixion-1", "Eighth Amendment to the United States Constitution-1"], "operation"]], [[["United States constitutional sentencing law-4"]], [["Cruel and unusual punishment-6"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the Bible the word of God?\nFacts: The Bible is the word of God. It is the word of God that is in the Bible. It is the word of God that is in the Bible. It is the word of God that is in the Bible. It is the word of God that is in the Bible. It is the word of God that is in the Bible.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does crucifixion violate US eighth amendment?\nFacts: The eighth amendment prohibits cruel and unusual punishment. Crucifixion was particularly barbaric as people do not die instantly and live for several days.\nAnswer (True or False):", "ct_subject": "The eighth amendment prohibits cruel and unusual punishment. Crucifixion was particularly barbaric as people do not die instantly and live for several days.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9672481"], [" the", "0.999933"], [" Bible", "0.89427626"], [" the", "0.9999546"], [" word", "0.8549281"], [" of", "1.0"], [" God", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Bible", "1.0"], [" is", "1.0"], [" the", "0.99999964"], [" word", "1.0"], [" of", "1.0"], [" God", "1.0"], [".", "1.0"], [" It", "0.9994423"], [" is", "1.0"], [" the", "1.0"], [" word", "0.998638"], [" of", "1.0"], [" God", "1.0"], [" that", "0.9998958"], [" is", "0.99909425"], [" in", "0.9403097"], [" the", "0.99999225"], [" Bible", "1.0"], [".", "1.0"], [" It", "0.9989316"], [" is", "1.0"], [" the", "0.99873596"], [" word", "1.0"], [" of", "1.0"], [" God", "1.0"], [" that", "1.0"], [" is", "1.0"], [" in", "1.0"], [" the", "1.0"], [" Bible", "1.0"], [".", "0.83772993"], [" It", "0.97039807"], [" is", "1.0"], [" the", "1.0"], [" word", "1.0"], [" of", "1.0"], [" God", "1.0"], [" that", "1.0"], [" is", "1.0"], [" in", "1.0"], [" the", "1.0"], [" Bible", "1.0"], [".", "1.0"], [" It", "0.99999416"], [" is", "1.0"], [" the", "1.0"], [" word", "1.0"], [" of", "1.0"], [" God", "1.0"], [" that", "1.0"], [" is", "1.0"], [" in", "1.0"], [" the", "1.0"], [" Bible", "1.0"], [".", "1.0"], [" It", "1.0"], [" is", "1.0"], [" the", "1.0"], [" word", "1.0"], [" of", "1.0"], [" God", "1.0"], [" that", "1.0"], [" is", "1.0"], [" in", "1.0"], [" the", "1.0"], [" Bible", "1.0"], [".", "1.0"]]}, "798": {"qid": "97cf5c63fb530d9fc492", "term": "Paul the Apostle", "description": "Early Christian apostle and missionary", "question": "Did Paul the Apostle's cause of death violate the tenets of Ahimsa?", "answer": true, "facts": ["Ahimsa is is an ancient Indian principle of nonviolence which applies to all living beings. ", "Ahimsa is a key virtue in Hinduism, Buddhism and Jainism.", "Paul the Apostle was violently beheaded."], "decomposition": ["What is Ahimsa?", "Does #1 believe in non-violence?", "Did Paul the Apostle die due to violence?", "Are #2 and #3 the same answer?"], "evidence": [[[["Ahi\u1e43s\u0101-1"]], [["Ahi\u1e43s\u0101-1"]], [["Decapitation-1", "Paul the Apostle-53", "Violence-1"]], ["operation"]], [[["Ahi\u1e43s\u0101-1"]], ["operation"], [["Paul the Apostle-53"]], ["operation"]], [[["Ahi\u1e43s\u0101-1"]], ["operation"], [["Paul the Apostle-53", "Paul the Apostle-54"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Apostle Paul's death violate the tenets of Ahimsa?\nFacts: Paul the Apostle was beheaded.\nAnswer (True or False): True\nQuestion: Did the Apostle Paul's death violate the tenets of Ahimsa?\nFacts: Paul", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Paul the Apostle's cause of death violate the tenets of Ahimsa?\nFacts: Ahimsa is is an ancient Indian principle of nonviolence which applies to all living beings.  Ahimsa is a key virtue in Hinduism, Buddhism and Jainism. Paul the Apostle was violently beheaded.\nAnswer (True or False):", "ct_subject": "Ahimsa is is an ancient Indian principle of nonviolence which applies to all living beings.  Ahimsa is a key virtue in Hinduism, Buddhism and Jainism. Paul the Apostle was violently beheaded.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999862"], [" the", "0.89912134"], [" Apostle", "0.9894784"], [" Paul", "1.0"], ["'s", "0.9919452"], [" death", "0.92364186"], [" violate", "1.0"], [" the", "1.0"], [" tenets", "1.0"], [" of", "1.0"], [" Ah", "0.9997906"], ["im", "1.0"], ["sa", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Paul", "0.7035491"], [" the", "0.9999938"], [" Apostle", "1.0"], [" was", "1.0"], [" be", "0.9998443"], ["headed", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "0.9998374"], [" Apostle", "1.0"], [" Paul", "1.0"], ["'s", "1.0"], [" death", "1.0"], [" violate", "1.0"], [" the", "1.0"], [" tenets", "1.0"], [" of", "1.0"], [" Ah", "1.0"], ["im", "1.0"], ["sa", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Paul", "1.0"]]}, "799": {"qid": "d4b9a903cd99f477a219", "term": "Athena", "description": "ancient Greek goddess of wisdom and war", "question": "Is Freya a combination of Athena and Aphrodite?", "answer": true, "facts": ["Athena was the Greek goddess of war.", "Aphrodite was the Greek goddess of love.", "Freya was the Norse goddess of war, love, and fertility."], "decomposition": ["What was Athena the Greek goddess of?", "What was Aphrodite's specialty?", "What was Freya the Norse goddess of?", "Does #3 have part of the same answer as #1 and #2?"], "evidence": [[[["Athena-1"]], [["Aphrodite-1"]], [["Freyja-1"]], ["no_evidence"]], [[["Athena-1"]], [["Aphrodite-1"]], [["Freyja-1"]], ["operation"]], [[["Athena-1"]], [["Aphrodite-1"]], [["Freyja-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the Roman god of the sun, Mars, a combination of the names of the Roman god of the sun, Jupiter, and the Roman god of the moon, Saturn?\nFacts: Mars was the Roman god of war. Jupiter was the Roman god of the sun. Saturn was the Roman god of the moon.\nAnswer (True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Freya a combination of Athena and Aphrodite?\nFacts: Athena was the Greek goddess of war. Aphrodite was the Greek goddess of love. Freya was the Norse goddess of war, love, and fertility.\nAnswer (True or False):", "ct_subject": "Athena was the Greek goddess of war. Aphrodite was the Greek goddess of love. Freya was the Norse goddess of war, love, and fertility.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9958099"], [" the", "1.0"], [" name", "0.9999825"], [" of", "0.9999399"], [" the", "1.0"], [" Roman", "0.9987128"], [" god", "0.9999999"], [" of", "1.0"], [" the", "0.9132097"], [" sun", "0.99950266"], [",", "1.0"], [" Mars", "1.0"], [",", "1.0"], [" a", "0.9999968"], [" combination", "1.0"], [" of", "1.0"], [" the", "1.0"], [" names", "0.99999785"], [" of", "1.0"], [" the", "1.0"], [" Roman", "0.9999951"], [" god", "0.99435765"], [" of", "1.0"], [" the", "0.9999993"], [" sun", "1.0"], [",", "1.0"], [" Jupiter", "1.0"], [",", "1.0"], [" and", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" god", "1.0"], [" of", "1.0"], [" the", "0.99998105"], [" moon", "0.9986368"], [",", "1.0"], [" Saturn", "0.99999607"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Mars", "0.96787137"], [" was", "0.99756503"], [" the", "1.0"], [" Roman", "1.0"], [" god", "1.0"], [" of", "1.0"], [" war", "1.0"], [".", "1.0"], [" Jupiter", "1.0"], [" was", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" god", "1.0"], [" of", "1.0"], [" the", "1.0"], [" sun", "1.0"], [".", "1.0"], [" Saturn", "1.0"], [" was", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" god", "1.0"], [" of", "1.0"], [" the", "1.0"], [" moon", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"]]}, "800": {"qid": "14a891550bef9ab64ef8", "term": "Foot (unit)", "description": "customary unit of length", "question": "When en route from China to France, must pilots know their altitude in the imperial foot?", "answer": true, "facts": ["Most international airports and aviators use the foot to measure altitude ", "China and North Korea require pilots to use meters for altitude", "Pilots must communicate their altitude with local air traffic control "], "decomposition": ["Which unit of altitude does France require pilots to use?", "Which unit of altitude does China require pilots to use?", "Is #1 or #2 the imperial foot?"], "evidence": [[[["Foot (unit)-2"]], [["Foot (unit)-2"]], ["operation"]], [[["Foot (unit)-26"], "no_evidence"], [["Foot (unit)-3"]], ["operation"]], [[["Foot (unit)-3"], "no_evidence"], [["Foot (unit)-3"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"C\" and a \"D\" in the word \"Coffee\"?\nFacts: The \"C\" stands for \"coffee\" and the \"D\" stands for \"diet.\"\nAnswer (True or False): True\nQuestion: What is the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: When en route from China to France, must pilots know their altitude in the imperial foot?\nFacts: Most international airports and aviators use the foot to measure altitude  China and North Korea require pilots to use meters for altitude Pilots must communicate their altitude with local air traffic control \nAnswer (True or False):", "ct_subject": "Most international airports and aviators use the foot to measure altitude  China and North Korea require pilots to use meters for altitude Pilots must communicate their altitude with local air traffic control ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999951"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.89076895"], [" between", "1.0"], [" a", "0.99999523"], [" \"", "1.0"], ["C", "0.35707498"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["D", "0.9992812"], ["\"", "0.8896338"], [" in", "0.9993339"], [" the", "1.0"], [" word", "0.8894688"], [" \"", "1.0"], ["C", "0.9757652"], ["off", "0.9015674"], ["ee", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.90200686"], [" \"", "0.99999595"], ["C", "1.0"], ["\"", "1.0"], [" stands", "0.9712289"], [" for", "1.0"], [" \"", "1.0"], ["co", "0.99998784"], ["ffee", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" the", "1.0"], [" \"", "1.0"], ["D", "1.0"], ["\"", "1.0"], [" stands", "1.0"], [" for", "1.0"], [" \"", "1.0"], ["d", "1.0"], ["iet", "0.31756577"], [".\"", "0.7402712"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"]]}, "801": {"qid": "5b0a4c75f55575ae7e83", "term": "Lieutenant", "description": "junior commissioned officer in many nations' armed forces", "question": "Would Gomer Pyle salute a lieutenant?", "answer": true, "facts": ["Gomer Pyle was a character on a television sitcom", "Pyle was in the US Marine Corp", "Lieutenants are junior commissioned officers in the USMC", "Marine custom dictates that officers are to be saluted by other Marines"], "decomposition": ["Which arm of the US Armed Forces did Gomer Pyle join?", "What was his rank in the #1?", "According to #1 tradition. would a #2 salute a lieutenant?"], "evidence": [[[["Gomer Pyle-4"]], [["Private first class-8"]], [["Lieutenant-1"], "operation"]], [[["Gomer Pyle, U.S.M.C.-2"]], [["Gomer Pyle, U.S.M.C.-12"]], [["Private (rank)-1"]]], [[["Gomer Pyle-7"]], [["Gomer Pyle-8"]], [["Lieutenant-12", "Salute-57"], "no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first man to walk on the moon?\nFacts: Neil Armstrong was the first man to walk on the moon.\nAnswer (True or False): True\nQuestion: What is the name of the first man to walk on the moon?\nFacts: Neil Armstrong was the first", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Gomer Pyle salute a lieutenant?\nFacts: Gomer Pyle was a character on a television sitcom Pyle was in the US Marine Corp Lieutenants are junior commissioned officers in the USMC Marine custom dictates that officers are to be saluted by other Marines\nAnswer (True or False):", "ct_subject": "Gomer Pyle was a character on a television sitcom Pyle was in the US Marine Corp Lieutenants are junior commissioned officers in the USMC Marine custom dictates that officers are to be saluted by other Marines", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999654"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9939522"], [" of", "1.0"], [" the", "1.0"], [" first", "0.091910325"], [" man", "0.988908"], [" to", "1.0"], [" walk", "0.96787196"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Neil", "1.0"], [" Armstrong", "1.0"], [" was", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [".", "0.9999999"], ["\n", "0.975172"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "0.9999999"], [" to", "1.0"], [" walk", "0.9991062"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Neil", "1.0"], [" Armstrong", "1.0"], [" was", "1.0"], [" the", "1.0"], [" first", "1.0"]]}, "802": {"qid": "f4a731f739533c52becd", "term": "Prime Minister of the United Kingdom", "description": "Head of UK Government", "question": "Does the Prime Minister of the United Kingdom have poor job security?", "answer": true, "facts": ["The Prime Minister of the United Kingdom is an elected official.", "Elected officials can be recalled with a vote of no confidence in UK parliament. "], "decomposition": ["How does the prime minister of UK get his position?", "Can people who are #1 be easily removed?"], "evidence": [[[["Prime Minister of the United Kingdom-60"]], [["Recall election-1"]]], [["no_evidence"], [["House of Commons of the United Kingdom-8"]]], [[["Prime minister-25"], "no_evidence"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the country that is the largest in the world?\nFacts: The United States of America is the largest country in the world.\nAnswer (True or False): True\nQuestion: What is the name of the country that is the largest in the world?\nFacts: The United States of America is the largest country in the world", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does the Prime Minister of the United Kingdom have poor job security?\nFacts: The Prime Minister of the United Kingdom is an elected official. Elected officials can be recalled with a vote of no confidence in UK parliament. \nAnswer (True or False):", "ct_subject": "The Prime Minister of the United Kingdom is an elected official. Elected officials can be recalled with a vote of no confidence in UK parliament. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9563403"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99999905"], [" of", "1.0"], [" the", "1.0"], [" country", "0.8297464"], [" that", "0.7016588"], [" is", "0.9983652"], [" the", "1.0"], [" largest", "0.99998283"], [" in", "0.9057845"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "0.9894643"], [" States", "1.0"], [" of", "0.9999988"], [" America", "1.0"], [" is", "0.99998367"], [" the", "1.0"], [" largest", "1.0"], [" country", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [".", "1.0"], ["\n", "0.99846727"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" country", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "1.0"], [" States", "1.0"], [" of", "1.0"], [" America", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" country", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"]]}, "803": {"qid": "a076d2cbd7e4eafad35b", "term": "The Jungle Book", "description": "1894 children's book by Rudyard Kipling", "question": "Does The Jungle Book contain racist subtext?", "answer": true, "facts": ["Baloo, the father character in The Jungle Book, refers to the money characters as \"flat-nosed flaky creeps\" mocking a common black feature feature.", "The antagonist snake character was made to sound like an Indian mag, and was said to hate men.", "King Louie is viewed socially as a racist archetype of a black man."], "decomposition": ["What term did Baloo use to refer to the monkey characters in \"The Jungle Book\"?", "What sentiment did the antagonistic snake express towards men?", "Do #1 and #2 have racist connotation?"], "evidence": [[[["Bandar-log-1"]], [["Kaa-3"]], [["Bandar-log-1"], "no_evidence"]], [[["The Jungle Book-1"], "no_evidence"], [["The Jungle Book (2016 film)-6"], "no_evidence"], [["The Jungle Book (1967 film)-26"], "operation"]], [["no_evidence"], ["no_evidence"], [["The Jungle Book-18"], "no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the movie The Jungle Book racist?\nFacts: The Jungle Book is a classic Disney movie. It is a story of a boy and his father who are forced to leave their home and go", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does The Jungle Book contain racist subtext?\nFacts: Baloo, the father character in The Jungle Book, refers to the money characters as \"flat-nosed flaky creeps\" mocking a common black feature feature. The antagonist snake character was made to sound like an Indian mag, and was said to hate men. King Louie is viewed socially as a racist archetype of a black man.\nAnswer (True or False):", "ct_subject": "Baloo, the father character in The Jungle Book, refers to the money characters as \"flat-nosed flaky creeps\" mocking a common black feature feature. The antagonist snake character was made to sound like an Indian mag, and was said to hate men. King Louie is viewed socially as a racist archetype of a black man.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.97740346"], [" the", "0.99997413"], [" movie", "0.9998331"], [" The", "0.9818636"], [" Jungle", "0.9999999"], [" Book", "1.0"], [" racist", "0.99999857"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Jungle", "1.0"], [" Book", "1.0"], [" is", "1.0"], [" a", "0.9999999"], [" classic", "0.95007247"], [" Disney", "1.0"], [" movie", "0.60202503"], [".", "0.39171946"], [" It", "0.99926907"], [" is", "0.9999646"], [" a", "0.99459934"], [" story", "0.9999995"], [" of", "0.8101834"], [" a", "1.0"], [" boy", "0.99999857"], [" and", "0.63151264"], [" his", "1.0"], [" father", "0.48540148"], [" who", "1.0"], [" are", "0.99999964"], [" forced", "0.998464"], [" to", "1.0"], [" leave", "1.0"], [" their", "1.0"], [" home", "1.0"], [" and", "0.999997"], [" go", "0.9446421"]]}, "804": {"qid": "dc2da0d915bc9d327b9a", "term": "Ginger", "description": "Species of plant", "question": "If you're pregnant, might you be recommended ginger?", "answer": true, "facts": ["Pregnancy often causes nausea and stomach upset.", "Small doses of ginger have been shown to help with vomiting and nausea.", "Doctors say that, in small doses, ginger is safe for pregnant women."], "decomposition": ["What symptoms can ginger help alleviate?", "Do pregnant women suffer from any symptoms in #1?"], "evidence": [[[["Ginger-36"]], [["Morning sickness-1"]]], [[["Ginger-36"]], [["Pregnancy-1"], "operation"]], [[["Jamaica ginger-2"]], [["Signs and symptoms of pregnancy-15"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"carpet\" and a \"carpeted\" house?\nFacts: A carpet is a carpeted house. A carpeted house is a carpeted house.\nAnswer (True or False): True\nQuestion: What is the difference between a \"carpet\" and a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: If you're pregnant, might you be recommended ginger?\nFacts: Pregnancy often causes nausea and stomach upset. Small doses of ginger have been shown to help with vomiting and nausea. Doctors say that, in small doses, ginger is safe for pregnant women.\nAnswer (True or False):", "ct_subject": "Pregnancy often causes nausea and stomach upset. Small doses of ginger have been shown to help with vomiting and nausea. Doctors say that, in small doses, ginger is safe for pregnant women.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.95724916"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9989329"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.9999999"], ["c", "0.40478006"], ["arp", "0.64885515"], ["et", "1.0"], ["\"", "0.9999925"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "0.99999964"], ["arp", "1.0"], ["et", "1.0"], ["ed", "1.0"], ["\"", "0.9999999"], [" house", "0.9999993"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" carpet", "1.0"], [" is", "0.99999917"], [" a", "1.0"], [" carpet", "0.9995677"], ["ed", "1.0"], [" house", "1.0"], [".", "1.0"], [" A", "0.9941089"], [" carpet", "1.0"], ["ed", "1.0"], [" house", "1.0"], [" is", "1.0"], [" a", "1.0"], [" carpet", "0.9608129"], ["ed", "1.0"], [" house", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["arp", "1.0"], ["et", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"]]}, "805": {"qid": "02435ed89673ef2eb808", "term": "Hanging", "description": "execution or suicide method involving suspension of a person by a ligature", "question": "Is hanging a viable execution method on a ship at sea?", "answer": true, "facts": ["Hanging is typically set up using rope.", "Ships have plenty of rope on board because their operation relies heavily on rope."], "decomposition": ["What materials are necessary for hanging?", "Would #1 be available on a ship?"], "evidence": [[[["Gallows-1"]], ["operation"]], [[["Hanging-4"]], [["Rigging-1"]]], [[["Hanging-15"], "operation"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"sailboat\" and a \"ship\"?\nFacts: Sailboats are smaller than ships. Sailboats are used for smaller boats. Sailboats are used for smaller boats. Sailboats are used for smaller boats. Sailboats are used for smaller boats. Sailboats are used for smaller boats. Sailboats are used for smaller boats. Sailboats are used for", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is hanging a viable execution method on a ship at sea?\nFacts: Hanging is typically set up using rope. Ships have plenty of rope on board because their operation relies heavily on rope.\nAnswer (True or False):", "ct_subject": "Hanging is typically set up using rope. Ships have plenty of rope on board because their operation relies heavily on rope.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.47388214"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.11333503"], [" between", "1.0"], [" a", "0.9999999"], [" \"", "0.9999999"], ["s", "0.9608773"], ["ail", "0.9998267"], ["boat", "0.9943877"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["ship", "0.99919313"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Sail", "0.9999999"], ["boats", "1.0"], [" are", "1.0"], [" smaller", "0.9980882"], [" than", "0.99994373"], [" ships", "1.0"], [".", "0.9999988"], [" Sail", "1.0"], ["boats", "1.0"], [" are", "0.9999999"], [" used", "0.99999297"], [" for", "0.99917597"], [" smaller", "0.8457204"], [" boats", "0.40680042"], [".", "0.9999999"], [" Sail", "1.0"], ["boats", "1.0"], [" are", "1.0"], [" used", "1.0"], [" for", "0.99999917"], [" smaller", "0.9938443"], [" boats", "1.0"], [".", "1.0"], [" Sail", "1.0"], ["boats", "1.0"], [" are", "1.0"], [" used", "1.0"], [" for", "1.0"], [" smaller", "1.0"], [" boats", "1.0"], [".", "1.0"], [" Sail", "1.0"], ["boats", "1.0"], [" are", "1.0"], [" used", "1.0"], [" for", "1.0"], [" smaller", "1.0"], [" boats", "1.0"], [".", "1.0"], [" Sail", "1.0"], ["boats", "1.0"], [" are", "1.0"], [" used", "1.0"], [" for", "1.0"], [" smaller", "1.0"], [" boats", "1.0"], [".", "1.0"], [" Sail", "1.0"], ["boats", "1.0"], [" are", "1.0"], [" used", "1.0"], [" for", "1.0"], [" smaller", "1.0"], [" boats", "1.0"], [".", "1.0"], [" Sail", "1.0"], ["boats", "1.0"], [" are", "1.0"], [" used", "1.0"], [" for", "1.0"]]}, "806": {"qid": "7fa631340ce8c42aba53", "term": "1980 United States presidential election", "description": "49th quadrennial presidential election in the United States", "question": "Were there greater landslides than 1980 United States presidential election?", "answer": true, "facts": ["A landslide refers to a competitor beating their opponent by a wide margin.", "Ronald Reagan defeated Jimmy carter in the 1980 United States presidential election by around 8 million votes.", "Franklin D. Roosevelt won the 1936 United States presidential election over Alf Landon by more than 11 million votes.", "In 1804 Thomas Jefferson received 162 (92%) of the electoral votes while Charles Cotesworth Pinckney received only 14 (8%)."], "decomposition": ["By what votes margin did Ronald Reagan defeat Jimmy Carter in the 1980 US Presidential election?", "By how many votes was Franklin D. Roosevelt leading Alf Landon in the 1936 US Presidential election?", "How many more votes did Thomas Jefferson receive than Charles Cotesworth Pinckney in the 1804 United States presidential election?", "Are #2 and #3 greater individually than #1?"], "evidence": [[[["Ronald Reagan-50"]], [["Franklin D. Roosevelt-52"]], [["Thomas Jefferson-73"], "no_evidence"], ["operation"]], [[["1980 United States presidential election-50"]], [["1936 United States presidential election-4"]], [["1804 United States presidential election-3", "Thomas Jefferson-73"]], ["operation"]], [[["1980 United States presidential election-4"]], [["1936 United States presidential election-4"]], [["1804 United States presidential election-3"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the United States have a president who was not a native-born American?\nFacts: The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Were there greater landslides than 1980 United States presidential election?\nFacts: A landslide refers to a competitor beating their opponent by a wide margin. Ronald Reagan defeated Jimmy carter in the 1980 United States presidential election by around 8 million votes. Franklin D. Roosevelt won the 1936 United States presidential election over Alf Landon by more than 11 million votes. In 1804 Thomas Jefferson received 162 (92%) of the electoral votes while Charles Cotesworth Pinckney received only 14 (8%).\nAnswer (True or False):", "ct_subject": "A landslide refers to a competitor beating their opponent by a wide margin. Ronald Reagan defeated Jimmy carter in the 1980 United States presidential election by around 8 million votes. Franklin D. Roosevelt won the 1936 United States presidential election over Alf Landon by more than 11 million votes. In 1804 Thomas Jefferson received 162 (92%) of the electoral votes while Charles Cotesworth Pinckney received only 14 (8%).", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.06332175"], [" the", "1.0"], [" United", "0.9990482"], [" States", "1.0"], [" have", "0.98806876"], [" a", "1.0"], [" president", "0.99973553"], [" who", "0.98270494"], [" was", "1.0"], [" not", "0.25126147"], [" a", "0.99999416"], [" native", "0.9971727"], ["-", "0.9999995"], ["born", "1.0"], [" American", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"]]}, "807": {"qid": "3b9f0e80d5929f447166", "term": "Swastika", "description": "a geometrical figure and an ancient religious icon in the cultures of Eurasia and 20th-century symbol of Nazism", "question": "Does the word swastika have meaning in sanskrit?", "answer": true, "facts": ["In Sanskrit, the word swastika is a combination of \u2018su\u2019 (meaning \u2018good\u2019) and \u2018asti\u2019 (meaning \u2018to exist\u2019)", "This meaning of swastika  from Sanskrit this gets translated as \u2018all is well.\u2019 "], "decomposition": ["Was the word swastika derived from a Sanskrit word?"], "evidence": [[[["Swastika-9"]]], [[["Swastika-3"], "operation"]], [[["Swastika-3"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the origin of the word \"candy\"?\nFacts: The word \"candy\" is derived from the Latin word \"cauda\" which means \"to eat\". The word \"candy\"", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does the word swastika have meaning in sanskrit?\nFacts: In Sanskrit, the word swastika is a combination of \u2018su\u2019 (meaning \u2018good\u2019) and \u2018asti\u2019 (meaning \u2018to exist\u2019) This meaning of swastika from Sanskrit this gets translated as \u2018all is well.\u2019 \nAnswer (True or False):", "ct_subject": "In Sanskrit, the word swastika is a combination of \u2018su\u2019 (meaning \u2018good\u2019) and \u2018asti\u2019 (meaning \u2018to exist\u2019) This meaning of swastika from Sanskrit this gets translated as \u2018all is well.\u2019 ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9995608"], [" is", "1.0"], [" the", "1.0"], [" origin", "0.95889884"], [" of", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "0.7473163"], ["andy", "0.9991309"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999448"], [" word", "1.0"], [" \"", "0.87938976"], ["c", "1.0"], ["andy", "1.0"], ["\"", "1.0"], [" is", "0.48576358"], [" derived", "0.998382"], [" from", "1.0"], [" the", "1.0"], [" Latin", "0.9811391"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["aud", "0.9981224"], ["a", "1.0"], ["\"", "1.0"], [" which", "1.0"], [" means", "1.0"], [" \"", "1.0"], ["to", "0.9999666"], [" eat", "0.9999999"], ["\".", "0.97140414"], [" The", "0.9878852"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"", "1.0"]]}, "808": {"qid": "e374088f76c6618c3459", "term": "Portuguese Colonial War", "description": "1961\u20131974 armed conflicts in Africa between Portugal and independence movements", "question": "Do all of the African regions that participated in the Portugese Colonial War share an official language?", "answer": true, "facts": [" The current African nations of Angola, Guinea-Bissau and Mozambique participated in the the Portugese Colonial War.", "The Portugese Colonial War was a decisive struggle in Lusophone Africa.", "Lusaphone countries are those that include Portugese as an official language."], "decomposition": ["Which African nations participated in the Portuguese Colonial War?", "Which African region did all of #1 belong to?", "Do all nations in #2 share official language?"], "evidence": [[[["Portuguese Colonial War-1"], "no_evidence"], [["Portuguese Colonial War-1"]], [["Lusophone-1"]]], [[["Portuguese Colonial War-24"]], [["Southern Africa-2"]], [["Swahili language-1"]]], [[["Portuguese Colonial War-2"]], [["Sub-Saharan Africa-1"], "no_evidence"], [["Portuguese Angola-36", "Portuguese Guinea-4", "Portuguese Mozambique-55"], "no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the country that is the largest land mass in the world?\nFacts: The United States of America is the largest land mass in the world.\nAnswer (True or", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do all of the African regions that participated in the Portugese Colonial War share an official language?\nFacts:  The current African nations of Angola, Guinea-Bissau and Mozambique participated in the the Portugese Colonial War. The Portugese Colonial War was a decisive struggle in Lusophone Africa. Lusaphone countries are those that include Portugese as an official language.\nAnswer (True or False):", "ct_subject": " The current African nations of Angola, Guinea-Bissau and Mozambique participated in the the Portugese Colonial War. The Portugese Colonial War was a decisive struggle in Lusophone Africa. Lusaphone countries are those that include Portugese as an official language.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999976"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" country", "0.97023755"], [" that", "1.0"], [" is", "0.99999833"], [" the", "0.9999989"], [" largest", "0.9999975"], [" land", "0.07220705"], [" mass", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999714"], [" United", "0.92385256"], [" States", "1.0"], [" of", "1.0"], [" America", "1.0"], [" is", "0.9999989"], [" the", "1.0"], [" largest", "1.0"], [" land", "1.0"], [" mass", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [".", "1.0"], ["\n", "0.9997365"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"]]}, "809": {"qid": "7c74e1234d292f83fbba", "term": "Land of Israel", "description": "Traditional Jewish name for an area of indefinite geographical extension in the Southern Levant.", "question": "Was Land of Israel in possession of an Islamic empire in 16th century?", "answer": true, "facts": ["Land of Israel was controlled by the Ottoman Empire in 16th century. ", "The religion of Ottoman Empire was Sunni Islam. "], "decomposition": ["Who ruled the geographic region of Israel in the 16th century?", "Was Islam the state religion of #1?"], "evidence": [[[["Israel-23", "Palestine (region)-20"]], [["Ottoman Empire-93"], "operation"]], [[["Israel-22"]], [["Ottoman Empire-96"], "no_evidence"]], [[["Israel-22"]], [["Ottoman Empire-96"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Was the Roman Empire in possession of an Islamic empire in the Middle Ages?\nFacts: The Roman Empire was in possession of an Islamic empire in the Middle Ages. \nAnswer (True or False): True\nQuestion: Was the Roman Empire in possession of an Islamic empire in the Middle Ages?\nFacts: The Roman Empire was in possession of an Islamic empire in", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Land of Israel in possession of an Islamic empire in 16th century?\nFacts: Land of Israel was controlled by the Ottoman Empire in 16th century.  The religion of Ottoman Empire was Sunni Islam. \nAnswer (True or False):", "ct_subject": "Land of Israel was controlled by the Ottoman Empire in 16th century.  The religion of Ottoman Empire was Sunni Islam. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "0.9967122"], [" the", "1.0"], [" Roman", "0.3484457"], [" Empire", "1.0"], [" in", "1.0"], [" possession", "1.0"], [" of", "1.0"], [" an", "1.0"], [" Islamic", "1.0"], [" empire", "1.0"], [" in", "1.0"], [" the", "0.9999925"], [" Middle", "0.32037392"], [" Ages", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [" was", "1.0"], [" in", "1.0"], [" possession", "1.0"], [" of", "1.0"], [" an", "1.0"], [" Islamic", "1.0"], [" empire", "1.0"], [" in", "1.0"], [" the", "1.0"], [" Middle", "1.0"], [" Ages", "1.0"], [".", "1.0"], [" ", "1.0"], ["\n", "0.9986206"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "0.9999999"], [" the", "1.0"], [" Roman", "0.99999976"], [" Empire", "1.0"], [" in", "1.0"], [" possession", "1.0"], [" of", "1.0"], [" an", "1.0"], [" Islamic", "1.0"], [" empire", "1.0"], [" in", "1.0"], [" the", "1.0"], [" Middle", "1.0"], [" Ages", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [" was", "1.0"], [" in", "1.0"], [" possession", "1.0"], [" of", "1.0"], [" an", "1.0"], [" Islamic", "1.0"], [" empire", "1.0"], [" in", "1.0"]]}, "810": {"qid": "a11734fceb222d0629c3", "term": "Mount Emei", "description": "mountain", "question": "Can a Liebherr LTM 11200-9.1 hypothetically lift Mount Emei?", "answer": true, "facts": ["Mount Emei is a 70 ton mountain located in China.", "The Liebherr LTM 11200-9.1 is the world's strongest crane that can lift 1200 tons."], "decomposition": ["How much does Mount Emei weigh?", "How much can a Liebherr LTM 11200-9.1 lift?", "Is #2 greater than #1?"], "evidence": [[[["Mount Emei-2"], "no_evidence"], [["Crane (machine)-43"], "no_evidence"], ["operation"]], [[["Mount Emei-1"], "no_evidence"], [["Liebherr Group-2"], "no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Can a Liebherr LTM 11200-9.1 hypothetically lift Mount Everest?\nFacts: Mount Everest is a 29,029 foot mountain located in Nepal. The Liebherr LTM 11200-9.1 is the world's strongest crane", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can a Liebherr LTM 11200-9.1 hypothetically lift Mount Emei?\nFacts: Mount Emei is a 70 ton mountain located in China. The Liebherr LTM 11200-9.1 is the world's strongest crane that can lift 1200 tons.\nAnswer (True or False):", "ct_subject": "Mount Emei is a 70 ton mountain located in China. The Liebherr LTM 11200-9.1 is the world's strongest crane that can lift 1200 tons.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Can", "0.99999523"], [" a", "1.0"], [" Lie", "1.0"], ["b", "1.0"], ["her", "1.0"], ["r", "1.0"], [" L", "1.0"], ["TM", "1.0"], [" 112", "1.0"], ["00", "1.0"], ["-", "1.0"], ["9", "1.0"], [".", "1.0"], ["1", "1.0"], [" hypot", "1.0"], ["hetically", "1.0"], [" lift", "1.0"], [" Mount", "0.99989486"], [" Everest", "0.9999726"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Mount", "1.0"], [" Everest", "1.0"], [" is", "1.0"], [" a", "0.99999917"], [" 29", "0.9937587"], [",", "1.0"], ["029", "0.999863"], [" foot", "1.0"], [" mountain", "0.8946039"], [" located", "1.0"], [" in", "1.0"], [" Nepal", "1.0"], [".", "1.0"], [" The", "1.0"], [" Lie", "1.0"], ["b", "1.0"], ["her", "1.0"], ["r", "1.0"], [" L", "1.0"], ["TM", "1.0"], [" 112", "1.0"], ["00", "1.0"], ["-", "1.0"], ["9", "1.0"], [".", "1.0"], ["1", "1.0"], [" is", "1.0"], [" the", "1.0"], [" world", "1.0"], ["'s", "1.0"], [" strongest", "1.0"], [" crane", "1.0"]]}, "811": {"qid": "7dc74fd9b62e514750c2", "term": "Bee", "description": "Clade of insects", "question": "Are queen bees unnecessary for growing apples?", "answer": true, "facts": ["Mason bees are solitary (they live alone)", "Mason bees are efficient pollinators for orchards", "Apple trees are grown in orchards"], "decomposition": ["What is the social structure of Mason bees?", "Where are Mason bees recognized as efficient pollinators?", "Is #1 needless of a queen, and are apples grown in #2?"], "evidence": [[[["Bee-27"]], [["Mason bee-2", "Orchard-7"], "no_evidence"], ["operation"]], [[["Mason bee-5"]], [["Osmia lignaria-8"]], [["Apple-38", "Mason bee-5"]]], [[["Mason bee-1"]], [["Mason bee-11"], "no_evidence"], [["Apple-1"], "no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any other animals that have been known to fly?\nFacts: Yes, there are many other animals that fly.\nAnswer (True or False): True\nQuestion: Are there any other animals that have been known to swim?\nFacts: Yes, there are many other animals that swim.\nAnswer (True or False): True\nQuestion: Are there any other animals that have", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are queen bees unnecessary for growing apples?\nFacts: Mason bees are solitary (they live alone) Mason bees are efficient pollinators for orchards Apple trees are grown in orchards\nAnswer (True or False):", "ct_subject": "Mason bees are solitary (they live alone) Mason bees are efficient pollinators for orchards Apple trees are grown in orchards", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.714876"], [" there", "1.0"], [" any", "0.99440295"], [" other", "0.99996257"], [" animals", "1.0"], [" that", "1.0"], [" have", "0.7299937"], [" been", "0.931384"], [" known", "0.6835364"], [" to", "1.0"], [" fly", "0.8480567"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Yes", "0.7336248"], [",", "1.0"], [" there", "0.9999919"], [" are", "1.0"], [" many", "0.9994961"], [" other", "0.99999833"], [" animals", "1.0"], [" that", "1.0"], [" fly", "0.9985349"], [".", "0.99997544"], ["\n", "0.9990778"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.9975508"], [" there", "1.0"], [" any", "1.0"], [" other", "0.9915822"], [" animals", "1.0"], [" that", "1.0"], [" have", "0.9999999"], [" been", "1.0"], [" known", "1.0"], [" to", "1.0"], [" swim", "0.88827056"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Yes", "1.0"], [",", "1.0"], [" there", "1.0"], [" are", "1.0"], [" many", "1.0"], [" other", "1.0"], [" animals", "1.0"], [" that", "1.0"], [" swim", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "1.0"], [" any", "1.0"], [" other", "1.0"], [" animals", "1.0"], [" that", "1.0"], [" have", "1.0"]]}, "812": {"qid": "341375fa1c7cb93278f5", "term": "Zucchini", "description": "Edible summer squash, typically green in color", "question": "Would 7 zucchini's satisfy potassium USDA daily recommendation?", "answer": true, "facts": ["The USDA recommends at least 3500 mg of potassium a day.", "One zucchini has 512 mg of potassium."], "decomposition": ["How much potassium is in a zucchini?", "How much potassium does the USDA recommend daily?", "Would seven times #1 be more than #2?"], "evidence": [[[["Zucchini-27"], "no_evidence"], [["Dietary Reference Intake-10"], "no_evidence"], ["no_evidence", "operation"]], [[["Zucchini-27"]], ["no_evidence"], ["operation"]], [["no_evidence"], [["Potassium-46"], "operation"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human being?\nFacts: The average human being weighs about 100 pounds.\nAnswer (True or False): True\nQuestion: What is the average weight of a horse?\nFacts: The average horse weighs about 1,000 pounds.\nAnswer (True or False): True\nQuestion: What is the average weight of a cow?\nFacts:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would 7 zucchini's satisfy potassium USDA daily recommendation?\nFacts: The USDA recommends at least 3500 mg of potassium a day. One zucchini has 512 mg of potassium.\nAnswer (True or False):", "ct_subject": "The USDA recommends at least 3500 mg of potassium a day. One zucchini has 512 mg of potassium.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9996371"], [" is", "1.0"], [" the", "1.0"], [" average", "0.99947566"], [" weight", "0.9999999"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "0.8352146"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.6708442"], [" being", "0.9996172"], [" weighs", "0.99784386"], [" about", "0.9999299"], [" 100", "0.97562975"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "1.0"], [" of", "1.0"], [" a", "1.0"], [" horse", "0.9142718"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" horse", "1.0"], [" weighs", "1.0"], [" about", "1.0"], [" 1", "0.99897087"], [",", "1.0"], ["000", "0.99999917"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "1.0"], [" of", "1.0"], [" a", "1.0"], [" cow", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"]]}, "813": {"qid": "57cace65de55cbb6b8d4", "term": "Colitis", "description": "inflammation of the colon or the large intestine", "question": "Is it best to avoid kola nuts with colitis?", "answer": true, "facts": ["Colitis is a disease in which the colon becomes inflamed.", "Many things can trigger colitis, including dairy, alcohol, and caffeine.", "The kola nut is the fruit of the tropical cola tree that contains caffeine inside."], "decomposition": ["What triggers colitis? ", "Are any of the triggers in #1 present in the kola nut?"], "evidence": [[[["Colitis-17"], "no_evidence"], [["Kola nut-2"], "operation"]], [[["Colitis-10"], "no_evidence"], [["Kola nut-1"], "no_evidence"]], [[["Colitis-3"], "no_evidence"], [["Kola nut-1"], "no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it best to avoid kola nuts with ulcerative colitis?\nFacts: Ulcerative colitis is a disease in which the colon becomes inflamed. Many things can trigger ulcerative colitis, including dairy, alcohol, and caffeine. The kola nut is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is it best to avoid kola nuts with colitis?\nFacts: Colitis is a disease in which the colon becomes inflamed. Many things can trigger colitis, including dairy, alcohol, and caffeine. The kola nut is the fruit of the tropical cola tree that contains caffeine inside.\nAnswer (True or False):", "ct_subject": "Colitis is a disease in which the colon becomes inflamed. Many things can trigger colitis, including dairy, alcohol, and caffeine. The kola nut is the fruit of the tropical cola tree that contains caffeine inside.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.999977"], [" it", "1.0"], [" best", "0.25466344"], [" to", "1.0"], [" avoid", "1.0"], [" k", "1.0"], ["ola", "1.0"], [" nuts", "1.0"], [" with", "1.0"], [" ul", "0.99150085"], ["cer", "1.0"], ["ative", "1.0"], [" col", "1.0"], ["itis", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Ul", "1.0"], ["cer", "1.0"], ["ative", "1.0"], [" col", "1.0"], ["itis", "1.0"], [" is", "1.0"], [" a", "1.0"], [" disease", "0.9999999"], [" in", "1.0"], [" which", "1.0"], [" the", "1.0"], [" colon", "1.0"], [" becomes", "1.0"], [" infl", "1.0"], ["amed", "1.0"], [".", "1.0"], [" Many", "1.0"], [" things", "1.0"], [" can", "1.0"], [" trigger", "1.0"], [" ul", "1.0"], ["cer", "1.0"], ["ative", "1.0"], [" col", "1.0"], ["itis", "1.0"], [",", "1.0"], [" including", "1.0"], [" dairy", "1.0"], [",", "1.0"], [" alcohol", "1.0"], [",", "1.0"], [" and", "1.0"], [" caffeine", "1.0"], [".", "1.0"], [" The", "1.0"], [" k", "1.0"], ["ola", "1.0"], [" nut", "1.0"], [" is", "1.0"]]}, "814": {"qid": "8a26ff980663f11b0c6e", "term": "QWERTY", "description": "keyboard layout where the first line is \"QWERTYUIOP\"", "question": "Can second row of QWERTY keyboard spell Abdastartus's kingdom?", "answer": true, "facts": ["QWERTY keyboards have one row of numbers followed by three rows of letters.", "The second row of the QWERTY keyboard has the letters: QWERTYUIOP.", "Abdastartus was king of Tyre from 929 \u2013 921 BC."], "decomposition": ["What letters are on the second row of a QWERTY keyboard?", "What was Abdastartus' kingdoms name?", "Are all the letters in #2 also found in #1?"], "evidence": [[[["QWERTY-1"], "no_evidence"], [["Abdastartus-1"]], ["operation"]], [[["QWERTY-1"], "no_evidence"], [["Abdastartus-3"]], ["operation"]], [[["QWERTY-9"]], [["Abdastartus-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to have a computer?\nFacts: The first computer was invented by the British mathematician Alan Turing in 1936.\nAnswer (True or False): True\nQuestion: What is the name of the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can second row of QWERTY keyboard spell Abdastartus's kingdom?\nFacts: QWERTY keyboards have one row of numbers followed by three rows of letters. The second row of the QWERTY keyboard has the letters: QWERTYUIOP. Abdastartus was king of Tyre from 929 \u2013 921 BC.\nAnswer (True or False):", "ct_subject": "QWERTY keyboards have one row of numbers followed by three rows of letters. The second row of the QWERTY keyboard has the letters: QWERTYUIOP. Abdastartus was king of Tyre from 929 \u2013 921 BC.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99548113"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9220322"], [" person", "0.98879683"], [" to", "1.0"], [" have", "0.19036789"], [" a", "1.0"], [" computer", "0.8185573"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" computer", "0.6669477"], [" was", "1.0"], [" invented", "0.99999344"], [" by", "0.9999926"], [" the", "0.8504673"], [" British", "0.53365827"], [" mathematician", "1.0"], [" Alan", "1.0"], [" Turing", "1.0"], [" in", "0.99997747"], [" 1936", "0.8607715"], [".", "1.0"], ["\n", "0.9999999"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"]]}, "815": {"qid": "1bed1742cceb9b51f41d", "term": "Eye surgery", "description": "medical specialty", "question": "Would Eye surgery on a fly be in vain?", "answer": true, "facts": ["Researchers at BYU have been developing smaller surgical instruments to improve medical procedures.", "BYU researchers  created robotically-controlled forceps that can pass through a hole about 3 millimeters in size.", "The eye of a fly is considerably small and estimates range from .5mm to 2mm."], "decomposition": ["What levels of precision can be reached by robot-assisted surgery?", "What is the size range of the eye of a fly?", "Is #1 considerably larger than the range of #2?"], "evidence": [[[["Robot-assisted surgery-21"], "no_evidence"], [["Fly-9"], "no_evidence"], ["operation"]], [[["Robot-assisted surgery-28"], "no_evidence"], [["Fly-2"], "no_evidence"], ["no_evidence", "operation"]], [[["Robot-assisted surgery-2"], "no_evidence"], [["Fly-14"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake with a hole in it\"?\nFacts: A snowflake is a single crystal. A snowflake with a hole in it is a cluster of crystals.\nAnswer", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Eye surgery on a fly be in vain?\nFacts: Researchers at BYU have been developing smaller surgical instruments to improve medical procedures. BYU researchers created robotically-controlled forceps that can pass through a hole about 3 millimeters in size. The eye of a fly is considerably small and estimates range from .5mm to 2mm.\nAnswer (True or False):", "ct_subject": "Researchers at BYU have been developing smaller surgical instruments to improve medical procedures. BYU researchers created robotically-controlled forceps that can pass through a hole about 3 millimeters in size. The eye of a fly is considerably small and estimates range from .5mm to 2mm.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999486"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9849006"], [" between", "1.0"], [" a", "0.99998856"], [" \"", "0.9999651"], ["s", "0.9929738"], ["now", "0.9998155"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.9998167"], ["now", "1.0"], ["flake", "1.0"], [" with", "0.9833543"], [" a", "1.0"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], ["\"?", "0.9999993"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9915176"], [" snow", "0.9999995"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "0.8040263"], [" crystal", "1.0"], [".", "0.3678312"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" with", "1.0"], [" a", "1.0"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], [" is", "1.0"], [" a", "1.0"], [" cluster", "0.999718"], [" of", "1.0"], [" crystals", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"]]}, "816": {"qid": "ec3eecce8a13f2742ee4", "term": "Milky Way", "description": "Spiral galaxy containing our Solar System", "question": "Is number of stars in Milky Way at least ten times earth's population?", "answer": true, "facts": ["The number of stars in the Milky Way galaxy is between 100 and 400 billion.", "Earth's population in 2018 was 7.5 billion people."], "decomposition": ["How many stars are in the Milky Way galaxy?", "What is the number of the human population on earth?", "Is #1 greater than or equal to ten times #2?"], "evidence": [[[["Milky Way-2"]], [["World population-1"]], ["operation"]], [[["Milky Way-2"]], [["World population-1"]], ["operation"]], [[["Milky Way-2"]], [["World population-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is there a person who has a DNA that can be traced to Genghis Khan?\nFacts: There are many people who have DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is there a person who has a DNA that can be traced to Genghis Khan?\nFacts: There", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is number of stars in Milky Way at least ten times earth's population?\nFacts: The number of stars in the Milky Way galaxy is between 100 and 400 billion. Earth's population in 2018 was 7.5 billion people.\nAnswer (True or False):", "ct_subject": "The number of stars in the Milky Way galaxy is between 100 and 400 billion. Earth's population in 2018 was 7.5 billion people.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99997926"], [" there", "0.9807725"], [" a", "0.99999523"], [" person", "0.9527476"], [" who", "0.9915862"], [" has", "0.99988997"], [" a", "0.08897048"], [" DNA", "0.9106181"], [" that", "1.0"], [" can", "0.99955255"], [" be", "1.0"], [" traced", "1.0"], [" to", "0.99631315"], [" Gen", "1.0"], ["gh", "1.0"], ["is", "1.0"], [" Khan", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "0.9999403"], [" are", "0.32464343"], [" many", "0.99977106"], [" people", "1.0"], [" who", "0.9978138"], [" have", "1.0"], [" DNA", "1.0"], [" that", "1.0"], [" can", "1.0"], [" be", "1.0"], [" traced", "1.0"], [" to", "1.0"], [" Gen", "1.0"], ["gh", "1.0"], ["is", "1.0"], [" Khan", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" there", "1.0"], [" a", "1.0"], [" person", "1.0"], [" who", "1.0"], [" has", "1.0"], [" a", "1.0"], [" DNA", "1.0"], [" that", "1.0"], [" can", "1.0"], [" be", "1.0"], [" traced", "1.0"], [" to", "1.0"], [" Gen", "1.0"], ["gh", "1.0"], ["is", "1.0"], [" Khan", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "1.0"]]}, "817": {"qid": "4c88f31810b240fd43a6", "term": "The Hobbit", "description": "Fantasy novel by J. R. R. Tolkien", "question": "Can The Hobbit be read in its entirety in four minutes?", "answer": true, "facts": ["The Hobbit is a 95,356 word book by J.R.R. Tolkien.", "Speed reader Howard Stephen Berg could read at the speed of 25,000 words per minute.", "Speed reader Maria Teresa Calderon from the Philippines claimed to be able to read 80,000 words per minute with 100% comprehension."], "decomposition": ["How many words are in the Hobbit?", "How many words per minute could Maria Teresa Calderon read?", "What is #2 multiplied by 4?", "Is #3 greater than or equal to #1?"], "evidence": [[[["The Hobbit-3"], "no_evidence"], [["Speed reading-19"], "no_evidence"], ["no_evidence", "operation"], ["no_evidence", "operation"]], [["no_evidence"], [["Speed reading-19"]], ["operation"], ["operation"]], [[["The Hobbit-1"], "no_evidence"], [["Speed reading-19"]], ["operation"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest word in the English language?\nFacts: The longest word in the English language is \"the\". The longest word in the English language is \"the\". The longest word in the English", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can The Hobbit be read in its entirety in four minutes?\nFacts: The Hobbit is a 95,356 word book by J.R.R. Tolkien. Speed reader Howard Stephen Berg could read at the speed of 25,000 words per minute. Speed reader Maria Teresa Calderon from the Philippines claimed to be able to read 80,000 words per minute with 100% comprehension.\nAnswer (True or False):", "ct_subject": "The Hobbit is a 95,356 word book by J.R.R. Tolkien. Speed reader Howard Stephen Berg could read at the speed of 25,000 words per minute. Speed reader Maria Teresa Calderon from the Philippines claimed to be able to read 80,000 words per minute with 100% comprehension.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99990165"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.99920005"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], [" is", "1.0"], [" \"", "0.9999989"], ["the", "0.9991584"], ["\".", "0.99937516"], [" The", "0.9995763"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], [" is", "1.0"], [" \"", "1.0"], ["the", "1.0"], ["\".", "1.0"], [" The", "0.9981376"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"]]}, "818": {"qid": "a505361d75535d674ebd", "term": "Paprika", "description": "spice made from dried fruits of Capsicum annuum", "question": "Can Paprika be made without a dehydrator?", "answer": true, "facts": ["Peppers can be dehydrated in the oven in lieu of a dehydrator.", "Sunlight and heat have been used for centuries to dry peppers and other foods."], "decomposition": ["What is paprika made from?", "Can #1 be dehydrated without using a dehydrator?"], "evidence": [[[["Paprika-4"], "no_evidence"], ["no_evidence", "operation"]], [[["Paprika-1"]], [["Food dehydrator-2"], "no_evidence", "operation"]], [[["Paprika-1"]], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"C\" and a \"K\"?\nFacts: The \"C\" stands for \"Carbon\" and the \"K\" stands for \"Kerosene\".\nAnswer (True or False): True\nQuestion: What is the difference between a \"C\" and a \"K\"?\nFacts: The \"C\" stands", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can Paprika be made without a dehydrator?\nFacts: Peppers can be dehydrated in the oven in lieu of a dehydrator. Sunlight and heat have been used for centuries to dry peppers and other foods.\nAnswer (True or False):", "ct_subject": "Peppers can be dehydrated in the oven in lieu of a dehydrator. Sunlight and heat have been used for centuries to dry peppers and other foods.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.65056413"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9997807"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["C", "0.9306812"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["K", "0.9994326"], ["\"?", "0.99999654"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99999106"], [" \"", "0.9647459"], ["C", "1.0"], ["\"", "1.0"], [" stands", "0.9999999"], [" for", "1.0"], [" \"", "1.0"], ["Car", "0.7045323"], ["bon", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" the", "1.0"], [" \"", "1.0"], ["K", "1.0"], ["\"", "1.0"], [" stands", "1.0"], [" for", "1.0"], [" \"", "1.0"], ["K", "1.0"], ["eros", "1.0"], ["ene", "1.0"], ["\".", "0.9999999"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["C", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["K", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" \"", "1.0"], ["C", "1.0"], ["\"", "1.0"], [" stands", "1.0"]]}, "819": {"qid": "16ced51d31cc386aa343", "term": "Pancake", "description": "Thin, round cake made of eggs, milk and flour", "question": "Are some types of pancakes named after coins?", "answer": true, "facts": ["Silver dollar pancakes are a variety that is smaller than traditional pancakes.", "Silver dollars are a type of American coin."], "decomposition": ["What are some common types of pancakes?", "Is any of #1 named after a coin?"], "evidence": [[[["Pancake-12"], "no_evidence"], ["operation"]], [[["Pancake-12"]], [["Silver dollar-1"]]], [[["Pancake-67", "Pancake-69", "Pancake-70"]], [["Pancake-69"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any types of pancakes that are named after the color of the syrup?\nFacts: Yes.\nAnswer (True or False): True\nQuestion: Are there any types of pancakes that are named after the type of syrup?\nFacts: Yes.\nAnswer (True or False): True\nQuestion: Are there any types of pancakes that are named after the type of syrup?\nFacts: Yes", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are some types of pancakes named after coins?\nFacts: Silver dollar pancakes are a variety that is smaller than traditional pancakes. Silver dollars are a type of American coin.\nAnswer (True or False):", "ct_subject": "Silver dollar pancakes are a variety that is smaller than traditional pancakes. Silver dollars are a type of American coin.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.99974364"], [" there", "0.99996936"], [" any", "0.99999905"], [" types", "0.9999132"], [" of", "1.0"], [" pancakes", "0.9999597"], [" that", "1.0"], [" are", "1.0"], [" named", "1.0"], [" after", "1.0"], [" the", "0.99859864"], [" color", "0.9759054"], [" of", "1.0"], [" the", "0.9999949"], [" syrup", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Yes", "0.9995683"], [".", "0.4978218"], ["\n", "0.9247506"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "1.0"], [" any", "1.0"], [" types", "1.0"], [" of", "1.0"], [" pancakes", "1.0"], [" that", "1.0"], [" are", "1.0"], [" named", "1.0"], [" after", "1.0"], [" the", "1.0"], [" type", "0.9595757"], [" of", "1.0"], [" syrup", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Yes", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "1.0"], [" any", "1.0"], [" types", "1.0"], [" of", "1.0"], [" pancakes", "1.0"], [" that", "1.0"], [" are", "1.0"], [" named", "1.0"], [" after", "1.0"], [" the", "1.0"], [" type", "0.9999999"], [" of", "1.0"], [" syrup", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Yes", "1.0"]]}, "820": {"qid": "378926a5a2874aa941a9", "term": "Fantasy", "description": "Genre of literature, film, television and other artforms", "question": "Would J.K Rowling's top sellers be on a fantasy shelf?", "answer": true, "facts": ["J.K Rowling's top sellers are her Harry Potter series.", "Harry Potter is a series about a boy who goes to a magical school to learn wizardry."], "decomposition": ["What is J. K. Rowling's top selling book?", "Is #1 fantasy?"], "evidence": [[[["J. K. Rowling-1"]], [["Harry Potter-1"], "operation"]], [[["J. K. Rowling-1"]], [["J. K. Rowling-1"]]], [[["J. K. Rowling-22"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: The first person to be executed in the United States was John Brown. John Brown was a slave owner who was executed for treason.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be executed in the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would J.K Rowling's top sellers be on a fantasy shelf?\nFacts: J.K Rowling's top sellers are her Harry Potter series. Harry Potter is a series about a boy who goes to a magical school to learn wizardry.\nAnswer (True or False):", "ct_subject": "J.K Rowling's top sellers are her Harry Potter series. Harry Potter is a series about a boy who goes to a magical school to learn wizardry.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999976"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9997073"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9983802"], [" person", "0.9982346"], [" to", "1.0"], [" be", "0.99961275"], [" executed", "0.9083826"], [" in", "0.99927133"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99835455"], [" first", "1.0"], [" person", "1.0"], [" to", "0.9995246"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" John", "0.9998795"], [" Brown", "1.0"], [".", "0.9885389"], [" John", "0.99462175"], [" Brown", "1.0"], [" was", "1.0"], [" a", "1.0"], [" slave", "0.94258684"], [" owner", "0.9977775"], [" who", "0.9999927"], [" was", "0.4610888"], [" executed", "1.0"], [" for", "0.99988484"], [" treason", "0.99988925"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"]]}, "821": {"qid": "4464ad7e6a0b908632ac", "term": "Limbic system", "description": "structures of the brain", "question": "Will The Exorcist stimulate limbic system?", "answer": true, "facts": ["The limbic system of the brain contains regions that detect fear, control bodily functions and perceive sensory information.", "The Exorcist has been called one of the scariest movies of all time.", "The Exorcist ranked number 3 on the American Film Institute's 100 Years/100 Thrills list."], "decomposition": ["Based on its functions, what kinds of stimuli is the limbic system likely to respond to?", "What is the general opinion of the movie The Exorcist?", "Does #2 suggest that the movie will generate #1?"], "evidence": [[[["Limbic system-17"]], [["The Exorcist-6"]], [["Limbic system-17", "The Exorcist (film)-1"]]], [[["Limbic system-17"]], [["The Exorcist (film series)-16", "The Exorcist (film)-78"]], ["operation"]], [[["Limbic system-9"]], [["The Exorcist (film)-1"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the Exorcist a good movie?\nFacts: The Exorcist is a good movie. It is a horror movie that is based on a true story. The movie is a classic that has been seen by millions", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Will The Exorcist stimulate limbic system?\nFacts: The limbic system of the brain contains regions that detect fear, control bodily functions and perceive sensory information. The Exorcist has been called one of the scariest movies of all time. The Exorcist ranked number 3 on the American Film Institute's 100 Years/100 Thrills list.\nAnswer (True or False):", "ct_subject": "The limbic system of the brain contains regions that detect fear, control bodily functions and perceive sensory information. The Exorcist has been called one of the scariest movies of all time. The Exorcist ranked number 3 on the American Film Institute's 100 Years/100 Thrills list.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.1636388"], [" the", "0.94448775"], [" Ex", "0.99999905"], ["orc", "1.0"], ["ist", "1.0"], [" a", "0.9999434"], [" good", "0.9987839"], [" movie", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Ex", "1.0"], ["orc", "1.0"], ["ist", "1.0"], [" is", "1.0"], [" a", "1.0"], [" good", "0.9991357"], [" movie", "1.0"], [".", "1.0"], [" It", "0.98808026"], [" is", "0.99997187"], [" a", "0.999818"], [" horror", "0.9998431"], [" movie", "0.99935764"], [" that", "0.99945194"], [" is", "0.99296737"], [" based", "0.99998105"], [" on", "1.0"], [" a", "0.99923086"], [" true", "1.0"], [" story", "1.0"], [".", "1.0"], [" The", "0.99928635"], [" movie", "0.99704427"], [" is", "0.9997836"], [" a", "0.97534627"], [" classic", "0.9465688"], [" that", "0.9967495"], [" has", "0.9999974"], [" been", "1.0"], [" seen", "0.9998971"], [" by", "1.0"], [" millions", "1.0"]]}, "822": {"qid": "3570d57f16a40488ff42", "term": "Judo", "description": "modern martial art, combat and Olympic sport", "question": "Do silicone suits make judo difficult?", "answer": true, "facts": ["Judo is a martial art that requires combatants to grip their opponents and throw them in various ways.", "Judo practitioners traditionally wear an outfit called a gi, which opponents use to grip and throw.", "Silicone is one of the slipperiest substances on the planet."], "decomposition": ["What maneuvers are required to do Judo?", "What characteristics does an article of clothing need to have in order to do #1 effectively?", "What characteristics does a silicone suit have? ", "Is #3 excluded from #2?"], "evidence": [[[["Judo-1"]], [["Keikogi-1"], "no_evidence"], [["Silicone rubber-2"], "no_evidence"], ["no_evidence", "operation"]], [[["Leopold's maneuvers-6"], "no_evidence"], ["no_evidence"], [["Silicone-47"]], ["operation"]], [[["Judo-1"]], [["Judo-48"]], [["Silicone-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake\" with a \"snowflake\"?\nFacts: A snowflake is a single, smooth, unbroken piece of ice. A snowflake with a \"snowflake\"", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do silicone suits make judo difficult?\nFacts: Judo is a martial art that requires combatants to grip their opponents and throw them in various ways. Judo practitioners traditionally wear an outfit called a gi, which opponents use to grip and throw. Silicone is one of the slipperiest substances on the planet.\nAnswer (True or False):", "ct_subject": "Judo is a martial art that requires combatants to grip their opponents and throw them in various ways. Judo practitioners traditionally wear an outfit called a gi, which opponents use to grip and throw. Silicone is one of the slipperiest substances on the planet.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999869"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9683828"], [" between", "1.0"], [" a", "0.9999999"], [" \"", "0.9999927"], ["s", "0.9942034"], ["now", "0.9987929"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.9999999"], ["now", "1.0"], ["flake", "1.0"], ["\"", "0.9642107"], [" with", "0.9999989"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\"?", "0.33396465"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.99999976"], [" snow", "0.98438245"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "0.99999857"], [",", "0.6450599"], [" smooth", "0.9953654"], [",", "1.0"], [" un", "0.99996924"], ["broken", "1.0"], [" piece", "0.9999969"], [" of", "1.0"], [" ice", "0.9999999"], [".", "1.0"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" with", "1.0"], [" a", "1.0"], [" \"", "0.9998092"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\"", "1.0"]]}, "823": {"qid": "5c3bf978d0f77907af6c", "term": "JAG (TV series)", "description": "American legal drama television series (1996-2005)", "question": "Could you watch all of JAG in six months?", "answer": true, "facts": ["JAG has 227 episodes in the entire series.", "Each episode of JAG is between 42-47 minutes long.", "There are over 200,000 minutes in a month. ", "The entire series of JAG is under 12,000 minutes. "], "decomposition": ["How many episodes of JAG are there?", "How long is an episode of JAG?", "What is #1 multiplied by #2?", "How many minutes are there in six months?", "Is #3 less than or equal to #4?"], "evidence": [[["no_evidence"], ["no_evidence"], ["operation"], [["Year-57"]], ["operation"]], [["no_evidence"], ["no_evidence"], ["operation"], ["no_evidence", "operation"], ["operation"]], [["no_evidence"], [["Television pilot-19"], "no_evidence"], ["operation"], ["no_evidence", "operation"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the character in JAG who is the most popular?\nFacts: The character who is the most popular is the character named \"The Man in the Iron Mask.\"\nAnswer (True or False): True\nQuestion: What is the name of the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could you watch all of JAG in six months?\nFacts: JAG has 227 episodes in the entire series. Each episode of JAG is between 42-47 minutes long. There are over 200,000 minutes in a month.  The entire series of JAG is under 12,000 minutes. \nAnswer (True or False):", "ct_subject": "JAG has 227 episodes in the entire series. Each episode of JAG is between 42-47 minutes long. There are over 200,000 minutes in a month.  The entire series of JAG is under 12,000 minutes. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99976987"], [" is", "1.0"], [" the", "1.0"], [" name", "0.969483"], [" of", "1.0"], [" the", "1.0"], [" character", "1.0"], [" in", "0.99885535"], [" J", "1.0"], ["AG", "1.0"], [" who", "0.58305496"], [" is", "0.99999845"], [" the", "1.0"], [" most", "1.0"], [" popular", "0.9346723"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" character", "1.0"], [" who", "0.9823137"], [" is", "1.0"], [" the", "0.9999994"], [" most", "1.0"], [" popular", "1.0"], [" is", "0.99989426"], [" the", "1.0"], [" character", "0.9170651"], [" named", "0.99944025"], [" \"", "1.0"], ["The", "0.99994445"], [" Man", "0.99999976"], [" in", "1.0"], [" the", "0.999731"], [" Iron", "0.99971014"], [" Mask", "1.0"], [".\"", "0.88503015"], ["\n", "0.99999905"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"]]}, "824": {"qid": "c921bdade889def26cf4", "term": "Solubility", "description": "Capacity of a designated solvent to hold a designated solute in homogeneous solution under specified conditions", "question": "In isopropyl alcohol, is the solubility of salt low?", "answer": true, "facts": ["Isopropyl alcohol is unique in the sense that salt remains visible.", "When salt has high solubility, it becomes invisible."], "decomposition": ["How does high solubility affect the visibility of salt?", "Does salt in isopropyl alcohol fail to exhibit #1?"], "evidence": [[[["Salt-15"]], [["Isopropyl alcohol-4"]]], [[["Isopropyl alcohol-4"]], ["operation"]], [[["Solubility-1"]], [["Isopropyl alcohol-4"], "no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"salt\" and a \"salt-and-pepper\" salt?\nFacts: A salt is a solid that has a high concentration of sodium. A salt-and-pepper salt is a solid that has a high concentration of chloride.\nAnswer (True or False): True\nQuestion: What is the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: In isopropyl alcohol, is the solubility of salt low?\nFacts: Isopropyl alcohol is unique in the sense that salt remains visible. When salt has high solubility, it becomes invisible.\nAnswer (True or False):", "ct_subject": "Isopropyl alcohol is unique in the sense that salt remains visible. When salt has high solubility, it becomes invisible.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9964976"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9997578"], [" between", "1.0"], [" a", "0.9999689"], [" \"", "0.9999995"], ["s", "0.98988134"], ["alt", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["alt", "1.0"], ["-", "0.0047937175"], ["and", "0.5150565"], ["-", "1.0"], ["pe", "1.0"], ["pper", "1.0"], ["\"", "0.9999999"], [" salt", "0.66422784"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9839939"], [" salt", "0.9968533"], [" is", "1.0"], [" a", "1.0"], [" solid", "0.9127528"], [" that", "0.9358604"], [" has", "0.99924755"], [" a", "0.99446917"], [" high", "0.999673"], [" concentration", "0.99501795"], [" of", "1.0"], [" sodium", "0.99997616"], [".", "1.0"], [" A", "1.0"], [" salt", "1.0"], ["-", "1.0"], ["and", "1.0"], ["-", "1.0"], ["pe", "1.0"], ["pper", "1.0"], [" salt", "1.0"], [" is", "1.0"], [" a", "1.0"], [" solid", "0.99998355"], [" that", "1.0"], [" has", "1.0"], [" a", "1.0"], [" high", "0.99999154"], [" concentration", "1.0"], [" of", "1.0"], [" chloride", "0.99962986"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"]]}, "825": {"qid": "6b9093fff19fdcb320e3", "term": "Noble gas", "description": "group of chemical elements tend to be chemically inert and thus form odorless, colorless, monatomic gases with low reactivity; consists of helium, neon, argon, krypton, xenon, radon, and possibly oganesson", "question": "Was the Japanese street aesthetic once illuminated by noble gasses?", "answer": true, "facts": ["Japan was known for their use of brightly lit businesses and signage.", "The signs in Japan were lit with neon for a long time."], "decomposition": ["What were the signs in Japan known for in the past?", "Were noble gasses the reason behind #1?"], "evidence": [[[["Neon lighting-3"], "no_evidence"], [["Neon lighting-1"]]], [["no_evidence"], [["Noble gas-1"], "operation"]], [[["Neon sign-1"], "no_evidence"], [["Neon-1", "Neon-3"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance between two points on Earth?\nFacts: The distance between the Earth and the Sun is about 93 million miles.\nAnswer (True or False): True\nQuestion: What is the longest distance between two points on the Moon?\nFacts: The distance between the Earth and the Moon is about 238,000 miles.\nAnswer (True or False):", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was the Japanese street aesthetic once illuminated by noble gasses?\nFacts: Japan was known for their use of brightly lit businesses and signage. The signs in Japan were lit with neon for a long time.\nAnswer (True or False):", "ct_subject": "Japan was known for their use of brightly lit businesses and signage. The signs in Japan were lit with neon for a long time.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99452835"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.49243802"], [" distance", "0.9987431"], [" between", "0.894732"], [" two", "1.0"], [" points", "1.0"], [" on", "0.99998665"], [" Earth", "0.9887475"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" distance", "0.99999523"], [" between", "1.0"], [" the", "1.0"], [" Earth", "1.0"], [" and", "1.0"], [" the", "1.0"], [" Sun", "0.99901056"], [" is", "1.0"], [" about", "1.0"], [" 93", "1.0"], [" million", "1.0"], [" miles", "1.0"], [".", "1.0"], ["\n", "0.986051"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.99999976"], [" distance", "0.9999995"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"], [" on", "1.0"], [" the", "1.0"], [" Moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" distance", "0.9999981"], [" between", "1.0"], [" the", "1.0"], [" Earth", "0.99182945"], [" and", "1.0"], [" the", "1.0"], [" Moon", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 238", "0.98149174"], [",", "1.0"], ["000", "1.0"], [" miles", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"]]}, "826": {"qid": "fd5f22b8ed969a08eea5", "term": "Rick and Morty", "description": "Animated sitcom", "question": "Could Rich and Morty be triggered for children of alcoholics?", "answer": true, "facts": ["Rick, one of the titular characters of Rick and Morty, is often seen drunk and speaking abusively to Morty.", "Morty's mother Beth is depicted multiple times neglecting her children while getting drunk on wine. ", "Trauma triggers can occur when someone is exposed to something that reminds them of a traumatic situation. "], "decomposition": ["What depictions are common triggers for children of alcoholics?", "Do any of the characters from Rick and Morty exhibit the characteristics in #1?"], "evidence": [[[["Alcoholism-13"], "no_evidence"], [["Rick and Morty-5"], "operation"]], [[["Alcoholism-13"], "no_evidence"], [["Rick and Morty-5"], "no_evidence", "operation"]], [[["Adult Children of Alcoholics-4"]], [["Adult Children of Alcoholics-4", "Rick and Morty-4", "Rick and Morty-5"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the character in the movie The Matrix?\nFacts: Neo is the main character of the movie. Neo is a computer programmer who is able to use his mind to create", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could Rich and Morty be triggered for children of alcoholics?\nFacts: Rick, one of the titular characters of Rick and Morty, is often seen drunk and speaking abusively to Morty. Morty's mother Beth is depicted multiple times neglecting her children while getting drunk on wine.  Trauma triggers can occur when someone is exposed to something that reminds them of a traumatic situation. \nAnswer (True or False):", "ct_subject": "Rick, one of the titular characters of Rick and Morty, is often seen drunk and speaking abusively to Morty. Morty's mother Beth is depicted multiple times neglecting her children while getting drunk on wine.  Trauma triggers can occur when someone is exposed to something that reminds them of a traumatic situation. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9936552"], [" is", "1.0"], [" the", "1.0"], [" name", "0.96788824"], [" of", "1.0"], [" the", "1.0"], [" character", "0.9888287"], [" in", "0.99999535"], [" the", "1.0"], [" movie", "1.0"], [" The", "0.79649943"], [" Matrix", "0.99219745"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Neo", "1.0"], [" is", "0.9740504"], [" the", "1.0"], [" main", "0.9969518"], [" character", "1.0"], [" of", "0.9995326"], [" the", "1.0"], [" movie", "0.88679624"], [".", "1.0"], [" Neo", "0.9575475"], [" is", "1.0"], [" a", "0.9725319"], [" computer", "0.99999964"], [" programmer", "0.9917241"], [" who", "0.9999999"], [" is", "0.9999999"], [" able", "1.0"], [" to", "1.0"], [" use", "0.9986576"], [" his", "0.99999964"], [" mind", "0.9999994"], [" to", "1.0"], [" create", "1.0"]]}, "827": {"qid": "f61fa68d73eb81ede181", "term": "Gorillaz", "description": "British virtual band", "question": "Does it seem like the Gorillaz is composed of more members than they have?", "answer": true, "facts": ["In music videos for Gorillaz songs, there are four animated bandmates playing.", "Gorillaz is a collaboration of 3 band members."], "decomposition": ["How many band members are in Gorillaz?", "How many animated band members are in Gorillaz videos?", "Is #2 more than #1?"], "evidence": [[[["Gorillaz-1"]], [["Gorillaz-1"]], ["operation"]], [[["Gorillaz-1"]], [["Gorillaz-1"]], ["operation"]], [[["Gorillaz-1"]], [["Gorillaz-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the band?\nFacts: The band is called Gorillaz.\nAnswer (True or False): True\nQuestion: What is the name of the band's album?\nFacts: The album is called Humanz.\nAnswer (True or False): True\nQuestion: What is the name of the band's", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does it seem like the Gorillaz is composed of more members than they have?\nFacts: In music videos for Gorillaz songs, there are four animated bandmates playing. Gorillaz is a collaboration of 3 band members.\nAnswer (True or False):", "ct_subject": "In music videos for Gorillaz songs, there are four animated bandmates playing. Gorillaz is a collaboration of 3 band members.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9350261"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" band", "0.9999647"], ["?", "0.5715196"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99969864"], [" band", "0.9999999"], [" is", "1.0"], [" called", "0.9999999"], [" Gor", "0.99999976"], ["ill", "1.0"], ["az", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" band", "1.0"], ["'s", "0.9999999"], [" album", "0.9987458"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" album", "1.0"], [" is", "1.0"], [" called", "0.9999999"], [" Human", "0.9954796"], ["z", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" band", "1.0"], ["'s", "1.0"]]}, "828": {"qid": "57e6b3d22162e254f7f0", "term": "United Airlines", "description": "Airline in the United States", "question": "Is Glycol something United Airlines would buy?", "answer": true, "facts": ["Glycol is a commonly used de-icing fluid for commercial planes.", "American Airlines flies all year round, including throughout the winter."], "decomposition": ["What is Glycol commonly used for?", "What cold season does American Airlines fly its planes?", "Would #1 be helpful in #2?"], "evidence": [[[["Diol-2", "Diol-4"]], [["American Airlines-1"], "no_evidence"], ["operation"]], [[["Ethylene glycol-1"]], [["Winter-1"]], [["Ethylene glycol-16"]]], [[["Diol-2", "Ethylene glycol-1"]], [["Winter-1"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city of New York City derived from the name of the city of New York?\nFacts: The name of the city of New York City is derived from the name of the city of New York.\nAnswer (True or False): True\nQuestion: Is the name of the city of New York City derived from the name of the city of New York?\nFacts", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Glycol something United Airlines would buy?\nFacts: Glycol is a commonly used de-icing fluid for commercial planes. American Airlines flies all year round, including throughout the winter.\nAnswer (True or False):", "ct_subject": "Glycol is a commonly used de-icing fluid for commercial planes. American Airlines flies all year round, including throughout the winter.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9776937"], [" the", "0.82205456"], [" name", "0.8896976"], [" of", "1.0"], [" the", "1.0"], [" city", "0.99999976"], [" of", "0.99854136"], [" New", "0.52465945"], [" York", "1.0"], [" City", "1.0"], [" derived", "0.97585344"], [" from", "1.0"], [" the", "1.0"], [" name", "0.99999523"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "0.9979013"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "0.9999751"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" City", "0.999894"], [" is", "0.9999999"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [".", "1.0"], ["\n", "0.9851959"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" City", "1.0"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"]]}, "829": {"qid": "49cd2c594e9715899734", "term": "Golden Gate Bridge", "description": "suspension bridge on the San Francisco Bay", "question": "Do depressed people travel to the Golden Gate Bridge often?", "answer": true, "facts": ["The Golden Gate Bridge is one of the most popular suicide spots in the USA.", "Suicide is often caused by severe depression."], "decomposition": ["What is the ultimate end that severe depression can lead to?", "Is the Golden Gate Bridge a place where #1 is known to often happen?"], "evidence": [[[["Major depressive disorder-22"]], [["Suicides at the Golden Gate Bridge-4"], "operation"]], [[["Suicide-7"]], [["Golden Gate Bridge-50"]]], [[["Suicide-1"]], [["Suicides at the Golden Gate Bridge-4"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance a person can travel in one day?\nFacts: The longest distance a person can travel in one day is about 1,000 miles.\nAnswer (True or False): True\nQuestion: What is the longest distance a person can travel in one year?\nFacts: The longest distance a person can travel in one year is about 1,000 miles.\nAnswer", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do depressed people travel to the Golden Gate Bridge often?\nFacts: The Golden Gate Bridge is one of the most popular suicide spots in the USA. Suicide is often caused by severe depression.\nAnswer (True or False):", "ct_subject": "The Golden Gate Bridge is one of the most popular suicide spots in the USA. Suicide is often caused by severe depression.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.96304804"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.25904262"], [" distance", "0.99999917"], [" a", "0.9992731"], [" person", "1.0"], [" can", "0.9999989"], [" travel", "0.7064626"], [" in", "0.9882981"], [" one", "0.9440585"], [" day", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" person", "1.0"], [" can", "1.0"], [" travel", "1.0"], [" in", "1.0"], [" one", "1.0"], [" day", "1.0"], [" is", "1.0"], [" about", "0.9999999"], [" 1", "0.9607149"], [",", "1.0"], ["000", "0.99999845"], [" miles", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" person", "1.0"], [" can", "1.0"], [" travel", "1.0"], [" in", "1.0"], [" one", "1.0"], [" year", "0.9999994"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" person", "1.0"], [" can", "1.0"], [" travel", "1.0"], [" in", "1.0"], [" one", "1.0"], [" year", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 1", "0.97889704"], [",", "1.0"], ["000", "1.0"], [" miles", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"]]}, "830": {"qid": "12f49bcc3fdaebe88bd9", "term": "Olive oil", "description": "liquid fat extracted by pressing olives", "question": "Do some people soak in olive oil and water?", "answer": true, "facts": ["Adding olive oil to bath water is a common practice for dry skin.", "In baths, people tend to soak for a period of time. "], "decomposition": ["During which activity do people soak in water for some time?", "Is it common to add olive oil water for dry skin during #1?"], "evidence": [[[["Bathing-1"]], [["Bathing-44"], "no_evidence"]], [[["Bathing-1"]], [["Bathing-44"], "no_evidence"]], [[["Bathing-1"]], [["Olive oil-28"], "no_evidence", "operation"]], [[["Bathing-1"]], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake with a hole in it\"?\nFacts: A snowflake is a solid, round, and shiny object. A snowflake with a hole in it is a solid, round, and shiny object that has a hole in it.\nAnswer (True or False): True\nQuestion: What is the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do some people soak in olive oil and water?\nFacts: Adding olive oil to bath water is a common practice for dry skin. In baths, people tend to soak for a period of time. \nAnswer (True or False):", "ct_subject": "Adding olive oil to bath water is a common practice for dry skin. In baths, people tend to soak for a period of time. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.87761587"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.99999225"], [" between", "1.0"], [" a", "0.9999999"], [" \"", "0.99979275"], ["s", "0.836129"], ["now", "0.18659352"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.9999846"], ["now", "1.0"], ["flake", "1.0"], [" with", "0.9162072"], [" a", "1.0"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], ["\"?", "0.9999976"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999572"], [" snow", "0.9999999"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" solid", "0.98018676"], [",", "0.99998343"], [" round", "0.9999262"], [",", "1.0"], [" and", "0.99964976"], [" shiny", "0.99779654"], [" object", "1.0"], [".", "0.9999999"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" with", "1.0"], [" a", "1.0"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], [" is", "1.0"], [" a", "1.0"], [" solid", "0.99995315"], [",", "1.0"], [" round", "1.0"], [",", "1.0"], [" and", "1.0"], [" shiny", "1.0"], [" object", "1.0"], [" that", "0.8848222"], [" has", "1.0"], [" a", "0.99999416"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"]]}, "831": {"qid": "c3ee8ac44eff4db19ffb", "term": "Bucharest", "description": "Capital of Romania", "question": "Could a wandering albatross fly from Bucharest to New York City without a rest?", "answer": true, "facts": ["Wandering albatross can travel at least 15,000 km (just under 10,000 miles) over the sea before returning to land. ", "It's 4766 miles or 7670 km from Bucharest to New York City."], "decomposition": ["How far can a Wandering albatross travel over the sea before returning to land?", "How far is it from Bucharest to New York City", "Is #2 less than #1?"], "evidence": [[[["Wandering albatross-4"], "no_evidence"], ["no_evidence"], ["operation"]], [[["Wandering albatross-8"], "no_evidence"], [["Bucharest-1", "New York City-1"], "no_evidence", "operation"], ["no_evidence", "operation"]], [[["Wandering albatross-5"]], ["no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance a person can travel in one day?\nFacts: The longest distance a person can travel in one day is about 1,000 km (621 miles).\nAnswer (True or False): True\nQuestion: What is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could a wandering albatross fly from Bucharest to New York City without a rest?\nFacts: Wandering albatross can travel at least 15,000 km (just under 10,000 miles) over the sea before returning to land.  It's 4766 miles or 7670 km from Bucharest to New York City.\nAnswer (True or False):", "ct_subject": "Wandering albatross can travel at least 15,000 km (just under 10,000 miles) over the sea before returning to land.  It's 4766 miles or 7670 km from Bucharest to New York City.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9996445"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.9999999"], [" distance", "1.0"], [" a", "0.5659173"], [" person", "0.99268687"], [" can", "0.7497004"], [" travel", "0.9994765"], [" in", "0.9999862"], [" one", "0.96764565"], [" day", "0.99999857"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999993"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" person", "1.0"], [" can", "1.0"], [" travel", "1.0"], [" in", "1.0"], [" one", "1.0"], [" day", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 1", "0.58900887"], [",", "1.0"], ["000", "0.9999993"], [" km", "0.9994481"], [" (", "1.0"], ["6", "1.0"], ["21", "1.0"], [" miles", "1.0"], [").", "0.9999893"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"]]}, "832": {"qid": "3540bb4c468f7b513aab", "term": "Aerosmith", "description": "American rock band", "question": "Can Aerosmith legally drive in the carpool lane?", "answer": true, "facts": ["Aerosmith is a rock band with five members", "Carpool lanes require at least two occupants in each vehicle"], "decomposition": ["How many people does the band Aerosmith have?", "What is the minimum number of occupants required to use the carpool lane?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Aerosmith-1"]], [["High-occupancy vehicle lane-1"]], ["operation"]], [[["Aerosmith-1"]], [["High-occupancy vehicle lane-1"]], ["operation"]], [[["Aerosmith-1"]], [["High-occupancy vehicle lane-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: The first person to be executed in the United States was John Brown, who was hanged in 1859\nAnswer (True or False): True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: The first person to be executed in the United States was John", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can Aerosmith legally drive in the carpool lane?\nFacts: Aerosmith is a rock band with five members Carpool lanes require at least two occupants in each vehicle\nAnswer (True or False):", "ct_subject": "Aerosmith is a rock band with five members Carpool lanes require at least two occupants in each vehicle", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999372"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9585071"], [" of", "1.0"], [" the", "1.0"], [" first", "0.20274515"], [" person", "0.9967122"], [" to", "1.0"], [" be", "0.99992955"], [" executed", "0.93808717"], [" in", "0.99999964"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.54194295"], [" first", "1.0"], [" person", "1.0"], [" to", "0.49306533"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" John", "0.9580339"], [" Brown", "1.0"], [",", "0.9381577"], [" who", "1.0"], [" was", "1.0"], [" hanged", "1.0"], [" in", "0.9998952"], [" 18", "0.9999999"], ["59", "1.0"], ["\n", "0.85408014"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99991333"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" John", "1.0"]]}, "833": {"qid": "88675643c5f299344f36", "term": "Tomato", "description": "Edible berry of the tomato plant, originating in South America", "question": "Do you need both hot and cold water to peel a tomato?", "answer": true, "facts": ["The first step in removing the skin from at tomato is to quickly submerge it in boiling water.", "The second step in removing the skin from a tomato is to take the tomatoes out of the boiling water and put them into ice water."], "decomposition": ["What are the various steps involved in peeling tomatoes?", "Does any of #1 use hot water?", "Does any of #1 use cold water?", "Are #2 and #3 positive?"], "evidence": [[[["Blanching (cooking)-1", "Peel (fruit)-1"]], [["Blanching (cooking)-7"], "operation"], [["Blanching (cooking)-9"], "operation"], ["operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Tomato-89"], "no_evidence"], ["no_evidence", "operation"], ["no_evidence", "operation"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake\"?\nFacts: A snowflake is a small, round, hard, and shiny object. A snowflake is made up of many small snowflakes.\nAnswer (True or False", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do you need both hot and cold water to peel a tomato?\nFacts: The first step in removing the skin from at tomato is to quickly submerge it in boiling water. The second step in removing the skin from a tomato is to take the tomatoes out of the boiling water and put them into ice water.\nAnswer (True or False):", "ct_subject": "The first step in removing the skin from at tomato is to quickly submerge it in boiling water. The second step in removing the skin from a tomato is to take the tomatoes out of the boiling water and put them into ice water.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9997663"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9999982"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.9999876"], ["s", "0.92649645"], ["now", "0.9999858"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\"?", "0.95915705"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999982"], [" snow", "0.9999994"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" small", "0.588227"], [",", "1.0"], [" round", "0.8556865"], [",", "1.0"], [" hard", "0.9985929"], [",", "1.0"], [" and", "0.9999697"], [" shiny", "1.0"], [" object", "0.99998426"], [".", "0.98344433"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" is", "1.0"], [" made", "0.99999607"], [" up", "0.99393046"], [" of", "1.0"], [" many", "1.0"], [" small", "0.998966"], [" snow", "0.9898565"], ["fl", "1.0"], ["akes", "1.0"], [".", "1.0"], ["\n", "0.9997429"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"]]}, "834": {"qid": "5c27625de0e7c35be856", "term": "Tony Bennett", "description": "American singer", "question": "Is Tony Bennett's middle name shared by a former UFC champion?", "answer": true, "facts": ["Tony Bennett's full name is Anthony Dominick Benedetto.", "Dominick Cruz is a two-time UFC Bantamweight Champion."], "decomposition": ["What is Tony Bennett's middle name?", "What are the names of the former UFC champions?", "Is #1 found in #2?"], "evidence": [[[["Tony Bennett-1"]], [["Dominick Cruz-1"]], ["operation"]], [[["Tony Bennett-1"]], [["Dominick Cruz-1"]], ["operation"]], [[["Tony Bennett-1"]], [["Dominick Reyes-1"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city of New York City the same as the name of the city of New York?\nFacts: The name of the city of New York is New York City. The name of the city of New York is New York.\nAnswer (True or False): True\nQuestion: Is the name of the city of New York City the same as the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Tony Bennett's middle name shared by a former UFC champion?\nFacts: Tony Bennett's full name is Anthony Dominick Benedetto. Dominick Cruz is a two-time UFC Bantamweight Champion.\nAnswer (True or False):", "ct_subject": "Tony Bennett's full name is Anthony Dominick Benedetto. Dominick Cruz is a two-time UFC Bantamweight Champion.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999975"], [" the", "0.9999999"], [" name", "1.0"], [" of", "0.9999989"], [" the", "1.0"], [" city", "0.70015824"], [" of", "0.58057874"], [" New", "0.9995734"], [" York", "1.0"], [" City", "1.0"], [" the", "0.9744106"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "0.99955636"], [" York", "1.0"], ["?", "0.7281809"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.95568824"], [" name", "0.99999917"], [" of", "1.0"], [" the", "0.99971586"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" is", "1.0"], [" New", "1.0"], [" York", "1.0"], [" City", "0.2259339"], [".", "1.0"], [" The", "0.9999993"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" is", "1.0"], [" New", "1.0"], [" York", "1.0"], [".", "0.99541134"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" City", "1.0"], [" the", "1.0"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"]]}, "835": {"qid": "a895b73bdfb2397b2e27", "term": "Friday", "description": "day of the week", "question": "Does 2015 have more unlucky Friday's than usual?", "answer": true, "facts": ["Friday the 13th is known as an unlucky Friday because of the number 13.", "A year can have as many as three Friday the 13ths.", "One Friday the 13th is the average per year.", "There were 3 Friday the 13ths in 2015."], "decomposition": ["How many Friday the 13ths were in 2015?", "What is the usual number of Friday the 13ths per year?", "Is #1 more than #2?"], "evidence": [[[["Friday the 13th-1"]], [["Friday the 13th-1"]], ["operation"]], [[["Friday the 13th-1"]], [["Friday the 13th-1"]], ["operation"]], [[["Friday the 13th-25"]], [["Friday the 13th-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the year 2000?\nFacts: The first person to be born in the year 2000 was a man named John Smith.\nAnswer (True or False): True\nQuestion: What is the name of the first person", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does 2015 have more unlucky Friday's than usual?\nFacts: Friday the 13th is known as an unlucky Friday because of the number 13. A year can have as many as three Friday the 13ths. One Friday the 13th is the average per year. There were 3 Friday the 13ths in 2015.\nAnswer (True or False):", "ct_subject": "Friday the 13th is known as an unlucky Friday because of the number 13. A year can have as many as three Friday the 13ths. One Friday the 13th is the average per year. There were 3 Friday the 13ths in 2015.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99590635"], [" is", "1.0"], [" the", "1.0"], [" name", "0.96800184"], [" of", "1.0"], [" the", "1.0"], [" first", "0.985463"], [" person", "0.9839684"], [" to", "1.0"], [" be", "0.9952265"], [" born", "0.9997092"], [" in", "1.0"], [" the", "1.0"], [" year", "0.99988735"], [" 2000", "0.9999827"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "0.99999905"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "1.0"], [" a", "0.99999964"], [" man", "0.22314076"], [" named", "1.0"], [" John", "0.9999604"], [" Smith", "1.0"], [".", "1.0"], ["\n", "0.99999285"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"]]}, "836": {"qid": "8e9607aa0cefcb84c0fb", "term": "Isaac Newton", "description": "Influential British physicist and mathematician", "question": "Is Isaac Newton buried at the same church as the author of Great Expectations?", "answer": true, "facts": ["Isaac Newton is buried at Westminster Abbey.", "Charles Dickens's book Great Expectations was published  in 1861.", "Charles Dickens is buried at the Poets' Corner of Westminster Abbey.", "Westminster Abbey is a large church in the City of Westminster, London, England."], "decomposition": ["Who is the author of 'Great Expectations'?", "Where is #1 resting place?", "Where was Isaac Newton buried?", "Are #2 and #3 the same?"], "evidence": [[[["Great Expectations-1"]], [["Charles Dickens-53"]], [["Isaac Newton-46"]], ["operation"]], [[["Great Expectations-1"]], [["Charles Dickens-53"]], [["Isaac Newton-46"]], ["operation"]], [[["Great Expectations-2"]], [["Charles Dickens-53"]], [["Isaac Newton-46"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city of New York City the same as the name of the city of New York?\nFacts: New York City is the name of the city of New York. New York is the name of the state of New York", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Isaac Newton buried at the same church as the author of Great Expectations?\nFacts: Isaac Newton is buried at Westminster Abbey. Charles Dickens's book Great Expectations was published in 1861. Charles Dickens is buried at the Poets' Corner of Westminster Abbey. Westminster Abbey is a large church in the City of Westminster, London, England.\nAnswer (True or False):", "ct_subject": "Isaac Newton is buried at Westminster Abbey. Charles Dickens's book Great Expectations was published in 1861. Charles Dickens is buried at the Poets' Corner of Westminster Abbey. Westminster Abbey is a large church in the City of Westminster, London, England.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99999714"], [" the", "1.0"], [" name", "0.9236374"], [" of", "0.9999999"], [" the", "1.0"], [" city", "0.9999999"], [" of", "1.0"], [" New", "0.99971396"], [" York", "1.0"], [" City", "1.0"], [" the", "0.99988496"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" New", "0.6802284"], [" York", "1.0"], [" City", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999995"], [" of", "1.0"], [" the", "0.99698263"], [" city", "1.0"], [" of", "0.39575067"], [" New", "1.0"], [" York", "1.0"], [".", "0.9999999"], [" New", "0.99999845"], [" York", "1.0"], [" is", "0.9954442"], [" the", "0.999992"], [" name", "1.0"], [" of", "1.0"], [" the", "0.9999846"], [" state", "0.9835014"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"]]}, "837": {"qid": "028eda7ded7825edb6eb", "term": "Jury", "description": "sworn body of people convened to render a verdict officially submitted to them by a court, or to set a penalty or judgment", "question": "Is a felony jury enough people for a Bunco game?", "answer": true, "facts": ["Felonies and other serious crimes have a jury of 12 people.", "Bunco is a parlour game requiring 12 or more players."], "decomposition": ["How many people are on a felony criminal jury?", "How many players are needed for a game of Bunco?", "Is #2 the same or less than #1?"], "evidence": [[[["Jury-4"]], [["Bunco-1"]], ["operation"]], [[["Jury-2"]], [["Bunco-1"]], ["operation"]], [[["Jury-4"]], [["Bunco-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a Bunco game and a Bunco game with a Bunco board?\nFacts: A Bunco game with a Bunco board is a game played with a Bunco board. A Bunco game with a Bunco board is a game played with a Bunco board.\nAnswer (True or False): True\nQuestion: What is the difference", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is a felony jury enough people for a Bunco game?\nFacts: Felonies and other serious crimes have a jury of 12 people. Bunco is a parlour game requiring 12 or more players.\nAnswer (True or False):", "ct_subject": "Felonies and other serious crimes have a jury of 12 people. Bunco is a parlour game requiring 12 or more players.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9983113"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9983992"], [" between", "1.0"], [" a", "1.0"], [" Bun", "0.9968785"], ["co", "1.0"], [" game", "1.0"], [" and", "1.0"], [" a", "1.0"], [" Bun", "1.0"], ["co", "1.0"], [" game", "1.0"], [" with", "1.0"], [" a", "1.0"], [" Bun", "0.9713108"], ["co", "1.0"], [" board", "0.9999621"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9374107"], [" Bun", "1.0"], ["co", "1.0"], [" game", "1.0"], [" with", "0.97137713"], [" a", "1.0"], [" Bun", "1.0"], ["co", "1.0"], [" board", "1.0"], [" is", "1.0"], [" a", "0.99994254"], [" game", "1.0"], [" played", "0.9999175"], [" with", "1.0"], [" a", "1.0"], [" Bun", "1.0"], ["co", "1.0"], [" board", "1.0"], [".", "1.0"], [" A", "0.9999999"], [" Bun", "1.0"], ["co", "1.0"], [" game", "1.0"], [" with", "1.0"], [" a", "1.0"], [" Bun", "1.0"], ["co", "1.0"], [" board", "1.0"], [" is", "1.0"], [" a", "0.99936706"], [" game", "1.0"], [" played", "1.0"], [" with", "1.0"], [" a", "1.0"], [" Bun", "1.0"], ["co", "1.0"], [" board", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"]]}, "838": {"qid": "9c3f11fb9b6b469a3681", "term": "Tax collector", "description": "person who collects taxes", "question": "Does Kenny G hold the qualifications to be a tax collector?", "answer": true, "facts": ["The qualifications to be a tax collector in the US inlude a bachelor's degree in accounting.", "Kenny G studied accounting at the University of Washington and graduated magna cum laude."], "decomposition": ["What are the qualifications to be a tax collector?", "Does Kenny G possess #1?"], "evidence": [[["no_evidence"], ["no_evidence"]], [[["Certified Public Accountant-16"], "no_evidence"], [["Kenny G-5"], "operation"]], [[["Audit-1", "Tax collector-1"], "no_evidence"], [["Kenny G-5"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the US that has the highest number of people who have a degree in accounting?\nFacts: The city with the highest number of people who have a degree in accounting is Seattle, Washington.\nAnswer (True or False): True\nQuestion: What is the name of the city in the US", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Kenny G hold the qualifications to be a tax collector?\nFacts: The qualifications to be a tax collector in the US inlude a bachelor's degree in accounting. Kenny G studied accounting at the University of Washington and graduated magna cum laude.\nAnswer (True or False):", "ct_subject": "The qualifications to be a tax collector in the US inlude a bachelor's degree in accounting. Kenny G studied accounting at the University of Washington and graduated magna cum laude.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.8576344"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99991"], [" of", "1.0"], [" the", "1.0"], [" city", "0.99973005"], [" in", "0.9801998"], [" the", "0.9999933"], [" US", "0.9810829"], [" that", "0.9999995"], [" has", "0.14493185"], [" the", "1.0"], [" highest", "0.99999213"], [" number", "0.999892"], [" of", "1.0"], [" people", "0.9334487"], [" who", "0.99999917"], [" have", "0.99927646"], [" a", "0.22084524"], [" degree", "0.88113254"], [" in", "1.0"], [" accounting", "0.9999503"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" city", "0.9999976"], [" with", "1.0"], [" the", "1.0"], [" highest", "1.0"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" who", "0.999073"], [" have", "1.0"], [" a", "1.0"], [" degree", "1.0"], [" in", "1.0"], [" accounting", "1.0"], [" is", "1.0"], [" Seattle", "0.955285"], [",", "0.99970406"], [" Washington", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" US", "1.0"]]}, "839": {"qid": "3aecd30e1212e2985d4b", "term": "Guitarist", "description": "person who plays the guitar", "question": "Do guitarists need both hands to play?", "answer": true, "facts": ["The left hand typically positions the chords on the fretboard.", "The right hand plays the strings, either strumming a whole chord or finger-picking individual strings.", "The position of the left hand on the fretboard changes the tones of the strings played by the right hand, so both hands are necessary."], "decomposition": ["Which musical instrument do guitarists play?", "How many hands are typically used to play #1?", "Is #2 equal to two?"], "evidence": [[[["Guitarist-1"]], [["Guitarist-2", "Guitarist-3"]], ["no_evidence", "operation"]], [[["Guitarist-1"]], [["Guitar-1"]], ["operation"]], [[["Guitarist-1"]], [["Guitar-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"chord\" and a \"string\"?\nFacts: A chord is a series of notes that are played in succession. A string is a single note played in succession.\nAnswer (True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do guitarists need both hands to play?\nFacts: The left hand typically positions the chords on the fretboard. The right hand plays the strings, either strumming a whole chord or finger-picking individual strings. The position of the left hand on the fretboard changes the tones of the strings played by the right hand, so both hands are necessary.\nAnswer (True or False):", "ct_subject": "The left hand typically positions the chords on the fretboard. The right hand plays the strings, either strumming a whole chord or finger-picking individual strings. The position of the left hand on the fretboard changes the tones of the strings played by the right hand, so both hands are necessary.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999987"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.99997556"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.9999641"], ["ch", "0.99155223"], ["ord", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["string", "0.99999654"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999993"], [" chord", "1.0"], [" is", "1.0"], [" a", "1.0"], [" series", "0.63576925"], [" of", "1.0"], [" notes", "1.0"], [" that", "0.9134367"], [" are", "0.9999869"], [" played", "1.0"], [" in", "0.9961293"], [" succession", "1.0"], [".", "1.0"], [" A", "1.0"], [" string", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "0.9429049"], [" note", "1.0"], [" played", "0.99917704"], [" in", "1.0"], [" succession", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"]]}, "840": {"qid": "43e2fe43169c07b5ab49", "term": "Karachi", "description": "Megacity in Sindh, Pakistan", "question": "Karachi was a part of Alexander the Great's success?", "answer": true, "facts": ["Karachi is a city in modern day Pakistan.", "Krokola was an ancient port located in what is now Karachi.", "Alexander the Great stationed his fleet in Krokola on his way to Babylon.", "Alexander the Great defeated Darius and conquered Babylon before expanding his empire."], "decomposition": ["What is Karachi?", "What was the name of the ancient port that was once located in #1?", "Before expanding his empire, what city did Alexander the Great conquer?", "Did Alexander the Great station his fleet at #2 prior to #3?"], "evidence": [[[["Karachi-1"]], [["Karachi-8"]], [["Achaemenid Assyria-41"]], ["operation"]], [[["Karachi-1"]], [["Karachi-8"]], ["no_evidence"], ["operation"]], [[["Karachi-1"]], [["Port of Karachi-2"]], [["Alexander the Great-51"], "no_evidence"], [["Karachi-8"], "no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city of Rome?\nFacts: Rome was a city in ancient Rome. The city was named after the Roman god of war, Mars.\nAnswer (True or False): True\nQuestion: What is the name of the city", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Karachi was a part of Alexander the Great's success?\nFacts: Karachi is a city in modern day Pakistan. Krokola was an ancient port located in what is now Karachi. Alexander the Great stationed his fleet in Krokola on his way to Babylon. Alexander the Great defeated Darius and conquered Babylon before expanding his empire.\nAnswer (True or False):", "ct_subject": "Karachi is a city in modern day Pakistan. Krokola was an ancient port located in what is now Karachi. Alexander the Great stationed his fleet in Krokola on his way to Babylon. Alexander the Great defeated Darius and conquered Babylon before expanding his empire.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99945205"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999994"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "0.9997913"], [" Rome", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "1.0"], [" was", "0.9942778"], [" a", "0.9999697"], [" city", "1.0"], [" in", "1.0"], [" ancient", "0.9689679"], [" Rome", "1.0"], [".", "1.0"], [" The", "0.99886465"], [" city", "0.40208244"], [" was", "0.9999838"], [" named", "0.7218012"], [" after", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" god", "0.9999988"], [" of", "1.0"], [" war", "1.0"], [",", "0.9999999"], [" Mars", "1.0"], [".", "1.0"], ["\n", "0.9999988"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"]]}, "841": {"qid": "8b08d90e8d537d92e6a8", "term": "King Kong (2005 film)", "description": "2005 film directed by Peter Jackson", "question": "Was King Kong (2005 film) the lead actress's biggest box office role?", "answer": true, "facts": ["Naomi Watts starred in King Kong (2005 film).", "Naomi Watts has starred in several movies such as Divergent and the RIng.", "Divergent: Insurgent made 295 million at the box office worldwide .", "The Ring made 248 million at the box office worldwide .", "King Kong (2005 film) made 550 million at the box office worldwide."], "decomposition": ["Who was the lead actress in King Kong (2005)?", "What other films has #1 starred in?", "How much did King Kong (2005) make at the box office?", "How much did each of #2 make at the box office?", "Is #3 greater than any of #4?"], "evidence": [[[["King Kong (2005 film)-1"]], [["Naomi Watts-1"]], [["King Kong (2005 film)-2"]], [["Flirting (film)-9", "For Love Alone-8"]], ["operation"]], [[["King Kong (2005 film)-1"]], [["Naomi Watts filmography-2"], "no_evidence"], [["King Kong (2005 film)-2"]], [["The Divergent Series: Insurgent-3"], "no_evidence"], ["no_evidence", "operation"]], [[["King Kong (2005 film)-1"]], [["Naomi Watts filmography-2"], "no_evidence"], [["King Kong (2005 film)-28"]], [["Naomi Watts-19"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: The first person to be born in the United States was Mary Ann", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was King Kong (2005 film) the lead actress's biggest box office role?\nFacts: Naomi Watts starred in King Kong (2005 film). Naomi Watts has starred in several movies such as Divergent and the RIng. Divergent: Insurgent made 295 million at the box office worldwide . The Ring made 248 million at the box office worldwide . King Kong (2005 film) made 550 million at the box office worldwide.\nAnswer (True or False):", "ct_subject": "Naomi Watts starred in King Kong (2005 film). Naomi Watts has starred in several movies such as Divergent and the RIng. Divergent: Insurgent made 295 million at the box office worldwide . The Ring made 248 million at the box office worldwide . King Kong (2005 film) made 550 million at the box office worldwide.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99913436"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9983902"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9999764"], [" person", "0.8900784"], [" to", "1.0"], [" be", "0.99992716"], [" born", "0.999949"], [" in", "1.0"], [" the", "1.0"], [" United", "0.9999472"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99999714"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" Mary", "0.76718277"], [" Ann", "0.9999995"]]}, "842": {"qid": "2bd29e26063a43572a19", "term": "Do it yourself", "description": "building, modifying, or repairing something without the aid of experts or professionals", "question": "Are some Do It Yourself projects potentially lethal?", "answer": true, "facts": ["Deep fried turkey can be made at home, but a small mistake can cause the entire setup to erupt into a grease fire.", "Home roofing repair can be a DIY project but without proper safety gear a fall can be deadly."], "decomposition": ["How could DIY deep fried turkey go wrong in case of a mistake?", "What accidents could DIY home roofing cause if something went wrong?", "Are #1 and #2 deadly?"], "evidence": [[[["Turkey fryer-5"]], [["Falling (accident)-1", "Roofer-1"], "no_evidence"], ["operation"]], [[["Turkey fryer-5"]], [["Falling (accident)-17"], "no_evidence"], [["Burn-4", "Falling (accident)-17"], "operation"]], [[["Turkey fryer-5"]], [["Home repair-8"]], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any other questions that you would like to ask the panel?\nFacts: The panel is comprised of a diverse group of experts in the field of genetics. We have a lot of respect for each other and we are all here to help you make the best decisions for your family.\nAnswer (", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are some Do It Yourself projects potentially lethal?\nFacts: Deep fried turkey can be made at home, but a small mistake can cause the entire setup to erupt into a grease fire. Home roofing repair can be a DIY project but without proper safety gear a fall can be deadly.\nAnswer (True or False):", "ct_subject": "Deep fried turkey can be made at home, but a small mistake can cause the entire setup to erupt into a grease fire. Home roofing repair can be a DIY project but without proper safety gear a fall can be deadly.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.6741256"], [" there", "1.0"], [" any", "1.0"], [" other", "0.5439523"], [" questions", "0.9997818"], [" that", "0.8811858"], [" you", "1.0"], [" would", "1.0"], [" like", "1.0"], [" to", "1.0"], [" ask", "0.9914386"], [" the", "0.8243117"], [" panel", "0.9999995"], ["?", "1.0"], ["\n", "1.0"], ["F", "0.9999999"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99614775"], [" panel", "1.0"], [" is", "0.99950445"], [" comprised", "0.99960536"], [" of", "1.0"], [" a", "0.9999565"], [" diverse", "1.0"], [" group", "1.0"], [" of", "1.0"], [" experts", "0.9999999"], [" in", "0.49166572"], [" the", "0.99996436"], [" field", "0.9870402"], [" of", "1.0"], [" genetics", "0.9999895"], [".", "0.98674136"], [" We", "0.99816567"], [" have", "0.5642821"], [" a", "1.0"], [" lot", "0.23768581"], [" of", "1.0"], [" respect", "0.99868554"], [" for", "1.0"], [" each", "1.0"], [" other", "1.0"], [" and", "0.99999964"], [" we", "1.0"], [" are", "0.99589705"], [" all", "1.0"], [" here", "0.9973858"], [" to", "1.0"], [" help", "0.99926347"], [" you", "0.9987167"], [" make", "0.99975485"], [" the", "0.99991393"], [" best", "0.99999964"], [" decisions", "0.9639245"], [" for", "0.99998033"], [" your", "1.0"], [" family", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "0.99999964"], [" (", "1.0"]]}, "843": {"qid": "a59367577ae81ab33531", "term": "Lettuce", "description": "Species of annual plant of the daisy family, most often grown as a leaf vegetable", "question": "Can lettuce result in spontaneous abortion?", "answer": true, "facts": ["Food-borne pathogens that can survive on lettuce include Listeria monocytogenes, ", "Listeria monocytogenes is the causative agent of listeriosis.", "The manifestations of listeriosis include intrauterine or cervical infections in pregnant women, which may result in spontaneous abortion."], "decomposition": ["What diseases can be caused by contaminated lettuce?", "Can any of #1 cause intrauterine or cervical infections?"], "evidence": [[[["Lettuce-4"]], [["Cervix-30", "Escherichia coli-1", "Salmonella-1"], "no_evidence", "operation"]], [[["Lettuce-4"]], [["Salmonella-22"], "no_evidence", "operation"]], [[["Lettuce-4"]], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Can a person be born with a heart defect?\nFacts: A heart defect is a congenital abnormality of the heart. The heart is a muscle that pumps blood throughout the body. The heart is made up of two chambers", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can lettuce result in spontaneous abortion?\nFacts: Food-borne pathogens that can survive on lettuce include Listeria monocytogenes,  Listeria monocytogenes is the causative agent of listeriosis. The manifestations of listeriosis include intrauterine or cervical infections in pregnant women, which may result in spontaneous abortion.\nAnswer (True or False):", "ct_subject": "Food-borne pathogens that can survive on lettuce include Listeria monocytogenes,  Listeria monocytogenes is the causative agent of listeriosis. The manifestations of listeriosis include intrauterine or cervical infections in pregnant women, which may result in spontaneous abortion.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Can", "0.08431457"], [" a", "0.9997032"], [" person", "0.9999999"], [" be", "0.9996778"], [" born", "0.9999589"], [" with", "1.0"], [" a", "0.99999857"], [" heart", "0.9999994"], [" defect", "0.9999975"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.96049863"], [" heart", "0.9999958"], [" defect", "1.0"], [" is", "1.0"], [" a", "1.0"], [" congen", "1.0"], ["ital", "1.0"], [" abnorm", "0.9910347"], ["ality", "1.0"], [" of", "0.9999958"], [" the", "1.0"], [" heart", "1.0"], [".", "0.99998665"], [" The", "0.39838374"], [" heart", "1.0"], [" is", "1.0"], [" a", "0.99999905"], [" muscle", "0.82972586"], [" that", "1.0"], [" pumps", "0.9993486"], [" blood", "1.0"], [" throughout", "0.99977237"], [" the", "1.0"], [" body", "1.0"], [".", "1.0"], [" The", "0.7298478"], [" heart", "1.0"], [" is", "1.0"], [" made", "0.245928"], [" up", "1.0"], [" of", "1.0"], [" two", "0.9999999"], [" chambers", "1.0"]]}, "844": {"qid": "0992c14cfd410f2d5c1e", "term": "Slot machine", "description": "Casino gambling machine", "question": "Do any video games about the end of civilization have slot machines?", "answer": true, "facts": ["Fallout New Vegas is a game that takes place after the apocalypse has ocurred. ", "In Fallout New Vegas, players can go to casinos and play on slot machines."], "decomposition": ["What video games take place in a post-apocalyptic world?", "Which video games have slot machines?", "Is at least one game in #1 found in #2?"], "evidence": [[[["Fallout (series)-2", "Fallout: New Vegas-1"]], [["Fallout: New Vegas-4"]], ["operation"]], [[["Fallout: New Vegas-1", "The Last of Us-1"], "no_evidence"], [["Fallout: New Vegas-4"], "no_evidence"], ["operation"]], [[["Fallout (series)-1"], "no_evidence"], [["Fallout: New Vegas-2", "Fallout: New Vegas-4"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the largest in size?\nFacts: The largest city in the United States is New York City.\nAnswer (True or False): True\nQuestion: What is the name of the city in the United States that is the largest in population?\nFacts: The largest", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do any video games about the end of civilization have slot machines?\nFacts: Fallout New Vegas is a game that takes place after the apocalypse has ocurred.  In Fallout New Vegas, players can go to casinos and play on slot machines.\nAnswer (True or False):", "ct_subject": "Fallout New Vegas is a game that takes place after the apocalypse has ocurred.  In Fallout New Vegas, players can go to casinos and play on slot machines.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9996049"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9994653"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9191999"], [" in", "0.9999993"], [" the", "1.0"], [" United", "0.9921212"], [" States", "1.0"], [" that", "1.0"], [" is", "0.9999982"], [" the", "1.0"], [" largest", "0.26621383"], [" in", "0.39621904"], [" size", "0.99930537"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9993761"], [" largest", "0.99999523"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "1.0"], [" New", "0.9527282"], [" York", "1.0"], [" City", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" in", "1.0"], [" population", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" largest", "1.0"]]}, "845": {"qid": "965b4af72fa11cbda5d1", "term": "Eminem", "description": "American rapper and actor", "question": "Would Terence Tao outperform Eminem in a math competition?", "answer": true, "facts": ["Eminem disliked math and social studies, and dropped out of high school at age 17.", "Terence Tao was a child prodigy in mathematics, attending university-level mathematics courses at the age of 9.", "From 1992 to 1996, Tao was a graduate student at Princeton University under the direction of Elias Stein, receiving his Ph.D. at the age of 21."], "decomposition": ["What achievements did Terence Tao make as a student of mathematics?", "What was Eminem's disposition to mathematics as a high-schooler?", "Does #1 indicate a higher mathematical ability than #2?"], "evidence": [[[["Terence Tao-2"]], [["Eminem-8"]], [["Fields Medal-2"]]], [[["Terence Tao-1"]], [["Eminem-8"]], ["operation"]], [[["Terence Tao-11", "Terence Tao-8"]], [["Eminem-8"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to ever be born in the year 2000?\nFacts: The year 2000 was the first year that the year zero", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Terence Tao outperform Eminem in a math competition?\nFacts: Eminem disliked math and social studies, and dropped out of high school at age 17. Terence Tao was a child prodigy in mathematics, attending university-level mathematics courses at the age of 9. From 1992 to 1996, Tao was a graduate student at Princeton University under the direction of Elias Stein, receiving his Ph.D. at the age of 21.\nAnswer (True or False):", "ct_subject": "Eminem disliked math and social studies, and dropped out of high school at age 17. Terence Tao was a child prodigy in mathematics, attending university-level mathematics courses at the age of 9. From 1992 to 1996, Tao was a graduate student at Princeton University under the direction of Elias Stein, receiving his Ph.D. at the age of 21.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999999"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99949837"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9986707"], [" person", "0.99829906"], [" to", "1.0"], [" ever", "0.6372249"], [" be", "0.99999976"], [" born", "0.99999976"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" year", "0.9999981"], [" 2000", "1.0"], [" was", "0.9999865"], [" the", "0.99844605"], [" first", "0.99994016"], [" year", "1.0"], [" that", "0.99964356"], [" the", "0.99919933"], [" year", "1.0"], [" zero", "0.99996865"]]}, "846": {"qid": "3b3765fab920695e5387", "term": "Peach", "description": "species of fruit tree (for the fruit use Q13411121)", "question": "Would a black widow woman have use for peaches?", "answer": true, "facts": ["A black widow woman refers to a woman who murders her husbands for money and remarries.", "Peach pits contain amygdalin, a type of cyanide.", "Cyanide is poisonous to humans."], "decomposition": ["What is a black widows main goal?", "What do peach pits contain?", "If a human ate #2, would #1 occur?"], "evidence": [[[["Stacey Castor-31"]], [["Amygdalin-5"]], [["Amygdalin-3"]]], [["no_evidence"], [["Amygdalin-2", "Peach-2", "Peach-22"], "no_evidence"], ["no_evidence", "operation"]], [[["Black Widow Murders-1"]], [["Peach (fruit)-16"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a black widow and a black widow spider?\nFacts: Black widow spiders are the only spiders that have venom. Black widow spiders are also known as black widow spiders.\nAnswer (True or False): True\nQuestion: What is the difference between a black widow and a black widow spider?\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a black widow woman have use for peaches?\nFacts: A black widow woman refers to a woman who murders her husbands for money and remarries. Peach pits contain amygdalin, a type of cyanide. Cyanide is poisonous to humans.\nAnswer (True or False):", "ct_subject": "A black widow woman refers to a woman who murders her husbands for money and remarries. Peach pits contain amygdalin, a type of cyanide. Cyanide is poisonous to humans.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999917"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9937954"], [" between", "1.0"], [" a", "1.0"], [" black", "0.9973865"], [" widow", "1.0"], [" and", "1.0"], [" a", "1.0"], [" black", "0.9990289"], [" widow", "1.0"], [" spider", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Black", "1.0"], [" widow", "1.0"], [" spiders", "1.0"], [" are", "1.0"], [" the", "0.9998103"], [" only", "0.899787"], [" spiders", "0.9999999"], [" that", "1.0"], [" have", "0.7209419"], [" venom", "0.9999995"], [".", "0.9795013"], [" Black", "1.0"], [" widow", "1.0"], [" spiders", "1.0"], [" are", "0.99999976"], [" also", "0.5665902"], [" known", "0.99965286"], [" as", "1.0"], [" black", "0.82979447"], [" widow", "1.0"], [" spiders", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" black", "1.0"], [" widow", "1.0"], [" and", "0.9999269"], [" a", "1.0"], [" black", "1.0"], [" widow", "1.0"], [" spider", "1.0"], ["?", "1.0"], ["\n", "1.0"]]}, "847": {"qid": "f3d87ec114acbb8c9dd7", "term": "Nordic countries", "description": "Geographical and cultural region in Northern Europe and the North Atlantic", "question": "Does someone from Japan need a passport to go to a Nordic country?", "answer": true, "facts": ["The Nordic Passport Union allows citizens of the Nordic countries: Denmark (Faroe Islands included since 1 January 1966, Greenland not included), Sweden, Norway (Svalbard, Bouvet Island and Queen Maud Land not included), Finland and Iceland (since 24 September 1965) to cross approved border districts without carrying and having their passport checked.", "Japan is not one of the approved countries."], "decomposition": ["What countries recognize the Nordic Passport Union?", "Is Japan included in #1?"], "evidence": [[[["Nordic Passport Union-1"]], ["operation"]], [[["Nordic Passport Union-1"]], ["operation"]], [[["Nordic Passport Union-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"Caucasian\" and a \"Negroid\" person?\nFacts: The term", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does someone from Japan need a passport to go to a Nordic country?\nFacts: The Nordic Passport Union allows citizens of the Nordic countries: Denmark (Faroe Islands included since 1 January 1966, Greenland not included), Sweden, Norway (Svalbard, Bouvet Island and Queen Maud Land not included), Finland and Iceland (since 24 September 1965) to cross approved border districts without carrying and having their passport checked. Japan is not one of the approved countries.\nAnswer (True or False):", "ct_subject": "The Nordic Passport Union allows citizens of the Nordic countries: Denmark (Faroe Islands included since 1 January 1966, Greenland not included), Sweden, Norway (Svalbard, Bouvet Island and Queen Maud Land not included), Finland and Iceland (since 24 September 1965) to cross approved border districts without carrying and having their passport checked. Japan is not one of the approved countries.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9747038"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9927564"], [" between", "1.0"], [" a", "0.9999317"], [" \"", "1.0"], ["C", "0.36759913"], ["aucas", "0.9997192"], ["ian", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "0.99999964"], [" \"", "1.0"], ["Neg", "1.0"], ["roid", "1.0"], ["\"", "0.99881834"], [" person", "0.9669589"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99999416"], [" term", "0.9999144"]]}, "848": {"qid": "92375474ecfe1fe8b2be", "term": "Tibia", "description": "larger of the two bones of the leg below the knee for vertebrates", "question": "Would you find a tibia beside parsley on a holiday plate?", "answer": true, "facts": ["The tibia of a goat is eaten during Passover, a Jewish holiday", "Parsley is served on a Passover seder plate beside the goat shank "], "decomposition": ["How is Passover celebrated?", "What part of a goat is eaten during #1?", "Is parsley typically served on the same plate as #2?"], "evidence": [[[["Passover-13", "Passover-7"]], [["Tibia-1", "Zeroa-1"]], [["Passover Seder plate-4"]]], [[["Passover-22"]], [["Passover-23"]], [["Parsley-15"], "no_evidence"]], [[["Passover-13"]], [["Passover-87"], "no_evidence"], [["Parsley-16"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest word in the English language?\nFacts: The longest word in the English language is \"the\". The longest word in the English language is \"the\". The longest word in the English language is \"the\". The longest word in the English language is \"the\". The longest word in the English language is \"the\". The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would you find a tibia beside parsley on a holiday plate?\nFacts: The tibia of a goat is eaten during Passover, a Jewish holiday Parsley is served on a Passover seder plate beside the goat shank \nAnswer (True or False):", "ct_subject": "The tibia of a goat is eaten during Passover, a Jewish holiday Parsley is served on a Passover seder plate beside the goat shank ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999785"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.98075587"], [" word", "0.9998784"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], [" is", "1.0"], [" \"", "0.99999535"], ["the", "0.9999485"], ["\".", "0.133387"], [" The", "0.97049576"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], [" is", "1.0"], [" \"", "1.0"], ["the", "1.0"], ["\".", "0.99999976"], [" The", "0.7627004"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], [" is", "1.0"], [" \"", "1.0"], ["the", "1.0"], ["\".", "1.0"], [" The", "0.9999993"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], [" is", "1.0"], [" \"", "1.0"], ["the", "1.0"], ["\".", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], [" is", "1.0"], [" \"", "1.0"], ["the", "1.0"], ["\".", "1.0"], [" The", "1.0"]]}, "849": {"qid": "d6ab89d323e040b5a28a", "term": "Groundhog Day", "description": "Traditional method of weather prediction", "question": "At Christmastime, do some films remind us that groundhog day is approaching?", "answer": true, "facts": ["Jack Frost is a 1979 stop motion Christmas film.", "In Jack Frost, the groundhog is a character and gets his own song reminding people of his own holiday."], "decomposition": ["What is the name of a stop motion Christmas film that was released in 1979?", "In #1, what does the groundhog get?", "Does #2 remind people of Groundhog Day?"], "evidence": [[[["Jack Frost (TV special)-1"]], [["Jack Frost (TV special)-6"]], ["operation"]], [[["Jack Frost (TV special)-1"]], [["Jack Frost (TV special)-2", "Jack Frost (TV special)-6"]], ["operation"]], [[["Jack Frost (TV special)-1"]], [["Jack Frost (TV special)-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the year 2000?\nFacts: The first person to be born in the year 2000 was born in the year 2000.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the year 2000?\nFacts", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: At Christmastime, do some films remind us that groundhog day is approaching?\nFacts: Jack Frost is a 1979 stop motion Christmas film. In Jack Frost, the groundhog is a character and gets his own song reminding people of his own holiday.\nAnswer (True or False):", "ct_subject": "Jack Frost is a 1979 stop motion Christmas film. In Jack Frost, the groundhog is a character and gets his own song reminding people of his own holiday.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99993765"], [" of", "1.0"], [" the", "1.0"], [" first", "0.52313006"], [" person", "0.9995066"], [" to", "1.0"], [" be", "0.9999734"], [" born", "0.99324507"], [" in", "0.9999995"], [" the", "1.0"], [" year", "0.999835"], [" 2000", "0.9604899"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "0.98139554"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "1.0"], [" born", "0.9902108"], [" in", "0.5034637"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"]]}, "850": {"qid": "86b893746c5eeece6760", "term": "Pearl hunting", "description": "Collecting pearls from wild mollusks", "question": "Would Michael Phelps be good at pearl hunting?", "answer": true, "facts": ["Pearl hunters swim underwater to collect pearls from oysters.", "Michael Phelps is the most decorated Olympic swimmer of all time."], "decomposition": ["What do pearl hunters do?", "What is Michael Phelps famous for?", "Does #2 help with accomplishing #1?"], "evidence": [[[["Pearl hunting-1"]], [["Michael Phelps-1"]], [["Pearl hunting-2"]]], [[["Pearl hunting-1"]], [["Michael Phelps-1"]], ["operation"]], [[["Pearl hunting-2"]], [["Michael Phelps-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the capital of the state of Texas?\nFacts: Austin\nAnswer (True or False): True\nQuestion: What is the name of the city in the United States that is the capital of the state of Texas?\nAnswer (True or False): Austin\nQuestion: What is the name of the city in the United States that is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Michael Phelps be good at pearl hunting?\nFacts: Pearl hunters swim underwater to collect pearls from oysters. Michael Phelps is the most decorated Olympic swimmer of all time.\nAnswer (True or False):", "ct_subject": "Pearl hunters swim underwater to collect pearls from oysters. Michael Phelps is the most decorated Olympic swimmer of all time.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999976"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99047285"], [" of", "1.0"], [" the", "1.0"], [" city", "0.280998"], [" in", "0.9830891"], [" the", "0.99999976"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "0.936801"], [" the", "1.0"], [" capital", "0.99998665"], [" of", "1.0"], [" the", "1.0"], [" state", "0.99991727"], [" of", "1.0"], [" Texas", "0.98180825"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Austin", "0.99819034"], ["\n", "0.7259796"], ["Answer", "1.0"], [" (", "0.9999995"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "0.99893373"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" Texas", "0.9999999"], ["?", "1.0"], ["\n", "1.0"], ["Answer", "0.30874357"], [" (", "0.99994814"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" Austin", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"]]}, "851": {"qid": "86f9275b46d101656634", "term": "Maritime pilot", "description": "mariner who manoeuvres ships through dangerous or congested waters", "question": "Can COVID-19 spread to maritime pilots?", "answer": true, "facts": ["Maritime pilots are human beings.", "COVID-19 can spread among human population. "], "decomposition": ["Which organisms are susceptible to COVID-19?", "Are maritime pilots one of #1?"], "evidence": [[[["Coronavirus disease 2019-1"], "no_evidence"], ["operation"]], [[["Coronavirus disease 2019-1"]], [["Maritime pilot-1"], "operation"]], [[["Coronavirus disease 2019-83"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Can COVID-19 spread to the United States?\nFacts: COVID-19 can spread to the United States. \nAnswer (True or False): True\nQuestion: Can COVID-19 spread to the United Kingdom?\nFacts: COVID-19 can spread to the United Kingdom. \nAnswer (True or False): True\nQuestion: Can COVID-19 spread to the United States?\nF", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can COVID-19 spread to maritime pilots?\nFacts: Maritime pilots are human beings. COVID-19 can spread among human population. \nAnswer (True or False):", "ct_subject": "Maritime pilots are human beings. COVID-19 can spread among human population. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Can", "0.9999963"], [" CO", "1.0"], ["VID", "1.0"], ["-", "1.0"], ["19", "1.0"], [" spread", "1.0"], [" to", "1.0"], [" the", "0.99966824"], [" United", "0.99999964"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" CO", "0.99999774"], ["VID", "1.0"], ["-", "1.0"], ["19", "1.0"], [" can", "1.0"], [" spread", "1.0"], [" to", "0.9999995"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [".", "1.0"], [" ", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Can", "1.0"], [" CO", "1.0"], ["VID", "1.0"], ["-", "1.0"], ["19", "1.0"], [" spread", "1.0"], [" to", "1.0"], [" the", "1.0"], [" United", "1.0"], [" Kingdom", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" CO", "1.0"], ["VID", "1.0"], ["-", "1.0"], ["19", "1.0"], [" can", "1.0"], [" spread", "1.0"], [" to", "1.0"], [" the", "1.0"], [" United", "1.0"], [" Kingdom", "1.0"], [".", "1.0"], [" ", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Can", "1.0"], [" CO", "1.0"], ["VID", "1.0"], ["-", "1.0"], ["19", "1.0"], [" spread", "1.0"], [" to", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "0.9999989"], ["\n", "1.0"], ["F", "1.0"]]}, "852": {"qid": "bbf5422e2b837a2e1ff6", "term": "B", "description": "letter in the Latin alphabet", "question": "Could B be mistaken for an Arabic numeral?", "answer": true, "facts": ["The letter 'B' resembles a figure-8 with a flattened left side.", "The Arabic numeral '8' is drawn as one large circle and a smaller circle immediately on top, intersecting each other. ", "A 'figure-8' is a shape consisting of two intersecting circles, the larger on the bottom."], "decomposition": ["Which figure is the letter B similar in appearance to?", "Is #1 an Arabic numeral?"], "evidence": [[[["8-1", "B-1"]], [["Arabic numerals-1"], "operation"]], [[["B-1", "Beta-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Bet (letter)-3"], "no_evidence"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: The first person to be born in the United States was a woman named Sarah Hensley.\nAnswer", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could B be mistaken for an Arabic numeral?\nFacts: The letter 'B' resembles a figure-8 with a flattened left side. The Arabic numeral '8' is drawn as one large circle and a smaller circle immediately on top, intersecting each other.  A 'figure-8' is a shape consisting of two intersecting circles, the larger on the bottom.\nAnswer (True or False):", "ct_subject": "The letter 'B' resembles a figure-8 with a flattened left side. The Arabic numeral '8' is drawn as one large circle and a smaller circle immediately on top, intersecting each other.  A 'figure-8' is a shape consisting of two intersecting circles, the larger on the bottom.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9995035"], [" is", "1.0"], [" the", "1.0"], [" name", "0.74006444"], [" of", "1.0"], [" the", "1.0"], [" first", "0.46834743"], [" person", "0.9903128"], [" to", "1.0"], [" be", "0.99954695"], [" born", "0.9999995"], [" in", "1.0"], [" the", "1.0"], [" United", "0.9999943"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9983835"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" a", "0.9423867"], [" woman", "0.99996173"], [" named", "1.0"], [" Sarah", "0.33491907"], [" H", "0.99989784"], ["ens", "1.0"], ["ley", "0.99999964"], [".", "0.9999999"], ["\n", "0.92446"], ["Answer", "1.0"]]}, "853": {"qid": "7b625d3d673751304dfe", "term": "Immanuel Kant", "description": "Prussian philosopher", "question": "Would Immanuel Kant be disgusted by the Black Lives Matter movement?", "answer": true, "facts": ["Immanuel Kant believed that Africans occupied the second lowest position on his racial hierarchy, below Whites and Asians.", "The Black Lives Matter movement advocates for racial equality and anti-racism."], "decomposition": ["What were Immanuel Kant's views on race?", "What are the main beliefs of the Black Lives Matter movement?", "Is #1 significantly different from #2?"], "evidence": [[["no_evidence"], [["Black Lives Matter-15"]], ["operation"]], [[["Scientific racism-41"]], [["Black Lives Matter-14", "Black Lives Matter-9"]], ["operation"]], [[["Scientific racism-1", "Scientific racism-41"]], [["Black Lives Matter-15"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: The first person executed in the United States was John Brown, who was hanged in 1859.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be executed in the United States?\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Immanuel Kant be disgusted by the Black Lives Matter movement?\nFacts: Immanuel Kant believed that Africans occupied the second lowest position on his racial hierarchy, below Whites and Asians. The Black Lives Matter movement advocates for racial equality and anti-racism.\nAnswer (True or False):", "ct_subject": "Immanuel Kant believed that Africans occupied the second lowest position on his racial hierarchy, below Whites and Asians. The Black Lives Matter movement advocates for racial equality and anti-racism.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999726"], [" is", "1.0"], [" the", "1.0"], [" name", "0.49257508"], [" of", "1.0"], [" the", "1.0"], [" first", "0.99949944"], [" person", "0.9793178"], [" to", "1.0"], [" be", "0.9982765"], [" executed", "0.9999865"], [" in", "0.9380212"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9996561"], [" first", "1.0"], [" person", "1.0"], [" executed", "0.9522325"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" John", "0.9998858"], [" Brown", "0.99999917"], [",", "0.9845222"], [" who", "1.0"], [" was", "1.0"], [" hanged", "1.0"], [" in", "0.99891365"], [" 18", "1.0"], ["59", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"]]}, "854": {"qid": "27aca37fe8213109c4ea", "term": "Burning Man", "description": "annual experimental festival based in Nevada, United States", "question": "Are people more likely than normal to get sunburn at Burning Man?", "answer": true, "facts": ["Burning Man often attracts lots of young people who are typically wearing minimal clothing due to the weather and for style. ", "Burning Man festivities occur in the hot summer sun and are often not in shaded areas."], "decomposition": ["What style of clothing do people wear to the burning man festival?", "Sun burning occurs easily while wearing what style of clothing?", "Is #1 and #2 the same?"], "evidence": [[[["Burning Man-37"], "no_evidence"], [["Nudity-1", "Swimsuit-2"], "no_evidence"], ["operation"]], [[["Burning Man-37"]], [["Sunburn-36"]], ["operation"]], [[["Burning Man-37"]], [["Bikini-71"]], [["Sunburn-26"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any other religions that are more likely to be true than the Christian religion?\nFacts: There are many religions that are more likely to be true than the Christian religion.\nAnswer (True or False): True\nQuestion: Are there any other religions that are more likely to be true than", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are people more likely than normal to get sunburn at Burning Man?\nFacts: Burning Man often attracts lots of young people who are typically wearing minimal clothing due to the weather and for style.  Burning Man festivities occur in the hot summer sun and are often not in shaded areas.\nAnswer (True or False):", "ct_subject": "Burning Man often attracts lots of young people who are typically wearing minimal clothing due to the weather and for style.  Burning Man festivities occur in the hot summer sun and are often not in shaded areas.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.99998283"], [" there", "1.0"], [" any", "0.80732536"], [" other", "0.9900489"], [" religions", "0.985437"], [" that", "0.9999958"], [" are", "0.9985226"], [" more", "0.95368016"], [" likely", "1.0"], [" to", "1.0"], [" be", "0.99784815"], [" true", "0.9999987"], [" than", "1.0"], [" the", "0.16151749"], [" Christian", "1.0"], [" religion", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "0.8845083"], [" are", "1.0"], [" many", "1.0"], [" religions", "0.48541674"], [" that", "1.0"], [" are", "1.0"], [" more", "0.9966468"], [" likely", "1.0"], [" to", "1.0"], [" be", "1.0"], [" true", "1.0"], [" than", "1.0"], [" the", "1.0"], [" Christian", "1.0"], [" religion", "1.0"], [".", "1.0"], ["\n", "0.9198957"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.999943"], [" there", "1.0"], [" any", "1.0"], [" other", "1.0"], [" religions", "1.0"], [" that", "1.0"], [" are", "1.0"], [" more", "1.0"], [" likely", "1.0"], [" to", "1.0"], [" be", "1.0"], [" true", "1.0"], [" than", "1.0"]]}, "855": {"qid": "614b682ff2b6ddd9ecba", "term": "Marco Polo", "description": "Italian explorer and merchant noted for travel to central and eastern Asia", "question": "Do you often hear Marco Polo's name shouted near water?", "answer": true, "facts": ["\"Marco Polo\" is a popular game among children and adults played while swimming.", "To play \"Marco Polo\", one person shouts \"Marco\" and the other shouts \"Polo\" while avoiding being tagged."], "decomposition": ["What is the game Marco Polo?", "When is #1 typically played?", "Does #2 occur near or in water?"], "evidence": [[[["Marco Polo (game)-1"]], [["Marco Polo (game)-1"]], [["Swimming pool-1"]]], [[["Marco Polo (game)-1", "Marco Polo (game)-2"]], ["no_evidence"], ["operation"]], [[["Marco Polo (game)-2"]], [["Marco Polo (game)-2"]], [["Marco Polo (game)-2"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance between two points on Earth?\nFacts: The distance between the Earth and the Moon is about 3,854,000 miles. The distance between the Earth and the Sun is about 93 million miles.\nAnswer (True or False): True\nQuestion: What is the longest distance", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do you often hear Marco Polo's name shouted near water?\nFacts: \"Marco Polo\" is a popular game among children and adults played while swimming. To play \"Marco Polo\", one person shouts \"Marco\" and the other shouts \"Polo\" while avoiding being tagged.\nAnswer (True or False):", "ct_subject": "\"Marco Polo\" is a popular game among children and adults played while swimming. To play \"Marco Polo\", one person shouts \"Marco\" and the other shouts \"Polo\" while avoiding being tagged.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99998176"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.064175986"], [" distance", "0.97685665"], [" between", "0.99793196"], [" two", "1.0"], [" points", "1.0"], [" on", "0.99897456"], [" Earth", "0.9939301"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" distance", "0.043816652"], [" between", "1.0"], [" the", "1.0"], [" Earth", "0.9995615"], [" and", "1.0"], [" the", "1.0"], [" Moon", "0.9999989"], [" is", "1.0"], [" about", "1.0"], [" 3", "0.35436463"], [",", "0.9319829"], ["8", "0.6632356"], ["54", "0.011186633"], [",", "1.0"], ["000", "1.0"], [" miles", "0.99988556"], [".", "0.9999993"], [" The", "0.68080896"], [" distance", "1.0"], [" between", "1.0"], [" the", "1.0"], [" Earth", "1.0"], [" and", "1.0"], [" the", "0.9999796"], [" Sun", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 93", "1.0"], [" million", "0.99999917"], [" miles", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "0.9999993"]]}, "856": {"qid": "3a20f77eb5aaebb051c7", "term": "D", "description": "letter in the Latin alphabet", "question": "Is the letter D influenced by the shape of ancient doors?", "answer": true, "facts": ["D is the fourth letter of the Latin alphabet", "D is a descendent of the ancient Phoenician Dalet", "Dalet was represented by a glyph of a door"], "decomposition": ["Which ancient language did the letter 'D' descend from?", "What was used to represent 'D' in #1?", "Was #2 a symbol of a door?"], "evidence": [[[["D-2"]], [["Dalet-2"]], [["Dalet-2"]]], [[["D-2"]], [["D-2"]], ["operation"]], [[["D-2"]], [["D-2"]], [["D-2", "Logogram-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: Thomas Jefferson was born in 1743.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: Thomas Jefferson was born in 1743.\nAnswer (True or", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is the letter D influenced by the shape of ancient doors?\nFacts: D is the fourth letter of the Latin alphabet D is a descendent of the ancient Phoenician Dalet Dalet was represented by a glyph of a door\nAnswer (True or False):", "ct_subject": "D is the fourth letter of the Latin alphabet D is a descendent of the ancient Phoenician Dalet Dalet was represented by a glyph of a door", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.7718569"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "0.96505964"], [" person", "0.82907504"], [" to", "1.0"], [" be", "0.99974173"], [" born", "0.9999379"], [" in", "1.0"], [" the", "1.0"], [" United", "0.9985752"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Thomas", "0.47920823"], [" Jefferson", "1.0"], [" was", "1.0"], [" born", "0.95528847"], [" in", "0.9947891"], [" 17", "0.99800724"], ["43", "1.0"], [".", "0.99984396"], ["\n", "0.58142626"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Thomas", "1.0"], [" Jefferson", "1.0"], [" was", "1.0"], [" born", "1.0"], [" in", "1.0"], [" 17", "1.0"], ["43", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"]]}, "857": {"qid": "d640dea4e362ceaa2a64", "term": "Acetylene", "description": "chemical compound", "question": "Did Julio Gonzalez like acetylene?", "answer": true, "facts": ["Julio Gonzalez was an artist who welded metal to create sculptures", "Welding is achieved by using a blowtorch on metal", "Blowtorches use acetylene as fuel"], "decomposition": ["What technique did Julio Gonzalez use to create his scultures?", "What is the main tool used for #1?", "What is a common fuel for #2?"], "evidence": [[[["Julio Gonz\u00e1lez (sculptor)-5"], "no_evidence"], [["Welding-10"]], [["Acetylene-14"], "operation"]], [[["Julio Gonz\u00e1lez (sculptor)-4"]], [["Oxy-fuel welding and cutting-3"]], [["Oxy-fuel welding and cutting-30"]]], [[["Julio Gonz\u00e1lez (sculptor)-1", "Julio Gonz\u00e1lez (sculptor)-4"]], [["Welding-10"]], [["Acetylene-14"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: The first person to be executed in the United States was John Brown. John Brown was hanged on May 1, 1859.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Julio Gonzalez like acetylene?\nFacts: Julio Gonzalez was an artist who welded metal to create sculptures Welding is achieved by using a blowtorch on metal Blowtorches use acetylene as fuel\nAnswer (True or False):", "ct_subject": "Julio Gonzalez was an artist who welded metal to create sculptures Welding is achieved by using a blowtorch on metal Blowtorches use acetylene as fuel", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.3453639"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9871027"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9638245"], [" person", "0.549089"], [" to", "1.0"], [" be", "0.96002805"], [" executed", "0.09876212"], [" in", "0.9992067"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9986833"], [" first", "1.0"], [" person", "1.0"], [" to", "0.90789574"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" John", "0.999987"], [" Brown", "1.0"], [".", "0.90746224"], [" John", "0.7172485"], [" Brown", "1.0"], [" was", "1.0"], [" hanged", "0.9998078"], [" on", "0.032179978"], [" May", "0.9999771"], [" 1", "0.9999999"], [",", "1.0"], [" 18", "1.0"], ["59", "1.0"], [".", "0.9768138"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"]]}, "858": {"qid": "9d5af50292804754a5d2", "term": "Supreme Court of Canada", "description": "highest court of Canada", "question": "Is clerk of Supreme Court of Canada safe profession for someone with seismophobia?", "answer": true, "facts": ["Seismophobia is the extreme fear of earthquakes.", "The Supreme Court of Canada is located in Ottawa.", "The Ottawa-Gattineau region is located far from active tectonic plates."], "decomposition": ["What is seismophobia a fear of?", "Movement of what causes #1?", "Where is the Supreme Court of Canada located?", "Is #3 located near active #2's?"], "evidence": [[[["2019\u201320 Puerto Rico earthquakes-23"]], [["Earthquake-3"]], ["no_evidence"], [["Earthquake-25"], "operation"]], [[["Earthquake-1"], "no_evidence"], [["Earthquake-3"]], [["Supreme Court of Canada-19"]], [["Ottawa-16"], "operation"]], [[["2019\u201320 Puerto Rico earthquakes-23"], "no_evidence"], [["Seismology-5"]], [["Supreme Court of Canada-19"]], [["Ottawa-16"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it true that the first person to be executed in the United States was a woman?\nFacts: The first person executed in the United States was a woman, Martha Washington.\nAnswer (True or False): True\nQuestion: Is it true that the first person to be executed in the United States was", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is clerk of Supreme Court of Canada safe profession for someone with seismophobia?\nFacts: Seismophobia is the extreme fear of earthquakes. The Supreme Court of Canada is located in Ottawa. The Ottawa-Gattineau region is located far from active tectonic plates.\nAnswer (True or False):", "ct_subject": "Seismophobia is the extreme fear of earthquakes. The Supreme Court of Canada is located in Ottawa. The Ottawa-Gattineau region is located far from active tectonic plates.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999989"], [" it", "0.99522877"], [" true", "1.0"], [" that", "1.0"], [" the", "1.0"], [" first", "0.99999857"], [" person", "0.9999901"], [" to", "1.0"], [" be", "0.99844474"], [" executed", "0.9999615"], [" in", "0.999759"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" a", "1.0"], [" woman", "0.9999999"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999993"], [" first", "1.0"], [" person", "1.0"], [" executed", "0.8136783"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" a", "1.0"], [" woman", "0.9987758"], [",", "0.88806313"], [" Martha", "0.8557565"], [" Washington", "0.9999999"], [".", "1.0"], ["\n", "0.9999958"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" true", "1.0"], [" that", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"]]}, "859": {"qid": "62239530032e5e88b8a7", "term": "Oceanography", "description": "The study of the physical and biological aspects of the ocean", "question": "Does an individual oceanographer study many sciences?", "answer": true, "facts": ["Study of the oceans involve many fields or science.", "To properly study their specific topic of research, an oceanographer must understand how their science interacts with the other involved sciences."], "decomposition": ["What other fields of science does oceanography cover?", "Does an individual oceanographer have to understand all of #1 to properly understand oceanography?"], "evidence": [[[["Oceanography-1"]], ["operation"]], [[["Oceanography-1"]], [["Oceanography-1"]]], [[["Oceanography-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a geologist and a geologist-at-large?\nFacts: A geologist is a scientist who studies the earth's crust and rocks. A geologist-at-large is a scientist who studies the earth's crust and rocks.\nAnswer (True or False): True\nQuestion: What is the difference between a ge", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does an individual oceanographer study many sciences?\nFacts: Study of the oceans involve many fields or science. To properly study their specific topic of research, an oceanographer must understand how their science interacts with the other involved sciences.\nAnswer (True or False):", "ct_subject": "Study of the oceans involve many fields or science. To properly study their specific topic of research, an oceanographer must understand how their science interacts with the other involved sciences.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99985945"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9997023"], [" between", "1.0"], [" a", "1.0"], [" ge", "0.24434416"], ["ologist", "1.0"], [" and", "1.0"], [" a", "0.9995808"], [" ge", "1.0"], ["ologist", "0.99964464"], ["-", "0.96033734"], ["at", "1.0"], ["-", "1.0"], ["large", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9989385"], [" ge", "1.0"], ["ologist", "1.0"], [" is", "0.9903278"], [" a", "1.0"], [" scientist", "0.6915393"], [" who", "1.0"], [" studies", "1.0"], [" the", "0.99918574"], [" earth", "0.99996805"], ["'s", "0.9999995"], [" crust", "0.9996933"], [" and", "0.9944133"], [" rocks", "0.33705541"], [".", "1.0"], [" A", "1.0"], [" ge", "1.0"], ["ologist", "1.0"], ["-", "1.0"], ["at", "1.0"], ["-", "1.0"], ["large", "1.0"], [" is", "1.0"], [" a", "1.0"], [" scientist", "1.0"], [" who", "1.0"], [" studies", "1.0"], [" the", "1.0"], [" earth", "1.0"], ["'s", "1.0"], [" crust", "0.99999976"], [" and", "1.0"], [" rocks", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" ge", "1.0"]]}, "860": {"qid": "bf5f8d8dc96fe5e04f4b", "term": "Cooper (profession)", "description": "Maker of staved vessels such as barrels", "question": "Are coopers required in the beverage industry?", "answer": true, "facts": ["Coopers make barrels.", "Barrels are used to store certain alcoholic beverages during production."], "decomposition": ["What liquids are barrels made for?", "Are any of #1 part of the beverage industry?"], "evidence": [[[["Barrel-2"]], [["Drink-1"], "operation"]], [[["Barrel-4"]], [["Sake-1"], "operation"]], [[["Barrel-1", "Barrel-2", "Barrel-4"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any other animals that have been used as food?\nFacts: There are many animals that have been used as food.\nAnswer (True or False): True\nQuestion: Are there any other animals that have been used as food?\nFacts: There are many animals that have been used as food.\nAnswer (True or False): True\nQuestion: Are there any other animals that have been used as food?\nF", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are coopers required in the beverage industry?\nFacts: Coopers make barrels. Barrels are used to store certain alcoholic beverages during production.\nAnswer (True or False):", "ct_subject": "Coopers make barrels. Barrels are used to store certain alcoholic beverages during production.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.99922633"], [" there", "1.0"], [" any", "0.9992588"], [" other", "0.99977237"], [" animals", "1.0"], [" that", "1.0"], [" have", "0.9238861"], [" been", "1.0"], [" used", "0.2628986"], [" as", "0.9934517"], [" food", "0.88583195"], ["?", "0.999992"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "0.6772539"], [" are", "1.0"], [" many", "0.9999999"], [" animals", "0.6825837"], [" that", "1.0"], [" have", "0.9999999"], [" been", "1.0"], [" used", "1.0"], [" as", "1.0"], [" food", "1.0"], [".", "1.0"], ["\n", "0.99913317"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "1.0"], [" any", "1.0"], [" other", "0.99908173"], [" animals", "1.0"], [" that", "1.0"], [" have", "1.0"], [" been", "1.0"], [" used", "1.0"], [" as", "1.0"], [" food", "0.9999225"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "1.0"], [" are", "1.0"], [" many", "1.0"], [" animals", "1.0"], [" that", "1.0"], [" have", "1.0"], [" been", "1.0"], [" used", "1.0"], [" as", "1.0"], [" food", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "1.0"], [" any", "1.0"], [" other", "1.0"], [" animals", "1.0"], [" that", "1.0"], [" have", "1.0"], [" been", "1.0"], [" used", "1.0"], [" as", "1.0"], [" food", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"]]}, "861": {"qid": "01254b50648f4fb08b51", "term": "American black bear", "description": "species of bear", "question": "Could two newborn American Black Bear cubs fit on a king size bed?", "answer": true, "facts": ["The average size of an American Black Bear cub is only 8 inches at birth.", "King size beds are 76\"x80\" in size."], "decomposition": ["What is the average length of an American Black Bear cub?", "What is the size of a king bed?", "Is two times #1 smaller than #2?"], "evidence": [[[["American black bear-29"]], [["Bed size-6"]], ["operation"]], [[["American black bear-19"]], [["Bed size-17"]], [["Bed size-17"], "operation"]], [[["American black bear-29"]], [["Bed size-23"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human?\nFacts: The average weight of a human is about 200 pounds.\nAnswer (True or False): True\nQuestion: What is the average height of a human?\nFacts: The average height of a human is about 5 feet, 8 inches.\nAnswer (True or False): True\nQuestion:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could two newborn American Black Bear cubs fit on a king size bed?\nFacts: The average size of an American Black Bear cub is only 8 inches at birth. King size beds are 76\"x80\" in size.\nAnswer (True or False):", "ct_subject": "The average size of an American Black Bear cub is only 8 inches at birth. King size beds are 76\"x80\" in size.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99997354"], [" is", "1.0"], [" the", "1.0"], [" average", "0.99973387"], [" weight", "0.9999999"], [" of", "1.0"], [" a", "0.999998"], [" human", "0.99999857"], ["?", "0.9195812"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" weight", "0.99999964"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" is", "1.0"], [" about", "0.9989813"], [" 200", "0.99707603"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" height", "0.75452816"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" height", "1.0"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 5", "0.9999459"], [" feet", "0.9134766"], [",", "0.96924937"], [" 8", "0.9657083"], [" inches", "1.0"], [".", "0.99997425"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"]]}, "862": {"qid": "7dcf18b23f679bc5a7cc", "term": "Gandalf", "description": "Fictional character created by J. R. R. Tolkien", "question": "Gandalf hypothetically defeats Rincewind in a wizard battle?", "answer": true, "facts": ["Gandalf is a 2000 plus year old wizard that has fought orcs and spirits in Middle Earth.", "Rincewind is the protagonist of the Discworld series.", "Rincewind is a failed student at the Unseen University for wizards in Ankh-Morpork.", "Rincewind is described by other wizards as the magical equivalent to the number zero."], "decomposition": ["How powerful is Gandalf as portrayed in LOTR?", "How powerful is Rincewind as portrayed at the Unseen University for wizards?", "Does #1 include far more experience and accomplishments than #2?"], "evidence": [[[["Gandalf-2"]], [["Rincewind-1"]], ["no_evidence"]], [[["Gandalf-2"], "no_evidence"], [["Rincewind-1"]], ["operation"]], [[["Gandalf-2"]], [["Rincewind-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to ever be born in the year 2000?\nFacts: The first person to ever be born in the year 2000 was", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Gandalf hypothetically defeats Rincewind in a wizard battle?\nFacts: Gandalf is a 2000 plus year old wizard that has fought orcs and spirits in Middle Earth. Rincewind is the protagonist of the Discworld series. Rincewind is a failed student at the Unseen University for wizards in Ankh-Morpork. Rincewind is described by other wizards as the magical equivalent to the number zero.\nAnswer (True or False):", "ct_subject": "Gandalf is a 2000 plus year old wizard that has fought orcs and spirits in Middle Earth. Rincewind is the protagonist of the Discworld series. Rincewind is a failed student at the Unseen University for wizards in Ankh-Morpork. Rincewind is described by other wizards as the magical equivalent to the number zero.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99355596"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9849155"], [" person", "0.965822"], [" to", "1.0"], [" ever", "0.9981371"], [" be", "1.0"], [" born", "0.9999999"], [" in", "0.9995338"], [" the", "0.9999993"], [" year", "1.0"], [" 2000", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "0.999653"], [" person", "1.0"], [" to", "1.0"], [" ever", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "0.9999999"]]}, "863": {"qid": "c253256fe0d8014da333", "term": "Alfa Romeo", "description": "Italian automotive manufacturer", "question": "Would an Alfa Romeo vehicle fit inside a barn?", "answer": true, "facts": ["Alfa Romeo makes cars.", "Barns are large enough to hold a car."], "decomposition": ["What is the average length of an Alfa Romeo?", "What is the average size of a barn?", "Is #1 smaller than #2?"], "evidence": [[[["Alfa Romeo-46"], "no_evidence"], ["no_evidence"], ["operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Alfa Romeo-1"], "no_evidence"], [["Barn-1"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance between two points on the earth?\nFacts: The earth is a sphere. The distance between the North Pole and the South Pole is about 24,000 miles.\nAnswer (True or False): True\nQuestion: What is the longest distance between two points on the earth?\nFacts: The earth is a sphere. The distance between the North Pole and the South Pole is about 24,000 miles.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would an Alfa Romeo vehicle fit inside a barn?\nFacts: Alfa Romeo makes cars. Barns are large enough to hold a car.\nAnswer (True or False):", "ct_subject": "Alfa Romeo makes cars. Barns are large enough to hold a car.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999976"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.037988067"], [" distance", "0.68942976"], [" between", "0.99999976"], [" two", "1.0"], [" points", "1.0"], [" on", "0.9984919"], [" the", "0.9702819"], [" earth", "0.9997298"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" earth", "0.9999999"], [" is", "1.0"], [" a", "0.99999964"], [" sphere", "1.0"], [".", "1.0"], [" The", "0.999984"], [" distance", "0.9999999"], [" between", "1.0"], [" the", "0.99931145"], [" North", "0.9977381"], [" Pole", "1.0"], [" and", "1.0"], [" the", "1.0"], [" South", "1.0"], [" Pole", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 24", "0.9966978"], [",", "1.0"], ["000", "1.0"], [" miles", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "0.9999987"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"], [" on", "1.0"], [" the", "1.0"], [" earth", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" earth", "1.0"], [" is", "1.0"], [" a", "1.0"], [" sphere", "1.0"], [".", "1.0"], [" The", "1.0"], [" distance", "1.0"], [" between", "1.0"], [" the", "1.0"], [" North", "1.0"], [" Pole", "1.0"], [" and", "1.0"], [" the", "1.0"], [" South", "1.0"], [" Pole", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 24", "1.0"], [",", "1.0"], ["000", "1.0"], [" miles", "1.0"], [".", "1.0"]]}, "864": {"qid": "a87b94672fd08c0a2f0e", "term": "United States Military Academy", "description": "U.S. Army's federal service academy in West Point, New York", "question": "Would the United States Military Academy reject an applicant with multiple sclerosis?", "answer": true, "facts": ["Multiple Sclerosis is a progressive condition affecting the brain and spinal chord.", "The US Military Academy does not give waivers for serious progressive conditions."], "decomposition": ["What kind of condition is Multiple Sclerosis?", "Would the US Military Academy have to reject someone with #1?"], "evidence": [[[["Multiple sclerosis-5"]], [["United States Military Academy-36"], "no_evidence"]], [[["Multiple sclerosis-1"]], [["United States Naval Academy-99"], "operation"]], [[["Multiple sclerosis-59"], "operation"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average life expectancy of a person born in the United States?\nFacts: The average life expectancy of a person born in the United States is 78.5 years.\nAnswer (True or False): True\nQuestion: What is the average life expectancy of a person born in the United States?\nFacts: The average life expectancy of a person born in", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would the United States Military Academy reject an applicant with multiple sclerosis?\nFacts: Multiple Sclerosis is a progressive condition affecting the brain and spinal chord. The US Military Academy does not give waivers for serious progressive conditions.\nAnswer (True or False):", "ct_subject": "Multiple Sclerosis is a progressive condition affecting the brain and spinal chord. The US Military Academy does not give waivers for serious progressive conditions.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99875"], [" is", "1.0"], [" the", "1.0"], [" average", "0.9998716"], [" life", "0.9865094"], [" expectancy", "0.9999565"], [" of", "1.0"], [" a", "1.0"], [" person", "0.73023146"], [" born", "0.9533538"], [" in", "1.0"], [" the", "1.0"], [" United", "0.9999914"], [" States", "1.0"], ["?", "0.9990565"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" life", "1.0"], [" expectancy", "1.0"], [" of", "0.9993556"], [" a", "1.0"], [" person", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "1.0"], [" 78", "1.0"], [".", "0.9989336"], ["5", "0.45076492"], [" years", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" life", "1.0"], [" expectancy", "1.0"], [" of", "1.0"], [" a", "1.0"], [" person", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" life", "1.0"], [" expectancy", "1.0"], [" of", "1.0"], [" a", "1.0"], [" person", "1.0"], [" born", "1.0"], [" in", "1.0"]]}, "865": {"qid": "bea6df46c8f218b4b5fb", "term": "Game engine", "description": "Software-development environment designed for building video games", "question": "Does Adobe Suite have video game engine coding?", "answer": true, "facts": ["Adobe applications runs on the C++ framework.", "Many video games are run on Unity game engine.", "The Unity game engine is a C++ coded engine."], "decomposition": ["What framework does Adobe Suite run on?", "What game engine do most video games run on?", "What type of engine is #2?", "Is the framework for #1 the same as the engine for #3?"], "evidence": [[[["Adobe Creative Suite-1", "Starling Framework-2"]], [["Starling Framework-1"], "no_evidence"], [["Starling Framework-3"], "no_evidence"], ["operation"]], [[["Adobe Creative Suite-1"], "no_evidence"], [["Game engine-27"], "no_evidence"], [["Game engine-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Adobe Creative Suite-1", "C++-2"], "no_evidence"], [["Unreal Engine-1"]], [["Unreal Engine-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is there a difference between the word \"citizen\" and \"citizen of the world\"?\nFacts: The word \"citizen\" is a legal term. The word \"citizen of the world\" is a political term.\nAnswer (True or False): True\nQuestion: Is there a difference between the word \"citizen\" and \"citizen of the world", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Adobe Suite have video game engine coding?\nFacts: Adobe applications runs on the C++ framework. Many video games are run on Unity game engine. The Unity game engine is a C++ coded engine.\nAnswer (True or False):", "ct_subject": "Adobe applications runs on the C++ framework. Many video games are run on Unity game engine. The Unity game engine is a C++ coded engine.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.34644282"], [" there", "0.28477407"], [" a", "1.0"], [" difference", "0.9999994"], [" between", "1.0"], [" the", "0.9791445"], [" word", "0.9999342"], [" \"", "1.0"], ["c", "0.99584275"], ["itizen", "0.9812469"], ["\"", "1.0"], [" and", "1.0"], [" \"", "0.99983096"], ["c", "1.0"], ["itizen", "1.0"], [" of", "1.0"], [" the", "1.0"], [" world", "0.99999976"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999511"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["itizen", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "0.6038589"], [" legal", "0.98398453"], [" term", "1.0"], [".", "0.9999975"], [" The", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["itizen", "1.0"], [" of", "1.0"], [" the", "1.0"], [" world", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" political", "0.99993074"], [" term", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.95078427"], [" there", "1.0"], [" a", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["itizen", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["itizen", "1.0"], [" of", "1.0"], [" the", "1.0"], [" world", "1.0"]]}, "866": {"qid": "9a7a4669a295c5f3ebea", "term": "Hunger", "description": "Sustained inability to eat sufficient food", "question": "Was Jean Valjean imprisoned due to hunger?", "answer": true, "facts": ["Jean Valjean was sentenced to imprisonment due to theft of property.", "The item Jean Valjean stole was a loaf of bread for his family."], "decomposition": ["What crime was Jean Valjean convicted of?", "What did Jean Valjean gain from #1?", "Who did he give #2 to?", "Is hunger experienced by #3 the main reason for wanting #2?"], "evidence": [[[["Jean Valjean-1"]], [["Jean Valjean-1"]], [["Jean Valjean-1"]], ["operation"]], [[["Jean Valjean-1"]], [["Jean Valjean-1"]], [["Jean Valjean-1"]], ["operation"]], [[["Jean Valjean-6"]], [["Jean Valjean-7"]], ["no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the American Revolution start with the signing of the Declaration of Independence?\nFacts: The American Revolution started with the signing of the Declaration of Independence.\nAnswer (True or False): True\nQuestion: Did the American Revolution end with the signing of the Treaty of Paris?\nFacts: The American Revolution ended with the signing of the Treaty of Paris.\nAnswer (", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Jean Valjean imprisoned due to hunger?\nFacts: Jean Valjean was sentenced to imprisonment due to theft of property. The item Jean Valjean stole was a loaf of bread for his family.\nAnswer (True or False):", "ct_subject": "Jean Valjean was sentenced to imprisonment due to theft of property. The item Jean Valjean stole was a loaf of bread for his family.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9979917"], [" the", "1.0"], [" American", "0.61414534"], [" Revolution", "0.9999893"], [" start", "0.996454"], [" with", "0.9999769"], [" the", "0.9999999"], [" signing", "0.3782222"], [" of", "1.0"], [" the", "1.0"], [" Declaration", "1.0"], [" of", "1.0"], [" Independence", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" American", "0.9973948"], [" Revolution", "1.0"], [" started", "1.0"], [" with", "1.0"], [" the", "1.0"], [" signing", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Declaration", "1.0"], [" of", "1.0"], [" Independence", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999423"], [" the", "1.0"], [" American", "1.0"], [" Revolution", "1.0"], [" end", "1.0"], [" with", "1.0"], [" the", "1.0"], [" signing", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Treaty", "0.99999976"], [" of", "1.0"], [" Paris", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" American", "1.0"], [" Revolution", "1.0"], [" ended", "1.0"], [" with", "1.0"], [" the", "1.0"], [" signing", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Treaty", "1.0"], [" of", "1.0"], [" Paris", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"]]}, "867": {"qid": "c864fb965099ee7f6ebd", "term": "Hypothermia", "description": "A human body core temperature below 35.0\u00b0C", "question": "Would someone on Venus be unlikely to experience hypothermia?", "answer": true, "facts": ["Hypothermia typically occurs from exposure to extreme cold.", "The average surface temperature on Venus is 863\u00b0F.", "A warmer surface temperature on the planet will result in a higher body temperature for people on that planet."], "decomposition": ["What is the average surface temperature on Venus?", "In order for the human body to experience hypothermia, it would have to be exposed to temperature that are what in relation to body temp?", "What is human body temperature?", "Does #1 meet the condition of #2 relative to #3?"], "evidence": [[[["Venus-23"]], [["Hypothermia-1"]], [["Human body temperature-4"]], ["operation"]], [[["Venus-2"]], [["Hypothermia-2"]], [["Human body temperature-7"]], ["operation"]], [[["Venus-19"]], [["Hypothermia-5"]], [["Human body temperature-4"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average temperature on Venus?\nFacts: Venus is a hot planet. The average surface temperature on Venus is 863\u00b0F.\nAnswer (True or False): True\nQuestion: What is the average temperature on Mars?\nFacts: Mars is a cold planet. The average", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would someone on Venus be unlikely to experience hypothermia?\nFacts: Hypothermia typically occurs from exposure to extreme cold. The average surface temperature on Venus is 863\u00b0F. A warmer surface temperature on the planet will result in a higher body temperature for people on that planet.\nAnswer (True or False):", "ct_subject": "Hypothermia typically occurs from exposure to extreme cold. The average surface temperature on Venus is 863\u00b0F. A warmer surface temperature on the planet will result in a higher body temperature for people on that planet.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99760723"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" temperature", "0.9999999"], [" on", "1.0"], [" Venus", "0.91819644"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Venus", "1.0"], [" is", "0.33898944"], [" a", "0.9999968"], [" hot", "0.9987404"], [" planet", "0.99993336"], [".", "0.9999995"], [" The", "1.0"], [" average", "1.0"], [" surface", "1.0"], [" temperature", "1.0"], [" on", "0.65939146"], [" Venus", "1.0"], [" is", "1.0"], [" 8", "1.0"], ["63", "1.0"], ["\u00b0", "1.0"], ["F", "1.0"], [".", "1.0"], ["\n", "0.9667526"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999976"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" temperature", "0.99843043"], [" on", "1.0"], [" Mars", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Mars", "1.0"], [" is", "1.0"], [" a", "1.0"], [" cold", "0.99999726"], [" planet", "1.0"], [".", "1.0"], [" The", "1.0"], [" average", "1.0"]]}, "868": {"qid": "7cfc1003f0a479b3487c", "term": "Jack Black", "description": "American actor, comedian, musician, music producer and youtuber.", "question": "Is Jack Black unlikely to compete with Bear McCreary for an award?", "answer": true, "facts": ["Jack Black is a musician but not a composer", "Bear McCreary is a composer", "Their interests are similar but their skills not overlap in awards categories"], "decomposition": ["What music-related occupation does Bear McCreary have?", "What types of awards are won by notable figures who work as #1? ", "What music-related occupation does Jack Black have?", "What types of awards have been won by notable figures who work as #3?", "Are #2 and #4 separate categories of awards?"], "evidence": [[[["Bear McCreary-1"]], [["American Society of Composers, Authors and Publishers-18"]], [["The Pick of Destiny-8"]], [["Jack Black-24"]], [["American Society of Composers, Authors and Publishers-18", "Jack Black-24"]]], [[["Bear McCreary-1"]], [["American Society of Composers, Authors and Publishers-18", "International Film Music Critics Association-1"], "no_evidence"], [["Jack Black-1"]], [["Grammy Award-1", "Grammy Award-2"], "no_evidence"], ["no_evidence", "operation"]], [[["Bear McCreary-1"]], [["Bear McCreary-2"]], [["Jack Black-16"]], [["Grammy Award for Best Hard Rock Performance-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city of New York City derived from the name of the city of New York?\nFacts: The name of the city of New York City is derived from the name of the city of New York. The name of the city of New York City is derived from the name of the city of New York.\nAnswer (True or False", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Jack Black unlikely to compete with Bear McCreary for an award?\nFacts: Jack Black is a musician but not a composer Bear McCreary is a composer Their interests are similar but their skills not overlap in awards categories\nAnswer (True or False):", "ct_subject": "Jack Black is a musician but not a composer Bear McCreary is a composer Their interests are similar but their skills not overlap in awards categories", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9857297"], [" the", "0.8314575"], [" name", "0.8199293"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9999982"], [" of", "0.9999993"], [" New", "0.99888235"], [" York", "1.0"], [" City", "1.0"], [" derived", "0.9823905"], [" from", "1.0"], [" the", "1.0"], [" name", "0.99999964"], [" of", "1.0"], [" the", "0.99999964"], [" city", "1.0"], [" of", "1.0"], [" New", "0.99978"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99999976"], [" name", "1.0"], [" of", "1.0"], [" the", "0.99999654"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" City", "0.9997148"], [" is", "0.99987257"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [".", "0.9999981"], [" The", "0.97156435"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" City", "0.95923674"], [" is", "0.9999993"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [".", "1.0"], ["\n", "0.99965024"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"]]}, "869": {"qid": "66101dcd75caa3fe265d", "term": "Solubility", "description": "Capacity of a designated solvent to hold a designated solute in homogeneous solution under specified conditions", "question": "Does Nigella Lawson care about solubility?", "answer": true, "facts": ["Nigella Lawson is a chef", "Chefs are concerned with cooking processes and nutrition", "Solubility plays a role in both the chemistry of cooking processes as well as the body's interaction with substances that it ingests"], "decomposition": ["What is Nigella Lawson's major occupation?", "What kind of substances and processes is the concept of solubility applicable to?", "What kind of substances and processes are of importance to #1?", "Are any of #2 included in #3?"], "evidence": [[[["Nigella Lawson-1"]], [["Solubility-1"], "no_evidence"], [["Cooking-34"], "no_evidence"], ["operation"]], [[["Nigella Lawson-1"]], [["Solubility-1"]], [["Cooking-17"]], ["operation"]], [[["Nigella Lawson-1"]], [["Solubility-1"], "no_evidence"], [["Cooking-14"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"salt\" and a \"salt and pepper\"?\nFacts: The difference between a \"salt\" and a \"salt and pepper\" is that the former is a salt that is used to season food and the latter is a salt that is used to season food.\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Nigella Lawson care about solubility?\nFacts: Nigella Lawson is a chef Chefs are concerned with cooking processes and nutrition Solubility plays a role in both the chemistry of cooking processes as well as the body's interaction with substances that it ingests\nAnswer (True or False):", "ct_subject": "Nigella Lawson is a chef Chefs are concerned with cooking processes and nutrition Solubility plays a role in both the chemistry of cooking processes as well as the body's interaction with substances that it ingests", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.93446934"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.99999917"], [" between", "1.0"], [" a", "0.99999976"], [" \"", "1.0"], ["s", "0.9972873"], ["alt", "0.99999523"], ["\"", "0.99999976"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["alt", "1.0"], [" and", "0.9930084"], [" pepper", "1.0"], ["\"?", "0.9512223"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9628515"], [" difference", "0.7105845"], [" between", "0.9999995"], [" a", "1.0"], [" \"", "0.99977714"], ["s", "1.0"], ["alt", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["alt", "1.0"], [" and", "1.0"], [" pepper", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" that", "1.0"], [" the", "0.99574536"], [" former", "0.4037788"], [" is", "1.0"], [" a", "1.0"], [" salt", "1.0"], [" that", "1.0"], [" is", "0.9932954"], [" used", "0.9804113"], [" to", "0.999326"], [" season", "0.9999994"], [" food", "0.99999857"], [" and", "0.5286398"], [" the", "1.0"], [" latter", "1.0"], [" is", "1.0"], [" a", "1.0"], [" salt", "0.9970886"], [" that", "1.0"], [" is", "1.0"], [" used", "1.0"], [" to", "1.0"], [" season", "0.9882058"], [" food", "1.0"], [".", "1.0"], ["\n", "1.0"]]}, "870": {"qid": "3ed6c807bfa51e5577d1", "term": "Daytona 500", "description": "Auto race held in Daytona, Florida, United States", "question": "Can E6000 cure before a hoverboard finishes the Daytona 500? ", "answer": true, "facts": ["The Daytona 500 is 500 miles", "A hoverboard can move at six to eight miles per hour", "E6000 fully cures in 24 to 72 hours"], "decomposition": ["How long is the Daytona 500?", "How fast can a hoverboard move in hours?", "What is #1 divided by #2?", "How many hours does it take for a E6000 to cure?", "Is #4 more less than #3?"], "evidence": [[[["Daytona 500-1"]], [["Franky Zapata-12"], "no_evidence"], ["operation"], [["Conroe (microprocessor)-8"], "no_evidence"], ["operation"]], [[["Daytona 500-1"]], [["Self-balancing scooter-9"]], ["operation"], [["Adhesive-28"], "no_evidence"], ["operation"]], [[["Daytona 500-1"]], [["Hoverboard-18"]], ["operation"], [["Adhesive-42"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the most expensive item in the world?\nFacts: The most expensive item in the world is the diamond ring of Princess Diana. The diamond ring cost $1.5 million.\nAnswer (True or False): True\nQuestion: What is the most expensive item in the world?\nFacts: The most expensive item in the world is the diamond ring", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can E6000 cure before a hoverboard finishes the Daytona 500? \nFacts: The Daytona 500 is 500 miles A hoverboard can move at six to eight miles per hour E6000 fully cures in 24 to 72 hours\nAnswer (True or False):", "ct_subject": "The Daytona 500 is 500 miles A hoverboard can move at six to eight miles per hour E6000 fully cures in 24 to 72 hours", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9966695"], [" is", "1.0"], [" the", "1.0"], [" most", "0.87142736"], [" expensive", "0.99907136"], [" item", "0.9999981"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" most", "1.0"], [" expensive", "1.0"], [" item", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" is", "1.0"], [" the", "0.99781275"], [" diamond", "0.9999721"], [" ring", "1.0"], [" of", "1.0"], [" Princess", "1.0"], [" Diana", "1.0"], [".", "0.9999993"], [" The", "0.9922126"], [" diamond", "0.999869"], [" ring", "1.0"], [" cost", "0.9822735"], [" $", "1.0"], ["1", "0.9999403"], [".", "0.99728966"], ["5", "0.9999951"], [" million", "1.0"], [".", "1.0"], ["\n", "0.99914765"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" most", "1.0"], [" expensive", "1.0"], [" item", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" most", "1.0"], [" expensive", "1.0"], [" item", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" is", "1.0"], [" the", "1.0"], [" diamond", "1.0"], [" ring", "1.0"]]}, "871": {"qid": "98c00a69ee9fd33c60f7", "term": "Pope", "description": "Leader of the Catholic Church", "question": "Could the Pope be on an episode of Pimp My Ride?", "answer": true, "facts": ["Pimp My Ride is a show where people's cars are upgraded and improved", "The Pope has a personal vehicle called the Popemobile"], "decomposition": ["In the show Pimp My Ride, what type of vehicles are upgraded and improved?", "Does the Pope have #1?"], "evidence": [[[["Pimp My Ride-5"]], [["Landaulet (car)-7"]]], [[["Pimp My Ride-1"]], [["Popemobile-1"]]], [[["Pimp My Ride-1"]], [["Popemobile-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the song that was played at the end of the movie \"The Godfather\"\nFacts: \"The Godfather\" is a movie about a mob boss who is trying to take over the world. The song played at the end of the movie is \"The Godfather Theme\"\nAnswer (True or False): True\nQuestion: What is the name", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could the Pope be on an episode of Pimp My Ride?\nFacts: Pimp My Ride is a show where people's cars are upgraded and improved The Pope has a personal vehicle called the Popemobile\nAnswer (True or False):", "ct_subject": "Pimp My Ride is a show where people's cars are upgraded and improved The Pope has a personal vehicle called the Popemobile", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99998736"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9986547"], [" of", "1.0"], [" the", "1.0"], [" song", "0.93169975"], [" that", "0.9859613"], [" was", "0.55924165"], [" played", "0.99999976"], [" at", "0.99781656"], [" the", "1.0"], [" end", "1.0"], [" of", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "0.9988657"], ["The", "1.0"], [" God", "0.99995446"], ["father", "1.0"], ["\"", "0.9586993"], ["\n", "0.98776054"], ["F", "0.9999999"], ["acts", "1.0"], [":", "1.0"], [" \"", "0.9999927"], ["The", "0.9999999"], [" God", "1.0"], ["father", "1.0"], ["\"", "1.0"], [" is", "0.99999404"], [" a", "1.0"], [" movie", "0.9722625"], [" about", "1.0"], [" a", "0.9615659"], [" mob", "0.999241"], [" boss", "0.2534133"], [" who", "0.9995351"], [" is", "0.9999999"], [" trying", "0.9407777"], [" to", "1.0"], [" take", "0.9997383"], [" over", "1.0"], [" the", "1.0"], [" world", "1.0"], [".", "1.0"], [" The", "1.0"], [" song", "1.0"], [" played", "0.95998544"], [" at", "1.0"], [" the", "1.0"], [" end", "1.0"], [" of", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" is", "1.0"], [" \"", "0.99998474"], ["The", "0.99998355"], [" God", "0.99993813"], ["father", "1.0"], [" Theme", "0.9999989"], ["\"", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"]]}, "872": {"qid": "0a1946f12a7b7dbbf9cd", "term": "United States Marine Corps", "description": "Amphibious warfare branch of the United States Armed Forces", "question": "Would a recruit for the United States Marine Corps be turned away for self harm?", "answer": true, "facts": ["Self harm is when someone intentionally causes injury or pain to themselves.", "Self harm is almost always related to a mental health condition.", "Those experiencing mental wellness related illnesses cannot join the Marines."], "decomposition": ["What medical condition is self harm usually related to?", "Are people who suffer from #1 turned away from the US Marine Corps?"], "evidence": [[[["Self-harm-3"]], [["United States Marine Corps-2"], "no_evidence", "operation"]], [[["Self-harm-22"]], ["no_evidence"]], [[["Self-harm-16", "Self-harm-17"]], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average life span of a human being?\nFacts: The average life span of a human being is about 100 years.\nAnswer (True or False): True\nQuestion: What is the average height of a human being?\nFacts: The average height of a human being is about 5 feet", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a recruit for the United States Marine Corps be turned away for self harm?\nFacts: Self harm is when someone intentionally causes injury or pain to themselves. Self harm is almost always related to a mental health condition. Those experiencing mental wellness related illnesses cannot join the Marines.\nAnswer (True or False):", "ct_subject": "Self harm is when someone intentionally causes injury or pain to themselves. Self harm is almost always related to a mental health condition. Those experiencing mental wellness related illnesses cannot join the Marines.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.999741"], [" is", "1.0"], [" the", "1.0"], [" average", "0.93514043"], [" life", "0.99184096"], [" span", "0.4150174"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "0.99417603"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" life", "0.99833506"], [" span", "1.0"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "1.0"], [" is", "1.0"], [" about", "0.99570376"], [" 100", "0.9999118"], [" years", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" height", "0.666552"], [" of", "1.0"], [" a", "1.0"], [" human", "0.9999987"], [" being", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" height", "1.0"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "1.0"], [" is", "1.0"], [" about", "0.9999926"], [" 5", "0.99999964"], [" feet", "0.9844825"]]}, "873": {"qid": "4173af9c4eeb33828e37", "term": "Frost", "description": "coating or deposit of ice that may form in humid air in cold conditions, usually overnight", "question": "Would it be unusual to see frost in September in Texas?", "answer": true, "facts": ["Texas is a Southern state of the United States, known for high heat.", "On average, Texas is between 68 and 89 degrees during the month of September.", "Frost forms at 32 degrees or lower."], "decomposition": ["What are the average temperatures in Texas during the month of September?", "What temperature does frost form at?", "Is #1 warmer than #2?"], "evidence": [[[["Climate of Dallas-3"], "no_evidence"], [["Frost (temperature)-1", "Frost-1"]], ["operation"]], [[["Texas-29", "Texas-30", "Texas-31"]], [["Dew point-1", "Frost-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Climate of Texas-3"], "no_evidence"], [["Dew point-1", "Frost-20", "Frost-5"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average temperature in the United States?\nFacts: The average temperature in the United States is 68 degrees.\nAnswer (True or False): True\nQuestion: What is the average temperature in the United States in the summer?\nFacts: The average temperature in the United States in the summer is 74", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would it be unusual to see frost in September in Texas?\nFacts: Texas is a Southern state of the United States, known for high heat. On average, Texas is between 68 and 89 degrees during the month of September. Frost forms at 32 degrees or lower.\nAnswer (True or False):", "ct_subject": "Texas is a Southern state of the United States, known for high heat. On average, Texas is between 68 and 89 degrees during the month of September. Frost forms at 32 degrees or lower.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999999"], [" is", "1.0"], [" the", "1.0"], [" average", "0.99677855"], [" temperature", "0.99992764"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "0.92276984"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" temperature", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "1.0"], [" 68", "0.7098435"], [" degrees", "1.0"], [".", "0.9999999"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" temperature", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" in", "0.98563296"], [" the", "0.9996346"], [" summer", "0.3657752"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" temperature", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" in", "1.0"], [" the", "1.0"], [" summer", "1.0"], [" is", "1.0"], [" 74", "0.99896634"]]}, "874": {"qid": "26cfd3b238056cc1dc82", "term": "Vitamin C", "description": "nutrient found in citrus fruits and other foods", "question": "Can vitamin C rich fruits be bad for health?", "answer": true, "facts": ["Oranges are fruits that are rich in vitamin C.", "Oranges are very acidic fruits that can wear down tooth enamel.", "Too much Vitamin C can cause nausea and diarrhea."], "decomposition": ["Which vitamin are oranges rich in?", "Is #1 the same as vitamin C?", "Can excess of #1 be harmful to a person's health?", "Can the acidity of oranges have adverse effects on human consumers?", "Are #2, #3 and #4 positive?"], "evidence": [[[["Orange (fruit)-20"]], ["operation"], [["Vitamin C megadosage-7"]], [["Citric acid-39", "Orange (fruit)-41"]], ["operation"]], [[["Orange (fruit)-20"]], ["operation"], ["no_evidence", "operation"], [["Citric acid-39"]], ["no_evidence", "operation"]], [[["Vitamin C-69"]], [["Vitamin C-69"]], [["Vitamin C-22"]], ["no_evidence"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Can you get cancer from eating too much meat?\nFacts: Meat is a very good source of protein. Meat is also a very good source of iron. Meat is also a very good source of vitamin B12.\nAnswer (True or False): True\nQuestion: Can you get cancer from eating too much meat?\nFacts: Meat", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can vitamin C rich fruits be bad for health?\nFacts: Oranges are fruits that are rich in vitamin C. Oranges are very acidic fruits that can wear down tooth enamel. Too much Vitamin C can cause nausea and diarrhea.\nAnswer (True or False):", "ct_subject": "Oranges are fruits that are rich in vitamin C. Oranges are very acidic fruits that can wear down tooth enamel. Too much Vitamin C can cause nausea and diarrhea.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Can", "0.57038814"], [" you", "0.9998652"], [" get", "0.9999999"], [" cancer", "0.5763817"], [" from", "1.0"], [" eating", "0.99999976"], [" too", "0.99999976"], [" much", "0.99992776"], [" meat", "0.50664866"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Meat", "0.9998822"], [" is", "1.0"], [" a", "1.0"], [" very", "0.8656381"], [" good", "0.9999999"], [" source", "1.0"], [" of", "1.0"], [" protein", "1.0"], [".", "0.9999975"], [" Meat", "0.9405419"], [" is", "1.0"], [" also", "0.9999882"], [" a", "0.99997854"], [" very", "0.90859"], [" good", "1.0"], [" source", "1.0"], [" of", "1.0"], [" iron", "1.0"], [".", "0.9962031"], [" Meat", "1.0"], [" is", "1.0"], [" also", "0.9999999"], [" a", "1.0"], [" very", "0.9999769"], [" good", "1.0"], [" source", "1.0"], [" of", "1.0"], [" vitamin", "0.999814"], [" B", "1.0"], ["12", "1.0"], [".", "1.0"], ["\n", "0.998126"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Can", "1.0"], [" you", "1.0"], [" get", "1.0"], [" cancer", "1.0"], [" from", "1.0"], [" eating", "0.99997425"], [" too", "1.0"], [" much", "1.0"], [" meat", "0.19865747"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Meat", "1.0"]]}, "875": {"qid": "8b9f3835041b7bbc30a3", "term": "Crucifixion", "description": "Method of capital punishment in which the victim is tied or nailed to a large wooden beam and left to hang until eventual death", "question": "If it socially acceptable to wear an icon depicting crucifixion? ", "answer": true, "facts": ["The crucifixion of Jesus is a common sign used by Catholics and Christian denominations. ", "Many jewelry stores offer necklaces with the Crucifixion of Jesus Christ."], "decomposition": ["Which common symbol is used by Catholics to depict crucifixion?", "Is #1 commonly found in jewelry stores?"], "evidence": [[[["Christian symbolism-6"], "no_evidence"], [["Christian cross variants-3"], "operation"]], [[["Crucifix-2"]], [["Crucifix-12"], "no_evidence", "operation"]], [[["Crucifixion-2"]], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: If it socially acceptable to wear an icon depicting the crucifixion of Jesus?\nFacts: The crucifixion of Jesus is a common sign used by Catholics and Christian denominations.  Many jewelry stores offer necklaces with the Crucifixion of Jesus Christ.\nAnswer (True or False): True\nQuestion: If it socially acceptable to", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: If it socially acceptable to wear an icon depicting crucifixion? \nFacts: The crucifixion of Jesus is a common sign used by Catholics and Christian denominations.  Many jewelry stores offer necklaces with the Crucifixion of Jesus Christ.\nAnswer (True or False):", "ct_subject": "The crucifixion of Jesus is a common sign used by Catholics and Christian denominations.  Many jewelry stores offer necklaces with the Crucifixion of Jesus Christ.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" If", "1.0"], [" it", "1.0"], [" socially", "0.9849963"], [" acceptable", "1.0"], [" to", "1.0"], [" wear", "1.0"], [" an", "0.19966865"], [" icon", "1.0"], [" depicting", "1.0"], [" the", "0.99999905"], [" crucifix", "0.87232023"], ["ion", "1.0"], [" of", "0.9972167"], [" Jesus", "1.0"], ["?", "1.0"], ["\n", "0.9999999"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" crucifix", "1.0"], ["ion", "1.0"], [" of", "1.0"], [" Jesus", "1.0"], [" is", "1.0"], [" a", "1.0"], [" common", "1.0"], [" sign", "1.0"], [" used", "1.0"], [" by", "1.0"], [" Catholics", "1.0"], [" and", "1.0"], [" Christian", "1.0"], [" denominations", "1.0"], [".", "1.0"], [" ", "1.0"], [" Many", "1.0"], [" jewelry", "1.0"], [" stores", "1.0"], [" offer", "1.0"], [" neck", "1.0"], ["l", "1.0"], ["aces", "1.0"], [" with", "1.0"], [" the", "1.0"], [" Cru", "1.0"], ["c", "1.0"], ["ifix", "1.0"], ["ion", "1.0"], [" of", "1.0"], [" Jesus", "1.0"], [" Christ", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" If", "1.0"], [" it", "1.0"], [" socially", "1.0"], [" acceptable", "1.0"], [" to", "1.0"]]}, "876": {"qid": "5f96f27de4c8cdafc070", "term": "Brake", "description": "mechanical device that inhibits motion", "question": "Can people die from brake failure?", "answer": true, "facts": ["Brake failure is the inability of brakes to function.", "When vehicles experience brake failure, they cannot be stopped safely, which results in a crash.", "People die in vehicular crashes."], "decomposition": ["What is a brake failure?", "What can #1 lead to in a car?", "Have people died from #2?"], "evidence": [[[["Disc brake-63"]], [["Traffic collision-1", "Traffic collision-50"]], [["Falco (musician)-22"], "operation"]], [[["Brake-1"], "no_evidence"], [["Traffic collision-1", "Traffic collision-24"], "no_evidence"], [["Traffic collision-3"], "operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Can people die from a broken neck?\nFacts: A broken neck is a condition in which the vertebrae in the neck are broken. The vertebrae are the bones that make up the neck. When the vertebrae are broken, the neck cannot be held in a straight position. People die from a broken neck.\nAnswer (True or", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can people die from brake failure?\nFacts: Brake failure is the inability of brakes to function. When vehicles experience brake failure, they cannot be stopped safely, which results in a crash. People die in vehicular crashes.\nAnswer (True or False):", "ct_subject": "Brake failure is the inability of brakes to function. When vehicles experience brake failure, they cannot be stopped safely, which results in a crash. People die in vehicular crashes.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Can", "0.98767054"], [" people", "0.9986619"], [" die", "0.9999975"], [" from", "1.0"], [" a", "0.99999976"], [" broken", "0.9997054"], [" neck", "0.99999046"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" broken", "1.0"], [" neck", "1.0"], [" is", "1.0"], [" a", "0.9988613"], [" condition", "0.33638692"], [" in", "0.99998915"], [" which", "1.0"], [" the", "1.0"], [" verte", "1.0"], ["bra", "1.0"], ["e", "1.0"], [" in", "0.19771783"], [" the", "1.0"], [" neck", "1.0"], [" are", "1.0"], [" broken", "0.97640926"], [".", "1.0"], [" The", "0.5673714"], [" verte", "1.0"], ["bra", "1.0"], ["e", "1.0"], [" are", "1.0"], [" the", "1.0"], [" bones", "1.0"], [" that", "1.0"], [" make", "0.99904734"], [" up", "1.0"], [" the", "1.0"], [" neck", "1.0"], [".", "1.0"], [" When", "0.74262804"], [" the", "0.99919885"], [" verte", "1.0"], ["bra", "1.0"], ["e", "1.0"], [" are", "1.0"], [" broken", "1.0"], [",", "1.0"], [" the", "0.9998977"], [" neck", "1.0"], [" cannot", "0.9894734"], [" be", "0.9716152"], [" held", "0.53749657"], [" in", "0.9999999"], [" a", "0.99970824"], [" straight", "1.0"], [" position", "1.0"], [".", "1.0"], [" People", "0.97089237"], [" die", "0.99999845"], [" from", "0.9971495"], [" a", "0.9532791"], [" broken", "1.0"], [" neck", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"]]}, "877": {"qid": "5a781808bf59dc835bd2", "term": "Humour", "description": "tendency of experiences to provoke laughter and provide amusement", "question": "Was the man who played the male lead in Mrs. Doubtfire known for his humour?", "answer": true, "facts": ["Robin Williams played the male lead in Mrs. Doubtfire.", "Robin Williams had a prolific standup and film comedy career."], "decomposition": ["Who played the male  lead in MRs. Doubtfire?", "What was the career of #1?", "Is #2 a humorous job?"], "evidence": [[[["Mrs. Doubtfire-1"]], [["Robin Williams-1"]], ["operation"]], [[["Mrs. Doubtfire-3"]], [["Robin Williams-14"]], ["operation"]], [[["Mrs. Doubtfire-1"]], [["Robin Williams-1"]], [["Comedian-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the man who played the male lead in The Godfather have a wife?\nFacts: The man who played the male lead in The Godfather had a wife.\nAnswer (True or False): True\nQuestion: Did the man who played the male lead in The Godfather have a son?\nFacts: The man who played the male", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was the man who played the male lead in Mrs. Doubtfire known for his humour?\nFacts: Robin Williams played the male lead in Mrs. Doubtfire. Robin Williams had a prolific standup and film comedy career.\nAnswer (True or False):", "ct_subject": "Robin Williams played the male lead in Mrs. Doubtfire. Robin Williams had a prolific standup and film comedy career.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.273806"], [" the", "1.0"], [" man", "1.0"], [" who", "1.0"], [" played", "1.0"], [" the", "1.0"], [" male", "1.0"], [" lead", "1.0"], [" in", "1.0"], [" The", "0.99999976"], [" God", "0.98309076"], ["father", "1.0"], [" have", "0.9981823"], [" a", "1.0"], [" wife", "0.070687704"], ["?", "0.99999964"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" man", "0.9999999"], [" who", "1.0"], [" played", "1.0"], [" the", "1.0"], [" male", "1.0"], [" lead", "1.0"], [" in", "1.0"], [" The", "1.0"], [" God", "1.0"], ["father", "1.0"], [" had", "1.0"], [" a", "1.0"], [" wife", "1.0"], [".", "1.0"], ["\n", "0.9993425"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999924"], [" the", "1.0"], [" man", "1.0"], [" who", "1.0"], [" played", "1.0"], [" the", "1.0"], [" male", "1.0"], [" lead", "1.0"], [" in", "1.0"], [" The", "1.0"], [" God", "1.0"], ["father", "1.0"], [" have", "1.0"], [" a", "0.99979"], [" son", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" man", "1.0"], [" who", "1.0"], [" played", "1.0"], [" the", "1.0"], [" male", "1.0"]]}, "878": {"qid": "16e41c83724a949fb983", "term": "Stone Cold Steve Austin", "description": "American professional wrestler", "question": "Coud every wife of Stone Cold Steve Austin fit in Audi TT?", "answer": true, "facts": ["Stone Cold Steve Austin has been married to 4 different women.", "The Audi TT is a sports car with 4 seats."], "decomposition": ["How many wives has Stone Cold Steve Austin had?", "How many people can sit in an Audi TT", "Is #2 at least #1?"], "evidence": [[[["Stone Cold Steve Austin-67"]], [["Audi TT-2"]], ["operation"]], [[["Stone Cold Steve Austin-67"]], [["2+2 (car body style)-1", "Audi TT-2"]], ["operation"]], [[["Stone Cold Steve Austin-67"]], [["Audi TT-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first man to be killed by a shark?\nFacts: The first man to be killed by a shark was a fisherman named John Smith.\nAnswer (True or False): True\nQuestion: What is the name of the first man to be killed by a shark?\nFacts: The first man to be killed by a shark was a fisherman named John", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Coud every wife of Stone Cold Steve Austin fit in Audi TT?\nFacts: Stone Cold Steve Austin has been married to 4 different women. The Audi TT is a sports car with 4 seats.\nAnswer (True or False):", "ct_subject": "Stone Cold Steve Austin has been married to 4 different women. The Audi TT is a sports car with 4 seats.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9959681"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999685"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9999435"], [" man", "0.74819714"], [" to", "1.0"], [" be", "0.46624774"], [" killed", "0.98055536"], [" by", "1.0"], [" a", "1.0"], [" shark", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" man", "0.99999905"], [" to", "1.0"], [" be", "1.0"], [" killed", "1.0"], [" by", "1.0"], [" a", "1.0"], [" shark", "1.0"], [" was", "1.0"], [" a", "1.0"], [" fisherman", "0.48664412"], [" named", "1.0"], [" John", "1.0"], [" Smith", "0.7162307"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" be", "1.0"], [" killed", "1.0"], [" by", "1.0"], [" a", "1.0"], [" shark", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" be", "1.0"], [" killed", "1.0"], [" by", "1.0"], [" a", "1.0"], [" shark", "1.0"], [" was", "1.0"], [" a", "1.0"], [" fisherman", "1.0"], [" named", "1.0"], [" John", "1.0"]]}, "879": {"qid": "6662006c915a62db5a89", "term": "Hamlet", "description": "tragedy by William Shakespeare", "question": "Is Hamlet more common on IMDB than Comedy of Errors?", "answer": true, "facts": ["IMDB, The Internet Movie Database, catalogs movies.", "IMDB lists 6 Hamlet movie adaptations.", "IMDB lists 5 Comedy of Errors movie adaptations."], "decomposition": ["How many listings of Hamlet are there on IMDB?", "How many listing of Comedy of Errors is there on IMDB?", "Is #1 greater than #2?"], "evidence": [[[["Hamlet-3"], "no_evidence"], [["The Comedy of Errors-24", "The Comedy of Errors-25"], "no_evidence"], ["no_evidence", "operation"]], [[["Hamlet-92"], "no_evidence"], [["The Comedy of Errors-24"]], ["operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the first man on the moon, Neil Armstrong, a reference to the first man on the moon, Neil Armstrong?\nFacts: Neil Armstrong was the first man to walk on the moon. Neil Armstrong was the first man to walk on the moon.\nAnswer (True or False): True\nQuestion: Is the name of the first", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Hamlet more common on IMDB than Comedy of Errors?\nFacts: IMDB, The Internet Movie Database, catalogs movies. IMDB lists 6 Hamlet movie adaptations. IMDB lists 5 Comedy of Errors movie adaptations.\nAnswer (True or False):", "ct_subject": "IMDB, The Internet Movie Database, catalogs movies. IMDB lists 6 Hamlet movie adaptations. IMDB lists 5 Comedy of Errors movie adaptations.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "0.97953767"], [" name", "0.9979474"], [" of", "0.9999988"], [" the", "1.0"], [" first", "0.87029046"], [" man", "0.9860109"], [" on", "0.9999995"], [" the", "1.0"], [" moon", "1.0"], [",", "0.7529788"], [" Neil", "1.0"], [" Armstrong", "1.0"], [",", "1.0"], [" a", "0.9999962"], [" reference", "0.99999297"], [" to", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [",", "0.999931"], [" Neil", "1.0"], [" Armstrong", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Neil", "0.99987245"], [" Armstrong", "1.0"], [" was", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "0.19074759"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [".", "1.0"], [" Neil", "0.9999261"], [" Armstrong", "1.0"], [" was", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"]]}, "880": {"qid": "5278f0501c540dff6407", "term": "Freemasonry", "description": "group of fraternal organizations", "question": "Has Freemasonry been represented on the Moon?", "answer": true, "facts": ["Freemasonry is a group of fraternal organizations rooted in fraternities of stonemasons of the fourteenth century.", "Buzz Aldrin was initiated into the Freemason fraternity in 1955", "Buzz Aldrin and Neil Armstrong were the first men to land on the moon in 1969."], "decomposition": ["What occupation goes into space?", "Have any #1 been Free Masons?", "Have any people listed in #2 been to the moon?"], "evidence": [[[["Astronaut-1"]], [["James Irwin-1", "James Irwin-23"]], [["James Irwin-1"]]], [[["Astronaut-1"]], [["Buzz Aldrin-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Astronaut-1"]], [["John Glenn-62"]], [["John Glenn-3"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first man to walk on the moon?\nFacts: The first man to walk on the moon was Neil Armstrong. Buzz Aldrin was the second man to walk on the moon.\nAnswer (True or False): True\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Has Freemasonry been represented on the Moon?\nFacts: Freemasonry is a group of fraternal organizations rooted in fraternities of stonemasons of the fourteenth century. Buzz Aldrin was initiated into the Freemason fraternity in 1955 Buzz Aldrin and Neil Armstrong were the first men to land on the moon in 1969.\nAnswer (True or False):", "ct_subject": "Freemasonry is a group of fraternal organizations rooted in fraternities of stonemasons of the fourteenth century. Buzz Aldrin was initiated into the Freemason fraternity in 1955 Buzz Aldrin and Neil Armstrong were the first men to land on the moon in 1969.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.997593"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99942183"], [" of", "1.0"], [" the", "1.0"], [" first", "0.99120754"], [" man", "0.9999988"], [" to", "0.99853575"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "0.99999774"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9997855"], [" first", "0.9999995"], [" man", "1.0"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" was", "1.0"], [" Neil", "0.9999999"], [" Armstrong", "1.0"], [".", "1.0"], [" Buzz", "0.9999994"], [" Ald", "1.0"], ["rin", "1.0"], [" was", "1.0"], [" the", "1.0"], [" second", "1.0"], [" man", "1.0"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"]]}, "881": {"qid": "93264708b0d600fb9650", "term": "Copper", "description": "Chemical element with atomic number 29", "question": "Would a fungal life-form be threatened by a pigment from copper?", "answer": true, "facts": ["Verdigris is a pigment made from copper", "Verdigris is also used as a fungicide "], "decomposition": ["Which element is the pigment verdigris derived from?", "Is #1 copper and verdigris also used as a fungicide?"], "evidence": [[[["Verdigris-1"]], [["Verdigris-6"]]], [[["Verdigris-1"]], [["Copper-5", "Verdigris-6"], "operation"]], [[["Verdigris-1"]], [["Copper-5", "Verdigris-6"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first man to be born in the United States?\nFacts: John Smith\nAnswer (True or False): True\nQuestion: What is the name of the first man to be born in the United States?\nFacts: John Smith\nAnswer (True or False): True\nQuestion: What is the name of the first man to be born in the United States", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a fungal life-form be threatened by a pigment from copper?\nFacts: Verdigris is a pigment made from copper Verdigris is also used as a fungicide \nAnswer (True or False):", "ct_subject": "Verdigris is a pigment made from copper Verdigris is also used as a fungicide ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999932"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9980259"], [" of", "1.0"], [" the", "1.0"], [" first", "0.99885905"], [" man", "0.9013604"], [" to", "0.8098781"], [" be", "0.60486573"], [" born", "0.813824"], [" in", "0.9999993"], [" the", "1.0"], [" United", "0.9998171"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "0.9999999"], [" Smith", "1.0"], ["\n", "0.9995401"], ["Answer", "1.0"], [" (", "0.9999999"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "0.9999989"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "1.0"], [" Smith", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"]]}, "882": {"qid": "810d006c5cb0e27081c8", "term": "Goldstone Deep Space Communications Complex", "description": "United States historic place", "question": "Do the telescopes at Goldstone Deep Space Communications Complex work the night shift?", "answer": true, "facts": ["The night shift is considered to be the hours of 11pm - 7am.", "The telescopes at Goldstone Deep Space Communications Complex are running 24 hours a day."], "decomposition": ["What hours are typically considered the night shift?", "What hours do the telescopes at Goldstone Deep Space Communications Complex run?", "Is there any overlap between #1 and #2?"], "evidence": [[[["Shift work-11"]], [["Goldstone Deep Space Communications Complex-2"], "no_evidence"], ["no_evidence", "operation"]], [[["Shift work-11"], "no_evidence"], ["no_evidence"], ["operation"]], [["no_evidence"], [["Astronomy-2", "Goldstone Deep Space Communications Complex-1"], "no_evidence"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the planet that is closest to the sun?\nFacts: The planet closest to the sun is called Proxima Centauri.\nAnswer (True or False): True\nQuestion: What is the name of the planet that is farthest from the sun?\nFacts: The planet farthest from the sun is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do the telescopes at Goldstone Deep Space Communications Complex work the night shift?\nFacts: The night shift is considered to be the hours of 11pm - 7am. The telescopes at Goldstone Deep Space Communications Complex are running 24 hours a day.\nAnswer (True or False):", "ct_subject": "The night shift is considered to be the hours of 11pm - 7am. The telescopes at Goldstone Deep Space Communications Complex are running 24 hours a day.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999722"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" planet", "0.999995"], [" that", "0.9999945"], [" is", "0.9988416"], [" closest", "0.98354465"], [" to", "1.0"], [" the", "1.0"], [" sun", "0.9999771"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" planet", "1.0"], [" closest", "0.9999987"], [" to", "1.0"], [" the", "1.0"], [" sun", "1.0"], [" is", "1.0"], [" called", "0.9999455"], [" Pro", "1.0"], ["x", "1.0"], ["ima", "1.0"], [" Centauri", "1.0"], [".", "1.0"], ["\n", "0.9999721"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" planet", "1.0"], [" that", "1.0"], [" is", "1.0"], [" fart", "1.0"], ["hest", "1.0"], [" from", "1.0"], [" the", "1.0"], [" sun", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" planet", "1.0"], [" fart", "1.0"], ["hest", "1.0"], [" from", "1.0"], [" the", "1.0"], [" sun", "1.0"], [" is", "1.0"]]}, "883": {"qid": "5812637ba98cba20a9af", "term": "Mercedes-Benz", "description": "automobile brand of Daimler AG", "question": "Is it legal for a licensed child driving Mercedes-Benz to be employed in US?", "answer": true, "facts": ["The minimum age for driving in the US is 16.", "Child labor laws in the US require a child to be 14 years of age or older to work."], "decomposition": ["What is the minimum driving age in the US?", "What is the minimum age for someone to be employed in the US?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Graduated driver licensing-35"], "no_evidence"], [["Child labour law-10"]], ["operation"]], [[["Driver's licenses in the United States-9"]], [["Child labor laws in the United States-2"]], ["operation"]], [[["Driver's licenses in the United States-9"]], [["Child labour-66", "Legal working age-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it legal for a licensed child driving a Mercedes-Benz to be employed in the US?\nFacts: The minimum age for driving in the US is 16. Child labor laws in the US require a child to be 14 years of age or older to work.\nAnswer (True or False): True\nQuestion: Is it legal", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is it legal for a licensed child driving Mercedes-Benz to be employed in US?\nFacts: The minimum age for driving in the US is 16. Child labor laws in the US require a child to be 14 years of age or older to work.\nAnswer (True or False):", "ct_subject": "The minimum age for driving in the US is 16. Child labor laws in the US require a child to be 14 years of age or older to work.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99867177"], [" it", "1.0"], [" legal", "1.0"], [" for", "1.0"], [" a", "1.0"], [" licensed", "1.0"], [" child", "1.0"], [" driving", "1.0"], [" a", "0.9868514"], [" Mercedes", "0.9979208"], ["-", "1.0"], ["Benz", "1.0"], [" to", "1.0"], [" be", "1.0"], [" employed", "1.0"], [" in", "1.0"], [" the", "0.995643"], [" US", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" minimum", "1.0"], [" age", "1.0"], [" for", "1.0"], [" driving", "1.0"], [" in", "1.0"], [" the", "1.0"], [" US", "1.0"], [" is", "1.0"], [" 16", "1.0"], [".", "1.0"], [" Child", "1.0"], [" labor", "1.0"], [" laws", "1.0"], [" in", "1.0"], [" the", "1.0"], [" US", "1.0"], [" require", "1.0"], [" a", "1.0"], [" child", "1.0"], [" to", "1.0"], [" be", "1.0"], [" 14", "1.0"], [" years", "1.0"], [" of", "1.0"], [" age", "1.0"], [" or", "1.0"], [" older", "1.0"], [" to", "1.0"], [" work", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" legal", "1.0"]]}, "884": {"qid": "cd920b1595f9bc70126f", "term": "Sofer", "description": "profession", "question": "Would a sofer be a bad job for a vegan?", "answer": true, "facts": ["A sofer is a transcriber of religious texts, and has its origins in Judaism.", "Sofers transcribe texts on a material known as vellum.", "Vellum is made of materials derived from calfskin.", "Vegans do not use any animal products."], "decomposition": ["What materials do sofers use?", "What products do vegans refuse to use?", "Is #1 included in #2?"], "evidence": [[[["Marshmallow sofa-5"]], [["Veganism-9"]], [["Leather-4"]]], [[["Sofer-1"]], [["Veganism-1"]], ["operation"]], [[["Parchment-1"]], [["Veganism-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a sofer and a translator?\nFacts: A sofer is a transcriber of religious texts, and has its origins in Judaism. Translators translate texts on a material known as vellum. Vegans do", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a sofer be a bad job for a vegan?\nFacts: A sofer is a transcriber of religious texts, and has its origins in Judaism. Sofers transcribe texts on a material known as vellum. Vellum is made of materials derived from calfskin. Vegans do not use any animal products.\nAnswer (True or False):", "ct_subject": "A sofer is a transcriber of religious texts, and has its origins in Judaism. Sofers transcribe texts on a material known as vellum. Vellum is made of materials derived from calfskin. Vegans do not use any animal products.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99592507"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" so", "0.9991918"], ["fer", "1.0"], [" and", "1.0"], [" a", "1.0"], [" translator", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" so", "1.0"], ["fer", "1.0"], [" is", "1.0"], [" a", "1.0"], [" transc", "0.9999995"], ["riber", "1.0"], [" of", "1.0"], [" religious", "1.0"], [" texts", "1.0"], [",", "1.0"], [" and", "1.0"], [" has", "1.0"], [" its", "1.0"], [" origins", "1.0"], [" in", "1.0"], [" Judaism", "1.0"], [".", "1.0"], [" Trans", "1.0"], ["l", "1.0"], ["ators", "1.0"], [" translate", "1.0"], [" texts", "0.99964"], [" on", "1.0"], [" a", "1.0"], [" material", "1.0"], [" known", "1.0"], [" as", "1.0"], [" ve", "1.0"], ["ll", "1.0"], ["um", "1.0"], [".", "1.0"], [" Veg", "0.99999094"], ["ans", "1.0"], [" do", "1.0"]]}, "885": {"qid": "cfba036236507374976d", "term": "Coen brothers", "description": "American filmmakers", "question": "Do people watching Coen brothers films in Guinea Bissau need subtitles?", "answer": true, "facts": ["The Coen brothers direct films primarily using English", "The primary languages used in Guinea Bissau are Creole, native African languages, and Portuguese"], "decomposition": ["In what country do the Coen Brothers make films?", "What is the primary language spoken in #1?", "What is the primary language spoken in Guinea Bissau?", "Is #3 different than #2?"], "evidence": [[[["Coen brothers-1"]], [["Languages of the United States-1"]], [["Guinea-Bissau-3"]], ["operation"]], [[["Coen brothers-1"]], [["American English-2"]], [["Guinea-Bissau-3"]], ["operation"]], [[["Coen brothers-13"], "no_evidence"], [["United States-80"]], [["Guinea-Bissau-3"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the country in Africa that is the largest land mass in the world?\nFacts: The country is called Africa\nAnswer (True or False): True\nQuestion: What is the name of the country in Africa that is the largest land mass in the world?\nFacts: The country is called Africa\nAnswer (True or False", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do people watching Coen brothers films in Guinea Bissau need subtitles?\nFacts: The Coen brothers direct films primarily using English The primary languages used in Guinea Bissau are Creole, native African languages, and Portuguese\nAnswer (True or False):", "ct_subject": "The Coen brothers direct films primarily using English The primary languages used in Guinea Bissau are Creole, native African languages, and Portuguese", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99346393"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99997735"], [" of", "1.0"], [" the", "1.0"], [" country", "0.69200164"], [" in", "0.6581025"], [" Africa", "0.79150647"], [" that", "1.0"], [" is", "0.9999496"], [" the", "1.0"], [" largest", "0.9934257"], [" land", "0.95188904"], [" mass", "1.0"], [" in", "0.9999994"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99999297"], [" country", "1.0"], [" is", "0.9999994"], [" called", "0.99994516"], [" Africa", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "0.9999999"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" country", "1.0"], [" in", "1.0"], [" Africa", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" land", "1.0"], [" mass", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "0.99953234"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" country", "1.0"], [" is", "1.0"], [" called", "1.0"], [" Africa", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"]]}, "886": {"qid": "59978a3e99a498e5567b", "term": "Great Pyramid of Giza", "description": "Largest pyramid in the Giza Necropolis, Egypt", "question": "Is Great Pyramid of Giza the last wonder of its kind?", "answer": true, "facts": ["The Great Pyramid of Giza is classified as one of the Seven Wonders of the Ancient World.", "Five of the ancient wonders were destroyed, and a sixth (the Hanging Gardens of Babylon) may not have existed.", "The Great Pyramid of Giza is largely intact as of 2020."], "decomposition": ["What are the wonders of the ancient world that are either destroyed or non-existent?", "What is the wonder of the ancient world that is still intact?", "Has #2 survived a much longer time than #1?"], "evidence": [[[["Seven Wonders of the Ancient World-1"]], [["Great Pyramid of Giza-1"]], ["operation"]], [[["Seven Wonders of the Ancient World-1"]], [["Great Pyramid of Giza-1"]], ["operation"]], [[["Seven Wonders of the Ancient World-1"]], [["Great Pyramid of Giza-1"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the Great Pyramid of Giza the only pyramid in the world?\nFacts: The Great Pyramid of Giza is the only pyramid in the world that is not a replica of a real pyramid. The Great Pyramid of Giza", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Great Pyramid of Giza the last wonder of its kind?\nFacts: The Great Pyramid of Giza is classified as one of the Seven Wonders of the Ancient World. Five of the ancient wonders were destroyed, and a sixth (the Hanging Gardens of Babylon) may not have existed. The Great Pyramid of Giza is largely intact as of 2020.\nAnswer (True or False):", "ct_subject": "The Great Pyramid of Giza is classified as one of the Seven Wonders of the Ancient World. Five of the ancient wonders were destroyed, and a sixth (the Hanging Gardens of Babylon) may not have existed. The Great Pyramid of Giza is largely intact as of 2020.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99719477"], [" the", "1.0"], [" Great", "1.0"], [" Pyramid", "1.0"], [" of", "1.0"], [" G", "1.0"], ["iza", "1.0"], [" the", "1.0"], [" only", "0.99638075"], [" pyramid", "0.25134215"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Great", "1.0"], [" Pyramid", "1.0"], [" of", "1.0"], [" G", "1.0"], ["iza", "1.0"], [" is", "1.0"], [" the", "1.0"], [" only", "1.0"], [" pyramid", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" that", "1.0"], [" is", "0.99999845"], [" not", "0.47250092"], [" a", "1.0"], [" replica", "1.0"], [" of", "0.9995364"], [" a", "0.87558603"], [" real", "0.99996924"], [" pyramid", "1.0"], [".", "1.0"], [" The", "0.9999318"], [" Great", "1.0"], [" Pyramid", "1.0"], [" of", "1.0"], [" G", "1.0"], ["iza", "1.0"]]}, "887": {"qid": "36c497860f72d148e4e8", "term": "Donkey", "description": "El burrito de sheck", "question": "Are Donkeys part of Christmas celebrations?", "answer": true, "facts": ["\"Dominic The Donkey\" is a popular Christmas song.", "\"Nestor The Ling Eared Christmas Donkey\" is a popular Christmas Movie."], "decomposition": ["Which animals have been popularly recognized as part of the Christmas culture?", "Are donkeys one of #1?"], "evidence": [[[["Nestor, the Long-Eared Christmas Donkey-2"]], ["operation"]], [[["Christmas-1"], "no_evidence"], [["Nativity of Jesus in art-12"], "no_evidence", "operation"]], [[["Nativity scene-20"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any animals that are not part of the Christmas story?\nFacts: There are many animals that are not part of the Christmas story.\nAnswer (True or False): True\nQuestion: Are there any animals that are not part of the Christmas story?\nFacts: There are many animals that are not part of the Christmas story.\nAnswer (True or False): True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are Donkeys part of Christmas celebrations?\nFacts: \"Dominic The Donkey\" is a popular Christmas song. \"Nestor The Ling Eared Christmas Donkey\" is a popular Christmas Movie.\nAnswer (True or False):", "ct_subject": "\"Dominic The Donkey\" is a popular Christmas song. \"Nestor The Ling Eared Christmas Donkey\" is a popular Christmas Movie.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.994189"], [" there", "1.0"], [" any", "0.9994381"], [" animals", "0.39590028"], [" that", "1.0"], [" are", "0.9999665"], [" not", "1.0"], [" part", "0.95719784"], [" of", "1.0"], [" the", "0.69394636"], [" Christmas", "1.0"], [" story", "0.1309164"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "0.99999964"], [" are", "1.0"], [" many", "0.99989283"], [" animals", "1.0"], [" that", "1.0"], [" are", "1.0"], [" not", "1.0"], [" part", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Christmas", "1.0"], [" story", "1.0"], [".", "1.0"], ["\n", "0.9999995"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.9999995"], [" there", "1.0"], [" any", "1.0"], [" animals", "1.0"], [" that", "1.0"], [" are", "1.0"], [" not", "1.0"], [" part", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Christmas", "1.0"], [" story", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "1.0"], [" are", "1.0"], [" many", "1.0"], [" animals", "1.0"], [" that", "1.0"], [" are", "1.0"], [" not", "1.0"], [" part", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Christmas", "1.0"], [" story", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"]]}, "888": {"qid": "d2af086597e0e03a4f57", "term": "Latitude", "description": "The angle between zenith at a point and the plane of the equator", "question": "Is latitude required to determine the coordinates of an area?", "answer": true, "facts": ["Longitude is one of the required data points needed for determining coordinates.", "Latitude is the other angle required to determine coordinates of an area. "], "decomposition": ["What are the two sets of data points that determine coordinates of a location?", "Is latitude one of the answers to #1?"], "evidence": [[[["Geographic coordinate system-15"]], ["operation"]], [[["Geographic coordinate system-4"]], ["operation"]], [[["Geographic coordinate system-15"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"C\" and a \"C\" in the word \"Coffee\"?\nFacts: The \"C\" in \"Coffee\" is pronounced like the \"C\" in \"Coffee\" and the \"C\" in \"Coffee\" is pronounced like the \"C\" in \"Coffee\".\nAnswer (", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is latitude required to determine the coordinates of an area?\nFacts: Longitude is one of the required data points needed for determining coordinates. Latitude is the other angle required to determine coordinates of an area. \nAnswer (True or False):", "ct_subject": "Longitude is one of the required data points needed for determining coordinates. Latitude is the other angle required to determine coordinates of an area. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9789411"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "0.9991284"], [" \"", "0.9999176"], ["C", "0.7502138"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["C", "0.833867"], ["\"", "0.9999999"], [" in", "0.99812514"], [" the", "0.9999957"], [" word", "0.9998447"], [" \"", "1.0"], ["C", "0.9511586"], ["off", "0.9564213"], ["ee", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99998677"], [" \"", "1.0"], ["C", "1.0"], ["\"", "1.0"], [" in", "0.9999938"], [" \"", "1.0"], ["C", "1.0"], ["off", "1.0"], ["ee", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" pronounced", "0.9999685"], [" like", "0.48111445"], [" the", "1.0"], [" \"", "1.0"], ["C", "1.0"], ["\"", "1.0"], [" in", "1.0"], [" \"", "1.0"], ["C", "0.9999999"], ["off", "1.0"], ["ee", "1.0"], ["\"", "0.9999976"], [" and", "0.99999976"], [" the", "0.9073413"], [" \"", "1.0"], ["C", "1.0"], ["\"", "1.0"], [" in", "1.0"], [" \"", "1.0"], ["C", "1.0"], ["off", "1.0"], ["ee", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" pronounced", "1.0"], [" like", "1.0"], [" the", "1.0"], [" \"", "1.0"], ["C", "1.0"], ["\"", "1.0"], [" in", "1.0"], [" \"", "1.0"], ["C", "1.0"], ["off", "1.0"], ["ee", "1.0"], ["\".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"]]}, "889": {"qid": "47fc029ca411d1f47914", "term": "Olive oil", "description": "liquid fat extracted by pressing olives", "question": "Would Carmine's kitchen staff be panicked if they had no olive oil?", "answer": true, "facts": ["Carmine's is an Italian restaurant.", "Olive oil is a large component of a lot of Italian cooking."], "decomposition": ["What kind of food does Carmine's serve?", "What are essential ingredients in #1?", "Is olive oil listed in #2?"], "evidence": [[["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], [["Pizza in the United States-3"]], ["operation"]], [[["Carmine Romano-2"], "no_evidence"], [["Italian cuisine-16"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the closest to the equator?\nFacts: The closest city to the equator is Cape Town, South Africa.\nAnswer (True or False): True\nQuestion: What is the name of the city in the United States that is the closest to the equator?\nFacts: The closest city to the equator", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Carmine's kitchen staff be panicked if they had no olive oil?\nFacts: Carmine's is an Italian restaurant. Olive oil is a large component of a lot of Italian cooking.\nAnswer (True or False):", "ct_subject": "Carmine's is an Italian restaurant. Olive oil is a large component of a lot of Italian cooking.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99930227"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99103874"], [" of", "1.0"], [" the", "1.0"], [" city", "0.98288673"], [" in", "0.989135"], [" the", "0.8380711"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "0.8119441"], [" the", "1.0"], [" closest", "0.5789695"], [" to", "1.0"], [" the", "1.0"], [" equ", "0.3340795"], ["ator", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99999774"], [" closest", "0.9999999"], [" city", "1.0"], [" to", "1.0"], [" the", "1.0"], [" equ", "1.0"], ["ator", "1.0"], [" is", "1.0"], [" Cape", "0.99999976"], [" Town", "1.0"], [",", "1.0"], [" South", "1.0"], [" Africa", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" closest", "1.0"], [" to", "1.0"], [" the", "1.0"], [" equ", "0.99999714"], ["ator", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" closest", "1.0"], [" city", "1.0"], [" to", "1.0"], [" the", "1.0"], [" equ", "1.0"], ["ator", "1.0"]]}, "890": {"qid": "5b8d0f04c83745464141", "term": "Snow leopard", "description": "species of mammal", "question": "Can a snow leopard eat twice its own body weight?", "answer": true, "facts": ["The average snow leopard weighs 72 pounds.", "The favorite food of snow leopards is an ibex.", "The average weight of an ibex is 150 pounds."], "decomposition": ["How much do snow leopards weigh on average?", "What is a snow leopard's favorite food?", "How much does #2 weigh?", "Is #3 at least twice as much as #1?"], "evidence": [[[["Snow leopard-17"]], [["Snow leopard-31"]], [["Bharal-3"]], [["Bharal-3", "Snow leopard-17"]]], [[["Snow leopard-17"]], [["Snow leopard-31"]], [["Snow leopard-31"]], ["no_evidence", "operation"]], [[["Snow leopard-17"]], [["Snow leopard-31"]], [["Snow leopard-31"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance a person can walk in one day?\nFacts: The average human can walk about 2,000 miles in a day. The longest distance a person can walk in one day is about 1,000 miles.\nAnswer (True or False): True\nQuestion: What is the longest distance a person can walk in one year", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can a snow leopard eat twice its own body weight?\nFacts: The average snow leopard weighs 72 pounds. The favorite food of snow leopards is an ibex. The average weight of an ibex is 150 pounds.\nAnswer (True or False):", "ct_subject": "The average snow leopard weighs 72 pounds. The favorite food of snow leopards is an ibex. The average weight of an ibex is 150 pounds.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99517184"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.87943375"], [" distance", "0.99999964"], [" a", "0.99999356"], [" person", "0.85785615"], [" can", "0.99486583"], [" walk", "0.99999976"], [" in", "1.0"], [" one", "0.99998474"], [" day", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "0.92791843"], [" human", "0.94479275"], [" can", "1.0"], [" walk", "1.0"], [" about", "0.57593834"], [" 2", "0.9709415"], [",", "0.99889624"], ["000", "0.9999989"], [" miles", "1.0"], [" in", "1.0"], [" a", "0.99641824"], [" day", "1.0"], [".", "1.0"], [" The", "0.999418"], [" longest", "0.97564733"], [" distance", "1.0"], [" a", "1.0"], [" person", "1.0"], [" can", "0.9999999"], [" walk", "1.0"], [" in", "1.0"], [" one", "1.0"], [" day", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 1", "0.35050634"], [",", "1.0"], ["000", "0.9999542"], [" miles", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" person", "1.0"], [" can", "1.0"], [" walk", "0.9995327"], [" in", "1.0"], [" one", "1.0"], [" year", "0.99781114"]]}, "891": {"qid": "31328c97391c0e047f44", "term": "Fibonacci number", "description": "integer in the infinite Fibonacci sequence", "question": "Are there five different single-digit Fibonacci numbers?", "answer": true, "facts": ["The first six numbers in the Fibonacci sequence are 1,1,2,3,5,8.", "Since 1 is doubled, there are only five different single digit numbers."], "decomposition": ["What are the single-digit numbers in the Fibonacci sequence?", "How many unique numbers are in #1?", "Does #2 equal 5?"], "evidence": [[[["Fibonacci-12"]], ["operation"], ["operation"]], [[["Random Fibonacci sequence-4"]], ["operation"], ["operation"]], [[["Fibonacci-12"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there five different Fibonacci numbers?\nFacts: The first six numbers in the Fibonacci sequence are 1,1,2,3,5,8. Since 1 is doubled, there are only five different single digit numbers.\nAnswer (True or False): True\nQuestion: Are there five different Fibon", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are there five different single-digit Fibonacci numbers?\nFacts: The first six numbers in the Fibonacci sequence are 1,1,2,3,5,8. Since 1 is doubled, there are only five different single digit numbers.\nAnswer (True or False):", "ct_subject": "The first six numbers in the Fibonacci sequence are 1,1,2,3,5,8. Since 1 is doubled, there are only five different single digit numbers.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.16808783"], [" there", "1.0"], [" five", "0.9996809"], [" different", "1.0"], [" Fib", "1.0"], ["on", "1.0"], ["acci", "1.0"], [" numbers", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" six", "1.0"], [" numbers", "1.0"], [" in", "1.0"], [" the", "1.0"], [" Fib", "1.0"], ["on", "1.0"], ["acci", "1.0"], [" sequence", "1.0"], [" are", "1.0"], [" 1", "1.0"], [",", "1.0"], ["1", "1.0"], [",", "1.0"], ["2", "1.0"], [",", "1.0"], ["3", "1.0"], [",", "1.0"], ["5", "1.0"], [",", "1.0"], ["8", "1.0"], [".", "1.0"], [" Since", "1.0"], [" 1", "1.0"], [" is", "1.0"], [" doubled", "1.0"], [",", "1.0"], [" there", "1.0"], [" are", "1.0"], [" only", "1.0"], [" five", "1.0"], [" different", "1.0"], [" single", "1.0"], [" digit", "1.0"], [" numbers", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "1.0"], [" five", "1.0"], [" different", "1.0"], [" Fib", "1.0"], ["on", "1.0"]]}, "892": {"qid": "e5f13851532305d2f932", "term": "Cucumber", "description": "species of plant", "question": "Is growing seedless cucumber good for a gardener with entomophobia?", "answer": true, "facts": ["Seedless cucumber fruit does not require pollination", "Cucumber plants need insects to pollinate them", "Entomophobia is a fear of insects"], "decomposition": ["What are people with Entomophobia fearful of?", "How do #1's usually help in the process of gardening?", "Do seedless cucumbers not require #2?"], "evidence": [[[["Entomophobia-1"]], [["Pollination-4"]], [["Cucumber-3"], "operation"]], [[["Entomophobia-1"]], [["Pollination-1"], "no_evidence"], [["Cucumber-3"], "operation"]], [[["Entomophobia-1"]], [["Cucumber beetle-1"]], [["Cucumber-4"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it true that the first person to use the word \"candy\" was a woman?\nFacts: Yes, the word \"candy\" was first used by a woman in the year 1776.\nAnswer (True or False): True\nQuestion: Is it true that the first person to use the word \"candy\" was a woman", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is growing seedless cucumber good for a gardener with entomophobia?\nFacts: Seedless cucumber fruit does not require pollination Cucumber plants need insects to pollinate them Entomophobia is a fear of insects\nAnswer (True or False):", "ct_subject": "Seedless cucumber fruit does not require pollination Cucumber plants need insects to pollinate them Entomophobia is a fear of insects", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99989486"], [" it", "0.9999943"], [" true", "1.0"], [" that", "1.0"], [" the", "1.0"], [" first", "0.99999416"], [" person", "0.8308565"], [" to", "1.0"], [" use", "0.99487585"], [" the", "0.9999981"], [" word", "1.0"], [" \"", "1.0"], ["c", "0.99999595"], ["andy", "0.7664267"], ["\"", "1.0"], [" was", "1.0"], [" a", "1.0"], [" woman", "0.32000583"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Yes", "0.86002487"], [",", "0.9997998"], [" the", "0.9999999"], [" word", "0.9114756"], [" \"", "0.9999846"], ["c", "1.0"], ["andy", "1.0"], ["\"", "1.0"], [" was", "1.0"], [" first", "0.9790565"], [" used", "1.0"], [" by", "0.9076364"], [" a", "1.0"], [" woman", "1.0"], [" in", "0.9942767"], [" the", "1.0"], [" year", "0.9457222"], [" 17", "0.9899638"], ["76", "0.8435843"], [".", "0.99976486"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" true", "1.0"], [" that", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" use", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"", "1.0"], [" was", "1.0"], [" a", "1.0"], [" woman", "1.0"]]}, "893": {"qid": "90e6d4060a0b911fe436", "term": "PayPal", "description": "Online financial services company based in San Jose, California", "question": "Would it be unusual to use paypal for drug deals?", "answer": true, "facts": ["Paypal prohibits the use of their platform for drugs or drug paraphernalia. ", "Using paypal leaves a digital footprint of any drug purchase."], "decomposition": ["Which kind of payments are prohibited on Paypal?", "Does #1 include payment for drug deals?"], "evidence": [[["no_evidence"], ["operation"]], [[["PayPal-55"]], ["operation"]], [[["Reception of WikiLeaks-37"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"Pizza Hut\" and a \"Pizza Hut Pizza\"?\nFacts: Pizza Hut is a chain of restaurants. Pizza Hut Pizza is a pizza.\nAnswer (True or False): True\nQuestion: What is the difference between a \"Pizza Hut\" and a \"Pizza Hut Pizza\"?\nFacts: Pizza Hut is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would it be unusual to use paypal for drug deals?\nFacts: Paypal prohibits the use of their platform for drugs or drug paraphernalia.  Using paypal leaves a digital footprint of any drug purchase.\nAnswer (True or False):", "ct_subject": "Paypal prohibits the use of their platform for drugs or drug paraphernalia.  Using paypal leaves a digital footprint of any drug purchase.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999943"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9999739"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["P", "0.4080987"], ["izza", "0.7711842"], [" Hut", "0.9985239"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["P", "1.0"], ["izza", "1.0"], [" Hut", "1.0"], [" Pizza", "0.99838936"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Pizza", "0.99886143"], [" Hut", "1.0"], [" is", "0.9999763"], [" a", "1.0"], [" chain", "0.99999225"], [" of", "1.0"], [" restaurants", "0.9999999"], [".", "0.9729435"], [" Pizza", "1.0"], [" Hut", "1.0"], [" Pizza", "1.0"], [" is", "1.0"], [" a", "1.0"], [" pizza", "0.9997379"], [".", "1.0"], ["\n", "0.9999999"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["P", "1.0"], ["izza", "1.0"], [" Hut", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["P", "1.0"], ["izza", "1.0"], [" Hut", "1.0"], [" Pizza", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Pizza", "1.0"], [" Hut", "1.0"], [" is", "1.0"]]}, "894": {"qid": "3f3050d10948d4d99cff", "term": "Casio", "description": "Japanese electronics company", "question": "Is Casio's founding year a composite number?", "answer": true, "facts": ["Electronics company Casio was founded in 1946.", "A composite number is a number that can be divided by numbers other than 1 and itself.", "1946 can be divided by 278 and 7."], "decomposition": ["What condition(s) makes a number composite?", "When was Casio founded?", "Does #2 satisfy #1?"], "evidence": [[[["Composite number-1"]], [["Casio-2"]], ["operation"]], [[["Composite number-2"]], [["Casio-2"]], ["operation"]], [[["Condition number-4"], "no_evidence"], [["Casio-2"], "operation"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the number of the year in the year of the year?\nFacts: The year of the year is the number of the year in the year of the year.\nAnswer (True or False): True\nQuestion: Is the number of the year in the year of the year?\nFacts: The year of the year is the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Casio's founding year a composite number?\nFacts: Electronics company Casio was founded in 1946. A composite number is a number that can be divided by numbers other than 1 and itself. 1946 can be divided by 278 and 7.\nAnswer (True or False):", "ct_subject": "Electronics company Casio was founded in 1946. A composite number is a number that can be divided by numbers other than 1 and itself. 1946 can be divided by 278 and 7.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99423695"], [" the", "1.0"], [" number", "1.0"], [" of", "1.0"], [" the", "0.6287468"], [" year", "1.0"], [" in", "0.97833884"], [" the", "0.99912995"], [" year", "1.0"], [" of", "0.99998856"], [" the", "1.0"], [" year", "0.9999901"], ["?", "0.9999995"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" year", "0.99349505"], [" of", "1.0"], [" the", "1.0"], [" year", "1.0"], [" is", "1.0"], [" the", "0.99999905"], [" number", "0.94969785"], [" of", "1.0"], [" the", "0.9999964"], [" year", "1.0"], [" in", "0.99249834"], [" the", "1.0"], [" year", "1.0"], [" of", "1.0"], [" the", "1.0"], [" year", "1.0"], [".", "1.0"], ["\n", "0.9999913"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" number", "1.0"], [" of", "1.0"], [" the", "1.0"], [" year", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" of", "1.0"], [" the", "1.0"], [" year", "1.0"], ["?", "0.99999917"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" year", "1.0"], [" of", "1.0"], [" the", "1.0"], [" year", "1.0"], [" is", "1.0"], [" the", "1.0"]]}, "895": {"qid": "610c03ac321041d868e2", "term": "Lord Voldemort", "description": "Fictional character of Harry Potter series", "question": "Would half muggle wizards fear Lord Voldemort?", "answer": true, "facts": ["Lord Voldemort created a slur to describe half muggle wizards, calling them mudbloods.", "Lord Voldemort sought to purge the wizarding world of half muggle wizards through death."], "decomposition": ["What did Lord Voldemort seek to do to half muggle wizards?", "Is #1 enough to instill fear in them?"], "evidence": [[[["Lord Voldemort-2"]], ["no_evidence"]], [[["Lord Voldemort-2"]], ["operation"]], [[["Lord Voldemort-4", "Lord Voldemort-6"], "no_evidence"], [["Fear-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a wizard and a witch?\nFacts: A wizard is a person who has been trained to use magic. A witch is a person who has been trained to use magic.\nAnswer (True or False): True\nQuestion: What is the difference between a wizard and a witch?\nFacts: A wizard is a person", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would half muggle wizards fear Lord Voldemort?\nFacts: Lord Voldemort created a slur to describe half muggle wizards, calling them mudbloods. Lord Voldemort sought to purge the wizarding world of half muggle wizards through death.\nAnswer (True or False):", "ct_subject": "Lord Voldemort created a slur to describe half muggle wizards, calling them mudbloods. Lord Voldemort sought to purge the wizarding world of half muggle wizards through death.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.91049093"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.92337686"], [" between", "1.0"], [" a", "1.0"], [" wizard", "0.9837969"], [" and", "1.0"], [" a", "1.0"], [" witch", "0.9999999"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.99999976"], [" wizard", "1.0"], [" is", "1.0"], [" a", "1.0"], [" person", "0.96119034"], [" who", "1.0"], [" has", "1.0"], [" been", "0.9994844"], [" trained", "0.9999994"], [" to", "0.99999523"], [" use", "1.0"], [" magic", "1.0"], [".", "1.0"], [" A", "1.0"], [" witch", "1.0"], [" is", "1.0"], [" a", "1.0"], [" person", "1.0"], [" who", "1.0"], [" has", "1.0"], [" been", "0.9999485"], [" trained", "1.0"], [" to", "1.0"], [" use", "1.0"], [" magic", "0.99965584"], [".", "0.9996592"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" wizard", "1.0"], [" and", "1.0"], [" a", "1.0"], [" witch", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" wizard", "1.0"], [" is", "1.0"], [" a", "1.0"], [" person", "1.0"]]}, "896": {"qid": "f51a52b400daa8f80388", "term": "Casablanca", "description": "City / State in Casablanca-Settat, Morocco", "question": "Is it hard to get a BLT in Casablanca?", "answer": true, "facts": ["A BLT is a sandwich consisting of bacon, lettuce and tomato", "Casablanca is predominantly Muslim", "Islam forbids the consumption of pork and pork products"], "decomposition": ["What is the predominant religion of Casablanca?", "What dietary restrictions does #1 impose?", "What all goes on a BLT?", "Are there items common to both #2 and #3?"], "evidence": [[[["Casablanca-43"]], [["Islamic culture-45"]], [["BLT-1"]], ["operation"]], [[["Casablanca-43"]], [["Haram-13"]], [["BLT-7"]], ["operation"]], [[["Casablanca-43"]], [["Islamic culture-45"]], [["BLT-1"]], [["Bacon-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it hard to get a BLT in Casablanca?\nFacts: A BLT is a sandwich consisting of bacon, lettuce and tomato Casablanca is predominantly Muslim Islam forbids the consumption of pork and pork products\nAnswer (True or False): True\nQuestion: Is it hard to get a BLT in Casablanca?\nFacts: A BL", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is it hard to get a BLT in Casablanca?\nFacts: A BLT is a sandwich consisting of bacon, lettuce and tomato Casablanca is predominantly Muslim Islam forbids the consumption of pork and pork products\nAnswer (True or False):", "ct_subject": "A BLT is a sandwich consisting of bacon, lettuce and tomato Casablanca is predominantly Muslim Islam forbids the consumption of pork and pork products", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99837035"], [" it", "1.0"], [" hard", "0.99999964"], [" to", "1.0"], [" get", "1.0"], [" a", "1.0"], [" BL", "1.0"], ["T", "1.0"], [" in", "1.0"], [" Cas", "1.0"], ["abl", "1.0"], ["anca", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" BL", "1.0"], ["T", "1.0"], [" is", "1.0"], [" a", "1.0"], [" sandwich", "1.0"], [" consisting", "1.0"], [" of", "1.0"], [" bacon", "1.0"], [",", "1.0"], [" lettuce", "1.0"], [" and", "1.0"], [" tomato", "1.0"], [" Cas", "1.0"], ["abl", "1.0"], ["anca", "1.0"], [" is", "1.0"], [" predominantly", "1.0"], [" Muslim", "1.0"], [" Islam", "1.0"], [" forbids", "1.0"], [" the", "1.0"], [" consumption", "1.0"], [" of", "1.0"], [" pork", "1.0"], [" and", "1.0"], [" pork", "1.0"], [" products", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" hard", "1.0"], [" to", "1.0"], [" get", "1.0"], [" a", "1.0"], [" BL", "1.0"], ["T", "1.0"], [" in", "1.0"], [" Cas", "1.0"], ["abl", "1.0"], ["anca", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" BL", "1.0"]]}, "897": {"qid": "39e2e14c7916898a717d", "term": "Ammonia", "description": "Chemical compound of nitrogen and hydrogen", "question": "Would a dog easily notice ammonia?", "answer": true, "facts": ["Ammonia has a characteristic pungent smell.", "Dogs have an extremely strong sense of smell, almost 40 times as sensitive as humans."], "decomposition": ["What common chemical has a characteristic pungent smell?", "What common pet has a sense of smell ten thousand to a hundred thousand times better than humans?", "Can #2 sense #1?"], "evidence": [[[["Ammonia-1"]], [["Dog anatomy-117"], "no_evidence"], ["operation"]], [[["Ammonia-1"], "no_evidence"], [["Dog anatomy-117"], "no_evidence"], ["operation"]], [[["Ammonia-1"]], [["Tracking (dog)-3"]], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a dog and a cat?\nFacts: A cat is a mammal. A dog is a carnivore.\nAnswer (True or False): True\nQuestion: What is the difference between a dog and a cat?\nFacts: A cat is a mammal. A dog is a carnivore.\nAnswer (True or False): True\nQuestion: What is the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a dog easily notice ammonia?\nFacts: Ammonia has a characteristic pungent smell. Dogs have an extremely strong sense of smell, almost 40 times as sensitive as humans.\nAnswer (True or False):", "ct_subject": "Ammonia has a characteristic pungent smell. Dogs have an extremely strong sense of smell, almost 40 times as sensitive as humans.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99899954"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9751984"], [" between", "1.0"], [" a", "1.0"], [" dog", "0.24245635"], [" and", "1.0"], [" a", "1.0"], [" cat", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.6375563"], [" cat", "0.99995565"], [" is", "0.9317688"], [" a", "1.0"], [" mammal", "0.99999917"], [".", "0.20391302"], [" A", "0.68845534"], [" dog", "1.0"], [" is", "1.0"], [" a", "1.0"], [" carniv", "0.99999857"], ["ore", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" dog", "1.0"], [" and", "1.0"], [" a", "1.0"], [" cat", "0.9999999"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" cat", "1.0"], [" is", "1.0"], [" a", "1.0"], [" mammal", "1.0"], [".", "1.0"], [" A", "1.0"], [" dog", "1.0"], [" is", "1.0"], [" a", "1.0"], [" carniv", "1.0"], ["ore", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"]]}, "898": {"qid": "d1ca8206707ec8277030", "term": "Snowdon", "description": "highest mountain in Wales", "question": "Would the yearly precipitation on Snowdon submerge an upright bowling pin?", "answer": true, "facts": ["Snowdown gets about 200 inches of precipitation a year ", "A standard bowling pin is one foot, three inches tall"], "decomposition": ["How much precipitation does Snowdown get yearly?", "How tall is a standard bowling pin?", "Is #1 more than #2?"], "evidence": [[[["Snowdon-9"]], [["Bowling pin-2"]], [["Bowling pin-2", "Snowdon-9"]]], [[["Snowdon-9"]], [["Bowling pin-2"]], ["operation"]], [[["Snowdon-9"]], [["Bowling pin-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human being?\nFacts: The average human weight is about 100 pounds\nAnswer (True or False): True\nQuestion: What is the average height of a human being?\nFacts: The average height of a human being is about 5 feet, 8 inches tall\nAnswer (True or False): True\nQuestion: What is the average weight of a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would the yearly precipitation on Snowdon submerge an upright bowling pin?\nFacts: Snowdown gets about 200 inches of precipitation a year  A standard bowling pin is one foot, three inches tall\nAnswer (True or False):", "ct_subject": "Snowdown gets about 200 inches of precipitation a year  A standard bowling pin is one foot, three inches tall", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9998733"], [" is", "1.0"], [" the", "1.0"], [" average", "0.9959675"], [" weight", "0.99991465"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "0.99349606"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99940443"], [" average", "1.0"], [" human", "0.99661404"], [" weight", "0.96624905"], [" is", "1.0"], [" about", "1.0"], [" 100", "0.99323374"], [" pounds", "1.0"], ["\n", "0.999747"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" height", "0.3422862"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" height", "0.9145653"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "0.9999982"], [" is", "1.0"], [" about", "1.0"], [" 5", "0.059931718"], [" feet", "1.0"], [",", "1.0"], [" 8", "0.8192063"], [" inches", "1.0"], [" tall", "0.42248604"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "1.0"], [" of", "1.0"], [" a", "1.0"]]}, "899": {"qid": "0b7787655f4a9d600dfc", "term": "Clark Gable", "description": "American actor", "question": "Did Clark Gable marry more women once than Richard Burton?", "answer": true, "facts": ["Richard Burton was married to Elizabeth Taylor twice. ", "Richard Burton married Sally Burton, Suzy Hunt, and Sybil Williams once.", "Clark Gable was married to five different women, one time each."], "decomposition": ["How many different women was Richard Burton married to?", "How many different wives did Clark Gable have?", "Is #2 more than #1?"], "evidence": [[[["Richard Burton-57"]], [["Clark Gable-78"]], ["operation"]], [[["Richard Burton-57"]], [["Clark Gable-14", "Clark Gable-34", "Clark Gable-39"]], ["operation"]], [[["Richard Burton-57"]], ["no_evidence"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Beatles have more than one baby?\nFacts: The Beatles had two children. Paul McCartney had two children. John Lennon had two children. George Harrison had two children. Ringo Starr had two children.\nAnswer (True or False): True\nQuestion: Did the Beatles have more than one child?", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Clark Gable marry more women once than Richard Burton?\nFacts: Richard Burton was married to Elizabeth Taylor twice.  Richard Burton married Sally Burton, Suzy Hunt, and Sybil Williams once. Clark Gable was married to five different women, one time each.\nAnswer (True or False):", "ct_subject": "Richard Burton was married to Elizabeth Taylor twice.  Richard Burton married Sally Burton, Suzy Hunt, and Sybil Williams once. Clark Gable was married to five different women, one time each.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999999"], [" the", "1.0"], [" Beatles", "0.9449159"], [" have", "0.99999976"], [" more", "0.99998915"], [" than", "1.0"], [" one", "1.0"], [" baby", "0.80801964"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Beatles", "1.0"], [" had", "1.0"], [" two", "0.99693227"], [" children", "1.0"], [".", "0.99989295"], [" Paul", "0.9999999"], [" McCartney", "1.0"], [" had", "0.99934644"], [" two", "0.3818136"], [" children", "1.0"], [".", "1.0"], [" John", "0.9099595"], [" Lennon", "1.0"], [" had", "1.0"], [" two", "0.99999297"], [" children", "1.0"], [".", "1.0"], [" George", "1.0"], [" Harrison", "1.0"], [" had", "1.0"], [" two", "1.0"], [" children", "1.0"], [".", "1.0"], [" Ring", "0.9999993"], ["o", "1.0"], [" Starr", "1.0"], [" had", "1.0"], [" two", "1.0"], [" children", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" Beatles", "1.0"], [" have", "1.0"], [" more", "1.0"], [" than", "1.0"], [" one", "1.0"], [" child", "0.9430156"], ["?", "1.0"]]}, "900": {"qid": "7b02fc57b8949146bd28", "term": "Comic book", "description": "Publication of comics art", "question": "Does Disney own a major comic book publisher?", "answer": true, "facts": ["The three biggest comic book publishers are DC, Marvel, and IDW.", "Disney has owned Marvel since 2007."], "decomposition": ["What are the top three biggest comic book publishers?", "What comic book publishers does Disney own?", "Is there any overlap between #1 and #2?"], "evidence": [[[["Big Two Comics-1"], "no_evidence"], [["Marvel Comics-1"]], ["operation"]], [[["American comic book-39", "Marvel Comics-1"], "no_evidence"], [["Marvel Comics-1"]], ["operation"]], [[["Comic book-2"]], [["Marvel Entertainment-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is there a Disney theme park in the United States?\nFacts: There is a Disney theme park in Orlando, Florida.\nAnswer (True or False): True\nQuestion: Is there a Disney theme park in the United Kingdom?\nFacts: There is a Disney theme park in London, England.\nAnswer (True or False): True\nQuestion: Is there a Disney theme park in Canada?\nF", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Disney own a major comic book publisher?\nFacts: The three biggest comic book publishers are DC, Marvel, and IDW. Disney has owned Marvel since 2007.\nAnswer (True or False):", "ct_subject": "The three biggest comic book publishers are DC, Marvel, and IDW. Disney has owned Marvel since 2007.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.032080896"], [" there", "0.18614745"], [" a", "1.0"], [" Disney", "0.9986777"], [" theme", "0.999997"], [" park", "1.0"], [" in", "0.9182045"], [" the", "0.98907346"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "0.9973537"], [" is", "1.0"], [" a", "0.9826027"], [" Disney", "1.0"], [" theme", "1.0"], [" park", "1.0"], [" in", "1.0"], [" Orlando", "0.9999777"], [",", "1.0"], [" Florida", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999988"], [" there", "1.0"], [" a", "1.0"], [" Disney", "1.0"], [" theme", "0.99999595"], [" park", "1.0"], [" in", "1.0"], [" the", "0.08220558"], [" United", "1.0"], [" Kingdom", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "1.0"], [" is", "1.0"], [" a", "1.0"], [" Disney", "1.0"], [" theme", "1.0"], [" park", "1.0"], [" in", "1.0"], [" London", "1.0"], [",", "0.9999733"], [" England", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" there", "1.0"], [" a", "1.0"], [" Disney", "1.0"], [" theme", "1.0"], [" park", "1.0"], [" in", "1.0"], [" Canada", "0.99989045"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"]]}, "901": {"qid": "d15cac26462700b9d6ca", "term": "Jackfruit", "description": "species of plant", "question": "Can jackfruit be used as a weapon?", "answer": true, "facts": ["Jackfruit is the fruit of a species of plant called the Jacktree.", "Jackfruit can weigh up to one hundred and twenty pounds.", "Jackfruit is covered in little spikes.", "Jackfruit can be thrown or flung at an enemy.", "A weapon is a thing that is used to cause bodily harm."], "decomposition": ["What are the prominent physical features of a jackfruit?", "Does #1 make it a suitable weapon?"], "evidence": [[[["Jackfruit-2"]], ["operation"]], [[["Jackfruit-12"]], [["Tubercle-1"], "operation"]], [[["Jackfruit-12"]], [["Jackfruit-12", "Weapon-2"], "no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a man and a woman?\nFacts: A man is a male human being. A woman is a female human being.\nAnswer (True or False): True\nQuestion: What is the difference between a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can jackfruit be used as a weapon?\nFacts: Jackfruit is the fruit of a species of plant called the Jacktree. Jackfruit can weigh up to one hundred and twenty pounds. Jackfruit is covered in little spikes. Jackfruit can be thrown or flung at an enemy. A weapon is a thing that is used to cause bodily harm.\nAnswer (True or False):", "ct_subject": "Jackfruit is the fruit of a species of plant called the Jacktree. Jackfruit can weigh up to one hundred and twenty pounds. Jackfruit is covered in little spikes. Jackfruit can be thrown or flung at an enemy. A weapon is a thing that is used to cause bodily harm.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99920005"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9289135"], [" between", "1.0"], [" a", "0.9999999"], [" man", "0.9010324"], [" and", "1.0"], [" a", "1.0"], [" woman", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" man", "1.0"], [" is", "1.0"], [" a", "1.0"], [" male", "0.27699533"], [" human", "0.9999999"], [" being", "1.0"], [".", "1.0"], [" A", "1.0"], [" woman", "1.0"], [" is", "1.0"], [" a", "1.0"], [" female", "1.0"], [" human", "1.0"], [" being", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"]]}, "902": {"qid": "763ef31236d06e080681", "term": "James Bond", "description": "Media franchise about a British spy", "question": "Do the James Bond and Doctor Who series have a similarity in format?", "answer": true, "facts": ["The character of James Bond has been played by numerous actors. ", "The character of The Doctor from Doctor Who has been played by many actors."], "decomposition": ["Who has played James Bond?", "Who has played the Doctor? ", "Are multiple actors listed for #1 and #2?"], "evidence": [[[["Portrayal of James Bond in film-4"]], [["The Doctor (Doctor Who)-111"]], ["operation"]], [[["Portrayal of James Bond in film-4"]], [["The Doctor (Doctor Who)-1"]], ["operation"]], [[["James Bond-2"], "no_evidence"], [["The Doctor (Doctor Who)-1"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: The first person to be born in the United States was Mary Ann Cotton.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: The first person to be born in the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do the James Bond and Doctor Who series have a similarity in format?\nFacts: The character of James Bond has been played by numerous actors.  The character of The Doctor from Doctor Who has been played by many actors.\nAnswer (True or False):", "ct_subject": "The character of James Bond has been played by numerous actors.  The character of The Doctor from Doctor Who has been played by many actors.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999999"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9935535"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9247087"], [" person", "0.93647295"], [" to", "1.0"], [" be", "0.9999989"], [" born", "0.99900323"], [" in", "1.0"], [" the", "1.0"], [" United", "0.9921029"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99999654"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" Mary", "0.6958597"], [" Ann", "0.9992142"], [" Cotton", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"]]}, "903": {"qid": "7deb49d3ba50fb75a04f", "term": "Capsaicin", "description": "chemical compound", "question": "Is Black Lives Matter connected with capsaicin?", "answer": true, "facts": ["Black Lives Matter has held numerous protests", "Protesters at Black Lives Matter events have had pepper spray used against them by police", "Capsaicin is the main ingredient of pepper spray"], "decomposition": ["What are the common practical applications of the capsaicin compund?", "What kind of activities does the Black Lives Matter movement engage in?", "Is any of #1 relevant to any of #2?"], "evidence": [[[["Capsaicin-11", "Capsaicin-5", "Capsaicin-7"]], [["Black Lives Matter-20"]], [["Pepper spray-1", "Riot control-1"], "operation"]], [[["Capsaicin-2"]], [["Black Lives Matter-30"]], ["operation"]], [[["Capsaicin-11"]], [["Movement for Black Lives-6"]], [["Movement for Black Lives-7"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the moon made of cheese?\nFacts: The moon is made of cheese\nAnswer (True or False): True\nQuestion: Is the moon made of cheese?\nFacts: The moon is made of cheese\nAnswer (True or False): True\nQuestion: Is the moon made of cheese?\nFacts: The moon is made of cheese\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Black Lives Matter connected with capsaicin?\nFacts: Black Lives Matter has held numerous protests Protesters at Black Lives Matter events have had pepper spray used against them by police Capsaicin is the main ingredient of pepper spray\nAnswer (True or False):", "ct_subject": "Black Lives Matter has held numerous protests Protesters at Black Lives Matter events have had pepper spray used against them by police Capsaicin is the main ingredient of pepper spray", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" moon", "0.9331"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" moon", "1.0"], [" is", "1.0"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], ["\n", "0.52479017"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" moon", "0.99997866"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" moon", "1.0"], [" is", "1.0"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" moon", "1.0"], [" is", "1.0"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], ["\n", "1.0"]]}, "904": {"qid": "4b2e265815601bfa4152", "term": "Astrophotography", "description": "specialized type of photography for recording images of astronomical objects and large areas of the night sky", "question": "Is it difficult to conduct astrophotography in the summer in Sweden?", "answer": true, "facts": ["Astrophotography is used to photograph the night sky.", "Swedish summers have short nights."], "decomposition": ["What does Astrophotography take photos of?", "Are #1's short in the summers of Sweden?"], "evidence": [[[["Astrophotography-1"]], [["Sweden-59"], "operation"]], [[["Astrophotography-1"]], [["Sweden-56"], "operation"]], [[["Astrophotography-1"]], [["Tourism in Sweden-6"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it difficult to conduct astrophotography in the summer in Sweden?\nFacts: Astrophotography is used to photograph the night sky. Swedish summers have short nights.\nAnswer (True or False): True\nQuestion: Is it difficult to conduct astrophotography in the summer in Sweden?\nFacts: Astrophotography is used to photograph the night sky. Swedish summers have short nights.\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is it difficult to conduct astrophotography in the summer in Sweden?\nFacts: Astrophotography is used to photograph the night sky. Swedish summers have short nights.\nAnswer (True or False):", "ct_subject": "Astrophotography is used to photograph the night sky. Swedish summers have short nights.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.94136804"], [" it", "1.0"], [" difficult", "0.99999666"], [" to", "1.0"], [" conduct", "1.0"], [" astroph", "1.0"], ["ot", "1.0"], ["ography", "1.0"], [" in", "1.0"], [" the", "1.0"], [" summer", "0.9999999"], [" in", "1.0"], [" Sweden", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Ast", "1.0"], ["roph", "1.0"], ["ot", "1.0"], ["ography", "1.0"], [" is", "1.0"], [" used", "1.0"], [" to", "1.0"], [" photograph", "1.0"], [" the", "1.0"], [" night", "1.0"], [" sky", "1.0"], [".", "1.0"], [" Swedish", "1.0"], [" summers", "1.0"], [" have", "1.0"], [" short", "1.0"], [" nights", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "0.99999964"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" difficult", "1.0"], [" to", "1.0"], [" conduct", "1.0"], [" astroph", "1.0"], ["ot", "1.0"], ["ography", "1.0"], [" in", "1.0"], [" the", "1.0"], [" summer", "1.0"], [" in", "1.0"], [" Sweden", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Ast", "1.0"], ["roph", "1.0"], ["ot", "1.0"], ["ography", "1.0"], [" is", "1.0"], [" used", "1.0"], [" to", "1.0"], [" photograph", "1.0"], [" the", "1.0"], [" night", "1.0"], [" sky", "1.0"], [".", "1.0"], [" Swedish", "1.0"], [" summers", "1.0"], [" have", "1.0"], [" short", "1.0"], [" nights", "1.0"], [".", "1.0"], ["\n", "1.0"]]}, "905": {"qid": "ef67433aeac38f7fada4", "term": "Tokyo Tower", "description": "observation tower", "question": "Did Tokyo Tower designers appreciate Stephen Sauvestre?", "answer": true, "facts": ["Tokyo Tower is a communications tower in Japan, built in 1958, that was inspired by the Eiffel Tower.", "Stephen Sauvestre was the architect of the the Eiffel Tower which was competed in 1889."], "decomposition": ["Which architectural design is Stephen Sauvestre famous for?", "Was #1 influential in the design of the Tokyo Tower?"], "evidence": [[[["Stephen Sauvestre-1"]], [["Tokyo Tower-1"]]], [[["Stephen Sauvestre-3"]], ["operation"]], [[["Stephen Sauvestre-1"]], [["Tokyo Tower-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the first person to use the word \"candy\" in the English language was a man named John Candy?\nFacts: John Candy was a candy maker in the United States. He was the first person to use the word \"candy\" in the English language.\nAnswer (True or False):", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Tokyo Tower designers appreciate Stephen Sauvestre?\nFacts: Tokyo Tower is a communications tower in Japan, built in 1958, that was inspired by the Eiffel Tower. Stephen Sauvestre was the architect of the the Eiffel Tower which was competed in 1889.\nAnswer (True or False):", "ct_subject": "Tokyo Tower is a communications tower in Japan, built in 1958, that was inspired by the Eiffel Tower. Stephen Sauvestre was the architect of the the Eiffel Tower which was competed in 1889.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9995346"], [" the", "1.0"], [" first", "0.9740752"], [" person", "0.99298966"], [" to", "1.0"], [" use", "0.8867858"], [" the", "0.9999988"], [" word", "1.0"], [" \"", "1.0"], ["c", "0.99980277"], ["andy", "0.8626862"], ["\"", "1.0"], [" in", "0.26328978"], [" the", "0.16796352"], [" English", "0.19006075"], [" language", "1.0"], [" was", "0.99969864"], [" a", "1.0"], [" man", "0.9363823"], [" named", "1.0"], [" John", "0.99997985"], [" Candy", "0.2361987"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "0.999923"], [" Candy", "1.0"], [" was", "1.0"], [" a", "0.99823594"], [" candy", "1.0"], [" maker", "0.059654642"], [" in", "0.9989317"], [" the", "0.9999999"], [" United", "0.99811697"], [" States", "1.0"], [".", "1.0"], [" He", "1.0"], [" was", "0.9999999"], [" the", "0.94917655"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" use", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["andy", "1.0"], ["\"", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"]]}, "906": {"qid": "b921f2496791cca37167", "term": "Oyster", "description": "salt-water bivalve mollusc", "question": "Can oysters be preserved without refrigeration? ", "answer": true, "facts": ["In some types of Korean kimchi, oysters are placed between the leaves of nappa cabbage. ", "Many grocery stores carry canned oysters in the shelf stable section. "], "decomposition": ["How are oysters preserved in various types of Korean kimchi?", "What are the common methods of preserving oysters in grocery stores?", "Do any of #1 or #2 not require refrigeration?"], "evidence": [[[["Kimchi-28"], "no_evidence"], [["Oyster-57", "Oyster-61"]], ["operation"]], [[["Korean cuisine-25"], "no_evidence"], ["no_evidence"], ["operation"]], [[["Oyster-48"], "no_evidence"], [["Oyster-35"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake\" with a \"snowflake\" with a \"snowflake\"?\nFacts: A snowflake is a single crystal. A snowflake with a \"snowflake\" with a \"snowflake\" is a cluster of crystals.\nAnswer (", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can oysters be preserved without refrigeration? \nFacts: In some types of Korean kimchi, oysters are placed between the leaves of nappa cabbage.  Many grocery stores carry canned oysters in the shelf stable section. \nAnswer (True or False):", "ct_subject": "In some types of Korean kimchi, oysters are placed between the leaves of nappa cabbage.  Many grocery stores carry canned oysters in the shelf stable section. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99996734"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.99828786"], [" between", "1.0"], [" a", "0.99999976"], [" \"", "1.0"], ["s", "0.80482304"], ["now", "0.8285634"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\"", "0.9310884"], [" with", "0.99998224"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\"", "0.99973863"], [" with", "0.92852855"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\"?", "0.99991167"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999757"], [" snow", "0.9976834"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "0.69856423"], [" crystal", "0.99768615"], [".", "0.8743311"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" with", "1.0"], [" a", "1.0"], [" \"", "0.9999956"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\"", "1.0"], [" with", "0.9999999"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" cluster", "0.9901007"], [" of", "1.0"], [" crystals", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"]]}, "907": {"qid": "02d58ebdbdb3a1cdc79e", "term": "Hollywood", "description": "District in Los Angeles, California, United States", "question": "Is it normally unnecessary to wear a coat in Hollywood in July?", "answer": true, "facts": ["The average high temperature in Hollywood in July is 77.2\u00b0F.", "The average low temperature in Hollywood in July is 61.5\u00b0F.", "A coat is a garment worn on the upper body for warmth."], "decomposition": ["What is the average high temperature in Hollywood in July?", "What is the average low temperature in Hollywood in July?", "What temperature does one usually wear a coat?", "Is #3 outside of #1 to #2?"], "evidence": [[[["Hollywood-1", "Los Angeles-36"]], [["Los Angeles-34"]], [["Overcoat-1", "Winter-5"]], ["operation"]], [[["Climate of Los Angeles-5", "Climate of Los Angeles-7"], "no_evidence"], [["Climate of Los Angeles-7"], "no_evidence"], [["Winter clothing-1"]], ["operation"]], [[["Los Angeles-35"], "no_evidence"], [["Los Angeles-35"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a man in the United States?\nFacts: The average weight of a man in the United States is about 220 pounds.\nAnswer (True or False): True\nQuestion: What is the average weight of a woman in the United States?\nFacts:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is it normally unnecessary to wear a coat in Hollywood in July?\nFacts: The average high temperature in Hollywood in July is 77.2\u00b0F. The average low temperature in Hollywood in July is 61.5\u00b0F. A coat is a garment worn on the upper body for warmth.\nAnswer (True or False):", "ct_subject": "The average high temperature in Hollywood in July is 77.2\u00b0F. The average low temperature in Hollywood in July is 61.5\u00b0F. A coat is a garment worn on the upper body for warmth.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9988508"], [" is", "1.0"], [" the", "1.0"], [" average", "0.9985461"], [" weight", "0.8960795"], [" of", "1.0"], [" a", "0.9999999"], [" man", "0.1843437"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" weight", "1.0"], [" of", "1.0"], [" a", "1.0"], [" man", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "1.0"], [" about", "0.99964225"], [" 220", "0.012267569"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "0.991618"], [" of", "1.0"], [" a", "1.0"], [" woman", "0.9994485"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"]]}, "908": {"qid": "ffd8a720264778c0fd6e", "term": "Toyota Hilux", "description": "Series of light commercial vehicles produced by the Japanese car-manufacturer Toyota.", "question": "Can the Toyota Hilux tip the scales against Mr. Ed?", "answer": true, "facts": ["The current generation of Toyota Hilux weighs at least 4,310 lbs", "Mr. Ed was portrayed by an adult horse", "The average adult horse weighs up to 2,000 lbs"], "decomposition": ["What does a Toyota Hilux weigh?", "What does an adult horse weigh?", "Is #1 greater than #2?"], "evidence": [[["no_evidence"], [["Horse-13"]], ["no_evidence", "operation"]], [["no_evidence"], [["Horse-13"]], ["operation"]], [[["Toyota Hilux-1"], "no_evidence"], [["Horse-12"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human?\nFacts: The average human weight is about 150 lbs.\nAnswer (True or False): True\nQuestion: What is the average weight of a horse?\nFacts: The average horse weight is about 1,000 lbs.\nAnswer (True or False): True\nQuestion: What", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can the Toyota Hilux tip the scales against Mr. Ed?\nFacts: The current generation of Toyota Hilux weighs at least 4,310 lbs Mr. Ed was portrayed by an adult horse The average adult horse weighs up to 2,000 lbs\nAnswer (True or False):", "ct_subject": "The current generation of Toyota Hilux weighs at least 4,310 lbs Mr. Ed was portrayed by an adult horse The average adult horse weighs up to 2,000 lbs", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9775063"], [" is", "1.0"], [" the", "1.0"], [" average", "0.9990482"], [" weight", "1.0"], [" of", "1.0"], [" a", "1.0"], [" human", "0.99851316"], ["?", "0.9999988"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.6035412"], [" weight", "0.37523505"], [" is", "1.0"], [" about", "0.61831874"], [" 150", "0.23656131"], [" lbs", "1.0"], [".", "0.9994192"], ["\n", "0.9999999"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999993"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "1.0"], [" of", "1.0"], [" a", "1.0"], [" horse", "0.99999976"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" horse", "0.99832624"], [" weight", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 1", "0.9998863"], [",", "1.0"], ["000", "0.99989986"], [" lbs", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"]]}, "909": {"qid": "a12eb8ae9b38c231a00f", "term": "Maroon 5", "description": "American pop punk band", "question": "Could Maroon 5 have hypothetically held a concert at Roman Colosseum?", "answer": true, "facts": ["The Roman Colosseum had a capacity of 87,000 people. ", "Maroon 5 has held concerts at Brazil's Allianz Parque, which has a capacity of close to 44,000.", "Almost 30,000 people attended Maroon 5's 2015 Madison Square Garden concert over two days."], "decomposition": ["How many spectators could the Roman Colosseum hold?", "How many people were in attendance at Maroon 5's largest concert?", "Is #1 greater than #2?"], "evidence": [[[["Colosseum-1"]], ["no_evidence"], ["operation"]], [[["Colosseum-1"]], [["Super Bowl LIII halftime show-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Colosseum-2"]], [["Maroon V Tour-1"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Could the Titanic have been sunk by a nuclear bomb?\nFacts: The Titanic was built to withstand a maximum of 10,000 tons of water.  The Titanic was built to withstand", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could Maroon 5 have hypothetically held a concert at Roman Colosseum?\nFacts: The Roman Colosseum had a capacity of 87,000 people.  Maroon 5 has held concerts at Brazil's Allianz Parque, which has a capacity of close to 44,000. Almost 30,000 people attended Maroon 5's 2015 Madison Square Garden concert over two days.\nAnswer (True or False):", "ct_subject": "The Roman Colosseum had a capacity of 87,000 people.  Maroon 5 has held concerts at Brazil's Allianz Parque, which has a capacity of close to 44,000. Almost 30,000 people attended Maroon 5's 2015 Madison Square Garden concert over two days.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Could", "0.94714826"], [" the", "0.99453163"], [" Titanic", "0.884716"], [" have", "1.0"], [" been", "0.9975858"], [" sunk", "0.99972266"], [" by", "1.0"], [" a", "1.0"], [" nuclear", "0.1653246"], [" bomb", "0.9999999"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Titanic", "1.0"], [" was", "0.99993193"], [" built", "0.2021802"], [" to", "1.0"], [" withstand", "0.8436316"], [" a", "1.0"], [" maximum", "0.99934393"], [" of", "1.0"], [" 10", "1.0"], [",", "1.0"], ["000", "1.0"], [" tons", "1.0"], [" of", "1.0"], [" water", "0.9999995"], [".", "1.0"], [" ", "0.049667414"], [" The", "0.999995"], [" Titanic", "0.96148425"], [" was", "1.0"], [" built", "0.9938659"], [" to", "1.0"], [" withstand", "1.0"]]}, "910": {"qid": "e5fa5f6c12bfa1aed955", "term": "Audiobook", "description": "recording of a text being read", "question": "Do Youtube viewers get unsolicited audiobook advice often?", "answer": true, "facts": ["Audible is one of the most common sponsors for Youtubers to have.", "Audible is an audiobook subscription service. ", "Audible ads typically involve discussing a book that the speaker has recently listened to."], "decomposition": ["What company is one of the most common sponsors for Youtubers to have?", "What do the ads for #1 typically involve?", "Does #2 involve someone giving audiobook advice?"], "evidence": [[[["Audible (store)-1"], "no_evidence"], [["Audible (store)-11"], "no_evidence"], ["operation"]], [[["YouTube-3"], "no_evidence"], [["YouTube-3"]], ["no_evidence"]], [[["Audible (store)-16"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Do you have a favorite book?\nFacts: I have a favorite book. It is called \"The Art of War\" by Sun Tzu. It is a book that I have read many times.\nAnswer (True or False): True\nQuestion: Do you have a favorite movie?\nF", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do Youtube viewers get unsolicited audiobook advice often?\nFacts: Audible is one of the most common sponsors for Youtubers to have. Audible is an audiobook subscription service.  Audible ads typically involve discussing a book that the speaker has recently listened to.\nAnswer (True or False):", "ct_subject": "Audible is one of the most common sponsors for Youtubers to have. Audible is an audiobook subscription service.  Audible ads typically involve discussing a book that the speaker has recently listened to.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Do", "0.30169043"], [" you", "1.0"], [" have", "0.99917126"], [" a", "0.99999857"], [" favorite", "1.0"], [" book", "0.2463914"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" I", "1.0"], [" have", "1.0"], [" a", "1.0"], [" favorite", "0.9936568"], [" book", "1.0"], [".", "0.9994479"], [" It", "0.9999995"], [" is", "0.40997356"], [" called", "0.11328957"], [" \"", "0.998684"], ["The", "1.0"], [" Art", "0.9999995"], [" of", "1.0"], [" War", "1.0"], ["\"", "1.0"], [" by", "1.0"], [" Sun", "1.0"], [" T", "1.0"], ["zu", "1.0"], [".", "1.0"], [" It", "0.96444273"], [" is", "1.0"], [" a", "1.0"], [" book", "0.7395929"], [" that", "1.0"], [" I", "0.9999987"], [" have", "0.9999981"], [" read", "1.0"], [" many", "1.0"], [" times", "1.0"], [".", "0.9948013"], ["\n", "0.74291307"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Do", "0.9999857"], [" you", "1.0"], [" have", "1.0"], [" a", "1.0"], [" favorite", "1.0"], [" movie", "0.9999999"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"]]}, "911": {"qid": "6b9f97d9cb0b477804df", "term": "Osama bin Laden", "description": "Co-founder of al-Qaeda", "question": "Did Osama bin Laden likely abstain from alcohol?", "answer": true, "facts": ["Osama bin Laden belonged to the religion of Islam.", "Islam prohibits the consumption of alcohol."], "decomposition": ["What religion was Osama bin Laden?", "Does #1 prohibit consumption of alcohol?"], "evidence": [[[["Osama bin Laden-10"]], [["Alcohol law-14"], "operation"]], [[["Osama bin Laden-10"]], [["Islamic culture-45"]]], [[["Osama bin Laden-16"]], [["Islamic dietary laws-9"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the United States have a \"War on Drugs\" during the 1980s?\nFacts: The United States did not have a \"War on Drugs\" during the 1980s. The United States had a \"War on Poverty\" during the 1980s.\nAnswer (True or False): True\nQuestion: Did the United States have a \"War on Drugs\" during the 1970s?\nFacts: The United States did not have", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Osama bin Laden likely abstain from alcohol?\nFacts: Osama bin Laden belonged to the religion of Islam. Islam prohibits the consumption of alcohol.\nAnswer (True or False):", "ct_subject": "Osama bin Laden belonged to the religion of Islam. Islam prohibits the consumption of alcohol.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" United", "0.9983575"], [" States", "1.0"], [" have", "0.9992048"], [" a", "1.0"], [" \"", "1.0"], ["War", "0.99996364"], [" on", "1.0"], [" Drugs", "1.0"], ["\"", "1.0"], [" during", "0.9212367"], [" the", "1.0"], [" 1980", "0.9999074"], ["s", "0.99999166"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "1.0"], [" States", "1.0"], [" did", "0.9671641"], [" not", "1.0"], [" have", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["War", "1.0"], [" on", "1.0"], [" Drugs", "1.0"], ["\"", "1.0"], [" during", "1.0"], [" the", "1.0"], [" 1980", "1.0"], ["s", "1.0"], [".", "1.0"], [" The", "0.99795246"], [" United", "1.0"], [" States", "1.0"], [" had", "0.99556196"], [" a", "1.0"], [" \"", "1.0"], ["War", "1.0"], [" on", "1.0"], [" Poverty", "0.90150416"], ["\"", "1.0"], [" during", "1.0"], [" the", "1.0"], [" 1980", "0.99998057"], ["s", "1.0"], [".", "1.0"], ["\n", "0.9998754"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" have", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["War", "1.0"], [" on", "1.0"], [" Drugs", "0.99999297"], ["\"", "1.0"], [" during", "1.0"], [" the", "1.0"], [" 1970", "0.022145081"], ["s", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "1.0"], [" States", "1.0"], [" did", "1.0"], [" not", "1.0"], [" have", "1.0"]]}, "912": {"qid": "c664e847b957da434f15", "term": "Hair", "description": "protein filament that grows from follicles found in the dermis, or skin", "question": "Is it safe to eat hair?", "answer": true, "facts": ["Hair is made of keratin.", "Food manufacturers use L-cysteine as a food additive.", "L-cysteine is made from keratin."], "decomposition": ["What is hair made of?", "What else is made from #1?", "Are any of #2 used in food production?"], "evidence": [[[["Hair-2"]], [["Alpha-keratin-1"]], ["no_evidence", "operation"]], [[["Hair-6"]], [["Hair-6"]], [["Food-1"]]], [[["Hair-1"]], [["Beef-1"]], [["Hamburger-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it safe to eat hair?\nFacts: Hair is made of keratin. Food manufacturers use L-cysteine as a food additive. L-cysteine is made from keratin.\nAnswer (True or False): True\nQuestion: Is it safe to eat hair?\nFacts: Hair is made of keratin. Food manufacturers use L-cyste", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is it safe to eat hair?\nFacts: Hair is made of keratin. Food manufacturers use L-cysteine as a food additive. L-cysteine is made from keratin.\nAnswer (True or False):", "ct_subject": "Hair is made of keratin. Food manufacturers use L-cysteine as a food additive. L-cysteine is made from keratin.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999933"], [" it", "1.0"], [" safe", "1.0"], [" to", "1.0"], [" eat", "0.9999665"], [" hair", "0.9999951"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Hair", "1.0"], [" is", "1.0"], [" made", "1.0"], [" of", "1.0"], [" ker", "1.0"], ["atin", "1.0"], [".", "1.0"], [" Food", "1.0"], [" manufacturers", "1.0"], [" use", "1.0"], [" L", "1.0"], ["-", "1.0"], ["cy", "1.0"], ["ste", "1.0"], ["ine", "1.0"], [" as", "1.0"], [" a", "1.0"], [" food", "1.0"], [" additive", "1.0"], [".", "1.0"], [" L", "1.0"], ["-", "1.0"], ["cy", "1.0"], ["ste", "1.0"], ["ine", "1.0"], [" is", "1.0"], [" made", "1.0"], [" from", "1.0"], [" ker", "1.0"], ["atin", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "0.99999464"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" safe", "1.0"], [" to", "1.0"], [" eat", "1.0"], [" hair", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Hair", "1.0"], [" is", "1.0"], [" made", "1.0"], [" of", "1.0"], [" ker", "1.0"], ["atin", "1.0"], [".", "1.0"], [" Food", "1.0"], [" manufacturers", "1.0"], [" use", "1.0"], [" L", "1.0"], ["-", "1.0"], ["cy", "1.0"], ["ste", "1.0"]]}, "913": {"qid": "fbcda00a5524a9fda557", "term": "Florence", "description": "Capital and most populous city of the Italian region of Tuscany", "question": "Was Florence a Theocracy during Italian Renaissance?", "answer": true, "facts": ["The Italian Renaissance was a period of history from the 13th century to 1600.", "A theocracy is a type of rule in which religious leaders have power.", "Friar Girolamo Savonarola was the ruler of Florence, after driving out the Medici family, from November 1494 \u2013 23 May 1498."], "decomposition": ["When was the Italian Renaissance?", "When did Friar Girolamo Savonarola rule Florence?", "Is #2 within the span of #1?", "Did Friar Girolamo Savonarola belong to a religious order during #3?"], "evidence": [[[["Italian Renaissance-1"]], [["Girolamo Savonarola-1"], "no_evidence"], ["operation"], [["Girolamo Savonarola-1"]]], [[["Italian Renaissance-1"]], [["Republic of Florence-38", "Republic of Florence-39", "Republic of Florence-40"]], ["operation"], [["Girolamo Savonarola-1"], "operation"]], [[["Italian Renaissance-1"]], [["Girolamo Savonarola-2", "Girolamo Savonarola-3"]], ["operation"], [["Dominican Order-1", "Girolamo Savonarola-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Was the Roman Empire a Democracy?\nFacts: The Roman Empire was a period of history from the 5th century BC to the 4th century AD. The Roman Republic was a period of history from the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Florence a Theocracy during Italian Renaissance?\nFacts: The Italian Renaissance was a period of history from the 13th century to 1600. A theocracy is a type of rule in which religious leaders have power. Friar Girolamo Savonarola was the ruler of Florence, after driving out the Medici family, from November 1494 \u2013 23 May 1498.\nAnswer (True or False):", "ct_subject": "The Italian Renaissance was a period of history from the 13th century to 1600. A theocracy is a type of rule in which religious leaders have power. Friar Girolamo Savonarola was the ruler of Florence, after driving out the Medici family, from November 1494 \u2013 23 May 1498.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "0.9998523"], [" the", "1.0"], [" Roman", "0.15103391"], [" Empire", "0.9999831"], [" a", "1.0"], [" Democracy", "0.99999917"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [" was", "1.0"], [" a", "1.0"], [" period", "0.9817901"], [" of", "1.0"], [" history", "1.0"], [" from", "1.0"], [" the", "1.0"], [" 5", "0.11962872"], ["th", "1.0"], [" century", "1.0"], [" BC", "1.0"], [" to", "1.0"], [" the", "1.0"], [" 4", "0.999995"], ["th", "1.0"], [" century", "1.0"], [" AD", "1.0"], [".", "1.0"], [" The", "0.9817857"], [" Roman", "1.0"], [" Republic", "0.99763787"], [" was", "1.0"], [" a", "1.0"], [" period", "1.0"], [" of", "1.0"], [" history", "0.99999297"], [" from", "1.0"], [" the", "1.0"]]}, "914": {"qid": "e9689243222e7afa28ad", "term": "Swan", "description": "large water bird", "question": "Would a Nike shoebox be too small to fit a swan in?", "answer": true, "facts": ["Nike Shoeboxes are usually 14\" x 10\" x 5\".", "An average swan is 4-5.6 ft in length."], "decomposition": ["What is the average size of a Nike Shoebox?", "What is the average length of a swan?", "Is #2 smaller than #1?"], "evidence": [[[["Shoe size-13", "Sneakers-16"], "no_evidence"], [["Swan-3"], "no_evidence"], ["operation"]], [[["Shoe-1"], "no_evidence"], [["Swan-3"]], ["operation"]], [["no_evidence"], [["Swan-3"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the largest number of people that can fit into a swan?\nFacts: The largest number of people that can fit into a swan is 4.\nAnswer (True or False): True\nQuestion: What is the largest number of people that can fit into a swan?\nFacts: The largest number of people that can", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a Nike shoebox be too small to fit a swan in?\nFacts: Nike Shoeboxes are usually 14\" x 10\" x 5\". An average swan is 4-5.6 ft in length.\nAnswer (True or False):", "ct_subject": "Nike Shoeboxes are usually 14\" x 10\" x 5\". An average swan is 4-5.6 ft in length.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999858"], [" is", "1.0"], [" the", "1.0"], [" largest", "0.90348035"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" that", "0.9999999"], [" can", "0.98372054"], [" fit", "1.0"], [" into", "0.52249146"], [" a", "1.0"], [" sw", "0.1452666"], ["an", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9998323"], [" largest", "1.0"], [" number", "0.055325802"], [" of", "1.0"], [" people", "1.0"], [" that", "1.0"], [" can", "1.0"], [" fit", "1.0"], [" into", "1.0"], [" a", "1.0"], [" sw", "1.0"], ["an", "1.0"], [" is", "1.0"], [" 4", "0.51745623"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "0.99999964"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" that", "1.0"], [" can", "1.0"], [" fit", "1.0"], [" into", "1.0"], [" a", "1.0"], [" sw", "1.0"], ["an", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" largest", "1.0"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" that", "1.0"], [" can", "1.0"]]}, "915": {"qid": "91bb99711affe05abe7b", "term": "Samsung", "description": "South Korean multinational conglomerate", "question": "Is Samsung accountable to shareholders?", "answer": true, "facts": ["Samsung is a publicly traded company.", "Publicly traded companies are ultimately accountable to shareholders. "], "decomposition": ["What kind of company is Samsung?", "Are #1's accountable to shareholders?"], "evidence": [[[["Samsung Electronics-1"], "no_evidence"], ["operation"]], [[["Samsung-1"]], [["Conglomerate (company)-21"], "operation"]], [[["Samsung-14"], "operation"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the United States of America a democracy?\nFacts: The United States of America is a constitutional republic. \nAnswer (True or False): True\nQuestion: Is the United States of America a republic?\nFacts: The United States of America is a constitutional republic. \nAnswer (True or False): True\nQuestion: Is the United States of America a democracy?\nFacts: The United States of America is a constitutional republic", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Samsung accountable to shareholders?\nFacts: Samsung is a publicly traded company. Publicly traded companies are ultimately accountable to shareholders. \nAnswer (True or False):", "ct_subject": "Samsung is a publicly traded company. Publicly traded companies are ultimately accountable to shareholders. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9930982"], [" the", "0.99998975"], [" United", "0.9989932"], [" States", "1.0"], [" of", "0.7688787"], [" America", "1.0"], [" a", "1.0"], [" democracy", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "1.0"], [" States", "1.0"], [" of", "1.0"], [" America", "1.0"], [" is", "1.0"], [" a", "0.9999999"], [" constitutional", "0.34704596"], [" republic", "1.0"], [".", "1.0"], [" ", "0.9999999"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" of", "1.0"], [" America", "1.0"], [" a", "1.0"], [" republic", "0.9999696"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "1.0"], [" States", "1.0"], [" of", "1.0"], [" America", "1.0"], [" is", "1.0"], [" a", "1.0"], [" constitutional", "1.0"], [" republic", "0.9999826"], [".", "1.0"], [" ", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" of", "1.0"], [" America", "1.0"], [" a", "1.0"], [" democracy", "0.9999999"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "1.0"], [" States", "1.0"], [" of", "1.0"], [" America", "1.0"], [" is", "1.0"], [" a", "1.0"], [" constitutional", "1.0"], [" republic", "1.0"]]}, "916": {"qid": "d4c4c4c7fb085e6d1d98", "term": "Alan Alda", "description": "American actor, director, and writer", "question": "Is Alan Alda old enough to have fought in the Vietnam War?", "answer": true, "facts": ["Alan Alda was born in 1936.", "The Vietnam War was from 1955 to 1975, with American involvement from 1965 to 1973.", "American soldiers must be at least 18 years old.", "Alan Alda was 29 in 1965."], "decomposition": ["When were US forces first involved in the Vietnam war?", "When was Alan Alda born?", "What is the minimum age required to join the US Army?", "What is #1 minus #2?", "Is #4 greater than or equal to #3?"], "evidence": [[[["Vietnam War-58"]], [["Alan Alda-2"]], [["United States Army Recruiting Command-13"]], ["operation"], ["operation"]], [[["Vietnam War-2"]], [["Alan Alda-2"]], ["no_evidence"], ["operation"], ["no_evidence", "operation"]], [[["Vietnam War-1"]], [["Alan Alda-2"]], [["United States Armed Forces-3"]], ["operation"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city of New York City the same as the name of the city of New York?\nFacts: The name of the city of New York is New York. The name of the city of New York is New York.\nAnswer (True or False): True\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Alan Alda old enough to have fought in the Vietnam War?\nFacts: Alan Alda was born in 1936. The Vietnam War was from 1955 to 1975, with American involvement from 1965 to 1973. American soldiers must be at least 18 years old. Alan Alda was 29 in 1965.\nAnswer (True or False):", "ct_subject": "Alan Alda was born in 1936. The Vietnam War was from 1955 to 1975, with American involvement from 1965 to 1973. American soldiers must be at least 18 years old. Alan Alda was 29 in 1965.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99981683"], [" the", "0.99732554"], [" name", "0.99419326"], [" of", "0.9998029"], [" the", "1.0"], [" city", "0.99999225"], [" of", "0.9999902"], [" New", "0.97824794"], [" York", "1.0"], [" City", "1.0"], [" the", "0.98996836"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "0.99999523"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.97654235"], [" name", "0.9999809"], [" of", "1.0"], [" the", "0.9999994"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" is", "0.9999999"], [" New", "1.0"], [" York", "1.0"], [".", "0.86906135"], [" The", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" is", "1.0"], [" New", "1.0"], [" York", "1.0"], [".", "0.9912539"], ["\n", "0.9999999"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"]]}, "917": {"qid": "f67e4140ba9ecdb6dcaa", "term": "Prophet", "description": "person claiming to speak for divine beings", "question": "Did Disney's second film rip off a prophet story?", "answer": true, "facts": ["Disney's second film, Pinocchio, was released in 1940.", "The biblical prophet Jonah was swallowed by a whale.", "In Pinocchio, Gepetto is swallowed by a giant whale while searching for Pinocchio."], "decomposition": ["What is Disney's second film?", "In #1, what happens to Gepetto while searching for Pinocchio?", "In a biblical prophet, what happens to Jonah?", "Is #2 the same as #3?"], "evidence": [[[["Pinocchio (1940 film)-1"]], [["Pinocchio (1940 film)-9"]], [["Jonah-1"]], ["operation"]], [[["Pinocchio (1940 film)-1"]], [["Pinocchio (1940 film)-9"]], [["Jonah-4"]], ["operation"]], [[["Pinocchio (1940 film)-1"]], [["Pinocchio (1940 film)-9"]], [["Jonah-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the first movie to be released in the United States, The Wizard of Oz, have a message?\nFacts: The first movie to be released in the United States, The Wizard of Oz, was released in 1939. The biblical prophet Jonah was swallowed by a whale. In The Wizard", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Disney's second film rip off a prophet story?\nFacts: Disney's second film, Pinocchio, was released in 1940. The biblical prophet Jonah was swallowed by a whale. In Pinocchio, Gepetto is swallowed by a giant whale while searching for Pinocchio.\nAnswer (True or False):", "ct_subject": "Disney's second film, Pinocchio, was released in 1940. The biblical prophet Jonah was swallowed by a whale. In Pinocchio, Gepetto is swallowed by a giant whale while searching for Pinocchio.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99999964"], [" the", "1.0"], [" first", "0.99963534"], [" movie", "0.99986947"], [" to", "0.3581247"], [" be", "0.99483055"], [" released", "0.99999976"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [",", "1.0"], [" The", "1.0"], [" Wizard", "1.0"], [" of", "1.0"], [" Oz", "1.0"], [",", "1.0"], [" have", "0.9999695"], [" a", "1.0"], [" message", "1.0"], ["?", "0.98647124"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "0.8009694"], [" movie", "1.0"], [" to", "1.0"], [" be", "1.0"], [" released", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [",", "1.0"], [" The", "1.0"], [" Wizard", "1.0"], [" of", "1.0"], [" Oz", "1.0"], [",", "1.0"], [" was", "1.0"], [" released", "1.0"], [" in", "1.0"], [" 1939", "1.0"], [".", "1.0"], [" The", "1.0"], [" biblical", "0.9999367"], [" prophet", "1.0"], [" Jonah", "0.999998"], [" was", "1.0"], [" swallowed", "1.0"], [" by", "1.0"], [" a", "1.0"], [" whale", "1.0"], [".", "1.0"], [" In", "1.0"], [" The", "0.9999956"], [" Wizard", "1.0"]]}, "918": {"qid": "07b9fcd269bd1f8b0803", "term": "Knight", "description": "An award of an honorary title for past or future service with its roots in chivalry in the Middle Ages", "question": "Can musicians become knights?", "answer": true, "facts": ["Elton John was knighted by the Queen of England.", "Tom Jones was knighted by the Queen of England.", "Elton John is a famous pop singer.", "Tom Jones is a famous musician."], "decomposition": ["Has Elton John been knighted?", "Has Tom Jones been knighted?", "Are #1 and #2 musicians?", "Are #1, #2 and #3 positive?"], "evidence": [[[["Elton John-3"]], [["Tom Jones (singer)-3"]], [["Elton John-1", "Tom Jones (singer)-1"]], ["operation"]], [[["Elton John-1"]], [["Tom Jones (singer)-1"]], [["Elton John-3", "Tom Jones (singer)-2"]], ["operation"]], [[["Elton John-3"]], [["Tom Jones (singer)-3"]], ["operation"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: John Smith.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: John Smith.\nAnswer (True or False): True\nQuestion: What is the name", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can musicians become knights?\nFacts: Elton John was knighted by the Queen of England. Tom Jones was knighted by the Queen of England. Elton John is a famous pop singer. Tom Jones is a famous musician.\nAnswer (True or False):", "ct_subject": "Elton John was knighted by the Queen of England. Tom Jones was knighted by the Queen of England. Elton John is a famous pop singer. Tom Jones is a famous musician.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9997986"], [" is", "1.0"], [" the", "1.0"], [" name", "0.6163797"], [" of", "1.0"], [" the", "1.0"], [" first", "0.97352487"], [" person", "0.45047647"], [" to", "1.0"], [" be", "0.9999205"], [" born", "0.999997"], [" in", "1.0"], [" the", "1.0"], [" United", "0.85644305"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "0.98348457"], [" Smith", "0.99999976"], [".", "0.9999957"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "1.0"], [" Smith", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"]]}, "919": {"qid": "751232e3ff80ee198616", "term": "Lemon", "description": "citrus fruit", "question": "Can a lemon aggravate dyspepsia?", "answer": true, "facts": ["Dyspepsia is a condition where the stomach is irritated.", "Lemons are highly acidic fruits.", "Common stomach irritants include alcohol, coffee, and acidic foods."], "decomposition": ["Which condition is referred to as dyspepsia?", "What are some common irritants that could aggravate #1?", "Is lemon an example of #2?"], "evidence": [[[["Indigestion-1"]], [["Indigestion-9"], "no_evidence"], ["no_evidence", "operation"]], [[["Indigestion-1"]], [["Indigestion-15"], "no_evidence"], [["Indigestion-25"], "no_evidence", "operation"]], [[["Indigestion-11"]], [["Indigestion-12"]], [["Lemon-17"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"carpet\" and a \"carpeted\" house?\nFacts: A carpet is a carpeted house. A carpeted house is a house that has been carpeted.\nAnswer (True or False): True\nQuestion: What is the difference between a \"carpet\" and a \"carp", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can a lemon aggravate dyspepsia?\nFacts: Dyspepsia is a condition where the stomach is irritated. Lemons are highly acidic fruits. Common stomach irritants include alcohol, coffee, and acidic foods.\nAnswer (True or False):", "ct_subject": "Dyspepsia is a condition where the stomach is irritated. Lemons are highly acidic fruits. Common stomach irritants include alcohol, coffee, and acidic foods.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9790285"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.99898857"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.99999475"], ["c", "0.080810934"], ["arp", "0.98100454"], ["et", "0.9999821"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "0.99999917"], ["arp", "1.0"], ["et", "1.0"], ["ed", "1.0"], ["\"", "0.99991834"], [" house", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" carpet", "1.0"], [" is", "0.9999988"], [" a", "1.0"], [" carpet", "0.9990164"], ["ed", "1.0"], [" house", "1.0"], [".", "1.0"], [" A", "0.9985355"], [" carpet", "1.0"], ["ed", "1.0"], [" house", "1.0"], [" is", "1.0"], [" a", "1.0"], [" house", "1.0"], [" that", "1.0"], [" has", "0.999992"], [" been", "1.0"], [" carpet", "1.0"], ["ed", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["arp", "1.0"], ["et", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["arp", "1.0"]]}, "920": {"qid": "22f35192d8d58cfb6ba9", "term": "John Key", "description": "38th Prime Minister of New Zealand", "question": "Could a fan of the Botany Swarm vote for John Key?", "answer": true, "facts": ["The Botany Swarm is a hockey team based in Auckland, New Zealand", "John Key is the Prime Minister of New Zealand"], "decomposition": ["What city is the Botany Swarm based in?", "In what country is #1?", "What country was John Key the Prime Minister of?", "Is #2 the same as #3?"], "evidence": [[[["Botany Swarm-1"]], [["East Auckland-1"]], [["John Key-1"]], ["operation"]], [[["Botany Swarm-1"]], [["Botany Swarm-1"]], [["John Key-1"]], ["operation"]], [[["Botany Swarm-1"]], [["Botany Swarm-1"]], [["John Key-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the country that is the largest exporter of bananas in the world?\nFacts: The country is the United States of America\nAnswer (True or False): True\nQuestion: What is the name of the country that is the largest exporter of bananas in the world?\nFacts: The country is the United States of America\nAnswer (True or False):", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could a fan of the Botany Swarm vote for John Key?\nFacts: The Botany Swarm is a hockey team based in Auckland, New Zealand John Key is the Prime Minister of New Zealand\nAnswer (True or False):", "ct_subject": "The Botany Swarm is a hockey team based in Auckland, New Zealand John Key is the Prime Minister of New Zealand", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9975387"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99999976"], [" of", "1.0"], [" the", "1.0"], [" country", "0.7427517"], [" that", "0.9998078"], [" is", "0.9464459"], [" the", "0.99999857"], [" largest", "0.99796116"], [" exp", "0.0053203157"], ["orter", "1.0"], [" of", "1.0"], [" bananas", "0.9473482"], [" in", "0.999992"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.866932"], [" country", "1.0"], [" is", "0.9883349"], [" the", "0.993097"], [" United", "0.9996245"], [" States", "1.0"], [" of", "1.0"], [" America", "1.0"], ["\n", "0.9999845"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" country", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" exp", "1.0"], ["orter", "1.0"], [" of", "1.0"], [" bananas", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" country", "1.0"], [" is", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" of", "1.0"], [" America", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"]]}, "921": {"qid": "57fefe73cfa1e7388aba", "term": "Jackson Pollock", "description": "American painter", "question": "Were Jackson Pollock's parents not required to say The Pledge of Allegiance as children?", "answer": true, "facts": ["Jackson Pollock's parents were both born and grew up in Tingley, Iowa.", "All states except California, Hawaii, Iowa, Vermont, and Wyoming require a regularly scheduled recitation of the pledge in public schools."], "decomposition": ["What state did Jackson Pollock's parents grow up in?", "What states do not require the pledge to be recited in school?", "Is #1 in the list in #2?"], "evidence": [[[["Jackson Pollock-4"]], [["Pledge of Allegiance-2"]], ["operation"]], [[["Jackson Pollock-4"]], [["Pledge of Allegiance-2"]], ["operation"]], [[["Jackson Pollock-4"]], [["Pledge of Allegiance-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the United States have a national holiday named after George Washington?\nFacts: The United States did not have a national holiday named after George Washington until the year 2000.\nAnswer (True or False): True\nQuestion: Did the United States have a national holiday", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Were Jackson Pollock's parents not required to say The Pledge of Allegiance as children?\nFacts: Jackson Pollock's parents were both born and grew up in Tingley, Iowa. All states except California, Hawaii, Iowa, Vermont, and Wyoming require a regularly scheduled recitation of the pledge in public schools.\nAnswer (True or False):", "ct_subject": "Jackson Pollock's parents were both born and grew up in Tingley, Iowa. All states except California, Hawaii, Iowa, Vermont, and Wyoming require a regularly scheduled recitation of the pledge in public schools.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99996555"], [" the", "1.0"], [" United", "0.55750126"], [" States", "1.0"], [" have", "0.17335863"], [" a", "1.0"], [" national", "0.9855623"], [" holiday", "1.0"], [" named", "0.99346066"], [" after", "1.0"], [" George", "0.9391959"], [" Washington", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99840456"], [" United", "0.999985"], [" States", "1.0"], [" did", "0.9999281"], [" not", "1.0"], [" have", "1.0"], [" a", "1.0"], [" national", "1.0"], [" holiday", "1.0"], [" named", "1.0"], [" after", "1.0"], [" George", "1.0"], [" Washington", "1.0"], [" until", "0.9997876"], [" the", "0.9969394"], [" year", "1.0"], [" 2000", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99999845"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" have", "1.0"], [" a", "1.0"], [" national", "1.0"], [" holiday", "1.0"]]}, "922": {"qid": "955a55c2c64209b0ab7d", "term": "Call of Duty", "description": "First-person shooter video game franchise", "question": "Will Conan the Barbarian hypothetically last a short time inside of Call of Duty?", "answer": true, "facts": ["Conan the Barbarian is a comic book character.", "Conan the Barbarian is equipped with a sword and does not typically wear armor.", "Call of Duty is a modern warfare video game.", "Soldiers in Call of Duty are equipped with weapons like sniper rifles, shotguns, and machine guns."], "decomposition": ["What equipment for fighting does Conan the Barbarian use?", "What equipment for fighting does Call of Duty use?", "Are the items listed in #2 deadlier than those in #1?"], "evidence": [[[["Conan the Barbarian-1"]], [["Call of Duty-1"]], ["no_evidence", "operation"]], [[["Conan the Barbarian-20"]], [["Call of Duty-46"]], [["Sword-58"], "operation"]], [[["Conan the Barbarian-16"]], [["Call of Duty-4"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the character in the movie \"The Matrix\"?\nFacts: Neo is the main character in the movie \"The Matrix\". Neo is a computer programmer. Neo is not a real person.\nAnswer (True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Will Conan the Barbarian hypothetically last a short time inside of Call of Duty?\nFacts: Conan the Barbarian is a comic book character. Conan the Barbarian is equipped with a sword and does not typically wear armor. Call of Duty is a modern warfare video game. Soldiers in Call of Duty are equipped with weapons like sniper rifles, shotguns, and machine guns.\nAnswer (True or False):", "ct_subject": "Conan the Barbarian is a comic book character. Conan the Barbarian is equipped with a sword and does not typically wear armor. Call of Duty is a modern warfare video game. Soldiers in Call of Duty are equipped with weapons like sniper rifles, shotguns, and machine guns.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99464744"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99655896"], [" of", "1.0"], [" the", "1.0"], [" character", "0.29912695"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "0.9999999"], ["The", "1.0"], [" Matrix", "0.97397804"], ["\"?", "0.9999918"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Neo", "0.9961383"], [" is", "0.8562531"], [" the", "0.9994885"], [" main", "0.90481883"], [" character", "1.0"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "0.99991345"], ["The", "1.0"], [" Matrix", "1.0"], ["\".", "1.0"], [" Neo", "1.0"], [" is", "1.0"], [" a", "0.99999857"], [" computer", "0.99999845"], [" programmer", "0.4828879"], [".", "0.9820088"], [" Neo", "1.0"], [" is", "1.0"], [" not", "0.9997954"], [" a", "1.0"], [" real", "0.99999917"], [" person", "1.0"], [".", "1.0"], ["\n", "0.9999999"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"]]}, "923": {"qid": "36352a5a049bf7ee6acd", "term": "Vitamin C", "description": "nutrient found in citrus fruits and other foods", "question": "Do pirates care about vitamin C?", "answer": true, "facts": ["Scurvy is caused by a prolonged period of time without Vitamin C", "People spending long periods of time at sea without to vitamin C are at high risk for scurvy", "Pirates spend long periods of time at sea"], "decomposition": ["What diseases are caused by a lack of vitamin C?", "What behaviors increase people risk of getting #1?", "Do pirate engage in #2?"], "evidence": [[[["Scurvy-1"]], [["Scurvy-4"]], [["Piracy-1"], "operation"]], [[["Vitamin C-12", "Vitamin C-2"]], [["Scurvy-4"]], ["operation"]], [[["Vitamin C-9"]], [["Scurvy-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake with a hole in it\"?\nFacts: A snowflake is a single crystal that is formed when a snowflake is struck by a falling snowflake. A snowflake with a hole in it is a single crystal that is formed when a snow", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do pirates care about vitamin C?\nFacts: Scurvy is caused by a prolonged period of time without Vitamin C People spending long periods of time at sea without to vitamin C are at high risk for scurvy Pirates spend long periods of time at sea\nAnswer (True or False):", "ct_subject": "Scurvy is caused by a prolonged period of time without Vitamin C People spending long periods of time at sea without to vitamin C are at high risk for scurvy Pirates spend long periods of time at sea", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99890375"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.99141073"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.99983513"], ["s", "0.9632562"], ["now", "0.98671746"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.999997"], ["now", "1.0"], ["flake", "1.0"], [" with", "0.99433386"], [" a", "1.0"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "0.99338865"], [" crystal", "0.99986005"], [" that", "0.96304464"], [" is", "0.9992855"], [" formed", "1.0"], [" when", "0.9996742"], [" a", "0.8863273"], [" snow", "0.914934"], ["flake", "1.0"], [" is", "1.0"], [" struck", "0.99879384"], [" by", "1.0"], [" a", "1.0"], [" falling", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [".", "1.0"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" with", "1.0"], [" a", "1.0"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "1.0"], [" crystal", "1.0"], [" that", "0.9999726"], [" is", "1.0"], [" formed", "1.0"], [" when", "1.0"], [" a", "1.0"], [" snow", "1.0"]]}, "924": {"qid": "80b3ba19b90c340ea5cc", "term": "Surveillance", "description": "monitoring of behavior, activities, or other changing information", "question": "Is video surveillance of a room possible without an obvious camera or new item?", "answer": true, "facts": ["Surveillance cameras can be built into light socket covers that look no different from a normal one.", "Surveillance cameras can be installed in special light bulbs to document activity in a room."], "decomposition": ["What are the various types of surveillance cameras based on installation?", "Are some of installed so as to be #1 hidden from view?"], "evidence": [[[["Closed-circuit television-2", "Closed-circuit television-3", "Closed-circuit television-4"]], [["Hidden camera-2"]]], [[["Hidden camera-1"], "no_evidence"], ["operation"]], [[["Hidden camera-1"]], [["Hidden camera-2"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it possible to make a bomb out of a piece of paper?\nFacts: Yes.\nAnswer (True or False): True\nQuestion: Is it possible to make a bomb out of a piece of paper?\nFacts: Yes.\nAnswer (True or False): True\nQuestion: Is it possible to make", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is video surveillance of a room possible without an obvious camera or new item?\nFacts: Surveillance cameras can be built into light socket covers that look no different from a normal one. Surveillance cameras can be installed in special light bulbs to document activity in a room.\nAnswer (True or False):", "ct_subject": "Surveillance cameras can be built into light socket covers that look no different from a normal one. Surveillance cameras can be installed in special light bulbs to document activity in a room.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9398671"], [" it", "1.0"], [" possible", "1.0"], [" to", "1.0"], [" make", "0.9862092"], [" a", "1.0"], [" bomb", "0.30512163"], [" out", "0.9288924"], [" of", "1.0"], [" a", "1.0"], [" piece", "0.9759076"], [" of", "1.0"], [" paper", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Yes", "0.85863554"], [".", "0.9543958"], ["\n", "0.9989773"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" possible", "1.0"], [" to", "1.0"], [" make", "1.0"], [" a", "1.0"], [" bomb", "1.0"], [" out", "1.0"], [" of", "1.0"], [" a", "1.0"], [" piece", "1.0"], [" of", "1.0"], [" paper", "1.0"], ["?", "0.9999908"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Yes", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" possible", "1.0"], [" to", "1.0"], [" make", "1.0"]]}, "925": {"qid": "72d2e2d76ec01e97f88f", "term": "Comma", "description": "Punctuation mark", "question": "Would a Fakir be surprised if they saw a comma in their religious book?", "answer": true, "facts": ["A Fakir is a Muslim Sufi holy man or woman that lives a simple life.", "The holy book for Muslims is the Quran.", "The comma is a punctuation mark in modern language.", "The Quran does not use any forms of modern punctuation."], "decomposition": ["What religion is a Fakir from?", "What is the name of #1's Holy Book?", "What kind of punctuation mark is the comma?", "Is #2 written without using #3?"], "evidence": [[[["Fakir-1"]], [["Quran-1"]], [["Comma-1"]], ["no_evidence"]], [[["Fakir-1"]], [["Quran-1"]], [["Comma-1"]], ["no_evidence", "operation"]], [[["Fakir-1"]], [["Quran-1"]], [["Comma-2"]], [["Classical Arabic-1"], "no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a Fakir and a Muslim?\nFacts: A Fakir is a Muslim holy man or woman that lives a simple life. The holy book for Muslims is the Quran. The Fakir is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a Fakir be surprised if they saw a comma in their religious book?\nFacts: A Fakir is a Muslim Sufi holy man or woman that lives a simple life. The holy book for Muslims is the Quran. The comma is a punctuation mark in modern language. The Quran does not use any forms of modern punctuation.\nAnswer (True or False):", "ct_subject": "A Fakir is a Muslim Sufi holy man or woman that lives a simple life. The holy book for Muslims is the Quran. The comma is a punctuation mark in modern language. The Quran does not use any forms of modern punctuation.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999026"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" F", "1.0"], ["ak", "1.0"], ["ir", "1.0"], [" and", "1.0"], [" a", "1.0"], [" Muslim", "0.9969543"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.99999976"], [" F", "1.0"], ["ak", "1.0"], ["ir", "1.0"], [" is", "1.0"], [" a", "1.0"], [" Muslim", "1.0"], [" holy", "0.9981818"], [" man", "1.0"], [" or", "1.0"], [" woman", "1.0"], [" that", "1.0"], [" lives", "1.0"], [" a", "1.0"], [" simple", "1.0"], [" life", "1.0"], [".", "1.0"], [" The", "1.0"], [" holy", "1.0"], [" book", "1.0"], [" for", "1.0"], [" Muslims", "1.0"], [" is", "1.0"], [" the", "1.0"], [" Quran", "1.0"], [".", "1.0"], [" The", "1.0"], [" F", "1.0"], ["ak", "1.0"], ["ir", "1.0"], [" is", "1.0"]]}, "926": {"qid": "99853e416a63527f954e", "term": "San Diego County, California", "description": "County in California, United States", "question": "Is San Diego County the home of a Shamu?", "answer": true, "facts": ["Shamu is the name of Sea World's mascot orca.", "Every Sea World has a Shamu.", "There is a Sea World location in San Diego."], "decomposition": ["What is Shamu the name of?", "Where can you find #1?", "Is there a #2 in San Diego?"], "evidence": [[[["Shamu-1"]], [["SeaWorld San Diego-1", "SeaWorld-1"]], ["operation"]], [[["Shamu-1"]], [["Captive killer whales-19"]], [["SeaWorld San Diego-1"]]], [[["Shamu-1"]], [["SeaWorld San Diego-27"]], [["SeaWorld San Diego-27"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city of San Diego derived from the word \"San\" or \"San Diego\"?\nFacts: The name of the city of San Diego is derived from the Spanish word \"San Diego\" which means \"the place of the sea\".\nAnswer (True or False): True\nQuestion: Is the name of the city of San Diego derived from", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is San Diego County the home of a Shamu?\nFacts: Shamu is the name of Sea World's mascot orca. Every Sea World has a Shamu. There is a Sea World location in San Diego.\nAnswer (True or False):", "ct_subject": "Shamu is the name of Sea World's mascot orca. Every Sea World has a Shamu. There is a Sea World location in San Diego.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99999833"], [" the", "0.9955591"], [" name", "0.9998572"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9999912"], [" of", "0.99999857"], [" San", "1.0"], [" Diego", "0.9950906"], [" derived", "0.85215104"], [" from", "1.0"], [" the", "1.0"], [" word", "0.9965114"], [" \"", "0.9911179"], ["San", "1.0"], ["\"", "0.9999492"], [" or", "0.9969838"], [" \"", "1.0"], ["San", "1.0"], [" Diego", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "0.99977905"], [" city", "1.0"], [" of", "1.0"], [" San", "1.0"], [" Diego", "1.0"], [" is", "1.0"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" Spanish", "0.9999372"], [" word", "1.0"], [" \"", "1.0"], ["San", "1.0"], [" Diego", "0.9999987"], ["\"", "0.9999937"], [" which", "0.99999774"], [" means", "1.0"], [" \"", "1.0"], ["the", "0.9999757"], [" place", "0.95643854"], [" of", "0.99998915"], [" the", "1.0"], [" sea", "0.9999982"], ["\".", "0.9999999"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" name", "0.99999976"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" San", "1.0"], [" Diego", "1.0"], [" derived", "1.0"], [" from", "1.0"]]}, "927": {"qid": "fee20d28322885672ccf", "term": "Bob Marley", "description": "Jamaican singer-songwriter", "question": "Is sunscreen unhelpful for the condition that killed Bob Marley?", "answer": true, "facts": ["Bob Marley died of acral lentiginous melanoma ", "Acral lentiginous melanoma occurs on skin that may not have any sun exposure "], "decomposition": ["What disease killed Bob Marley?", "What is the cause of #1?", "Would sunscreen help with preventing #2?"], "evidence": [[[["Bob Marley-4"]], [["Acral lentiginous melanoma-4"]], ["no_evidence", "operation"]], [[["Bob Marley-4"]], [["Acral lentiginous melanoma-1"]], ["operation"]], [[["Bob Marley-26"]], [["Acral lentiginous melanoma-4"]], [["Acral lentiginous melanoma-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it true that the first person to be buried in the United States was a slave?\nFacts: The first person buried in the United States was a slave.\nAnswer (True or False): True\nQuestion: Is it true that the first person to be buried in the United States was a slave?\nFacts: The first person buried", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is sunscreen unhelpful for the condition that killed Bob Marley?\nFacts: Bob Marley died of acral lentiginous melanoma  Acral lentiginous melanoma occurs on skin that may not have any sun exposure \nAnswer (True or False):", "ct_subject": "Bob Marley died of acral lentiginous melanoma  Acral lentiginous melanoma occurs on skin that may not have any sun exposure ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999975"], [" it", "0.9969773"], [" true", "0.99999905"], [" that", "1.0"], [" the", "1.0"], [" first", "0.9997482"], [" person", "0.9984328"], [" to", "1.0"], [" be", "0.73118305"], [" buried", "0.38953033"], [" in", "1.0"], [" the", "1.0"], [" United", "0.99919313"], [" States", "1.0"], [" was", "1.0"], [" a", "1.0"], [" slave", "0.83193743"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9997695"], [" first", "1.0"], [" person", "1.0"], [" buried", "0.99986935"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" a", "1.0"], [" slave", "1.0"], [".", "0.99999154"], ["\n", "0.90185714"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" true", "1.0"], [" that", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" buried", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" a", "1.0"], [" slave", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" buried", "1.0"]]}, "928": {"qid": "7cf376afb78c20dd6f18", "term": "Stanford University", "description": "Private research university in Stanford, California", "question": "Was John Gall from same city as Stanford University?", "answer": true, "facts": ["John Gall is a former major league baseball player born in Stanford, California.", "Stanford University was founded by Leland and Jane Stanford  in Stanford, California."], "decomposition": ["Where was John Gall (baseball player) born?", "Where is Stanford University located?", "Is #1 the same as #2?"], "evidence": [[[["John Gall (baseball)-2"]], [["Stanford University-1"]], ["operation"]], [[["John Gall (baseball)-2"]], [["Stanford University-1"]], ["operation"]], [[["John Gall (baseball)-2"]], [["Stanford University-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Was the first person to be born in the year 2000 born in the year 2000?\nFacts: The first person to be born in the year 2000 was born in the year 2000.\nAnswer (True or False): True\nQuestion: Was the first person to be born in the year 2000 born in the year 2000?\nFacts: The first person to be born", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was John Gall from same city as Stanford University?\nFacts: John Gall is a former major league baseball player born in Stanford, California. Stanford University was founded by Leland and Jane Stanford in Stanford, California.\nAnswer (True or False):", "ct_subject": "John Gall is a former major league baseball player born in Stanford, California. Stanford University was founded by Leland and Jane Stanford in Stanford, California.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "0.94198245"], [" the", "0.9999999"], [" first", "0.99997747"], [" person", "0.33350283"], [" to", "1.0"], [" be", "0.99911517"], [" born", "0.99999404"], [" in", "1.0"], [" the", "1.0"], [" year", "0.999413"], [" 2000", "0.9999999"], [" born", "0.9998765"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Was", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"]]}, "929": {"qid": "2b2a1d9d526e9a99a8ca", "term": "Snowboarding", "description": "winter sport", "question": "Snowboarding is a rarity in Hilo?", "answer": true, "facts": ["Snowboarding is a sport that involves descending snow-covered slopes.", "Hilo, a town in Hawaii, has not had snow in almost 200 years."], "decomposition": ["What kind of surface is needed for snowboarding?", "Is #1 likely to be nonexistent in Hilo, going by the prevailing climatic conditions?"], "evidence": [[[["Snowboarding-1"]], [["Hilo, Hawaii-12"], "operation"]], [[["Snowboarding-1"]], [["Hilo, Hawaii-13"], "operation"]], [[["Snowboarding-1"]], [["Hilo, Hawaii-12"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in Hawaii that is the capital of Hawaii?\nFacts: Honolulu\nAnswer (True or False): False\nQuestion: What is the name of the city in Hawaii that is the capital of Hawaii?\nFacts: Honolulu\nAnswer (True or False): False\nQuestion: What is the name of the city in Hawaii that is the capital", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Snowboarding is a rarity in Hilo?\nFacts: Snowboarding is a sport that involves descending snow-covered slopes. Hilo, a town in Hawaii, has not had snow in almost 200 years.\nAnswer (True or False):", "ct_subject": "Snowboarding is a sport that involves descending snow-covered slopes. Hilo, a town in Hawaii, has not had snow in almost 200 years.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999765"], [" is", "1.0"], [" the", "1.0"], [" name", "0.93970925"], [" of", "1.0"], [" the", "1.0"], [" city", "0.99954873"], [" in", "1.0"], [" Hawaii", "0.9999845"], [" that", "0.99999976"], [" is", "0.9979845"], [" the", "1.0"], [" capital", "0.5783652"], [" of", "1.0"], [" Hawaii", "0.9948474"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Honolulu", "1.0"], ["\n", "0.33621264"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" False", "0.5992506"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" Hawaii", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" Hawaii", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "0.94120824"], ["acts", "1.0"], [":", "1.0"], [" Honolulu", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" False", "0.8619273"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" Hawaii", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"]]}, "930": {"qid": "07ea848474b8dff89299", "term": "Portuguese people", "description": "ethnic group", "question": "Did King of Portuguese people in 1515 have familial ties to the Tudors?", "answer": true, "facts": ["Manuel I was King of Portugal from 1495 to 1521.", "Manuel I of Portugal married Maria of Aragon.", "Maria of Aragon was the sister of Catherine of Aragon.", "Catherine of Aragon was the first wife of Henry VIII and was one of a handful that he did not behead."], "decomposition": ["Who was the King of Portugal in 1515?", "Who were in #1's immediate family?", "Were any of #2 related to the Tudors?"], "evidence": [[[["Manuel I of Portugal-1"]], [["Manuel I of Portugal-5"], "no_evidence"], ["no_evidence", "operation"]], [[["Manuel I of Portugal-1"]], [["Isabella of Portugal-4"]], [["Mary I of England-1", "Philip II of Spain-1", "Philip II of Spain-2"], "operation"]], [[["Manuel I of Portugal-1"]], ["no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the first person to be born in the year 2000 have a genetic link to the year 2000?\nFacts: The first person to be born in the year 2000 was a woman named Mary.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did King of Portuguese people in 1515 have familial ties to the Tudors?\nFacts: Manuel I was King of Portugal from 1495 to 1521. Manuel I of Portugal married Maria of Aragon. Maria of Aragon was the sister of Catherine of Aragon. Catherine of Aragon was the first wife of Henry VIII and was one of a handful that he did not behead.\nAnswer (True or False):", "ct_subject": "Manuel I was King of Portugal from 1495 to 1521. Manuel I of Portugal married Maria of Aragon. Maria of Aragon was the sister of Catherine of Aragon. Catherine of Aragon was the first wife of Henry VIII and was one of a handful that he did not behead.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "0.9999956"], [" first", "1.0"], [" person", "0.9873741"], [" to", "1.0"], [" be", "0.94853485"], [" born", "0.99996746"], [" in", "1.0"], [" the", "1.0"], [" year", "0.9731312"], [" 2000", "1.0"], [" have", "1.0"], [" a", "1.0"], [" genetic", "0.94720215"], [" link", "0.48708162"], [" to", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "0.07493488"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999999"], [" first", "0.99998224"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "1.0"], [" a", "0.9485283"], [" woman", "0.99492455"], [" named", "1.0"], [" Mary", "0.9941729"], [".", "0.9999596"]]}, "931": {"qid": "06adc3a703c49b96a7e5", "term": "Artillery", "description": "Heavy ranged guns or weapons", "question": "Would a slingshot be improperly classified as artillery?", "answer": true, "facts": ["Artillery refers to ranged weaponry that is predominantly used in breaching fortifications.", "Examples of artillery include: howitzers, mortars, and rockets.", "Mortars can have a range up to 4,680m.", "A slingshot is a string weapon that propels a rock or other small projectile.", "Some slingshots can fire projectiles up to 9m."], "decomposition": ["What are the basic characteristics of a weapon considered artillery?", "Does a slingshot fail to possess all of #1?"], "evidence": [[[["Artillery-9"]], [["Slingshot-8"]]], [[["Artillery-1"]], [["Slingshot-1"], "operation"]], [[["Artillery-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake-shaped\" snowflake?\nFacts: A snowflake is a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a slingshot be improperly classified as artillery?\nFacts: Artillery refers to ranged weaponry that is predominantly used in breaching fortifications. Examples of artillery include: howitzers, mortars, and rockets. Mortars can have a range up to 4,680m. A slingshot is a string weapon that propels a rock or other small projectile. Some slingshots can fire projectiles up to 9m.\nAnswer (True or False):", "ct_subject": "Artillery refers to ranged weaponry that is predominantly used in breaching fortifications. Examples of artillery include: howitzers, mortars, and rockets. Mortars can have a range up to 4,680m. A slingshot is a string weapon that propels a rock or other small projectile. Some slingshots can fire projectiles up to 9m.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999993"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9948447"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.992878"], ["now", "1.0"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["-", "0.9767362"], ["shaped", "1.0"], ["\"", "1.0"], [" snow", "1.0"], ["flake", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999999"], [" snow", "1.0"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"]]}, "932": {"qid": "9fb41cefb010f47033f3", "term": "Gladiator", "description": "combatant who entertained audiences in the Roman Republic and Roman Empire", "question": "Did Gladiator's weapon of choice require less hands than Soul Calibur's Faust?", "answer": true, "facts": ["Faust is a zweihander sword in the Soul Calibur video game series.", "A zweihander is a giant sword that requires two hands to wield.", "Gladiators used the Gladius which was a short one handed sword."], "decomposition": ["Faust in the Soul Calibur video game series is what kind of sword?", "How many hands would be needed to lift #1?", "How many hands would be needed to lift a typical Gladiator's sword?", "Is #3 less than #2?"], "evidence": [[[["Siegfried and Nightmare-17"]], [["Siegfried and Nightmare-17"]], [["Gladius-1", "Gladius-2"]], ["operation"]], [[["Siegfried and Nightmare-17"]], ["operation"], [["Gladius-20"], "no_evidence"], ["operation"]], [[["Soulcalibur (video game)-1"], "no_evidence"], ["no_evidence"], [["Gladius-1", "Gladius-2"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Roman Empire have a \"Golden Age\" of peace and prosperity?\nFacts: The Roman Empire was a very violent and bloody empire. The Roman Empire was the largest empire in the world at its height. The Roman Empire was the largest empire in", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Gladiator's weapon of choice require less hands than Soul Calibur's Faust?\nFacts: Faust is a zweihander sword in the Soul Calibur video game series. A zweihander is a giant sword that requires two hands to wield. Gladiators used the Gladius which was a short one handed sword.\nAnswer (True or False):", "ct_subject": "Faust is a zweihander sword in the Soul Calibur video game series. A zweihander is a giant sword that requires two hands to wield. Gladiators used the Gladius which was a short one handed sword.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.997738"], [" the", "1.0"], [" Roman", "0.9999521"], [" Empire", "1.0"], [" have", "1.0"], [" a", "1.0"], [" \"", "0.99980515"], ["Golden", "0.71222174"], [" Age", "1.0"], ["\"", "1.0"], [" of", "0.912864"], [" peace", "0.25875074"], [" and", "0.99999964"], [" prosperity", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [" was", "1.0"], [" a", "0.9999974"], [" very", "0.87266326"], [" violent", "0.9999999"], [" and", "1.0"], [" bloody", "0.99439174"], [" empire", "0.026123768"], [".", "0.9999999"], [" The", "0.99998987"], [" Roman", "1.0"], [" Empire", "1.0"], [" was", "1.0"], [" the", "0.914323"], [" largest", "0.9829937"], [" empire", "1.0"], [" in", "1.0"], [" the", "0.8561408"], [" world", "1.0"], [" at", "0.99595696"], [" its", "0.99410015"], [" height", "0.99979025"], [".", "1.0"], [" The", "0.9999869"], [" Roman", "1.0"], [" Empire", "1.0"], [" was", "1.0"], [" the", "0.9995877"], [" largest", "0.9999957"], [" empire", "1.0"], [" in", "1.0"]]}, "933": {"qid": "601db2d5fba830c71962", "term": "Batman (1989 film)", "description": "1989 film directed by Tim Burton", "question": "Is Batman (1989 film) likely to be shown on flight from NY to Kansas City?", "answer": true, "facts": ["A flight from NY to Kansas City is four and a half hours.", "The run time of Batman (1989 film) is two hours and six minutes.", "Batman (1989 film) is rated PG-13", "The average age group of passengers is 18-34.", "Airlines have relaxed their rules for in-flight movies in last few years and even R rated movies have been shown."], "decomposition": ["How long is a flight from NY to Kansas City?", "How long is the 1989 Batman film? ", "Is #2 less than #1?"], "evidence": [[["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Kansas City metropolitan area-1", "New York City-1"], "no_evidence"], [["Batman (1989 film)-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Flight length-5"], "no_evidence"], [["Batman (1989 film)-23"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city in the movie \"The Big Lebowski\" a reference to the city of Leb", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Batman (1989 film) likely to be shown on flight from NY to Kansas City?\nFacts: A flight from NY to Kansas City is four and a half hours. The run time of Batman (1989 film) is two hours and six minutes. Batman (1989 film) is rated PG-13 The average age group of passengers is 18-34. Airlines have relaxed their rules for in-flight movies in last few years and even R rated movies have been shown.\nAnswer (True or False):", "ct_subject": "A flight from NY to Kansas City is four and a half hours. The run time of Batman (1989 film) is two hours and six minutes. Batman (1989 film) is rated PG-13 The average age group of passengers is 18-34. Airlines have relaxed their rules for in-flight movies in last few years and even R rated movies have been shown.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.98897356"], [" the", "0.9486144"], [" name", "0.999997"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9998709"], [" in", "0.8788059"], [" the", "0.9999999"], [" movie", "1.0"], [" \"", "1.0"], ["The", "1.0"], [" Big", "0.98915595"], [" Le", "1.0"], ["b", "1.0"], ["owski", "1.0"], ["\"", "1.0"], [" a", "0.999984"], [" reference", "1.0"], [" to", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Le", "1.0"], ["b", "1.0"]]}, "934": {"qid": "10bcf9c1d9026f741f75", "term": "Supreme Court of Canada", "description": "highest court of Canada", "question": "Can the Supreme Court of Canada fight a Lucha trios match?", "answer": true, "facts": ["A Lucha trios match requires at least two teams of three wrestlers each", "The Supreme Court of Canada has nine justices"], "decomposition": ["How many Justices are in the Supreme Court of Canada?", "What is the total number of people needed to fight in a Lucha trios match?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Supreme Court of Canada-5"]], [["Lucha libre-16"]], ["operation"]], [[["Supreme Court of Canada-5"]], [["Lucha libre-1"]], ["operation"]], [[["Supreme Court of Canada-30"]], [["Lucha libre-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: John Adams was born on July 4, 1797\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: John Adams was born on July 4, 1797\nAnswer (True or False): True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can the Supreme Court of Canada fight a Lucha trios match?\nFacts: A Lucha trios match requires at least two teams of three wrestlers each The Supreme Court of Canada has nine justices\nAnswer (True or False):", "ct_subject": "A Lucha trios match requires at least two teams of three wrestlers each The Supreme Court of Canada has nine justices", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999888"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99993634"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9967619"], [" person", "0.94493335"], [" to", "1.0"], [" be", "0.9999895"], [" born", "0.040907267"], [" in", "1.0"], [" the", "1.0"], [" United", "0.99588543"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "0.92626756"], [" Adams", "0.99927586"], [" was", "0.97642606"], [" born", "0.9960672"], [" on", "0.95902437"], [" July", "0.99878985"], [" 4", "1.0"], [",", "1.0"], [" 17", "1.0"], ["97", "0.999997"], ["\n", "0.9997484"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "0.99999917"], [" Adams", "1.0"], [" was", "1.0"], [" born", "1.0"], [" on", "1.0"], [" July", "1.0"], [" 4", "1.0"], [",", "1.0"], [" 17", "1.0"], ["97", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"]]}, "935": {"qid": "2e95e20da8a6cfacaa83", "term": "Dessert", "description": "A course that concludes a meal; usually sweet", "question": "Would an ancient visitor to Persia probably consume crocus threads?", "answer": true, "facts": ["Ancient Persians would have several desserts after a simple meal", "Saffron is made from crocus styles or threads", "Saffron is a common ingredient in Persian desserts"], "decomposition": ["What would Ancient Persians typically have after a simple meal?", "What was a common ingredient in #1?", "Is #2 made from crocus threads?"], "evidence": [[[["History of saffron-16"]], [["History of saffron-16"]], [["Saffron-1"]]], [[["Tahchin-1"], "no_evidence"], [["Saffron (color)-1"]], ["operation"]], [["no_evidence"], ["no_evidence"], [["Crocus sativus-6"], "no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city of Rome?\nFacts: Rome is the capital of the Roman Empire\nAnswer (True or False): True\nQuestion: What is the name of the city of Rome?\nFacts: Rome is the capital of the Roman Empire\nAnswer (True or False): True\nQuestion: What is the name of the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would an ancient visitor to Persia probably consume crocus threads?\nFacts: Ancient Persians would have several desserts after a simple meal Saffron is made from crocus styles or threads Saffron is a common ingredient in Persian desserts\nAnswer (True or False):", "ct_subject": "Ancient Persians would have several desserts after a simple meal Saffron is made from crocus styles or threads Saffron is a common ingredient in Persian desserts", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999957"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9996069"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9974273"], [" of", "0.95316035"], [" Rome", "0.99981457"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "0.99999976"], [" is", "0.9949273"], [" the", "1.0"], [" capital", "0.9570451"], [" of", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], ["\n", "0.97679377"], ["Answer", "1.0"], [" (", "0.99999976"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"]]}, "936": {"qid": "4931b5e5a55f092e40a8", "term": "The Who", "description": "English rock band", "question": "Would the Who concert in international space station be audible?", "answer": true, "facts": ["Sound travels through the vibration of atoms and molecules in a medium (such as air or water). ", "There is air in the international space station. "], "decomposition": ["What is the most common medium of transmission of sound?", "Is #1 present in the international space station?"], "evidence": [[[["Speech science-10"], "no_evidence"], [["International Space Station-90"], "no_evidence", "operation"]], [[["Transmission medium-2"], "no_evidence"], [["Space station-20"], "no_evidence"]], [[["Sound-1", "Sound-5"]], [["International Space Station-89"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake\" with a \"snowflake\" with a \"snowflake\"?\nFacts: A snowflake is a single crystal.  A snowflake with a \"snowflake\" with a \"snowflake\" is a collection of crystals. \nAnswer (True or False", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would the Who concert in international space station be audible?\nFacts: Sound travels through the vibration of atoms and molecules in a medium (such as air or water).  There is air in the international space station. \nAnswer (True or False):", "ct_subject": "Sound travels through the vibration of atoms and molecules in a medium (such as air or water).  There is air in the international space station. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99998784"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9051547"], [" between", "1.0"], [" a", "0.9999926"], [" \"", "0.99981874"], ["s", "0.88303286"], ["now", "1.0"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.99999523"], ["now", "1.0"], ["flake", "1.0"], ["\"", "0.06596391"], [" with", "0.99999726"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\"", "0.99990654"], [" with", "0.98930347"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\"?", "0.993425"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.99999964"], [" snow", "0.99992335"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "0.9998996"], [" crystal", "1.0"], [".", "0.25292596"], [" ", "0.7033967"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" with", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\"", "1.0"], [" with", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\"", "1.0"], [" is", "0.99999976"], [" a", "1.0"], [" collection", "0.9461159"], [" of", "1.0"], [" crystals", "1.0"], [".", "1.0"], [" ", "0.9999957"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"]]}, "937": {"qid": "ee62f46a39caaa860b26", "term": "King Kong (2005 film)", "description": "2005 film directed by Peter Jackson", "question": "Was King Kong (2005 film) solvent?", "answer": true, "facts": ["Solvent refers to the assets of a project being greater than the liabilities.", "The assets of a movie film are the box office receipts, and the liabilities is the budget.", "King Kong (2005) had box office receipts of 562 million.", "King Kong (2005) had a budget of 207 million."], "decomposition": ["What does it mean to be solvent in business/finance?", "What was the budget (liabilities) of the 2005 movie King Kong?", "How much did 2005 movie King Kong gross (assets) worldwide?", "Does #3 compare favorably with #2 as defined in #1?"], "evidence": [[[["Solvency-1"]], [["King Kong (2005 film)-2"]], [["King Kong (2005 film)-2"]], ["operation"]], [[["Solvency-1"]], [["King Kong (2005 film)-2"]], [["King Kong (2005 film)-2"]], ["operation"]], [[["Solvency-1"]], [["King Kong (2005 film)-2"]], [["King Kong (2005 film)-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: Thomas Jefferson was the first person to be born in the United States.\nAnswer (True or False): True\nQuestion: What is", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was King Kong (2005 film) solvent?\nFacts: Solvent refers to the assets of a project being greater than the liabilities. The assets of a movie film are the box office receipts, and the liabilities is the budget. King Kong (2005) had box office receipts of 562 million. King Kong (2005) had a budget of 207 million.\nAnswer (True or False):", "ct_subject": "Solvent refers to the assets of a project being greater than the liabilities. The assets of a movie film are the box office receipts, and the liabilities is the budget. King Kong (2005) had box office receipts of 562 million. King Kong (2005) had a budget of 207 million.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999845"], [" is", "1.0"], [" the", "1.0"], [" name", "0.83691704"], [" of", "1.0"], [" the", "1.0"], [" first", "0.99974483"], [" person", "0.8981331"], [" to", "1.0"], [" be", "0.99892884"], [" born", "0.99900985"], [" in", "1.0"], [" the", "1.0"], [" United", "0.7953925"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Thomas", "0.7945469"], [" Jefferson", "1.0"], [" was", "0.9999919"], [" the", "0.99997425"], [" first", "1.0"], [" person", "1.0"], [" to", "0.9999995"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"]]}, "938": {"qid": "2b5b926a732d43620065", "term": "Don't ask, don't tell", "description": "Former policy on gay people serving in the United States military", "question": "During the time immediately after 9/11, was don't ask don't tell still in place?", "answer": true, "facts": ["Don't ask don't tell was the official military policy for LGBT service members until 2011.", "9/11 Occured on September 11th, 2001."], "decomposition": ["Until what year was \"Don't ask; Don't tell.\" in place?", "In what year did 9/11 occur?", "Is #1 more recent than #2?"], "evidence": [[[["Don't ask, don't tell-23"]], [["Post-9/11-2"]], ["operation"]], [[["Don't ask, don't tell-1"]], [["September 11 attacks-1"]], ["operation"]], [[["Don't ask, don't tell-1"]], [["September 11 attacks-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the capital of the state of Texas?\nFacts: Austin\nAnswer (True or False): True\nQuestion: What is the name of the city in the United States that is the capital of the state of Texas?\nFacts: Austin\nAnswer (", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: During the time immediately after 9/11, was don't ask don't tell still in place?\nFacts: Don't ask don't tell was the official military policy for LGBT service members until 2011. 9/11 Occured on September 11th, 2001.\nAnswer (True or False):", "ct_subject": "Don't ask don't tell was the official military policy for LGBT service members until 2011. 9/11 Occured on September 11th, 2001.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999857"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99911517"], [" of", "1.0"], [" the", "1.0"], [" city", "0.15413226"], [" in", "0.9766997"], [" the", "0.9999994"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "0.999931"], [" the", "1.0"], [" capital", "0.9993686"], [" of", "1.0"], [" the", "1.0"], [" state", "0.9999995"], [" of", "1.0"], [" Texas", "0.9248024"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Austin", "0.14688952"], ["\n", "0.9733299"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "0.93825775"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" Texas", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "0.99999857"], ["acts", "1.0"], [":", "1.0"], [" Austin", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"]]}, "939": {"qid": "f1d755a68c3627fa938a", "term": "Cook (profession)", "description": "occupation involving cooking food", "question": "Can Michael Jordan become a professional cook in America? ", "answer": true, "facts": ["Based on the American Culinary Federation, the minimum requirements for entering culinary apprenticeships include being 17 years old and having a high school diploma or equivalent.", "Michael Jordan graduated from Laney High School in 1981.", "Michael Jordan was born on February 17, 1963, which makes him 57 years old in 2020."], "decomposition": ["What are the minimum requirements to become a professional cook in America?", "Does Michael Jordan satisfy all of #1?"], "evidence": [[[["Cook (profession)-16"]], [["Michael Jordan-2"], "operation"]], [[["Chef-2"]], ["no_evidence", "operation"]], [[["Chef-17"]], [["Michael Jordan-1"], "no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human being?\nFacts: The average human being weighs approximately 120 pounds.\nAnswer (True or False): True\nQuestion: What is the average height of a human being?\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can Michael Jordan become a professional cook in America? \nFacts: Based on the American Culinary Federation, the minimum requirements for entering culinary apprenticeships include being 17 years old and having a high school diploma or equivalent. Michael Jordan graduated from Laney High School in 1981. Michael Jordan was born on February 17, 1963, which makes him 57 years old in 2020.\nAnswer (True or False):", "ct_subject": "Based on the American Culinary Federation, the minimum requirements for entering culinary apprenticeships include being 17 years old and having a high school diploma or equivalent. Michael Jordan graduated from Laney High School in 1981. Michael Jordan was born on February 17, 1963, which makes him 57 years old in 2020.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9996037"], [" is", "1.0"], [" the", "1.0"], [" average", "0.999962"], [" weight", "0.9996171"], [" of", "1.0"], [" a", "0.99999344"], [" human", "0.9998234"], [" being", "0.7051319"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.9940474"], [" being", "0.99998605"], [" weighs", "0.9999995"], [" approximately", "0.8912302"], [" 120", "0.55093485"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" height", "0.9929755"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "1.0"], ["?", "1.0"], ["\n", "1.0"]]}, "940": {"qid": "1b3dfeb62e7e27c2461a", "term": "Irish mythology", "description": "Pre-Christian Mythology of Ireland", "question": "Did Irish mythology inspire Washington Irving?", "answer": true, "facts": ["Washington Irving's most famous novel was The Legend of Sleepy Hollow.", "The antagonist of the Legend of Sleepy Hollow, The Headless Horseman, was a man on horseback that was missing his head.", "The Dullahan is a mythological creature in Irish mythology.", "The Dullahan comes from the Irish word Gan Ceann, meaning \"without a head\".", "The Dullahan is depicted as a headless rider, usually on a black horse, who carries their own head in their arm."], "decomposition": ["Which Washington Irving novel was the most popular?", "What are the features of the antagonist of #1?", "What are the features of Dullahan from Irish mythology?", "Does #2 considerably match #3?"], "evidence": [[[["The Legend of Sleepy Hollow-1"]], [["The Legend of Sleepy Hollow-1"]], [["Dullahan-2"]], ["operation"]], [[["The Legend of Sleepy Hollow-1"], "no_evidence"], [["The Legend of Sleepy Hollow-2"]], [["Dullahan-2"]], ["operation"]], [[["Washington Irving-1"]], [["Headless Horseman-1"]], [["Dullahan-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the first American flag have a blue field and a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Irish mythology inspire Washington Irving?\nFacts: Washington Irving's most famous novel was The Legend of Sleepy Hollow. The antagonist of the Legend of Sleepy Hollow, The Headless Horseman, was a man on horseback that was missing his head. The Dullahan is a mythological creature in Irish mythology. The Dullahan comes from the Irish word Gan Ceann, meaning \"without a head\". The Dullahan is depicted as a headless rider, usually on a black horse, who carries their own head in their arm.\nAnswer (True or False):", "ct_subject": "Washington Irving's most famous novel was The Legend of Sleepy Hollow. The antagonist of the Legend of Sleepy Hollow, The Headless Horseman, was a man on horseback that was missing his head. The Dullahan is a mythological creature in Irish mythology. The Dullahan comes from the Irish word Gan Ceann, meaning \"without a head\". The Dullahan is depicted as a headless rider, usually on a black horse, who carries their own head in their arm.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.999964"], [" the", "1.0"], [" first", "0.9244121"], [" American", "0.996366"], [" flag", "0.9999999"], [" have", "0.38192865"], [" a", "0.99993193"], [" blue", "0.9999999"], [" field", "1.0"], [" and", "0.99998724"], [" a", "1.0"]]}, "941": {"qid": "3103007c7b0ca91df965", "term": "Times Square", "description": "Neighborhood in Manhattan in New York City, New York", "question": "At midnight in Times Square on New Years Eve, are you likely to meet people in diapers?", "answer": true, "facts": ["Times Square holds an enormous gathering on New Year's Eve.", "There are no portable restrooms in Times Square.", "People begin waiting for the midnight event in Times Square before 10 a.m."], "decomposition": ["How early do people arrive to wait for the ball to drop on New Years Eve?", "When does the ball drop?", "How much time is between #1 and #2?", "Within #3 hours, would someone need to use the bathroom?"], "evidence": [[[["New Year's Eve-37"], "no_evidence"], [["New Year's Eve-37"]], ["no_evidence", "operation"], ["no_evidence", "operation"]], [[["New Year's Eve-39"], "no_evidence"], [["Time ball-8"]], ["operation"], [["Urinary system-8"]]], [[["Times Square-39"], "no_evidence"], [["Times Square Ball-8"]], ["operation"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in China that is the largest in the world?\nFacts: The largest city in the world is Beijing.\nAnswer (True or False): True\nQuestion: What is the name of the city in China that is the largest in the world?\nF", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: At midnight in Times Square on New Years Eve, are you likely to meet people in diapers?\nFacts: Times Square holds an enormous gathering on New Year's Eve. There are no portable restrooms in Times Square. People begin waiting for the midnight event in Times Square before 10 a.m.\nAnswer (True or False):", "ct_subject": "Times Square holds an enormous gathering on New Year's Eve. There are no portable restrooms in Times Square. People begin waiting for the midnight event in Times Square before 10 a.m.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9969528"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99170107"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9993425"], [" in", "0.9977385"], [" China", "0.8569059"], [" that", "0.9999988"], [" is", "0.9892793"], [" the", "1.0"], [" largest", "0.99998057"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.1774187"], [" largest", "0.99999"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" is", "1.0"], [" Beijing", "0.99992096"], [".", "0.9999999"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" China", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "0.9999938"]]}, "942": {"qid": "cdd68b5d76364d35779f", "term": "United States Air Force", "description": "Air and space warfare branch of the United States Armed Forces", "question": "Would a member of the United States Air Force get a discount at Dunkin Donuts?", "answer": true, "facts": ["The United States Air Force is part of the military.", "Dunkin Donuts offers a military discount. "], "decomposition": ["What is the The United States Air Force a branch of?", "What groups of people get a discount at Dunkin Donuts?", "Is there any overlap between #1 and #2?"], "evidence": [[[["United States Air Force-1"]], ["no_evidence"], ["no_evidence", "operation"]], [[["United States Air Force-1"]], ["no_evidence"], ["no_evidence", "operation"]], [[["United States Air Force-1"]], [["Discounts and allowances-30"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the capital of the state of New York?\nFacts: The city in the United States that is the capital of the state of New York is Albany.\nAnswer (True or False): True\nQuestion: What is the name of the city in the United States that is the capital of the state of New York", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a member of the United States Air Force get a discount at Dunkin Donuts?\nFacts: The United States Air Force is part of the military. Dunkin Donuts offers a military discount. \nAnswer (True or False):", "ct_subject": "The United States Air Force is part of the military. Dunkin Donuts offers a military discount. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999964"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9996699"], [" of", "1.0"], [" the", "1.0"], [" city", "0.39014426"], [" in", "0.9999994"], [" the", "0.99971586"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "0.99871445"], [" the", "1.0"], [" capital", "0.9984074"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" New", "0.9991991"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99932444"], [" city", "0.9999137"], [" in", "0.9823959"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" is", "1.0"], [" Albany", "0.99982435"], [".", "1.0"], ["\n", "0.9999999"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "0.9999993"]]}, "943": {"qid": "111dac3a619eaba99951", "term": "Bitcoin", "description": "decentralized cryptocurrency", "question": "Can you put bitcoin in your pocket?", "answer": true, "facts": ["Bitcoin is a digital crypto currency.", "Bitcoin can be stored in tangible wallets, called hard wallets.", "Cryptocurrency hard wallets are the size of a thumb drive.", "Thumb drives can fit in your pocket."], "decomposition": ["What kind of currency is bitcoin?", "What are some common ways of storing #1?", "Which of #2 is a physical object?", "Is #3 small enough to fit in a pocket?"], "evidence": [[[["Bitcoin-1"]], [["Bitcoin-44"]], [["Bitcoin-47"]], ["no_evidence"]], [[["Cryptocurrency-2"]], [["Bitcoin-47"]], ["operation"], ["operation"]], [[["Bitcoin-1"]], [["Bitcoin-46", "Bitcoin-49"]], ["operation"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"bump\" and a \"fork\"?\nFacts: A \"bump\" is a small change in the blockchain. A \"fork\" is a change in the blockchain that is larger than a \"bump\".\nAnswer (True or False): True\nQuestion: What is the difference between", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can you put bitcoin in your pocket?\nFacts: Bitcoin is a digital crypto currency. Bitcoin can be stored in tangible wallets, called hard wallets. Cryptocurrency hard wallets are the size of a thumb drive. Thumb drives can fit in your pocket.\nAnswer (True or False):", "ct_subject": "Bitcoin is a digital crypto currency. Bitcoin can be stored in tangible wallets, called hard wallets. Cryptocurrency hard wallets are the size of a thumb drive. Thumb drives can fit in your pocket.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999989"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.94206285"], ["b", "0.14763132"], ["ump", "0.30472413"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["fork", "0.9740268"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" \"", "1.0"], ["b", "1.0"], ["ump", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" small", "0.9554225"], [" change", "1.0"], [" in", "1.0"], [" the", "0.9999999"], [" blockchain", "0.99710125"], [".", "1.0"], [" A", "1.0"], [" \"", "1.0"], ["fork", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" change", "0.40595648"], [" in", "1.0"], [" the", "1.0"], [" blockchain", "1.0"], [" that", "1.0"], [" is", "1.0"], [" larger", "0.7147214"], [" than", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["b", "1.0"], ["ump", "1.0"], ["\".", "1.0"], ["\n", "0.99999976"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"]]}, "944": {"qid": "a73446e82b72b4a99e5b", "term": "Glutamic acid", "description": "amino acid", "question": "Does Masaharu Morimoto rely on glutamic acid?", "answer": true, "facts": ["Masaharu Morimoto is a Japanese chef", "Japanese cuisine relies on several forms of seaweed as ingredients and flavorings for broth like kombu dashi", "Glutamic acid has been identified as the flavoring component in kombu seaweed"], "decomposition": ["What is Masaharu Morimoto's profession?", "What cuisine does #1 make?", "What is a main ingredient in #2?", "Is glutamic acid a flavoring component in #3?"], "evidence": [[[["Masaharu Morimoto-1"]], [["Masaharu Morimoto-2"]], [["Monosodium glutamate-2"]], [["Glutamic acid-3"]]], [[["Masaharu Morimoto-1"]], [["Masaharu Morimoto-1"]], [["Rice-8"]], ["no_evidence"]], [[["Masaharu Morimoto-1"]], [["Masaharu Morimoto-1"]], [["Japanese cuisine-2", "Soy sauce-6"], "no_evidence"], [["Glutamic acid-22"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the Japanese dish \"Kombu\" derived from the word \"Kombu\" meaning \"to eat\" or \"to eat with\"?\nFacts: The name of the dish \"Kombu\" is derived from the word \"Kombu", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Masaharu Morimoto rely on glutamic acid?\nFacts: Masaharu Morimoto is a Japanese chef Japanese cuisine relies on several forms of seaweed as ingredients and flavorings for broth like kombu dashi Glutamic acid has been identified as the flavoring component in kombu seaweed\nAnswer (True or False):", "ct_subject": "Masaharu Morimoto is a Japanese chef Japanese cuisine relies on several forms of seaweed as ingredients and flavorings for broth like kombu dashi Glutamic acid has been identified as the flavoring component in kombu seaweed", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.994909"], [" the", "0.98177755"], [" name", "0.9066457"], [" of", "0.9999713"], [" the", "1.0"], [" Japanese", "0.9999969"], [" dish", "0.775519"], [" \"", "0.9999982"], ["K", "1.0"], ["omb", "1.0"], ["u", "1.0"], ["\"", "0.9999999"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["K", "0.99996614"], ["omb", "1.0"], ["u", "1.0"], ["\"", "1.0"], [" meaning", "0.99984694"], [" \"", "1.0"], ["to", "0.9999999"], [" eat", "1.0"], ["\"", "0.9890415"], [" or", "1.0"], [" \"", "1.0"], ["to", "1.0"], [" eat", "1.0"], [" with", "1.0"], ["\"?", "0.9470423"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999938"], [" name", "0.9998462"], [" of", "0.9944352"], [" the", "1.0"], [" dish", "0.99971336"], [" \"", "1.0"], ["K", "1.0"], ["omb", "1.0"], ["u", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["K", "1.0"], ["omb", "1.0"], ["u", "1.0"]]}, "945": {"qid": "b7ee54ccf00c2de84abb", "term": "Koala", "description": "An arboreal herbivorous marsupial native to Australia.", "question": "Would a nickel fit inside a koala pouch?", "answer": true, "facts": ["Koala joeys (babies) enter their mother's pouch when they are about 2 to 3 centimeters long.", "An American nickel is 2.12 centimeters in diameter."], "decomposition": ["Who usually sits in a koala's pouch?", "What is the size of #1?", "How big is a nickel?", "Is #2 more than #3?"], "evidence": [[[["Koala-2"]], [["Koala-25", "Marsupial-26"]], [["Nickel (United States coin)-1"]], ["operation"]], [[["Koala-2"]], [["Koala-1"], "no_evidence"], [["Nickel (United States coin)-1"]], ["operation"]], [[["Koala-2"]], [["Koala-23", "Koala-24"]], [["Nickel (United States coin)-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest word in the English language?\nFacts: The longest word in the English language is \"the\". The longest word in the English language is \"the\".\nAnswer (True or False): True\nQuestion: What is the longest word in the English language?\nFacts: The longest word in the English language is \"the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a nickel fit inside a koala pouch?\nFacts: Koala joeys (babies) enter their mother's pouch when they are about 2 to 3 centimeters long. An American nickel is 2.12 centimeters in diameter.\nAnswer (True or False):", "ct_subject": "Koala joeys (babies) enter their mother's pouch when they are about 2 to 3 centimeters long. An American nickel is 2.12 centimeters in diameter.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.999966"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.984453"], [" word", "0.13832913"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], [" is", "1.0"], [" \"", "1.0"], ["the", "0.9958662"], ["\".", "0.9041147"], [" The", "0.9999839"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], [" is", "1.0"], [" \"", "1.0"], ["the", "1.0"], ["\".", "1.0"], ["\n", "0.998437"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.9999999"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "0.9999869"], [" language", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], [" is", "1.0"], [" \"", "1.0"], ["the", "1.0"]]}, "946": {"qid": "16d8da02bc5e1975a1d9", "term": "Christmas carol", "description": "Song or hymn or carol on the theme of Christmas", "question": "When the shuttle Columbia 11 landed, was it the season for Christmas carols?", "answer": true, "facts": ["The Columbia 11 shuttle landed on December 10th 1990.", "Christmas is celebrated during the month of December every year."], "decomposition": ["What month did the space shuttle Columbia 11 land?", "In what month are Christmas carols typically sung?", "Are #1 and #2 the same answer?"], "evidence": [[[["STS-40-1"], "no_evidence"], [["Christmas-1"]], ["operation"]], [[["STS-40-7"]], [["Christmas and holiday season-1", "Christmas carol-1"]], ["operation"]], [[["Space Shuttle Columbia-1"], "no_evidence"], [["Christmas and holiday season-1", "Christmas carol-1"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first man to walk on the moon?\nFacts: Neil Armstrong was the first man to walk on the moon on July 20th 1969.\nAnswer (True or False): True\nQuestion: What is the name of the first man to walk on the moon?\nFacts: Neil Armstrong was the first man to walk on the moon on July 20", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: When the shuttle Columbia 11 landed, was it the season for Christmas carols?\nFacts: The Columbia 11 shuttle landed on December 10th 1990. Christmas is celebrated during the month of December every year.\nAnswer (True or False):", "ct_subject": "The Columbia 11 shuttle landed on December 10th 1990. Christmas is celebrated during the month of December every year.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.997684"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999995"], [" of", "1.0"], [" the", "1.0"], [" first", "0.046981152"], [" man", "0.40429768"], [" to", "0.9999999"], [" walk", "0.9999008"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Neil", "1.0"], [" Armstrong", "1.0"], [" was", "0.99999785"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" on", "0.9999968"], [" July", "1.0"], [" 20", "1.0"], ["th", "0.7711818"], [" 1969", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "0.9997868"], [" to", "1.0"], [" walk", "0.99979883"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Neil", "1.0"], [" Armstrong", "1.0"], [" was", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" on", "1.0"], [" July", "1.0"], [" 20", "1.0"]]}, "947": {"qid": "694ac8d334fc4545ee31", "term": "Kurt Cobain", "description": "American singer, composer, and musician", "question": "Did Kurt Cobain's music genre survive after his death?", "answer": true, "facts": ["Kurt Cobain was the lead singer of Nirvana.", "Nirvana's music is classified as Grunge rock.", "Kurt Cobain died on April 5, 1994.", "Some of the major Grunge rock bands included Alice in Chains, Pearl Jam, and Soundgarden.", "Alice in Chains and Pearl Jam released their latest albums in 2018 and 2020 respectively."], "decomposition": ["What is the musiucal genre associated with both Kurt Cobain and Pearl Jam?", "What year did Kurt Cobain die? ", "Did Pearl Jam release a #1 genre album after #2?", "Is #3 yes?"], "evidence": [[[["Grunge-2"]], [["Kurt Cobain-1"]], [["No Code-1"]], [["No Code-1"]]], [[["Kurt Cobain-2", "Pearl Jam-2"]], [["Kurt Cobain-55"]], [["Vitalogy-9"], "no_evidence"], ["operation"]], [[["Grunge-2"]], [["Kurt Cobain-1"]], [["Lightning Bolt (Pearl Jam album)-11"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the year 2000?\nFacts: The year 2000 was the first year that the year zero was used.\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Kurt Cobain's music genre survive after his death?\nFacts: Kurt Cobain was the lead singer of Nirvana. Nirvana's music is classified as Grunge rock. Kurt Cobain died on April 5, 1994. Some of the major Grunge rock bands included Alice in Chains, Pearl Jam, and Soundgarden. Alice in Chains and Pearl Jam released their latest albums in 2018 and 2020 respectively.\nAnswer (True or False):", "ct_subject": "Kurt Cobain was the lead singer of Nirvana. Nirvana's music is classified as Grunge rock. Kurt Cobain died on April 5, 1994. Some of the major Grunge rock bands included Alice in Chains, Pearl Jam, and Soundgarden. Alice in Chains and Pearl Jam released their latest albums in 2018 and 2020 respectively.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.5331125"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999993"], [" of", "1.0"], [" the", "1.0"], [" first", "0.8851375"], [" person", "0.98197067"], [" to", "1.0"], [" be", "0.99967563"], [" born", "0.9999974"], [" in", "1.0"], [" the", "1.0"], [" year", "0.99991107"], [" 2000", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" year", "0.9945322"], [" 2000", "1.0"], [" was", "0.9999999"], [" the", "1.0"], [" first", "0.9889374"], [" year", "1.0"], [" that", "0.999998"], [" the", "0.968633"], [" year", "1.0"], [" zero", "0.99282783"], [" was", "1.0"], [" used", "0.9997272"], [".", "1.0"], ["\n", "0.5214102"]]}, "948": {"qid": "7e552e58565771a2008c", "term": "Noah's Ark", "description": "the vessel in the Genesis flood narrative", "question": "Were there eight humans on Noah's Ark?", "answer": true, "facts": ["Noah only took his family aboard the Ark.", "Noah brought his wife, three sons, and his sons' wives.", "Four couples lived on the Ark, eight total people."], "decomposition": ["How many people entered Noah's Ark?", "Is #1 greater than or equal to eight?"], "evidence": [[[["Wives aboard Noah's Ark-6"]], [["Wives aboard Noah's Ark-6"]]], [[["Wives aboard Noah's Ark-6"]], ["operation"]], [[["Wives aboard Noah's Ark-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Ark have a compass?\nFacts: The Ark had a compass, but it was not a compass that was used by the Ark's crew. The Ark's compass was a set of stones that were used to measure the distance between the two poles.\nAnswer (True or False): True\nQuestion: Did the Ark have a compass?\nF", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Were there eight humans on Noah's Ark?\nFacts: Noah only took his family aboard the Ark. Noah brought his wife, three sons, and his sons' wives. Four couples lived on the Ark, eight total people.\nAnswer (True or False):", "ct_subject": "Noah only took his family aboard the Ark. Noah brought his wife, three sons, and his sons' wives. Four couples lived on the Ark, eight total people.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999833"], [" the", "1.0"], [" Ark", "0.9993093"], [" have", "0.99999964"], [" a", "1.0"], [" compass", "0.091920696"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.47898197"], [" Ark", "1.0"], [" had", "0.32071427"], [" a", "0.9999994"], [" compass", "1.0"], [",", "0.99939895"], [" but", "1.0"], [" it", "1.0"], [" was", "1.0"], [" not", "1.0"], [" a", "0.9597807"], [" compass", "0.93937916"], [" that", "1.0"], [" was", "0.9983632"], [" used", "1.0"], [" by", "0.9386227"], [" the", "1.0"], [" Ark", "0.9990833"], ["'s", "0.990928"], [" crew", "1.0"], [".", "1.0"], [" The", "0.9994393"], [" Ark", "1.0"], ["'s", "1.0"], [" compass", "0.9993255"], [" was", "1.0"], [" a", "1.0"], [" set", "0.9999043"], [" of", "1.0"], [" stones", "0.99999726"], [" that", "1.0"], [" were", "1.0"], [" used", "0.98692894"], [" to", "1.0"], [" measure", "0.92596966"], [" the", "1.0"], [" distance", "0.9651747"], [" between", "0.9535158"], [" the", "1.0"], [" two", "0.64690125"], [" poles", "1.0"], [".", "0.999746"], ["\n", "0.9999713"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "0.999995"], [" Ark", "1.0"], [" have", "1.0"], [" a", "1.0"], [" compass", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"]]}, "949": {"qid": "f3e238989015dd72bfda", "term": "Queen Elizabeth The Queen Mother", "description": "Queen consort of King George VI, mother of Queen Elizabeth II", "question": "Did Queen Elizabeth The Queen Mother and her daughter share name with Tudor queen?", "answer": true, "facts": ["Queen Elizabeth the Queen Mother gave birth to Queen Elizabeth II in 1926.", "The Tudor dynasty had a number of Queens including: Mary I of England, Elizabeth I of England, and Margaret Tudor, Queen of Scots."], "decomposition": ["Which name did the Queen Mother and Queen Elizabeth have in common?", "What are the names of some queens from the Tudor dynasty?", "Is #1 included in any of #2?"], "evidence": [[[["Elizabeth II-1", "Queen Elizabeth The Queen Mother-1"]], [["House of Tudor-1"]], ["operation"]], [[["Queen Elizabeth The Queen Mother-1"]], [["Elizabeth I of England-1", "Mary I of England-1"]], ["operation"]], [[["Queen Elizabeth The Queen Mother-1"]], [["House of Tudor-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the first person to be born in the year 2000 have a name that was not the year 2000?\nFacts: The first person to be born in the year 2000 was born in the year 2000.\nAnswer (True or False): True\nQuestion: Did the first", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Queen Elizabeth The Queen Mother and her daughter share name with Tudor queen?\nFacts: Queen Elizabeth the Queen Mother gave birth to Queen Elizabeth II in 1926. The Tudor dynasty had a number of Queens including: Mary I of England, Elizabeth I of England, and Margaret Tudor, Queen of Scots.\nAnswer (True or False):", "ct_subject": "Queen Elizabeth the Queen Mother gave birth to Queen Elizabeth II in 1926. The Tudor dynasty had a number of Queens including: Mary I of England, Elizabeth I of England, and Margaret Tudor, Queen of Scots.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999541"], [" the", "1.0"], [" first", "0.91361797"], [" person", "0.9916963"], [" to", "1.0"], [" be", "0.9999696"], [" born", "0.9999987"], [" in", "1.0"], [" the", "0.7050748"], [" year", "0.9941334"], [" 2000", "1.0"], [" have", "0.9602448"], [" a", "0.9999776"], [" name", "1.0"], [" that", "0.9993292"], [" was", "0.99980384"], [" not", "0.86223197"], [" the", "0.9271232"], [" year", "0.9993591"], [" 2000", "0.9825267"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "0.9999517"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "1.0"], [" born", "1.0"], [" in", "0.9999989"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [".", "1.0"], ["\n", "0.999951"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99999976"], [" the", "1.0"], [" first", "1.0"]]}, "950": {"qid": "a0362ec2c3b195e02315", "term": "Last Supper", "description": "Final meal that, in the Gospel accounts, Jesus shared with his apostles in Jerusalem before his crucifixion", "question": "Is anyone at the Last Supper celebrated in Islam?", "answer": true, "facts": ["The Last Supper was a meal between Jesus and his twelve disciples in Christianity.", "In Islam, Jesus is one of many revered prophets.", "In Islam, Jesus returns in a Second Coming to fight the \"False Messiah\" and establish peace on earth."], "decomposition": ["Who was present at the Last Supper?", "Are any of #1 celebrated in Islam?"], "evidence": [[[["Last Supper-1"]], [["Jesus-4"], "operation"]], [[["Last Supper-1"]], [["Jesus in Islam-2"]]], [[["Apostles-1", "Last Supper-1"]], [["Disciples of Jesus in Islam-1"], "no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the Bible the only book of the Bible?\nFacts: The Bible is the only book of the Bible. The Bible is the only book of the Bible. The Bible is the only book of the Bible. The Bible is the only book of the Bible. The Bible", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is anyone at the Last Supper celebrated in Islam?\nFacts: The Last Supper was a meal between Jesus and his twelve disciples in Christianity. In Islam, Jesus is one of many revered prophets. In Islam, Jesus returns in a Second Coming to fight the \"False Messiah\" and establish peace on earth.\nAnswer (True or False):", "ct_subject": "The Last Supper was a meal between Jesus and his twelve disciples in Christianity. In Islam, Jesus is one of many revered prophets. In Islam, Jesus returns in a Second Coming to fight the \"False Messiah\" and establish peace on earth.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999999"], [" the", "0.9963812"], [" Bible", "0.9999995"], [" the", "0.9657984"], [" only", "0.89445645"], [" book", "0.9999856"], [" of", "0.9850044"], [" the", "1.0"], [" Bible", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Bible", "1.0"], [" is", "1.0"], [" the", "1.0"], [" only", "0.9988525"], [" book", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Bible", "1.0"], [".", "1.0"], [" The", "0.9932162"], [" Bible", "1.0"], [" is", "1.0"], [" the", "1.0"], [" only", "1.0"], [" book", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Bible", "1.0"], [".", "0.99999976"], [" The", "1.0"], [" Bible", "1.0"], [" is", "1.0"], [" the", "1.0"], [" only", "1.0"], [" book", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Bible", "1.0"], [".", "1.0"], [" The", "1.0"], [" Bible", "1.0"], [" is", "1.0"], [" the", "1.0"], [" only", "1.0"], [" book", "1.0"], [" of", "1.0"], [" the", "1.0"], [" Bible", "1.0"], [".", "1.0"], [" The", "1.0"], [" Bible", "1.0"]]}, "951": {"qid": "a7b23152e978745fa82b", "term": "League of Legends", "description": "Multiplayer online battle arena video game", "question": "Could Cosmic Girls play League of Legends alone?", "answer": true, "facts": ["Cosmic Girls is a 13 member kpop group", "League of Legends is a video game requiring two teams of five players each"], "decomposition": ["How many players are needed for a League of Legends match?", "How many people are in the group \"Cosmic Girls\"?", "Is #2 greater than or equal to #1?"], "evidence": [[[["League of Legends: Wild Rift-2"], "no_evidence"], [["Cosmic Girls-1"]], ["operation"]], [[["League of Legends: Wild Rift-4"]], [["Cosmic Girls-1"]], ["operation"]], [[["League of Legends-10"]], [["Cosmic Girls-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the song that is playing in the background of the video?\nFacts: \"I'm a Star\" by Girls' Generation\nAnswer (True or False): True\nQuestion: What is the name of the song that is playing in the background of the video?\nFacts: \"I'm a Star\" by Girls' Generation\nAnswer (True or False): True\nQuestion:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could Cosmic Girls play League of Legends alone?\nFacts: Cosmic Girls is a 13 member kpop group League of Legends is a video game requiring two teams of five players each\nAnswer (True or False):", "ct_subject": "Cosmic Girls is a 13 member kpop group League of Legends is a video game requiring two teams of five players each", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.999912"], [" is", "1.0"], [" the", "1.0"], [" name", "0.97041714"], [" of", "1.0"], [" the", "1.0"], [" song", "0.8966936"], [" that", "0.99885607"], [" is", "0.7853905"], [" playing", "0.35292506"], [" in", "1.0"], [" the", "1.0"], [" background", "1.0"], [" of", "1.0"], [" the", "1.0"], [" video", "1.0"], ["?", "0.9843051"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" \"", "0.99998116"], ["I", "0.99999774"], ["'m", "1.0"], [" a", "0.99980825"], [" Star", "1.0"], ["\"", "1.0"], [" by", "1.0"], [" Girls", "0.94559366"], ["'", "1.0"], [" Generation", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" song", "1.0"], [" that", "1.0"], [" is", "1.0"], [" playing", "1.0"], [" in", "1.0"], [" the", "1.0"], [" background", "1.0"], [" of", "1.0"], [" the", "1.0"], [" video", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" \"", "1.0"], ["I", "1.0"], ["'m", "1.0"], [" a", "1.0"], [" Star", "1.0"], ["\"", "1.0"], [" by", "1.0"], [" Girls", "1.0"], ["'", "1.0"], [" Generation", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"]]}, "952": {"qid": "df538a945f0d1c012cd2", "term": "Super Mario", "description": "platform video game series from Nintendo's Mario franchise", "question": "Does Super Mario require electricity to play?", "answer": true, "facts": ["Super Mario is a video game.", "Video games are played on electronic devices.", "Electronic devices require electricity to function."], "decomposition": ["What is Super Mario?", "Where are #1 played?", "Do #2 require electricity?"], "evidence": [[[["Super Mario-1"]], [["Nintendo video game consoles-1"]], [["Nintendo video game consoles-1"]]], [[["Super Mario-1"]], [["Nintendo Entertainment System-2"]], [["Nintendo Entertainment System-12"], "operation"]], [[["Super Mario-1"]], [["Super Mario-1", "Video game console-3"]], [["Video game console-3"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: The first person to be born in the United States was John Smith.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: The first person to be born in the United States was John Smith.\nAnswer (True or False):", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Super Mario require electricity to play?\nFacts: Super Mario is a video game. Video games are played on electronic devices. Electronic devices require electricity to function.\nAnswer (True or False):", "ct_subject": "Super Mario is a video game. Video games are played on electronic devices. Electronic devices require electricity to function.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99944955"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99974626"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9999497"], [" person", "0.99955875"], [" to", "1.0"], [" be", "0.9960936"], [" born", "0.9481939"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.62549573"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" John", "0.9047801"], [" Smith", "0.9999988"], [".", "1.0"], ["\n", "0.99221987"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" John", "1.0"], [" Smith", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"]]}, "953": {"qid": "d47c38cb1ab436fe2f64", "term": "2008 Summer Olympics", "description": "Games of the XXIX Olympiad, held in Beijing in 2008", "question": "Could all of the 2008 Summer Olympics women find a hookup athlete partner?", "answer": true, "facts": ["The 2008 Summer Olympics had 4,637 women compete.", "The 2008 Summer Olympics had 6,305 men compete."], "decomposition": ["How many women participated in the 2008 Summer Olympics?", "How many men participated in the 2008 Summer Olympics?", "Is #2 at least equal to #1?"], "evidence": [[[["2008 Summer Olympics-1", "2008 Summer Olympics-2"], "no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Athletics at the 2008 Summer Olympics-2"], "no_evidence"], [["Athletics at the 2008 Summer Olympics-2"], "no_evidence"], ["operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human?\nFacts: The average weight of a human is about 100 pounds.\nAnswer (True or False): True\nQuestion: What is the average height of a human?\nFacts: The average height of a human is about 5 feet, 8 inches.\nAnswer (True or False): True\nQuestion: What is the average weight", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could all of the 2008 Summer Olympics women find a hookup athlete partner?\nFacts: The 2008 Summer Olympics had 4,637 women compete. The 2008 Summer Olympics had 6,305 men compete.\nAnswer (True or False):", "ct_subject": "The 2008 Summer Olympics had 4,637 women compete. The 2008 Summer Olympics had 6,305 men compete.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.98630667"], [" is", "1.0"], [" the", "1.0"], [" average", "0.9999727"], [" weight", "0.99503005"], [" of", "1.0"], [" a", "0.9999999"], [" human", "0.995316"], ["?", "0.9766764"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" weight", "0.99987924"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" is", "1.0"], [" about", "0.9988708"], [" 100", "0.97870123"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" height", "1.0"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" height", "1.0"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 5", "0.997823"], [" feet", "0.9999999"], [",", "0.9992999"], [" 8", "0.8984088"], [" inches", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "1.0"]]}, "954": {"qid": "93029b0b5b4f19ab150a", "term": "Skull", "description": "bony structure that forms the skeleton of head in most vertebrates", "question": "Can an adult human skull hypothetically pass through the birth canal?", "answer": true, "facts": ["The largest baby ever born was 22 pounds. ", "The average human skull weighs between 10 and 11 pounds."], "decomposition": ["How big is the average baby ever delivered vaginally?", "How big is the average adult skull?", "Is #1 greater than #2?"], "evidence": [[[["Childbirth-29"], "no_evidence"], [["Human head-18"]], [["Obstetrical dilemma-14"], "operation"]], [[["Infant-9"]], [["Human head-18"]], [["Human head-18", "Infant-9"]]], [[["Infant-5", "Infant-7"]], [["Skull-1"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human being?\nFacts: The average human being weighs between 10 and 11 pounds.\nAnswer (True or False): True\nQuestion: What is the average weight of a human being?\nFacts: The average human being weighs between 10 and 11 pounds.\nAnswer (True or False): True\nQuestion: What is the average weight of a human being?", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can an adult human skull hypothetically pass through the birth canal?\nFacts: The largest baby ever born was 22 pounds.  The average human skull weighs between 10 and 11 pounds.\nAnswer (True or False):", "ct_subject": "The largest baby ever born was 22 pounds.  The average human skull weighs between 10 and 11 pounds.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99632025"], [" is", "1.0"], [" the", "1.0"], [" average", "0.9597125"], [" weight", "1.0"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "0.96838367"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.99999976"], [" being", "0.9999999"], [" weighs", "1.0"], [" between", "1.0"], [" 10", "0.99834204"], [" and", "1.0"], [" 11", "1.0"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "0.99999976"], [" of", "1.0"], [" a", "1.0"], [" human", "0.99999976"], [" being", "0.99977094"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "1.0"], [" being", "1.0"], [" weighs", "1.0"], [" between", "1.0"], [" 10", "1.0"], [" and", "1.0"], [" 11", "1.0"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "1.0"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "1.0"], ["?", "1.0"]]}, "955": {"qid": "ccb63564094c6ff09a14", "term": "Julia Roberts", "description": "American actress and producer", "question": "Does Julia Roberts lose the prolific acting contest in her family?", "answer": true, "facts": ["As of May 2020, Julia Roberts has acted in 64 projects.", "Julia Roberts has a brother in acting, Eric Roberts, and a niece in acting, Emma Roberts.", "As of May 2020, Eric Roberts has acted in 577 projects."], "decomposition": ["Who is Julia Roberts brother?", "Is #1 an actor?", "How many projects has #2 appeared in?", "How many projects has Julia Roberts acted in?", "Is #3 larger than #4?"], "evidence": [[[["Julia Roberts-4"]], [["Eric Roberts-1"]], [["Eric Roberts-2"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Eric Roberts-1"]], [["Eric Roberts-1"]], [["Eric Roberts filmography-4"], "no_evidence"], [["Julia Roberts-2"], "no_evidence"], ["no_evidence", "operation"]], [[["Eric Roberts-3"]], [["Eric Roberts-1"]], [["Eric Roberts-2"]], [["Julia Roberts filmography-1"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the year 2000?\nFacts: The year 2000 was the first year that the year 2000 was the first year that the year 2000 was the first year that the year 2000 was the first year that the year 2000 was the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Julia Roberts lose the prolific acting contest in her family?\nFacts: As of May 2020, Julia Roberts has acted in 64 projects. Julia Roberts has a brother in acting, Eric Roberts, and a niece in acting, Emma Roberts. As of May 2020, Eric Roberts has acted in 577 projects.\nAnswer (True or False):", "ct_subject": "As of May 2020, Julia Roberts has acted in 64 projects. Julia Roberts has a brother in acting, Eric Roberts, and a niece in acting, Emma Roberts. As of May 2020, Eric Roberts has acted in 577 projects.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999747"], [" is", "1.0"], [" the", "1.0"], [" name", "0.948356"], [" of", "1.0"], [" the", "1.0"], [" first", "0.99947447"], [" person", "0.99933475"], [" to", "1.0"], [" be", "0.9999347"], [" born", "0.9999987"], [" in", "1.0"], [" the", "1.0"], [" year", "0.999278"], [" 2000", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" year", "0.9995283"], [" 2000", "1.0"], [" was", "0.9999974"], [" the", "1.0"], [" first", "0.98171276"], [" year", "1.0"], [" that", "1.0"], [" the", "0.99999034"], [" year", "1.0"], [" 2000", "0.9995658"], [" was", "1.0"], [" the", "1.0"], [" first", "1.0"], [" year", "1.0"], [" that", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "1.0"], [" the", "1.0"], [" first", "1.0"], [" year", "1.0"], [" that", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "1.0"], [" the", "1.0"], [" first", "1.0"], [" year", "1.0"], [" that", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "1.0"], [" the", "1.0"]]}, "956": {"qid": "280da2485d6504022d34", "term": "Eid al-Fitr", "description": "Islamic holiday that marks the end of Ramadan", "question": "Is Eid al-Fitr holiday inappropriate to watch entire US Office?", "answer": true, "facts": ["Eid al-Fitr is an Islamic holiday dedicated to prayer.", "Eid al_fitr lasts from 1 to 3 days depending on the country.", "The entire US Office tv show would take 4 days, three hours, and 30 minutes to watch."], "decomposition": ["How long does Eid al-Fitr last?", "What is the run time of the Office?", "Is #2 longer than #1?"], "evidence": [[[["Eid al-Fitr-4"]], [["The Office (American TV series)-2"], "no_evidence"], ["operation"]], [[["Eid al-Fitr-4"]], [["Finale (The Office)-1", "The Office (American TV series)-8"]], ["operation"]], [[["Eid al-Fitr-1"]], [["The Office-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it true that the US government has a secret plan to kill all the Muslims in the world?\nFacts: The US government has no secret plan to kill all the Muslims in the world. The US government has a secret plan to kill all the Muslims in", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Eid al-Fitr holiday inappropriate to watch entire US Office?\nFacts: Eid al-Fitr is an Islamic holiday dedicated to prayer. Eid al_fitr lasts from 1 to 3 days depending on the country. The entire US Office tv show would take 4 days, three hours, and 30 minutes to watch.\nAnswer (True or False):", "ct_subject": "Eid al-Fitr is an Islamic holiday dedicated to prayer. Eid al_fitr lasts from 1 to 3 days depending on the country. The entire US Office tv show would take 4 days, three hours, and 30 minutes to watch.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "0.9903632"], [" true", "0.9999932"], [" that", "1.0"], [" the", "1.0"], [" US", "0.99883705"], [" government", "0.65973747"], [" has", "0.73778737"], [" a", "0.9997446"], [" secret", "1.0"], [" plan", "1.0"], [" to", "1.0"], [" kill", "0.9755366"], [" all", "0.9999981"], [" the", "0.99975866"], [" Muslims", "0.0052467505"], [" in", "0.9481027"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.97653985"], [" US", "1.0"], [" government", "1.0"], [" has", "0.9999999"], [" no", "0.97904384"], [" secret", "0.92858773"], [" plan", "1.0"], [" to", "1.0"], [" kill", "1.0"], [" all", "1.0"], [" the", "1.0"], [" Muslims", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [".", "1.0"], [" The", "0.95742047"], [" US", "1.0"], [" government", "1.0"], [" has", "0.9999994"], [" a", "0.9999995"], [" secret", "1.0"], [" plan", "1.0"], [" to", "1.0"], [" kill", "1.0"], [" all", "1.0"], [" the", "1.0"], [" Muslims", "0.99998295"], [" in", "1.0"]]}, "957": {"qid": "2ee33d6353893e4bd69c", "term": "Japan Airlines", "description": "airline headquartered in Tokyo, Japan", "question": "Are any of the destinations of Japan Airlines former Axis Powers?", "answer": true, "facts": ["Japan Airlines flies all over the world to places such as Germany, Ireland, and Australia.", "The Axis Powers were the countries that fought against the Allies during World War II.", "Axis Powers included countries such as Germany, Italy, and Japan."], "decomposition": ["Which countries does Japan Airlines fly to?", "Which counties were part of the Axis powers?", "Are there any similarities or overlap between #1 and #2?"], "evidence": [[[["Japan Airlines-2"], "no_evidence"], [["Axis powers-24"]], ["no_evidence"]], [[["Japan Airlines-2", "Japan Airlines-84"], "no_evidence"], [["Axis powers-2"]], ["operation"]], [[["Japan Airlines-63", "Japan Airlines-84"], "no_evidence"], [["Axis powers-1", "Axis powers-225"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the country that is the largest producer of oil in the world?\nFacts: Saudi Arabia.\nAnswer (True or False): True\nQuestion: What is the name of the country that is the largest producer of oil in the world?\nFacts", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are any of the destinations of Japan Airlines former Axis Powers?\nFacts: Japan Airlines flies all over the world to places such as Germany, Ireland, and Australia. The Axis Powers were the countries that fought against the Allies during World War II. Axis Powers included countries such as Germany, Italy, and Japan.\nAnswer (True or False):", "ct_subject": "Japan Airlines flies all over the world to places such as Germany, Ireland, and Australia. The Axis Powers were the countries that fought against the Allies during World War II. Axis Powers included countries such as Germany, Italy, and Japan.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.997889"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" country", "0.7864669"], [" that", "1.0"], [" is", "0.9967829"], [" the", "1.0"], [" largest", "1.0"], [" producer", "1.0"], [" of", "1.0"], [" oil", "0.9141998"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Saudi", "0.9998585"], [" Arabia", "1.0"], [".", "0.8031979"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" country", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" producer", "1.0"], [" of", "1.0"], [" oil", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"]]}, "958": {"qid": "bd9497b8ba9a2085a775", "term": "WWE Raw", "description": "WWE television program", "question": "Would a viewer of Monday Night Football be able to catch WWE Raw during commercial breaks?", "answer": true, "facts": ["Monday Night Football begins at 8pm EST on Monday nights during the NFL season", "WWE Raw airs on Monday nights between 8pm and 11pm EST"], "decomposition": ["When does Monday Night Football air?", "When does WWE Raw air?", "Is there and overlap between #1 and #2?"], "evidence": [[[["Monday Night Football-68"]], [["WWE Raw-1"]], ["operation"]], [[["Monday Night Football-7"], "operation"], [["WWE Raw-59"], "operation"], ["operation"]], [[["Monday Night Football-68"]], [["WWE Raw-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be killed by a shark?\nFacts: The first person to be killed by a shark was a man named John D. Rockefeller.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be killed by a shark?\nFacts: The first person to", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a viewer of Monday Night Football be able to catch WWE Raw during commercial breaks?\nFacts: Monday Night Football begins at 8pm EST on Monday nights during the NFL season WWE Raw airs on Monday nights between 8pm and 11pm EST\nAnswer (True or False):", "ct_subject": "Monday Night Football begins at 8pm EST on Monday nights during the NFL season WWE Raw airs on Monday nights between 8pm and 11pm EST", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999917"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99999475"], [" of", "1.0"], [" the", "1.0"], [" first", "0.922946"], [" person", "0.98838913"], [" to", "1.0"], [" be", "0.9999759"], [" killed", "0.45608112"], [" by", "0.9997662"], [" a", "1.0"], [" shark", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99999845"], [" first", "1.0"], [" person", "0.99795437"], [" to", "1.0"], [" be", "1.0"], [" killed", "1.0"], [" by", "1.0"], [" a", "1.0"], [" shark", "1.0"], [" was", "1.0"], [" a", "1.0"], [" man", "0.9999949"], [" named", "1.0"], [" John", "0.9999974"], [" D", "0.81374127"], [".", "1.0"], [" Rockefeller", "1.0"], [".", "0.9999999"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" killed", "1.0"], [" by", "1.0"], [" a", "1.0"], [" shark", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"]]}, "959": {"qid": "fab08cce89ad00ba8339", "term": "Christmas carol", "description": "Song or hymn or carol on the theme of Christmas", "question": "Are multiple Christmas Carol's named after Saints?", "answer": true, "facts": ["The Christmas Carol Good King Wenceslas is based on the Bohemian king Wenceslaus I.", "Wenceslaus I was named a Saint in the Catholic Church and has a feast day of September 28.", "Jolly Old Saint Nicholas is a Christmas Carol named after an early Christian bishop who became a Saint."], "decomposition": ["Who was Christmas carol 'Good King Wenceslas' about?", "Who was  Christmas carol 'Jolly Old Saint Nicholas' named for?", "Are #1 and #2 Saints?"], "evidence": [[[["Good King Wenceslas-3"]], [["Jolly Old Saint Nicholas-1"]], [["Good King Wenceslas-3", "Jolly Old Saint Nicholas-1"]]], [[["Good King Wenceslas-1"]], [["Saint Nicholas-1"]], [["Saint Nicholas-1", "Wenceslaus I, Duke of Bohemia-2"]]], [[["Good King Wenceslas-1"]], [["Saint Nicholas-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there more than one God?\nFacts: There are many Gods in the world. The Bible says that there are three Gods: God the Father, God the Son, and God the Holy Spirit.\nAnswer (True or", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are multiple Christmas Carol's named after Saints?\nFacts: The Christmas Carol Good King Wenceslas is based on the Bohemian king Wenceslaus I. Wenceslaus I was named a Saint in the Catholic Church and has a feast day of September 28. Jolly Old Saint Nicholas is a Christmas Carol named after an early Christian bishop who became a Saint.\nAnswer (True or False):", "ct_subject": "The Christmas Carol Good King Wenceslas is based on the Bohemian king Wenceslaus I. Wenceslaus I was named a Saint in the Catholic Church and has a feast day of September 28. Jolly Old Saint Nicholas is a Christmas Carol named after an early Christian bishop who became a Saint.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.9992354"], [" there", "0.99999905"], [" more", "0.9999989"], [" than", "0.9999777"], [" one", "0.99870956"], [" God", "0.99123865"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "1.0"], [" are", "0.97702676"], [" many", "0.99998176"], [" Gods", "1.0"], [" in", "0.98850924"], [" the", "1.0"], [" world", "1.0"], [".", "0.99999964"], [" The", "0.42173785"], [" Bible", "0.99999976"], [" says", "1.0"], [" that", "0.9999981"], [" there", "1.0"], [" are", "0.9757262"], [" three", "0.99649733"], [" Gods", "1.0"], [":", "1.0"], [" God", "0.9994413"], [" the", "1.0"], [" Father", "1.0"], [",", "1.0"], [" God", "1.0"], [" the", "1.0"], [" Son", "1.0"], [",", "0.99999964"], [" and", "1.0"], [" God", "1.0"], [" the", "1.0"], [" Holy", "1.0"], [" Spirit", "1.0"], [".", "1.0"], ["\n", "0.9998653"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"]]}, "960": {"qid": "9594919b1927f406a344", "term": "Bobby Jindal", "description": "American politician", "question": "Would Bobby Jindal's high school mascot eat kibble?", "answer": true, "facts": ["Bobby Jindal attended Baton Rouge Magnet High School.", "Baton Rouge Magnet High School's mascot is the bulldog.", "Kibble is another name for the dry form of dog/pet food."], "decomposition": ["Which school did Bobby Jindal attend?", "What is #1's mascot?", "What does kibble refer to?", "Would a #2 eat #3?"], "evidence": [[[["Bobby Jindal-7"]], ["no_evidence"], [["Kibbles 'n Bits-1"]], ["operation"]], [[["Bobby Jindal-7"]], [["Baton Rouge Magnet High School-1"], "no_evidence"], [["Dog food-1"], "no_evidence"], ["operation"]], [[["Bobby Jindal-7"]], ["no_evidence"], [["Dog food-16"]], [["Dog food-16"], "no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that has the highest percentage of people who are obese?\nFacts: The city with the highest percentage of people who are obese is New York City.\nAnswer (True or False): True\nQuestion: What is the name of the city in the United States that has the highest percentage", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Bobby Jindal's high school mascot eat kibble?\nFacts: Bobby Jindal attended Baton Rouge Magnet High School. Baton Rouge Magnet High School's mascot is the bulldog. Kibble is another name for the dry form of dog/pet food.\nAnswer (True or False):", "ct_subject": "Bobby Jindal attended Baton Rouge Magnet High School. Baton Rouge Magnet High School's mascot is the bulldog. Kibble is another name for the dry form of dog/pet food.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.98679787"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999938"], [" of", "1.0"], [" the", "1.0"], [" city", "0.78620845"], [" in", "0.8730779"], [" the", "0.99999976"], [" United", "0.9994611"], [" States", "1.0"], [" that", "1.0"], [" has", "0.8953226"], [" the", "1.0"], [" highest", "0.9820457"], [" percentage", "0.96134365"], [" of", "1.0"], [" people", "1.0"], [" who", "1.0"], [" are", "0.9999664"], [" obese", "0.9999864"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" city", "1.0"], [" with", "1.0"], [" the", "1.0"], [" highest", "1.0"], [" percentage", "1.0"], [" of", "1.0"], [" people", "0.9994437"], [" who", "1.0"], [" are", "1.0"], [" obese", "1.0"], [" is", "1.0"], [" New", "1.0"], [" York", "1.0"], [" City", "1.0"], [".", "1.0"], ["\n", "0.99855334"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" has", "1.0"], [" the", "1.0"], [" highest", "1.0"], [" percentage", "1.0"]]}, "961": {"qid": "73a229ef0fd252aaeb39", "term": "Honey badger", "description": "species of mammal", "question": "Would a honey badger's dentures be different from a wolverine's?", "answer": true, "facts": ["Dentures are false teeth that resemble the wearer's natural teeth", "Honey badgers and wolverines are physically very similar, but they can be differentiated by their dentition."], "decomposition": ["What subfamily does the honey badger belong to?", "What subfamily does the wolverine belong to?", "What helps distinguish #1 from #2?", "Does #3 include dental shape?"], "evidence": [[[["Honey badger-2"]], [["Wolverine-1"]], [["Honey badger-15"], "no_evidence"], ["operation"]], [[["Honey badger-2"]], [["Wolverine-1"]], [["Honey badger-15"]], [["Wolverine-9"], "operation"]], [[["Honey badger-6"]], [["Wolverine-1"], "no_evidence"], [["Honey badger-15", "Wolverine-14"], "no_evidence"], [["Honey badger-15"], "no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a man and a woman?\nFacts: A man is a male human being. A woman is a female human being.\nAnswer (True or False): True\nQuestion: What is the difference between a man and a woman?\nFacts: A man is a male human being.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a honey badger's dentures be different from a wolverine's?\nFacts: Dentures are false teeth that resemble the wearer's natural teeth Honey badgers and wolverines are physically very similar, but they can be differentiated by their dentition.\nAnswer (True or False):", "ct_subject": "Dentures are false teeth that resemble the wearer's natural teeth Honey badgers and wolverines are physically very similar, but they can be differentiated by their dentition.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999833"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9999726"], [" between", "1.0"], [" a", "1.0"], [" man", "0.9387379"], [" and", "1.0"], [" a", "1.0"], [" woman", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" man", "1.0"], [" is", "1.0"], [" a", "1.0"], [" male", "0.99998975"], [" human", "1.0"], [" being", "1.0"], [".", "0.99999964"], [" A", "1.0"], [" woman", "1.0"], [" is", "1.0"], [" a", "1.0"], [" female", "1.0"], [" human", "1.0"], [" being", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" man", "1.0"], [" and", "1.0"], [" a", "1.0"], [" woman", "1.0"], ["?", "0.9999964"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" man", "1.0"], [" is", "1.0"], [" a", "1.0"], [" male", "1.0"], [" human", "1.0"], [" being", "1.0"], [".", "1.0"]]}, "962": {"qid": "c22eb5bb5ca50900abc4", "term": "Green", "description": "Additive primary color visible between blue and yellow", "question": "Did the color green help Theodor Geisel become famous?", "answer": true, "facts": ["Green is a color made by mixing blue and yellow", "Theodor Geisel is the American writer known as Dr. Seuss", "One of Dr. Seuss's most famous books and lines is Green Eggs and Ham"], "decomposition": ["What was Theodor Geisel's pen name?", "Which books authored by #1 made him famous?", "Are any of #2 particularly related to the color green?"], "evidence": [[[["Geisel Award-2"]], [["Green Eggs and Ham-5"]], [["Green Eggs and Ham-5"]]], [[["Dr. Seuss-1"]], [["Dr. Seuss-3"]], [["Green Eggs and Ham-3"], "operation"]], [[["Dr. Seuss-1"]], [["Dr. Seuss-3"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city of Rome?\nFacts: Rome is the capital of the Roman Empire The name of the city of Rome is Rome The name of the city of Rome is Rome The name of the city of Rome is Rome The name of the city of Rome is Rome The name of the city", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did the color green help Theodor Geisel become famous?\nFacts: Green is a color made by mixing blue and yellow Theodor Geisel is the American writer known as Dr. Seuss One of Dr. Seuss's most famous books and lines is Green Eggs and Ham\nAnswer (True or False):", "ct_subject": "Green is a color made by mixing blue and yellow Theodor Geisel is the American writer known as Dr. Seuss One of Dr. Seuss's most famous books and lines is Green Eggs and Ham", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.994041"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999895"], [" of", "1.0"], [" the", "1.0"], [" city", "0.37574765"], [" of", "0.95901763"], [" Rome", "0.9924233"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "1.0"], [" is", "0.9999858"], [" the", "0.99996805"], [" capital", "0.9999999"], [" of", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [" The", "0.9999783"], [" name", "0.98037004"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "0.98461086"], [" Rome", "1.0"], [" is", "0.9999945"], [" Rome", "0.99999976"], [" The", "0.99997866"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], [" is", "1.0"], [" Rome", "1.0"], [" The", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], [" is", "1.0"], [" Rome", "1.0"], [" The", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], [" is", "1.0"], [" Rome", "1.0"], [" The", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"]]}, "963": {"qid": "3b70f1178fdb45ad2a24", "term": "Ariana Grande", "description": "American singer, songwriter, and actress", "question": "Does Ariana Grande's signature style combine comfort items and high fashion?", "answer": true, "facts": ["Ariana Grande's signature style is a long, over-sized pullover sweater with thigh high heels.", "Oversized pullovers are considered lounge wear, for relaxing at home in. ", "High heels are associated with high style. "], "decomposition": ["What is Ariana Grande's signature top?", "What is Ariana Grande's signature shoewear?", "What type of clothing is #1 considered?", "Is #3 considered a comfort and item and is #2 considered a high style item?"], "evidence": [[[["Ariana Grande-34"], "no_evidence"], [["Ariana Grande-34"], "no_evidence"], ["no_evidence"], ["no_evidence"]], [[["Ariana Grande-34"]], ["no_evidence"], ["no_evidence"], ["operation"]], [[["Ariana Grande-34"]], [["Ariana Grande-34"]], [["Crop top-3"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the most popular type of car in the United States?\nFacts: The most popular type of car in the United States is a car. \nAnswer (True or False): True\nQuestion: What is the most popular type of car in the United States?", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Ariana Grande's signature style combine comfort items and high fashion?\nFacts: Ariana Grande's signature style is a long, over-sized pullover sweater with thigh high heels. Oversized pullovers are considered lounge wear, for relaxing at home in.  High heels are associated with high style. \nAnswer (True or False):", "ct_subject": "Ariana Grande's signature style is a long, over-sized pullover sweater with thigh high heels. Oversized pullovers are considered lounge wear, for relaxing at home in.  High heels are associated with high style. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99997306"], [" is", "1.0"], [" the", "1.0"], [" most", "0.32711956"], [" popular", "0.9678297"], [" type", "0.8895274"], [" of", "1.0"], [" car", "0.40332103"], [" in", "0.9999875"], [" the", "1.0"], [" United", "0.99998057"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" most", "1.0"], [" popular", "1.0"], [" type", "1.0"], [" of", "1.0"], [" car", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "1.0"], [" a", "1.0"], [" car", "0.9859606"], [".", "0.99072355"], [" ", "1.0"], ["\n", "0.9952551"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" most", "1.0"], [" popular", "1.0"], [" type", "1.0"], [" of", "1.0"], [" car", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "0.99999905"], ["?", "1.0"]]}, "964": {"qid": "2eba35467e49d7e9041d", "term": "Richard III of England", "description": "15th-century King of England", "question": "Did Richard III's father have greater longevity than him?", "answer": true, "facts": ["Richard III lived until the age of 32.", "Richard, Duke of York, Richard III's father, lived until the age of 49."], "decomposition": ["How many years did Richard III live to be?", "Who was Richard III's father?", "How old did #2 live to be?", "Is #3 greater than #1?"], "evidence": [[[["Richard III of England-1"]], [["Richard of York, 3rd Duke of York-1", "Richard of York, 3rd Duke of York-2"]], [["Richard of York, 3rd Duke of York-1"]], ["operation"]], [[["Richard III of England-1"]], [["Richard III of England-5"]], [["Richard of York, 3rd Duke of York-1"]], ["operation"]], [[["Richard III (disambiguation)-1"]], [["Richard III of England-5"]], [["Richard of York, 3rd Duke of York-3", "Sandal Castle-9"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Roman Empire have a greater population than the United States?\nFacts: The Roman Empire had a population of about 1.5 billion people. The United States has a population of about 320 million people.\nAnswer (True or False): True\nQuestion: Did the Roman Empire have a greater population than the United States?\nFacts: The Roman Empire had a population of", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Richard III's father have greater longevity than him?\nFacts: Richard III lived until the age of 32. Richard, Duke of York, Richard III's father, lived until the age of 49.\nAnswer (True or False):", "ct_subject": "Richard III lived until the age of 32. Richard, Duke of York, Richard III's father, lived until the age of 49.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999999"], [" the", "1.0"], [" Roman", "0.990457"], [" Empire", "1.0"], [" have", "1.0"], [" a", "0.999998"], [" greater", "0.015362036"], [" population", "0.9989736"], [" than", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [" had", "1.0"], [" a", "1.0"], [" population", "1.0"], [" of", "1.0"], [" about", "0.9895159"], [" 1", "0.12852217"], [".", "1.0"], ["5", "0.9999685"], [" billion", "1.0"], [" people", "1.0"], [".", "1.0"], [" The", "1.0"], [" United", "1.0"], [" States", "1.0"], [" has", "0.9984451"], [" a", "1.0"], [" population", "1.0"], [" of", "1.0"], [" about", "1.0"], [" 320", "0.9999815"], [" million", "1.0"], [" people", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [" have", "1.0"], [" a", "1.0"], [" greater", "1.0"], [" population", "1.0"], [" than", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [" had", "1.0"], [" a", "1.0"], [" population", "1.0"], [" of", "1.0"]]}, "965": {"qid": "5858ad50cf60b252c7e7", "term": "Frost", "description": "coating or deposit of ice that may form in humid air in cold conditions, usually overnight", "question": "Is it common to see frost during some college commencements?", "answer": true, "facts": ["College commencement ceremonies often happen during the months of December, May, and sometimes June. ", "Frost isn't uncommon to see during the month of December, as it is the winter."], "decomposition": ["What seasons can you expect see frost?", "What months do college commencements occur?", "Do any of #2 occur during #1?"], "evidence": [[[["Frost-1"]], [["Graduation-11"], "no_evidence"], ["operation"]], [[["Frost-30"], "no_evidence"], [["Commencement speech-2"], "no_evidence"], ["no_evidence", "operation"]], [[["Frost-25"], "no_evidence"], [["Commencement at Central Connecticut State University-18"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human?\nFacts: The average weight of a human is about 100 pounds.\nAnswer (True or False): True\nQuestion: What is the average height of a human?\nFacts: The average height of a human is about 5 feet, 8 inches.\nAnswer (True or False", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is it common to see frost during some college commencements?\nFacts: College commencement ceremonies often happen during the months of December, May, and sometimes June.  Frost isn't uncommon to see during the month of December, as it is the winter.\nAnswer (True or False):", "ct_subject": "College commencement ceremonies often happen during the months of December, May, and sometimes June.  Frost isn't uncommon to see during the month of December, as it is the winter.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.20740278"], [" is", "1.0"], [" the", "1.0"], [" average", "0.99069804"], [" weight", "0.85271674"], [" of", "1.0"], [" a", "0.9999956"], [" human", "0.9999926"], ["?", "0.9297901"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" weight", "0.99947256"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" is", "1.0"], [" about", "0.9997217"], [" 100", "0.67001146"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" height", "0.9999883"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" height", "1.0"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 5", "0.99984527"], [" feet", "0.9999851"], [",", "0.9996101"], [" 8", "0.98339164"], [" inches", "1.0"], [".", "0.9999887"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"]]}, "966": {"qid": "ab1f7f17560c5b0bc203", "term": "Eddie Murphy", "description": "American stand-up comedian and actor", "question": "Could Eddie Murphy's children hypothetically fill a basketball court by themselves?", "answer": true, "facts": ["Eddie Murphy has ten children.", "Basketball is played with two teams, each having five players on the court at one time."], "decomposition": ["How many children does Eddie Murphy have?", "How many players are on a basketball team?", "How many teams are on the basketball court at the same time?", "How much is #2 multiplied by #3?", "Is #1 greater than or equal to #4?"], "evidence": [[[["Eddie Murphy-40", "Eddie Murphy-41", "Eddie Murphy-43"]], [["Basketball-1"]], [["Basketball-1"]], ["operation"], ["operation"]], [[["Eddie Murphy-40"], "no_evidence"], [["Basketball-3"]], [["Basketball-1"]], ["operation"], ["operation"]], [[["Eddie Murphy-41"], "no_evidence"], [["Basketball-3"]], [["Basketball-1"]], ["operation"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average height of a man in the United States?\nFacts: The average height of a man in the United States is 5'10\".\nAnswer (True or False): True\nQuestion: What is the average weight of a man in the United States?\nFacts: The average weight of a man in the United States is about 180 pounds.\nAnswer (", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could Eddie Murphy's children hypothetically fill a basketball court by themselves?\nFacts: Eddie Murphy has ten children. Basketball is played with two teams, each having five players on the court at one time.\nAnswer (True or False):", "ct_subject": "Eddie Murphy has ten children. Basketball is played with two teams, each having five players on the court at one time.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.6739253"], [" is", "1.0"], [" the", "1.0"], [" average", "0.9969162"], [" height", "0.95365363"], [" of", "1.0"], [" a", "1.0"], [" man", "0.99999726"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "0.9999939"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" height", "1.0"], [" of", "1.0"], [" a", "1.0"], [" man", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "1.0"], [" 5", "0.9999875"], ["'", "1.0"], ["10", "0.8340348"], ["\".", "0.9999989"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "0.9999889"], [" of", "1.0"], [" a", "1.0"], [" man", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" weight", "1.0"], [" of", "1.0"], [" a", "1.0"], [" man", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" is", "1.0"], [" about", "0.99998534"], [" 180", "0.9999944"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"]]}, "967": {"qid": "0cc643196b24a6d37b46", "term": "United Nations Conference on Trade and Development", "description": "organization", "question": "Could Edward Snowden have visited the headquarters of United Nations Conference on Trade and Development?", "answer": true, "facts": ["The headquarters of the United Nations Conference on Trade and Development is in Geneva, Switzerland.", "Edward Snowden was stationed in Geneva in 2007 with the task of representing the US at the UN."], "decomposition": ["What city and country is the United Nations Conference on Trade and Development located in?", "In 2007, what was Edward Snowden's tasked with?", "Was Edward Snowden stationed in #1 in 2007 to accomplish #2? "], "evidence": [[[["United Nations Conference on Trade and Development-7"]], [["Edward Snowden-13"]], ["operation"]], [[["Palace of Nations-1"]], [["Edward Snowden-13"]], ["operation"]], [[["United Nations Conference on Trade and Development-3"]], [["Edward Snowden-13"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the capital of the state of Washington?\nFacts: The city of Washington is the capital of the state of Washington.\nAnswer (True or False): True\nQuestion: What is the name of the city in the United States that is the capital", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could Edward Snowden have visited the headquarters of United Nations Conference on Trade and Development?\nFacts: The headquarters of the United Nations Conference on Trade and Development is in Geneva, Switzerland. Edward Snowden was stationed in Geneva in 2007 with the task of representing the US at the UN.\nAnswer (True or False):", "ct_subject": "The headquarters of the United Nations Conference on Trade and Development is in Geneva, Switzerland. Edward Snowden was stationed in Geneva in 2007 with the task of representing the US at the UN.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.8418843"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9262498"], [" in", "0.9989711"], [" the", "0.9999987"], [" United", "1.0"], [" States", "1.0"], [" that", "0.9994861"], [" is", "0.9991177"], [" the", "1.0"], [" capital", "0.99999976"], [" of", "1.0"], [" the", "1.0"], [" state", "0.9999995"], [" of", "1.0"], [" Washington", "0.9917725"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9997359"], [" city", "0.88791525"], [" of", "0.95280385"], [" Washington", "1.0"], [" is", "1.0"], [" the", "0.98909783"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" Washington", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"]]}, "968": {"qid": "4db6d375c79e86c2818e", "term": "Sony", "description": "Japanese multinational conglomerate corporation", "question": "Did Sony definitively win the video game war against Sega?", "answer": true, "facts": ["Sony is the maker of the Playstation which has sold over 108 million PS4 units by March 2020.", "Sega's last console, the Sega Dreamcast, was discontinued in 2001.", "Sony Playstation competed with Sega's Dreamcast and Saturn systems in the 1990s.", "Sega now makes games for its former competitor, Sony, including Team Sonic Racing in 2019.", "At the height of the console wars, Sega Saturn sold 9.5 million units while Sony Playstation sold 102 million units."], "decomposition": ["How many console did Sega Saturn sell?", "How many console did Sony Playstation?", "Is #2 greater than #1?"], "evidence": [[[["Sega Saturn-25"], "no_evidence"], [["PlayStation-2"]], ["operation"]], [[["Sega Saturn-3"]], [["PlayStation-81"]], [["PlayStation-81"]]], [[["Sega Saturn-3"]], [["PlayStation (console)-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to ever be born", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Sony definitively win the video game war against Sega?\nFacts: Sony is the maker of the Playstation which has sold over 108 million PS4 units by March 2020. Sega's last console, the Sega Dreamcast, was discontinued in 2001. Sony Playstation competed with Sega's Dreamcast and Saturn systems in the 1990s. Sega now makes games for its former competitor, Sony, including Team Sonic Racing in 2019. At the height of the console wars, Sega Saturn sold 9.5 million units while Sony Playstation sold 102 million units.\nAnswer (True or False):", "ct_subject": "Sony is the maker of the Playstation which has sold over 108 million PS4 units by March 2020. Sega's last console, the Sega Dreamcast, was discontinued in 2001. Sony Playstation competed with Sega's Dreamcast and Saturn systems in the 1990s. Sega now makes games for its former competitor, Sony, including Team Sonic Racing in 2019. At the height of the console wars, Sega Saturn sold 9.5 million units while Sony Playstation sold 102 million units.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9985708"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999927"], [" of", "1.0"], [" the", "1.0"], [" first", "0.99977726"], [" person", "0.9973477"], [" to", "1.0"], [" ever", "0.7266277"], [" be", "0.9999608"], [" born", "0.9999987"]]}, "969": {"qid": "75bcf0203ee31aeeddd5", "term": "United States Capitol", "description": "seat of the United States Congress", "question": "Is the United States Capitol located near the White House?", "answer": true, "facts": ["The Capitol building is located at one end of the National Mall in downtown Washington DC.", "The White House is located next to the Washington Monument a short way down from the Mall."], "decomposition": ["What city is the United States Capitol located in?", "What city is the White House located in?", "Is #1 the same as #2?"], "evidence": [[[["United States Capitol-1"]], [["White House-1"]], ["operation"]], [[["United States-1"]], [["White House-1"]], ["operation"]], [[["United States Capitol-1"]], [["White House-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the United States Constitution written in English?\nFacts: The Constitution is written in the English language.\nAnswer (True or False): True\nQuestion: Is the United States Constitution written in the English language?\nFacts: The Constitution is written in the English language.\nAnswer (True or False): True\nQuestion: Is the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is the United States Capitol located near the White House?\nFacts: The Capitol building is located at one end of the National Mall in downtown Washington DC. The White House is located next to the Washington Monument a short way down from the Mall.\nAnswer (True or False):", "ct_subject": "The Capitol building is located at one end of the National Mall in downtown Washington DC. The White House is located next to the Washington Monument a short way down from the Mall.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.990267"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" Constitution", "0.9999441"], [" written", "1.0"], [" in", "1.0"], [" English", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Constitution", "0.9999989"], [" is", "0.9999994"], [" written", "1.0"], [" in", "1.0"], [" the", "0.6362929"], [" English", "0.998635"], [" language", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" Constitution", "1.0"], [" written", "1.0"], [" in", "1.0"], [" the", "0.99995625"], [" English", "1.0"], [" language", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Constitution", "1.0"], [" is", "1.0"], [" written", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"]]}, "970": {"qid": "0688bd3291c81ffcfea1", "term": "Radioactive waste", "description": "wastes that contain nuclear material", "question": "Does the United States Navy create radioactive waste?", "answer": true, "facts": ["Radioactive waste is created by nuclear material processing", "The United States Navy uses many nuclear submarines"], "decomposition": ["Radioactive waste is a byproduct of what process?", "Does the US Navy engage in any of the activities in #1?"], "evidence": [[[["Radioactive waste-1"]], [["Nuclear submarine-4"]]], [[["Radioactive waste-1"]], [["United States Navy Nuclear Propulsion-1"], "operation"]], [[["Radioactive waste-1"]], [["United States Navy-5"], "no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city of New York?\nFacts: New York City is the largest city in the United States.\nAnswer (True or False): True\nQuestion: What is the name of the city of Chicago?\nFacts: Chicago is the largest city in the United States.\nAnswer (True or False): True\nQuestion: What is the name of the city of Los Angeles?\nFacts: Los Angeles", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does the United States Navy create radioactive waste?\nFacts: Radioactive waste is created by nuclear material processing The United States Navy uses many nuclear submarines\nAnswer (True or False):", "ct_subject": "Radioactive waste is created by nuclear material processing The United States Navy uses many nuclear submarines", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999485"], [" is", "1.0"], [" the", "1.0"], [" name", "0.98783433"], [" of", "1.0"], [" the", "1.0"], [" city", "0.6861749"], [" of", "0.24416019"], [" New", "0.2806027"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" New", "1.0"], [" York", "1.0"], [" City", "0.99999964"], [" is", "0.3329472"], [" the", "1.0"], [" largest", "0.9259966"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [".", "0.9486033"], ["\n", "0.99999976"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "0.9999999"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Chicago", "0.9179129"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Chicago", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "0.9051992"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Los", "0.9999963"], [" Angeles", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Los", "1.0"], [" Angeles", "1.0"]]}, "971": {"qid": "1f187be8cef09e713156", "term": "Black fly", "description": "family of insects", "question": "Was Black fly upstaged by another insect in Jeff Goldblum's 1986 film?", "answer": true, "facts": ["Jeff Goldnlum starred in the 1986 movie The Fly.", "The fly used in the movie The Fly was a common Housefly.", "The Black fly is most closely related to Chironomidae since they both feed on mammals."], "decomposition": ["Which fly was used in the 1986 movie The Fly?", "is #1 a black fly?"], "evidence": [[["no_evidence"], ["operation"]], [[["The Fly (1986 film)-4"]], [["Black fly-1", "Housefly-1"]]], [[["The Fly (1986 film)-4"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first man to be born in the year 2000?\nFacts: The first man to be born in the year 2000 was born in the year 2000.\nAnswer (True or False): True\nQuestion: What is the name of", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Black fly upstaged by another insect in Jeff Goldblum's 1986 film?\nFacts: Jeff Goldnlum starred in the 1986 movie The Fly. The fly used in the movie The Fly was a common Housefly. The Black fly is most closely related to Chironomidae since they both feed on mammals.\nAnswer (True or False):", "ct_subject": "Jeff Goldnlum starred in the 1986 movie The Fly. The fly used in the movie The Fly was a common Housefly. The Black fly is most closely related to Chironomidae since they both feed on mammals.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999604"], [" is", "1.0"], [" the", "1.0"], [" name", "0.999995"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9901316"], [" man", "0.95585084"], [" to", "0.9999325"], [" be", "0.999566"], [" born", "0.99990547"], [" in", "0.9999999"], [" the", "0.99544936"], [" year", "0.9991861"], [" 2000", "0.9999995"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "1.0"], [" born", "1.0"], [" in", "0.9938368"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"]]}, "972": {"qid": "dab785cc72fbb866c986", "term": "Alan Rickman", "description": "British actor", "question": "Did Alan Rickman have an improperly functioning organ?", "answer": true, "facts": ["Alan Rickman died of pancreatic cancer on 14 January 2016 at age 69.", "Pancreatic cancer arises when cells in the pancreas, a glandular organ behind the stomach, begin to multiply out of control and form a mass."], "decomposition": ["What medical conditions did Alan Rickman have?", "Does any of the conditions in #1 involve an organ?"], "evidence": [[[["Alan Rickman-25"]], [["Pancreatic cancer-1"]]], [[["Alan Rickman-25"]], [["Pancreatic cancer-1"]]], [[["Alan Rickman-25"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Titanic sink because of a faulty steering wheel?\nFacts: The Titanic sank because of a faulty steering wheel. The steering wheel was faulty because the ship was traveling at a speed of more than 30 knots (about 50 mph).\nAnswer (True or False): True\nQuestion", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Alan Rickman have an improperly functioning organ?\nFacts: Alan Rickman died of pancreatic cancer on 14 January 2016 at age 69. Pancreatic cancer arises when cells in the pancreas, a glandular organ behind the stomach, begin to multiply out of control and form a mass.\nAnswer (True or False):", "ct_subject": "Alan Rickman died of pancreatic cancer on 14 January 2016 at age 69. Pancreatic cancer arises when cells in the pancreas, a glandular organ behind the stomach, begin to multiply out of control and form a mass.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99782085"], [" the", "1.0"], [" Titanic", "0.99991333"], [" sink", "1.0"], [" because", "1.0"], [" of", "1.0"], [" a", "0.999938"], [" faulty", "0.9999826"], [" steering", "0.11486293"], [" wheel", "0.9987602"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Titanic", "1.0"], [" sank", "0.99480385"], [" because", "1.0"], [" of", "0.99999785"], [" a", "1.0"], [" faulty", "1.0"], [" steering", "1.0"], [" wheel", "1.0"], [".", "1.0"], [" The", "0.9875665"], [" steering", "0.9582518"], [" wheel", "1.0"], [" was", "1.0"], [" faulty", "0.9976186"], [" because", "1.0"], [" the", "0.9962022"], [" ship", "0.99995804"], [" was", "0.9999943"], [" traveling", "0.9837841"], [" at", "0.6973256"], [" a", "0.99999535"], [" speed", "1.0"], [" of", "1.0"], [" more", "0.99701333"], [" than", "1.0"], [" 30", "0.9997309"], [" knots", "1.0"], [" (", "0.98869705"], ["about", "0.9999989"], [" 50", "0.2296774"], [" mph", "0.99971765"], [").", "0.9999603"], ["\n", "0.9777696"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"]]}, "973": {"qid": "34ef40fff88e669f154f", "term": "4", "description": "Natural number", "question": "Would four shoes be insufficient for a set of octuplets?", "answer": true, "facts": ["There are eight children in a set of octuplets.", "The typical child is born with two feet.", "Normally each foot needs to be covered with a single shoe."], "decomposition": ["How many children are in a set of octuplets?", "How many shoes does a person wear?", "What is #1 multiplied by #2?", "Is #3 greater than 4?"], "evidence": [[[["Multiple birth-6"]], [["Shoe-1"]], ["operation"], ["operation"]], [[["Multiple birth-19"]], [["Shoe-1"]], ["operation"], ["operation"]], [[["Suleman octuplets-1"]], [["Shoe-1"]], ["operation"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human being?\nFacts: The average human being weighs about 100 pounds.\nAnswer (True or False): True\nQuestion: What is the average height of a human being?\nFacts: The average human being is about 5 feet, 8 inches tall.\nAnswer (True or False): True\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would four shoes be insufficient for a set of octuplets?\nFacts: There are eight children in a set of octuplets. The typical child is born with two feet. Normally each foot needs to be covered with a single shoe.\nAnswer (True or False):", "ct_subject": "There are eight children in a set of octuplets. The typical child is born with two feet. Normally each foot needs to be covered with a single shoe.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9998258"], [" is", "1.0"], [" the", "1.0"], [" average", "0.43023846"], [" weight", "0.9983577"], [" of", "1.0"], [" a", "1.0"], [" human", "0.99999845"], [" being", "0.68717915"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.99755365"], [" being", "0.99999714"], [" weighs", "0.9999982"], [" about", "0.9999994"], [" 100", "0.99842995"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" height", "0.992469"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.9999671"], [" being", "1.0"], [" is", "1.0"], [" about", "0.96024454"], [" 5", "0.9999995"], [" feet", "1.0"], [",", "0.92770773"], [" 8", "0.9992042"], [" inches", "1.0"], [" tall", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"]]}, "974": {"qid": "ba1242c0a924787b6e03", "term": "Logging", "description": "the cutting, skidding, on-site processing, and loading of trees or logs onto transport vehicles", "question": "Would it be hard to get toilet paper if there were no loggers?", "answer": true, "facts": ["Logging produces products such as pulp.", "Pulp is used to make paper products such as toilet paper."], "decomposition": ["What material is used to make paper products such as toilet paper?", "Is logging an important step in  producing #1?"], "evidence": [[[["Pulp (paper)-1"]], [["Logging-1"]]], [[["Paper-1"]], [["Logging-1"]]], [[["Toilet paper-37"]], [["Logging-11"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a man and a woman?\nFacts: A man is a male human being. A woman is a female human being.\nAnswer (True or False): True\nQuestion: What is the difference between a man and a woman?\nFacts: A man is a male human being. A woman is a female human being.\nAnswer (True or False):", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would it be hard to get toilet paper if there were no loggers?\nFacts: Logging produces products such as pulp. Pulp is used to make paper products such as toilet paper.\nAnswer (True or False):", "ct_subject": "Logging produces products such as pulp. Pulp is used to make paper products such as toilet paper.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99950516"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9990609"], [" between", "1.0"], [" a", "1.0"], [" man", "0.766862"], [" and", "1.0"], [" a", "1.0"], [" woman", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" man", "1.0"], [" is", "1.0"], [" a", "1.0"], [" male", "0.99384874"], [" human", "0.9999951"], [" being", "1.0"], [".", "1.0"], [" A", "1.0"], [" woman", "1.0"], [" is", "1.0"], [" a", "1.0"], [" female", "1.0"], [" human", "1.0"], [" being", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" man", "1.0"], [" and", "1.0"], [" a", "1.0"], [" woman", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" man", "1.0"], [" is", "1.0"], [" a", "1.0"], [" male", "1.0"], [" human", "1.0"], [" being", "1.0"], [".", "1.0"], [" A", "1.0"], [" woman", "1.0"], [" is", "1.0"], [" a", "1.0"], [" female", "1.0"], [" human", "1.0"], [" being", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"]]}, "975": {"qid": "ddfbe25915ba85ef4716", "term": "Bohai Sea", "description": "The innermost gulf of the Yellow Sea and Korea Bay on the coast of Northeastern and North China", "question": "Could Rhode Island sink into the Bohai Sea?", "answer": true, "facts": ["The Bohai Sea is 30,000 square miles", "Rhode Island is 1,214 square miles"], "decomposition": ["How many square miles is the Bohai Sea?", "How many square miles is Rhode Island?", "Is #1 greater than or equal to #2?"], "evidence": [[[["Bohai Sea-1"]], [["Rhode Island-29"]], [["Rhode Island-29"], "operation"]], [[["Bohai Sea-1"]], [["Rhode Island-29"]], ["operation"]], [[["Bohai Sea-1"]], [["Rhode Island-29"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the largest number of people that can fit on a single football field?\nFacts: The largest number of people that can fit on a single football field is 2,000.\nAnswer (True or False): True\nQuestion: What is the largest number of people that can fit on a single football field?\nFacts: The largest number of people that can fit on a single football field is 2,000.\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could Rhode Island sink into the Bohai Sea?\nFacts: The Bohai Sea is 30,000 square miles Rhode Island is 1,214 square miles\nAnswer (True or False):", "ct_subject": "The Bohai Sea is 30,000 square miles Rhode Island is 1,214 square miles", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999993"], [" is", "1.0"], [" the", "1.0"], [" largest", "0.99564266"], [" number", "0.61655325"], [" of", "1.0"], [" people", "1.0"], [" that", "0.9995097"], [" can", "0.985322"], [" fit", "1.0"], [" on", "0.9972683"], [" a", "0.9999999"], [" single", "1.0"], [" football", "0.9999993"], [" field", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9992593"], [" largest", "1.0"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" that", "1.0"], [" can", "1.0"], [" fit", "1.0"], [" on", "1.0"], [" a", "1.0"], [" single", "0.9944693"], [" football", "1.0"], [" field", "1.0"], [" is", "1.0"], [" 2", "0.9975631"], [",", "1.0"], ["000", "0.9987237"], [".", "0.999995"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" that", "1.0"], [" can", "1.0"], [" fit", "1.0"], [" on", "1.0"], [" a", "1.0"], [" single", "1.0"], [" football", "0.99999976"], [" field", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" largest", "1.0"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" that", "1.0"], [" can", "1.0"], [" fit", "1.0"], [" on", "1.0"], [" a", "1.0"], [" single", "1.0"], [" football", "1.0"], [" field", "1.0"], [" is", "1.0"], [" 2", "1.0"], [",", "1.0"], ["000", "1.0"], [".", "1.0"], ["\n", "1.0"]]}, "976": {"qid": "0aad1627ee2984c8c147", "term": "1980 United States presidential election", "description": "49th quadrennial presidential election in the United States", "question": "Was the 1980 presidential election won by a member of the Grand Old Party?", "answer": true, "facts": ["The Republican party is nicknamed the Grand Old Party.", "The 1980 election was won by Ronald Reagan.", "Reagan was a Republican."], "decomposition": ["Which political party is also known as the Grand Old Party?", "Who won the 1980 presidential election?", "What political party did #2 belong to?", "Is #3 the same as #1?"], "evidence": [[[["Republican Party (United States)-1"]], [["1980 United States presidential election-1"]], [["Ronald Reagan-3"]], ["operation"]], [[["Republican Party (United States)-1"]], [["1980 United States presidential election-1"]], [["Ronald Reagan-3"]], ["operation"]], [[["Republican Party (United States)-1"]], [["1980 United States presidential election-1"]], [["Ronald Reagan-3"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the country that is the largest in the world?\nFacts: The United States of America.\nAnswer (True or False): True\nQuestion: What is the name of the country that is the largest in the world?\nFacts: The United States of America.\nAnswer (True or False): True\nQuestion: What is the name of", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was the 1980 presidential election won by a member of the Grand Old Party?\nFacts: The Republican party is nicknamed the Grand Old Party. The 1980 election was won by Ronald Reagan. Reagan was a Republican.\nAnswer (True or False):", "ct_subject": "The Republican party is nicknamed the Grand Old Party. The 1980 election was won by Ronald Reagan. Reagan was a Republican.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999213"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" country", "0.9641414"], [" that", "0.9932974"], [" is", "0.9997191"], [" the", "1.0"], [" largest", "0.9994543"], [" in", "0.9991732"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "1.0"], [" States", "1.0"], [" of", "0.99710554"], [" America", "1.0"], [".", "0.99999857"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" country", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "1.0"], [" States", "1.0"], [" of", "1.0"], [" America", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"]]}, "977": {"qid": "f8de728cbf65a3f0f1d6", "term": "Kaffir lime", "description": "A citrus fruit native to tropical Southeast Asia and southern China", "question": "Would a kaffir lime be a good ingredient for making a candle?", "answer": true, "facts": ["Kaffir limes are citrus fruits originating in tropical climates.", "The Kaffir lime leaves and rind emit an intense citrus fragrance when crushed up.", "Yankee Candle, one of the largest candle companies, sells several popular varieties of citrus candles.", "Sage and Citrus is one of the highest rated scents that Yankee Candle sells."], "decomposition": ["Which fragrance do Kaffir lime leaves emit when crushed?", "What are the scents of some popular varieties of candles that Yankee Candle sells?", "Is #2 included in #1?"], "evidence": [[[["Kaffir lime-2"]], [["Yankee Candle-14"], "no_evidence"], ["operation"]], [[["Kaffir lime-8"], "no_evidence"], [["Yankee Candle-14"], "no_evidence"], ["no_evidence", "operation"]], [[["Kaffir lime-2"]], [["Yankee Candle-14"], "no_evidence"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a kaffir lime and a kaffir lime tree?\nFacts: The kaffir lime tree is a tropical tree that grows in", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a kaffir lime be a good ingredient for making a candle?\nFacts: Kaffir limes are citrus fruits originating in tropical climates. The Kaffir lime leaves and rind emit an intense citrus fragrance when crushed up. Yankee Candle, one of the largest candle companies, sells several popular varieties of citrus candles. Sage and Citrus is one of the highest rated scents that Yankee Candle sells.\nAnswer (True or False):", "ct_subject": "Kaffir limes are citrus fruits originating in tropical climates. The Kaffir lime leaves and rind emit an intense citrus fragrance when crushed up. Yankee Candle, one of the largest candle companies, sells several popular varieties of citrus candles. Sage and Citrus is one of the highest rated scents that Yankee Candle sells.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999976"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9999305"], [" between", "1.0"], [" a", "1.0"], [" k", "1.0"], ["aff", "1.0"], ["ir", "1.0"], [" lime", "1.0"], [" and", "1.0"], [" a", "1.0"], [" k", "1.0"], ["aff", "1.0"], ["ir", "1.0"], [" lime", "0.9998976"], [" tree", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9873066"], [" k", "1.0"], ["aff", "1.0"], ["ir", "1.0"], [" lime", "1.0"], [" tree", "0.99999857"], [" is", "1.0"], [" a", "1.0"], [" tropical", "1.0"], [" tree", "1.0"], [" that", "0.99763155"], [" grows", "0.99996805"], [" in", "1.0"]]}, "978": {"qid": "08bd7c4d339a2838f173", "term": "Pulitzer Prize", "description": "U.S. award for achievements in newspaper and online journalism, literature, and musical composition", "question": "Is it impossible for Cheb Mami to win a Pulitzer Prize for musical composition?", "answer": true, "facts": ["The history Pulitzer Prize can be won by any citizen, all other Pulitzer Prize winners must be a US Citizen.", "Cheb Mami is an Algerian singer.", "Cheb Mami is a citizen of Algeria."], "decomposition": ["The Pulitzer Prize for musical composition is exclusive to the citizens of which country?", "Which country is Cheb Mami from?", "Is #1 different from #2?"], "evidence": [[[["Pulitzer Prize for Music-1"]], [["Cheb Mami-1"]], ["operation"]], [[["Pulitzer Prize for Music-1"]], [["Cheb Mami-2"]], ["operation"]], [[["Pulitzer Prize for Music-2"]], [["Cheb Mami-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it possible for a man to be a homosexual?\nFacts: Homosexuality is not a crime in the United States. Homosexuality is not a crime in any other country in the world.\nAnswer (True or False): True\nQuestion: Is it possible for", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is it impossible for Cheb Mami to win a Pulitzer Prize for musical composition?\nFacts: The history Pulitzer Prize can be won by any citizen, all other Pulitzer Prize winners must be a US Citizen. Cheb Mami is an Algerian singer. Cheb Mami is a citizen of Algeria.\nAnswer (True or False):", "ct_subject": "The history Pulitzer Prize can be won by any citizen, all other Pulitzer Prize winners must be a US Citizen. Cheb Mami is an Algerian singer. Cheb Mami is a citizen of Algeria.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99999356"], [" it", "1.0"], [" possible", "0.99999976"], [" for", "0.9998417"], [" a", "1.0"], [" man", "0.9942812"], [" to", "1.0"], [" be", "0.9687187"], [" a", "0.9773917"], [" homosexual", "0.7441526"], ["?", "0.99972624"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Hom", "0.9805588"], ["osexual", "1.0"], ["ity", "1.0"], [" is", "1.0"], [" not", "0.8360972"], [" a", "1.0"], [" crime", "0.98548955"], [" in", "1.0"], [" the", "0.9999914"], [" United", "1.0"], [" States", "1.0"], [".", "1.0"], [" Hom", "0.97613984"], ["osexual", "1.0"], ["ity", "1.0"], [" is", "1.0"], [" not", "0.99030066"], [" a", "1.0"], [" crime", "0.9847436"], [" in", "1.0"], [" any", "0.9004543"], [" other", "0.9999999"], [" country", "1.0"], [" in", "0.69105446"], [" the", "1.0"], [" world", "1.0"], [".", "1.0"], ["\n", "0.9999881"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" possible", "1.0"], [" for", "1.0"]]}, "979": {"qid": "11e20bbf1f44625b8349", "term": "Family Guy", "description": "American animated sitcom", "question": "Does the art from Family Guy look a lot like the art in American Dad?", "answer": true, "facts": ["Family Guy and American Dad are both Fox Animated Sitcoms animated by Seth MacFarlane.", "Family Guy and American Dad characters all share common facial features and movement styles."], "decomposition": ["Who is the animator for Family Guy?", "Who is the animator for American Dad?", "Is #1 the same as #2?"], "evidence": [[[["Seth MacFarlane-14"]], [["Seth MacFarlane-21"]], ["operation"]], [[["Family Guy-1"]], [["American Dad!-1"]], ["operation"]], [[["Family Guy-2"]], [["American Dad!-14"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the character in the movie The Last Airbender?\nFacts: The character in The Last Airbender is named Aang.\nAnswer (True or False): True\nQuestion: What is the name of the character in the movie The Last Airbender?\nFacts: The character in The Last", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does the art from Family Guy look a lot like the art in American Dad?\nFacts: Family Guy and American Dad are both Fox Animated Sitcoms animated by Seth MacFarlane. Family Guy and American Dad characters all share common facial features and movement styles.\nAnswer (True or False):", "ct_subject": "Family Guy and American Dad are both Fox Animated Sitcoms animated by Seth MacFarlane. Family Guy and American Dad characters all share common facial features and movement styles.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9963174"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" character", "0.9999281"], [" in", "0.9999999"], [" the", "1.0"], [" movie", "1.0"], [" The", "0.9220004"], [" Last", "0.99275935"], [" Air", "0.99691296"], ["bender", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" character", "0.99540496"], [" in", "1.0"], [" The", "0.8013535"], [" Last", "1.0"], [" Air", "1.0"], ["bender", "1.0"], [" is", "1.0"], [" named", "1.0"], [" A", "1.0"], ["ang", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" character", "1.0"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" The", "1.0"], [" Last", "0.99998796"], [" Air", "1.0"], ["bender", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" character", "1.0"], [" in", "1.0"], [" The", "1.0"], [" Last", "1.0"]]}, "980": {"qid": "faa475c30066e8b522f1", "term": "Alexander Graham Bell", "description": "scientist and inventor known for his work on the telephone", "question": "Would Alexander Graham Bell hypothetically support Nazi eugenics?", "answer": true, "facts": ["Eugenics was the idea of selective breeding or sterilization to rid the human populace of certain traits.", "Nazis used eugenics to justify mass sterilization and mass murder.", "Alexander Graham Bell Alexander Graham Bell advocated against the use of sign language and hoped to eradicate deafness through selective breeding."], "decomposition": ["What did the Nazi's use to justify mass sterilization and mass murder?", "What is the definition of #1?", "What did Alexander Graham Bell advocate against the use of?", "Did Alexander Graham Bell use #2 to get rid of #3?"], "evidence": [[[["Nazi eugenics-3"]], [["Eugenics-1"]], [["History of eugenics-21"], "no_evidence"], ["no_evidence"]], [[["Eugenics-4"]], [["Eugenics-1"]], [["History of eugenics-21"]], ["operation"]], [[["Nazism and race-3", "Nazism-59", "Nazism-60"]], [["Racial hierarchy-11"]], [["History of eugenics-21"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"geneticist\" and a \"geneticist-on-the-make\"?\nFacts: A geneticist is a scientist who studies the genetic makeup of organisms. A geneticist-", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Alexander Graham Bell hypothetically support Nazi eugenics?\nFacts: Eugenics was the idea of selective breeding or sterilization to rid the human populace of certain traits. Nazis used eugenics to justify mass sterilization and mass murder. Alexander Graham Bell Alexander Graham Bell advocated against the use of sign language and hoped to eradicate deafness through selective breeding.\nAnswer (True or False):", "ct_subject": "Eugenics was the idea of selective breeding or sterilization to rid the human populace of certain traits. Nazis used eugenics to justify mass sterilization and mass murder. Alexander Graham Bell Alexander Graham Bell advocated against the use of sign language and hoped to eradicate deafness through selective breeding.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.8450566"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.99994934"], [" between", "1.0"], [" a", "0.97953254"], [" \"", "1.0"], ["gen", "0.9998324"], ["etic", "0.99997675"], ["ist", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["gen", "1.0"], ["etic", "1.0"], ["ist", "1.0"], ["-", "0.9622813"], ["on", "0.99628276"], ["-", "1.0"], ["the", "1.0"], ["-", "1.0"], ["make", "0.99999607"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9998122"], [" genetic", "1.0"], ["ist", "1.0"], [" is", "1.0"], [" a", "0.99999785"], [" scientist", "1.0"], [" who", "1.0"], [" studies", "1.0"], [" the", "1.0"], [" genetic", "0.99883634"], [" makeup", "0.9999472"], [" of", "1.0"], [" organisms", "0.76975745"], [".", "1.0"], [" A", "1.0"], [" genetic", "1.0"], ["ist", "1.0"], ["-", "1.0"]]}, "981": {"qid": "81cb69ab13c3195331c4", "term": "Black swan", "description": "species of bird", "question": "Can black swan's formation type help spell longest word in Dictionary?", "answer": true, "facts": ["Black swan's fly in a \"V\" formation.", "The longest word in the dictionary is pneumonoultramicroscopicsilicovolcanoconiosis."], "decomposition": ["What letter does the formation of black swans in flight resemble?", "What is the longest word in English language?", "Can #1 be found in #2?"], "evidence": [[[["Black swan-6"]], [["Longest word in English-4"]], ["operation"]], [[["Black swan-6"]], [["Pneumonoultramicroscopicsilicovolcanoconiosis-1"]], ["operation"]], [[["Swan-6"], "no_evidence"], [["Pneumonoultramicroscopicsilicovolcanoconiosis-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest word in the English language?\nFacts: The longest word in the English language is \"I love you\".\nAnswer (True or False): True\nQuestion: What is the longest word in the English language?\nFacts: The longest word in the English language is \"I love you\".\nAnswer (", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can black swan's formation type help spell longest word in Dictionary?\nFacts: Black swan's fly in a \"V\" formation. The longest word in the dictionary is pneumonoultramicroscopicsilicovolcanoconiosis.\nAnswer (True or False):", "ct_subject": "Black swan's fly in a \"V\" formation. The longest word in the dictionary is pneumonoultramicroscopicsilicovolcanoconiosis.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99642074"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "0.980095"], [" language", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], [" is", "1.0"], [" \"", "0.9999999"], ["I", "0.6868627"], [" love", "0.9390777"], [" you", "1.0"], ["\".", "0.03933077"], ["\n", "0.99999166"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" word", "1.0"], [" in", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], [" is", "1.0"], [" \"", "1.0"], ["I", "1.0"], [" love", "1.0"], [" you", "1.0"], ["\".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"]]}, "982": {"qid": "636b557a0eb102ce04b5", "term": "Glutamic acid", "description": "amino acid", "question": "Do you find glutamic acid in a severed finger?", "answer": true, "facts": ["Glutamic acid is an amino acid and neurotransmitter", "As a neurotransmitter, glutamic acid is the most abundant in the vertebrate nervous system", "A severed finger contains parts of a vertebrate's nervous system"], "decomposition": ["What kind of transmitter is glutamic acid?", "In which bodily system is #1 the most abundant?", "Does a severed finger contain #2?"], "evidence": [[[["Glutamic acid-1"]], [["Glutamic acid-1"]], ["no_evidence", "operation"]], [[["Glutamic acid-1"]], [["Glutamic acid-1"]], ["no_evidence", "operation"]], [[["Glutamic acid-1"]], [["Neurotransmitter-16"], "no_evidence"], [["Dendrite-3"], "no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Do you know what a \"sperm whale\" is?\nFacts: A sperm whale is a large, baleen whale that is the largest animal on earth. It is the largest animal that can be seen from the surface of the water. It is the largest animal that can be seen from the surface of", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do you find glutamic acid in a severed finger?\nFacts: Glutamic acid is an amino acid and neurotransmitter As a neurotransmitter, glutamic acid is the most abundant in the vertebrate nervous system A severed finger contains parts of a vertebrate's nervous system\nAnswer (True or False):", "ct_subject": "Glutamic acid is an amino acid and neurotransmitter As a neurotransmitter, glutamic acid is the most abundant in the vertebrate nervous system A severed finger contains parts of a vertebrate's nervous system", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Do", "0.39950308"], [" you", "1.0"], [" know", "0.49603528"], [" what", "0.9974101"], [" a", "0.99783933"], [" \"", "1.0"], ["s", "0.24731894"], ["perm", "1.0"], [" whale", "1.0"], ["\"", "1.0"], [" is", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" sperm", "1.0"], [" whale", "1.0"], [" is", "1.0"], [" a", "1.0"], [" large", "0.9999949"], [",", "1.0"], [" b", "0.99759835"], ["ale", "1.0"], ["en", "1.0"], [" whale", "1.0"], [" that", "1.0"], [" is", "0.99614245"], [" the", "0.9159227"], [" largest", "1.0"], [" animal", "0.99854076"], [" on", "0.67955935"], [" earth", "0.9998361"], [".", "1.0"], [" It", "0.999998"], [" is", "0.99999344"], [" the", "0.92312586"], [" largest", "1.0"], [" animal", "1.0"], [" that", "0.9999685"], [" can", "0.98391634"], [" be", "0.50975025"], [" seen", "0.92211497"], [" from", "0.93767434"], [" the", "1.0"], [" surface", "1.0"], [" of", "1.0"], [" the", "1.0"], [" water", "1.0"], [".", "1.0"], [" It", "0.99999225"], [" is", "0.9992538"], [" the", "0.79419637"], [" largest", "0.99999976"], [" animal", "1.0"], [" that", "1.0"], [" can", "1.0"], [" be", "1.0"], [" seen", "1.0"], [" from", "1.0"], [" the", "1.0"], [" surface", "1.0"], [" of", "1.0"]]}, "983": {"qid": "01c3faf4915a44133f60", "term": "Iggy Pop", "description": "American rock singer-songwriter, musician, and actor", "question": "Was Iggy Pop named after his father?", "answer": true, "facts": ["Iggy Pop's birth name was James Newell Osterberg Jr.", "The father of Iggy Pop was James Newell Osterberg Sr."], "decomposition": ["What is Iggy Pop's real name?", "What is Iggy Pop's father's name?", "Is #1 the same as #2?"], "evidence": [[[["Iggy Pop-1"]], [["Iggy Pop-5"]], ["operation"]], [[["Iggy Pop-1"]], [["Iggy Pop-5"]], ["operation"]], [[["Iggy Pop-1"]], [["Iggy Pop-5"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Beatles have a secret song called \"I Want to Hold Your Hand\"?\nFacts: The Beatles had a song called \"I Want to Hold Your Hand\" but it was not written by them.\nAnswer (True or False): True\nQuestion: Did the Beatles have a secret song called \"I Want to Hold Your Hand\"?\nFacts: The Beatles had a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Iggy Pop named after his father?\nFacts: Iggy Pop's birth name was James Newell Osterberg Jr. The father of Iggy Pop was James Newell Osterberg Sr.\nAnswer (True or False):", "ct_subject": "Iggy Pop's birth name was James Newell Osterberg Jr. The father of Iggy Pop was James Newell Osterberg Sr.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.6757033"], [" the", "1.0"], [" Beatles", "1.0"], [" have", "0.9904253"], [" a", "1.0"], [" secret", "0.9999987"], [" song", "1.0"], [" called", "0.9296809"], [" \"", "1.0"], ["I", "1.0"], [" Want", "0.9965307"], [" to", "1.0"], [" Hold", "1.0"], [" Your", "1.0"], [" Hand", "1.0"], ["\"?", "0.56342304"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Beatles", "1.0"], [" had", "0.27444136"], [" a", "1.0"], [" song", "0.9997528"], [" called", "1.0"], [" \"", "1.0"], ["I", "1.0"], [" Want", "1.0"], [" to", "1.0"], [" Hold", "1.0"], [" Your", "1.0"], [" Hand", "1.0"], ["\"", "1.0"], [" but", "0.9764866"], [" it", "1.0"], [" was", "1.0"], [" not", "0.99999905"], [" written", "0.9999013"], [" by", "1.0"], [" them", "0.9455978"], [".", "1.0"], ["\n", "0.99409086"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999237"], [" the", "1.0"], [" Beatles", "1.0"], [" have", "1.0"], [" a", "1.0"], [" secret", "1.0"], [" song", "1.0"], [" called", "1.0"], [" \"", "1.0"], ["I", "1.0"], [" Want", "1.0"], [" to", "1.0"], [" Hold", "1.0"], [" Your", "1.0"], [" Hand", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Beatles", "1.0"], [" had", "1.0"], [" a", "1.0"]]}, "984": {"qid": "3a373a74e76c72176e39", "term": "B\u00f6rek", "description": "Stuffed phyllo pastry", "question": "Would \u015eerafeddin Sabuncuo\u011flu have eaten B\u00f6rek?", "answer": true, "facts": ["B\u00f6rek originated in Ottoman cuisine", "\u015eerafeddin Sabuncuo\u011flu was an Ottoman scientist"], "decomposition": ["Where did Borek originate from?", "Was Serafeddin Sabuncuoglu from #1?"], "evidence": [[[["B\u00f6rek-1"]], [["Ottoman Empire-1", "Sabuncuo\u011flu \u015eerafeddin-1"], "operation"]], [[["B\u00f6rek-3"]], [["Sabuncuo\u011flu \u015eerafeddin-2"]]], [[["B\u00f6rek-1"]], [["Amasya-1", "Sabuncuo\u011flu \u015eerafeddin-2"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Ottoman Empire have a capital city?\nFacts: The Ottoman Empire had a capital city called Constantinople\nAnswer (True or False): True\nQuestion: Did the Ottoman Empire have a capital city?\nFacts: The Ottoman Empire had a capital city called Constantinople\nAnswer (True or False): True\nQuestion: Did the Ottoman Empire have a capital city?\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would \u015eerafeddin Sabuncuo\u011flu have eaten B\u00f6rek?\nFacts: B\u00f6rek originated in Ottoman cuisine \u015eerafeddin Sabuncuo\u011flu was an Ottoman scientist\nAnswer (True or False):", "ct_subject": "B\u00f6rek originated in Ottoman cuisine \u015eerafeddin Sabuncuo\u011flu was an Ottoman scientist", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.16354792"], [" the", "1.0"], [" Ottoman", "0.9992256"], [" Empire", "1.0"], [" have", "1.0"], [" a", "1.0"], [" capital", "0.9697889"], [" city", "0.99782"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Ottoman", "0.93433714"], [" Empire", "1.0"], [" had", "0.9038778"], [" a", "1.0"], [" capital", "1.0"], [" city", "1.0"], [" called", "0.9772872"], [" Constantinople", "0.5061528"], ["\n", "0.9999827"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99835855"], [" the", "1.0"], [" Ottoman", "1.0"], [" Empire", "1.0"], [" have", "1.0"], [" a", "1.0"], [" capital", "1.0"], [" city", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Ottoman", "1.0"], [" Empire", "1.0"], [" had", "1.0"], [" a", "1.0"], [" capital", "1.0"], [" city", "1.0"], [" called", "1.0"], [" Constantinople", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" Ottoman", "1.0"], [" Empire", "1.0"], [" have", "1.0"], [" a", "1.0"], [" capital", "1.0"], [" city", "1.0"], ["?", "1.0"], ["\n", "1.0"]]}, "985": {"qid": "6bed2a566da292a36593", "term": "Rice pudding", "description": "Dish made from rice mixed with water or milk", "question": "Would Cyndi Lauper use milk substitute in her rice pudding?", "answer": true, "facts": ["Cyndi Lauper wrote a song about lactose intolerance.", "Lactose intolerance leads to gastrointestinal discomfort upon eating dairy."], "decomposition": ["What conditions lead people to using milk substitutes?", "Does Cyndi Lauper suffer from any conditions listed in #1?"], "evidence": [[[["Lactose intolerance-1", "Veganism-1"]], [["Cyndi Lauper-1"], "no_evidence", "operation"]], [[["Milk allergy-9"]], [["Cyndi Lauper-76"], "no_evidence"]], [[["Milk substitute-17"]], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: The first person to be born in the United States was a woman named Sarah Hensley.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: The first person to be born in", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Cyndi Lauper use milk substitute in her rice pudding?\nFacts: Cyndi Lauper wrote a song about lactose intolerance. Lactose intolerance leads to gastrointestinal discomfort upon eating dairy.\nAnswer (True or False):", "ct_subject": "Cyndi Lauper wrote a song about lactose intolerance. Lactose intolerance leads to gastrointestinal discomfort upon eating dairy.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999999"], [" is", "1.0"], [" the", "1.0"], [" name", "0.80762696"], [" of", "1.0"], [" the", "1.0"], [" first", "0.999723"], [" person", "0.9982193"], [" to", "1.0"], [" be", "0.9896986"], [" born", "0.25506705"], [" in", "1.0"], [" the", "1.0"], [" United", "0.99560946"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.999705"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" a", "0.99996996"], [" woman", "1.0"], [" named", "1.0"], [" Sarah", "0.8247091"], [" H", "0.99724054"], ["ens", "1.0"], ["ley", "0.999985"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"]]}, "986": {"qid": "a905e30e6cfc76377348", "term": "Bitcoin", "description": "decentralized cryptocurrency", "question": "Could a single bitcoin ever cover cost of a Volkswagen Jetta?", "answer": true, "facts": ["The all time high price of bitcoin was $19,783 in 2017.", "The suggested retail price of a 2020 Volkswagen Jetta is $18,895."], "decomposition": ["What is the highest price for a bitcoin?", "What is the cheapest price of a Jetta?", "Is #1 greater than #2?"], "evidence": [[[["Bitcoin-22"]], ["no_evidence"], ["no_evidence", "operation"]], [[["Bitcoin-22"]], ["no_evidence"], ["operation"]], [[["Economics of bitcoin-16"]], [["Volkswagen Jetta-1"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average life expectancy of a human being?\nFacts: The average life expectancy of a human being is about 100 years.\nAnswer (True or False): True\nQuestion: What is the average weight of a human being?\nFacts: The average weight of a human being is about 100 pounds.\nAnswer (True or False", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could a single bitcoin ever cover cost of a Volkswagen Jetta?\nFacts: The all time high price of bitcoin was $19,783 in 2017. The suggested retail price of a 2020 Volkswagen Jetta is $18,895.\nAnswer (True or False):", "ct_subject": "The all time high price of bitcoin was $19,783 in 2017. The suggested retail price of a 2020 Volkswagen Jetta is $18,895.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99986255"], [" is", "1.0"], [" the", "1.0"], [" average", "0.9865785"], [" life", "0.9764607"], [" expectancy", "0.70821255"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "0.5975198"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" life", "1.0"], [" expectancy", "1.0"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "1.0"], [" is", "1.0"], [" about", "0.9995727"], [" 100", "0.9843946"], [" years", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "0.97468615"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" weight", "1.0"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 100", "0.9994667"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"]]}, "987": {"qid": "df5d1bdd7a56f0090f60", "term": "Drew Carey", "description": "American actor, comedian, game show host, libertarian and photographer", "question": "Has Drew Carey outshined Doug Davidson's tenure?", "answer": true, "facts": ["Drew Carey has been the host of the Price is Right for over 13 years.", "Doug Davidson hosted the Price is Right from 1994-1995."], "decomposition": ["How long has Drew Carey hosted the Price is Right?", "How long did Doug Davidson host the Price is Right?", "Is #1 longer than #2?"], "evidence": [[[["Drew Carey-14"]], [["Doug Davidson-2"]], ["operation"]], [[["The Price Is Right-1"]], [["The New Price Is Right (1994 game show)-1"]], ["operation"]], [[["Drew Carey-1"]], [["Doug Davidson-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: The first person to be born in the United States was a woman named Mary Ann Cotton.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: The first person to be born in the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Has Drew Carey outshined Doug Davidson's tenure?\nFacts: Drew Carey has been the host of the Price is Right for over 13 years. Doug Davidson hosted the Price is Right from 1994-1995.\nAnswer (True or False):", "ct_subject": "Drew Carey has been the host of the Price is Right for over 13 years. Doug Davidson hosted the Price is Right from 1994-1995.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.977086"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9787459"], [" of", "1.0"], [" the", "1.0"], [" first", "0.91736674"], [" person", "0.9999037"], [" to", "1.0"], [" be", "0.9994042"], [" born", "0.79155254"], [" in", "1.0"], [" the", "1.0"], [" United", "0.99815995"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9995863"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" a", "0.9830253"], [" woman", "0.9999976"], [" named", "1.0"], [" Mary", "0.4965821"], [" Ann", "0.9983365"], [" Cotton", "1.0"], [".", "1.0"], ["\n", "0.99999094"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"]]}, "988": {"qid": "b4c61fdd96959a52f996", "term": "High Speed 1", "description": "high-speed railway between London and the Channel Tunnel", "question": "Would the tunnels at CERN fit onto the High Speed 1 rails?", "answer": true, "facts": ["High Speed 1 (HS1), legally the Channel Tunnel Rail Link (CTRL), is a 67-mile (108 km) high-speed railway.", "The CERN collider is contained in a circular tunnel, with a circumference of 26.7 kilometres (16.6 mi).", "The circumference of a circle is the length of the enclosing boundary."], "decomposition": ["How long are the tunnels at the CERN collider?", "How long is the High Speed 1 railway?", "Is #1 less than or equal to #2?"], "evidence": [[[["Large Electron\u2013Positron Collider-2"]], [["High Speed 1-1"]], ["operation"]], [[["CERN-18"]], [["High Speed 1-1"]], ["operation"]], [[["CERN-18"]], [["High Speed 1-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the planet that is closest to the Sun?\nFacts: The planet closest to the Sun is called Proxima Centauri.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would the tunnels at CERN fit onto the High Speed 1 rails?\nFacts: High Speed 1 (HS1), legally the Channel Tunnel Rail Link (CTRL), is a 67-mile (108 km) high-speed railway. The CERN collider is contained in a circular tunnel, with a circumference of 26.7 kilometres (16.6 mi). The circumference of a circle is the length of the enclosing boundary.\nAnswer (True or False):", "ct_subject": "High Speed 1 (HS1), legally the Channel Tunnel Rail Link (CTRL), is a 67-mile (108 km) high-speed railway. The CERN collider is contained in a circular tunnel, with a circumference of 26.7 kilometres (16.6 mi). The circumference of a circle is the length of the enclosing boundary.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.999987"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99985385"], [" of", "1.0"], [" the", "1.0"], [" planet", "0.7313436"], [" that", "0.99990654"], [" is", "0.82607937"], [" closest", "0.70186484"], [" to", "1.0"], [" the", "0.9999908"], [" Sun", "0.9934661"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" planet", "1.0"], [" closest", "1.0"], [" to", "1.0"], [" the", "1.0"], [" Sun", "1.0"], [" is", "1.0"], [" called", "1.0"], [" Pro", "1.0"], ["x", "1.0"], ["ima", "1.0"], [" Centauri", "1.0"], [".", "1.0"]]}, "989": {"qid": "29551a9093ea19c1e7c3", "term": "2000", "description": "Year", "question": "Would 1996 leap year baby technically be 1 year old in 2000?", "answer": true, "facts": ["A leap year happens once every 4 years and has 29 days in February.", "The years 1996, 2000, 2004, 2008, 2012, 2016, and 2020 are the last 7 leap years that have happened.", "1996 to 2000 is one leap year."], "decomposition": ["How many years apart are consecutive leap years?", "What is 2000 minus 1996?", "Is #2 divided by #1 equal to one?"], "evidence": [[[["Leap year-2"]], ["operation"], ["operation"]], [[["Leap year-2"]], ["operation"], ["operation"]], [[["Leap year-16"]], ["operation"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a leap year and a leap day?\nFacts: Leap years are a special type of day that occurs every 4 years. A leap day is a day that occurs every year.\nAnswer (True or False): True\nQuestion: What", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would 1996 leap year baby technically be 1 year old in 2000?\nFacts: A leap year happens once every 4 years and has 29 days in February. The years 1996, 2000, 2004, 2008, 2012, 2016, and 2020 are the last 7 leap years that have happened. 1996 to 2000 is one leap year.\nAnswer (True or False):", "ct_subject": "A leap year happens once every 4 years and has 29 days in February. The years 1996, 2000, 2004, 2008, 2012, 2016, and 2020 are the last 7 leap years that have happened. 1996 to 2000 is one leap year.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999917"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.99998486"], [" between", "1.0"], [" a", "0.9999317"], [" leap", "0.9999974"], [" year", "1.0"], [" and", "1.0"], [" a", "1.0"], [" leap", "1.0"], [" day", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Leap", "0.41705632"], [" years", "0.99999547"], [" are", "0.99981433"], [" a", "0.98428243"], [" special", "0.32966015"], [" type", "0.99995565"], [" of", "1.0"], [" day", "0.9782256"], [" that", "0.85032296"], [" occurs", "0.99999964"], [" every", "0.9527132"], [" 4", "1.0"], [" years", "1.0"], [".", "1.0"], [" A", "0.99975103"], [" leap", "1.0"], [" day", "1.0"], [" is", "0.7379379"], [" a", "1.0"], [" day", "1.0"], [" that", "1.0"], [" occurs", "1.0"], [" every", "1.0"], [" year", "0.98125905"], [".", "0.99999976"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"]]}, "990": {"qid": "82bde29394d6aba258a1", "term": "Preventive healthcare", "description": "Prevent and minimize the occurrence of diseases", "question": "Can preventive healthcare reduce STI transmission?", "answer": true, "facts": ["Preventive healthcare includes screenings for STI/STD's. ", "Increases in testing for STI's allow for citizens to protect themselves from infection and contain outbreaks."], "decomposition": ["What are the effects of preventive measures on STI transmission?", "Does #1 involve a reduction in their spread?"], "evidence": [[[["Sexually transmitted infection-22"], "no_evidence"], ["no_evidence", "operation"]], [[["Preventive healthcare-18"]], [["Pre-exposure prophylaxis-8"], "operation"]], [[["Condom-85"]], [["Condom-85"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it true that the average American has more than one STD?\nFacts: The average American has more than one STD.  The average American has more than one STD.  The average American has more than one STD.  The average American has more than one STD.  The average American has more than one STD.  The average American has more than one STD.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can preventive healthcare reduce STI transmission?\nFacts: Preventive healthcare includes screenings for STI/STD's.  Increases in testing for STI's allow for citizens to protect themselves from infection and contain outbreaks.\nAnswer (True or False):", "ct_subject": "Preventive healthcare includes screenings for STI/STD's.  Increases in testing for STI's allow for citizens to protect themselves from infection and contain outbreaks.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9387337"], [" it", "0.93881524"], [" true", "0.9999746"], [" that", "1.0"], [" the", "1.0"], [" average", "0.97809654"], [" American", "0.9996056"], [" has", "0.9783764"], [" more", "0.93997705"], [" than", "1.0"], [" one", "0.9997739"], [" STD", "0.9165478"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" American", "1.0"], [" has", "1.0"], [" more", "1.0"], [" than", "1.0"], [" one", "1.0"], [" STD", "1.0"], [".", "1.0"], [" ", "0.9999999"], [" The", "1.0"], [" average", "0.9987662"], [" American", "1.0"], [" has", "1.0"], [" more", "1.0"], [" than", "1.0"], [" one", "1.0"], [" STD", "1.0"], [".", "1.0"], [" ", "0.9999993"], [" The", "1.0"], [" average", "1.0"], [" American", "1.0"], [" has", "1.0"], [" more", "1.0"], [" than", "1.0"], [" one", "1.0"], [" STD", "1.0"], [".", "1.0"], [" ", "1.0"], [" The", "1.0"], [" average", "1.0"], [" American", "1.0"], [" has", "1.0"], [" more", "1.0"], [" than", "1.0"], [" one", "1.0"], [" STD", "1.0"], [".", "1.0"], [" ", "1.0"], [" The", "1.0"], [" average", "1.0"], [" American", "1.0"], [" has", "1.0"], [" more", "1.0"], [" than", "1.0"], [" one", "1.0"], [" STD", "1.0"], [".", "1.0"], [" ", "1.0"], [" The", "1.0"], [" average", "1.0"], [" American", "1.0"], [" has", "1.0"], [" more", "1.0"], [" than", "1.0"], [" one", "1.0"], [" STD", "1.0"], [".", "1.0"]]}, "991": {"qid": "e3f5b70bb16f336cc7fc", "term": "Snowdon", "description": "highest mountain in Wales", "question": "Would Snowdon mountain be a piece of cake for Tenzing Norgay?", "answer": true, "facts": ["Tenzing Norgay was a mountaineer that climbed Mount Everest in 1953.", "Snowdon Mountain has a peak of 3,560 feet.", "Mount Everest has a peak of over 29,000 feet."], "decomposition": ["How high is Snowdon Mountain?", "What was the highest peak ever climbed by Tenzing Norgay", "How high is #2?", "Is #3 greater than #1?"], "evidence": [[[["Snowdon-1"]], [["Tenzing Norgay-1"]], [["Mount Everest-2"]], ["operation"]], [[["Snowdon-1"]], [["Mount Everest-1", "Tenzing Norgay-1"]], [["Mount Everest-2"]], ["operation"]], [[["Snowdon-1"], "no_evidence"], [["Tenzing Norgay-1"]], [["Mount Everest-2"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city of New York?\nFacts: New York City is the largest city in the United States. It is located in New York State.\nAnswer (True or False): True\nQuestion: What is the name of the city of New York?\nF", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Snowdon mountain be a piece of cake for Tenzing Norgay?\nFacts: Tenzing Norgay was a mountaineer that climbed Mount Everest in 1953. Snowdon Mountain has a peak of 3,560 feet. Mount Everest has a peak of over 29,000 feet.\nAnswer (True or False):", "ct_subject": "Tenzing Norgay was a mountaineer that climbed Mount Everest in 1953. Snowdon Mountain has a peak of 3,560 feet. Mount Everest has a peak of over 29,000 feet.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99994993"], [" is", "1.0"], [" the", "1.0"], [" name", "0.688664"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9996692"], [" of", "0.99951863"], [" New", "0.96404505"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" New", "1.0"], [" York", "1.0"], [" City", "0.48808897"], [" is", "1.0"], [" the", "1.0"], [" largest", "0.9952054"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [".", "1.0"], [" It", "0.9998883"], [" is", "1.0"], [" located", "1.0"], [" in", "1.0"], [" New", "0.6563899"], [" York", "1.0"], [" State", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "0.8524517"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"]]}, "992": {"qid": "5165ea01d214c4e74188", "term": "Reconstruction era", "description": "Era of military occupation in the Southern United States after the American Civil War (1865\u20131877)", "question": "Can a Reconstruction era coin buy DJI Mavic Pro Drone?", "answer": true, "facts": ["The DJI Mavic Pro Drone retails for around $1,000 dollars.", "THE Reconstruction Era took place from 1865-1877.", "Mint condition 1870 Seated Liberty Silver Dollar's can sell for between $2,283 to $4,933."], "decomposition": ["How much does a DJI Mavic Pro Drone retail for?", "During what years did the Reconstruction era occur?", "Of the US coins minted during the years in #2, are any of them now worth at least as much as #1?"], "evidence": [[[["DJI-26"], "no_evidence"], [["Reconstruction era-2"]], [["Three-cent silver-28", "Two-cent piece (United States)-21"], "operation"]], [[["DJI-26"], "no_evidence"], [["Reconstruction era-2"]], [["Three-cent piece-4"], "no_evidence", "operation"]], [[["Mavic (UAV)-2"], "no_evidence"], [["Reconstruction era-2"]], [["Economic history of the United States-201"], "no_evidence"]], [[["Mavic (UAV)-17"], "no_evidence"], [["Reconstruction era-1"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"Penny\" and a \"Dime\"?\nFacts: The penny is a coin that is made of copper and nickel. The dime is a coin that is made of zinc and copper.\nAnswer (", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can a Reconstruction era coin buy DJI Mavic Pro Drone?\nFacts: The DJI Mavic Pro Drone retails for around $1,000 dollars. THE Reconstruction Era took place from 1865-1877. Mint condition 1870 Seated Liberty Silver Dollar's can sell for between $2,283 to $4,933.\nAnswer (True or False):", "ct_subject": "The DJI Mavic Pro Drone retails for around $1,000 dollars. THE Reconstruction Era took place from 1865-1877. Mint condition 1870 Seated Liberty Silver Dollar's can sell for between $2,283 to $4,933.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99220955"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.99999034"], [" between", "1.0"], [" a", "0.99766856"], [" \"", "1.0"], ["P", "0.4499007"], ["enny", "0.99790025"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["D", "0.9098147"], ["ime", "0.99969804"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.88894117"], [" penny", "0.36135104"], [" is", "0.9999896"], [" a", "0.9999747"], [" coin", "0.9914056"], [" that", "0.9999958"], [" is", "0.8605963"], [" made", "0.84956974"], [" of", "0.9999999"], [" copper", "1.0"], [" and", "0.89681995"], [" nickel", "1.0"], [".", "1.0"], [" The", "1.0"], [" dime", "1.0"], [" is", "1.0"], [" a", "0.9999312"], [" coin", "1.0"], [" that", "1.0"], [" is", "1.0"], [" made", "1.0"], [" of", "1.0"], [" zinc", "0.99615085"], [" and", "1.0"], [" copper", "0.94330025"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"]]}, "993": {"qid": "7460108c3996aca011d5", "term": "September", "description": "ninth month in the Julian and Gregorian calendars", "question": "Does Home Depot sell item in late September zodiac sign symbol?", "answer": true, "facts": ["The zodiac sign for late September is Libra.", "The Libra zodiac sign is represented by scales. ", "Home Depot sells a number of scales including Pelouze and Ozeri brands."], "decomposition": ["What is the zodiac sign that represents late September?", "What is symbol of #1?", "What kinds of goods (and services) does The Home Depot deal in?", "Is #2 likely to be included in #3?"], "evidence": [[[["Libra (astrology)-1"]], [["Weighing scale-1"]], [["The Home Depot-1"]], ["operation"]], [[["Libra (astrology)-1"]], [["Libra (astrology)-2"]], [["The Home Depot-1"]], ["operation"]], [[["Libra-1"]], [["Libra (astrology)-2"]], [["The Home Depot-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: The first person to be executed in the United States was John Brown.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be executed in the United States?\nF", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Home Depot sell item in late September zodiac sign symbol?\nFacts: The zodiac sign for late September is Libra. The Libra zodiac sign is represented by scales.  Home Depot sells a number of scales including Pelouze and Ozeri brands.\nAnswer (True or False):", "ct_subject": "The zodiac sign for late September is Libra. The Libra zodiac sign is represented by scales.  Home Depot sells a number of scales including Pelouze and Ozeri brands.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9998388"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999939"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9380288"], [" person", "0.9973333"], [" to", "1.0"], [" be", "0.9999999"], [" executed", "0.96343505"], [" in", "0.8438147"], [" the", "1.0"], [" United", "0.99999607"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.91274536"], [" first", "1.0"], [" person", "1.0"], [" to", "0.9948454"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" John", "0.9999995"], [" Brown", "0.9999995"], [".", "0.9999535"], ["\n", "0.7180425"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"]]}, "994": {"qid": "a97758e32a875a48e83a", "term": "Beauty and the Beast (1991 film)", "description": "1991 American animated musical fantasy romance film", "question": "Do inanimate objects come alive in Beauty and the Beast?", "answer": true, "facts": ["Beauty and the Beast features a castle full of items that move and speak on their own. ", "An inanimate object is one that is not alive in any way.", "Main characters of Beauty and the Beast include a talking teacup and a sassy duster."], "decomposition": ["Who are the main characters in Beauty and the Beast?", "What type of entities are the characters listed in #1?", "Are any of the types listed in #2 usually inanimate objects?"], "evidence": [[[["Beauty and the Beast (2017 film)-6"]], [["Beauty and the Beast (2017 film)-6"]], [["Beauty and the Beast (2017 film)-6"]]], [[["Beauty and the Beast (1991 film)-6", "Beauty and the Beast (1991 film)-7"], "no_evidence"], [["Beauty and the Beast (1991 film)-7"], "no_evidence"], ["operation"]], [[["Beauty and the Beast (1991 film)-7"]], [["Beauty and the Beast (1991 film)-10"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the character in Beauty and the Beast who is the most beautiful?\nFacts: Belle.\nAnswer (True or False): True\nQuestion: What is the name of the character in Beauty and the Beast who is the most", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do inanimate objects come alive in Beauty and the Beast?\nFacts: Beauty and the Beast features a castle full of items that move and speak on their own.  An inanimate object is one that is not alive in any way. Main characters of Beauty and the Beast include a talking teacup and a sassy duster.\nAnswer (True or False):", "ct_subject": "Beauty and the Beast features a castle full of items that move and speak on their own.  An inanimate object is one that is not alive in any way. Main characters of Beauty and the Beast include a talking teacup and a sassy duster.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999815"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" character", "0.9999974"], [" in", "1.0"], [" Beauty", "0.9999999"], [" and", "1.0"], [" the", "1.0"], [" Beast", "1.0"], [" who", "0.9981184"], [" is", "0.9999994"], [" the", "1.0"], [" most", "1.0"], [" beautiful", "0.9999987"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Belle", "1.0"], [".", "0.9353539"], ["\n", "0.9999194"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" character", "1.0"], [" in", "1.0"], [" Beauty", "1.0"], [" and", "1.0"], [" the", "1.0"], [" Beast", "1.0"], [" who", "1.0"], [" is", "1.0"], [" the", "1.0"], [" most", "1.0"]]}, "995": {"qid": "3ca0a58b1697a63521b6", "term": "Rick and Morty", "description": "Animated sitcom", "question": "Can you watch Rick and Morty in Mariana Trench?", "answer": true, "facts": ["Rick and Morty is available in blu-ray format.", "You can play blu-ray on a laptop computer ", "It is possible to go to Mariana Trench inside a deep-diving submersible vehicle with a laptop."], "decomposition": ["What portable media format is Rick and Morty available in?", "What electronics do deep-diving submersibles have?", "Can any of #1 be played on any of #2?"], "evidence": [[[["Rick and Morty-28"]], [["Deep-submergence vehicle-1"], "no_evidence"], ["operation"]], [[["Rick and Morty-27"]], [["DVD player-1", "Submersible-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Rick and Morty-28"]], [["Deep diving-11"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the character in the movie \"The Big Lebowski\"?\nFacts: The Big Lebowski is a movie about a man who is a drug dealer. The character is named \"The Dude\".\nAnswer (True or False): True\nQuestion: What", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can you watch Rick and Morty in Mariana Trench?\nFacts: Rick and Morty is available in blu-ray format. You can play blu-ray on a laptop computer  It is possible to go to Mariana Trench inside a deep-diving submersible vehicle with a laptop.\nAnswer (True or False):", "ct_subject": "Rick and Morty is available in blu-ray format. You can play blu-ray on a laptop computer  It is possible to go to Mariana Trench inside a deep-diving submersible vehicle with a laptop.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999651"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" character", "0.8997788"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "0.9992198"], ["The", "1.0"], [" Big", "0.9934917"], [" Le", "1.0"], ["b", "1.0"], ["owski", "1.0"], ["\"?", "0.9516095"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Big", "0.9935103"], [" Le", "1.0"], ["b", "1.0"], ["owski", "1.0"], [" is", "1.0"], [" a", "1.0"], [" movie", "0.9999944"], [" about", "1.0"], [" a", "1.0"], [" man", "0.99999666"], [" who", "1.0"], [" is", "0.99304116"], [" a", "0.73504466"], [" drug", "0.6135342"], [" dealer", "0.98885953"], [".", "0.99967027"], [" The", "0.9975272"], [" character", "0.9999994"], [" is", "0.99998677"], [" named", "1.0"], [" \"", "0.9999999"], ["The", "1.0"], [" Dude", "1.0"], ["\".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"]]}, "996": {"qid": "8141eefa48407dfb759b", "term": "Oprah Winfrey", "description": "American businesswoman, talk show host, actress, producer, and philanthropist", "question": "Does Oprah Winfrey have a degree from an Ivy League university?", "answer": true, "facts": ["Oprah Winfrey has received honorary doctorates from Duke and Harvard Universities", "Harvard University is a member of the Ivy League"], "decomposition": ["What schools does Oprah Winfrey have honorary degrees from?", "Is #1 Ivy league?"], "evidence": [[[["Oprah Winfrey-4"]], [["Harvard University-1"], "operation"]], [[["Oprah Winfrey-4"]], [["Ivy League-1"], "operation"]], [[["Oprah Winfrey-4"]], [["Outline of Harvard University-2"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city in the United States that is the capital of the state of Texas?\nFacts: The capital of the state of Texas is Austin, Texas\nAnswer (True or False): True\nQuestion: What is the name of the city in the United States that is the capital of the state of California?\nFacts: The capital of the state of California is Los", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Oprah Winfrey have a degree from an Ivy League university?\nFacts: Oprah Winfrey has received honorary doctorates from Duke and Harvard Universities Harvard University is a member of the Ivy League\nAnswer (True or False):", "ct_subject": "Oprah Winfrey has received honorary doctorates from Duke and Harvard Universities Harvard University is a member of the Ivy League", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9873547"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999443"], [" of", "1.0"], [" the", "1.0"], [" city", "0.9401375"], [" in", "0.9186523"], [" the", "0.99999976"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "0.9999989"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" Texas", "0.9966582"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999578"], [" capital", "0.9990239"], [" of", "1.0"], [" the", "0.29667336"], [" state", "1.0"], [" of", "1.0"], [" Texas", "1.0"], [" is", "1.0"], [" Austin", "1.0"], [",", "0.6934971"], [" Texas", "1.0"], ["\n", "0.99956006"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "0.9999999"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" that", "1.0"], [" is", "1.0"], [" the", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" California", "0.9999989"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" capital", "1.0"], [" of", "1.0"], [" the", "1.0"], [" state", "1.0"], [" of", "1.0"], [" California", "1.0"], [" is", "1.0"], [" Los", "0.81291604"]]}, "997": {"qid": "746b563db21efc4cfa92", "term": "Led Zeppelin", "description": "English rock band", "question": "Did the lead singer of Led Zepplin ever perform with Ernest Chataway?", "answer": true, "facts": ["Robert Plant is the lead singer of Led Zepplin", "Robert Plant was in the band The Honeydrippers", "Ernest Chataway was in the band The Honeydrippers"], "decomposition": ["Who was the lead singer of Led Zepplin?", "Who are the members of the Honeydrippers?", "Is Ernest Chataway also part of #2?", "Is #1 in #2?", "Is #3 and #4 both yes?"], "evidence": [[[["Led Zeppelin-1"]], [["The Honeydrippers-1"]], ["operation"], ["operation"], ["operation"]], [[["Robert Plant-1"]], [["The Honeydrippers-1"]], [["The Honeydrippers-1"]], ["operation"], ["operation"]], [[["The Honeydrippers-1"]], [["The Honeydrippers-1"]], ["operation"], ["operation"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the lead singer of Led Zepplin ever perform with Ernest Chataway?\nFacts: Robert Plant is the lead singer of Led Zepplin Robert Plant was in the band The Honeydrippers Ernest Chataway was in the band The Honeydrippers\nAnswer (True or False): True\nQuestion: Did the lead singer of", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did the lead singer of Led Zepplin ever perform with Ernest Chataway?\nFacts: Robert Plant is the lead singer of Led Zepplin Robert Plant was in the band The Honeydrippers Ernest Chataway was in the band The Honeydrippers\nAnswer (True or False):", "ct_subject": "Robert Plant is the lead singer of Led Zepplin Robert Plant was in the band The Honeydrippers Ernest Chataway was in the band The Honeydrippers", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99975926"], [" the", "1.0"], [" lead", "1.0"], [" singer", "1.0"], [" of", "1.0"], [" Led", "1.0"], [" Ze", "1.0"], ["pp", "1.0"], ["lin", "1.0"], [" ever", "1.0"], [" perform", "1.0"], [" with", "1.0"], [" Ernest", "0.7754248"], [" Chat", "1.0"], ["away", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Robert", "0.9836849"], [" Plant", "1.0"], [" is", "1.0"], [" the", "1.0"], [" lead", "1.0"], [" singer", "1.0"], [" of", "1.0"], [" Led", "1.0"], [" Ze", "1.0"], ["pp", "1.0"], ["lin", "1.0"], [" Robert", "1.0"], [" Plant", "1.0"], [" was", "1.0"], [" in", "1.0"], [" the", "1.0"], [" band", "1.0"], [" The", "1.0"], [" Honey", "1.0"], ["dri", "1.0"], ["ppers", "1.0"], [" Ernest", "1.0"], [" Chat", "1.0"], ["away", "1.0"], [" was", "1.0"], [" in", "1.0"], [" the", "1.0"], [" band", "1.0"], [" The", "1.0"], [" Honey", "1.0"], ["dri", "1.0"], ["ppers", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" lead", "1.0"], [" singer", "1.0"], [" of", "1.0"]]}, "998": {"qid": "563394228ae6952dd267", "term": "Coca", "description": "group of plant varieties cultivated for coca production", "question": "Would someone with a nosebleed benefit from Coca?", "answer": true, "facts": ["Coca constricts blood vessels.", "As a result, it serves to stop bleeding. ", "Someone with a nosebleed would want the bleeding to stop."], "decomposition": ["What does Coca do to blood vessels?", "What happens to blood when #1 occurs?", "Would someone with a nose want #2 to occur?"], "evidence": [[[["Coca-30"]], [["Blood vessel-16"]], ["operation"]], [[["Coca-30"]], ["no_evidence"], ["no_evidence"]], [[["Coca-30"]], ["no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a man and a woman?\nFacts: A man is a male human being. A woman is a female human being.\nAnswer (True or False): True\nQuestion: What is the difference between a man and a woman?\nFacts: A man is a male human being. A woman is a female human being.\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would someone with a nosebleed benefit from Coca?\nFacts: Coca constricts blood vessels. As a result, it serves to stop bleeding.  Someone with a nosebleed would want the bleeding to stop.\nAnswer (True or False):", "ct_subject": "Coca constricts blood vessels. As a result, it serves to stop bleeding.  Someone with a nosebleed would want the bleeding to stop.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9998913"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.99999964"], [" between", "1.0"], [" a", "1.0"], [" man", "0.38667208"], [" and", "1.0"], [" a", "1.0"], [" woman", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" man", "1.0"], [" is", "1.0"], [" a", "1.0"], [" male", "0.98776746"], [" human", "0.78242385"], [" being", "1.0"], [".", "1.0"], [" A", "1.0"], [" woman", "1.0"], [" is", "1.0"], [" a", "1.0"], [" female", "1.0"], [" human", "1.0"], [" being", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" man", "1.0"], [" and", "1.0"], [" a", "1.0"], [" woman", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" man", "1.0"], [" is", "1.0"], [" a", "1.0"], [" male", "1.0"], [" human", "1.0"], [" being", "1.0"], [".", "1.0"], [" A", "1.0"], [" woman", "1.0"], [" is", "1.0"], [" a", "1.0"], [" female", "1.0"], [" human", "1.0"], [" being", "1.0"], [".", "1.0"], ["\n", "1.0"]]}, "999": {"qid": "48cac3b98391f6da285f", "term": "Wednesday", "description": "Day of the week", "question": "Will Communion be denied to Wednesday name origin followers?", "answer": true, "facts": ["Communion is the body and blood of Christ given out during mass.", "Communion is only given to believers baptized in the Christian Church.", "Wednesday comes from Old English Wodnesdaeg referring to Woden, also called Odin.", "Odin was the pagan god of Norse mythology.", "Vikings, believers in Norse mythology, clashed with Christians in Wessex and Northumbria for hundreds of years."], "decomposition": ["Which deity is related to the origin of the name 'Wednesday'?", "Who are the worshipers of #1?", "Which group of people are allowed to take the Communion?", "Are #2 included in #3?"], "evidence": [[[["Odin-2"]], [["Odin-2"], "no_evidence"], [["Eucharist-1"]], ["operation"]], [[["Wednesday-1"]], [["Odin-2"]], [["First Communion-1"]], ["operation"]], [[["Wednesday-1"]], [["Anglo-Saxon paganism-1"]], [["Eucharist-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Will the name of the new church be changed to \"The Church of the First Born\"?\nFacts: The name of the new", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Will Communion be denied to Wednesday name origin followers?\nFacts: Communion is the body and blood of Christ given out during mass. Communion is only given to believers baptized in the Christian Church. Wednesday comes from Old English Wodnesdaeg referring to Woden, also called Odin. Odin was the pagan god of Norse mythology. Vikings, believers in Norse mythology, clashed with Christians in Wessex and Northumbria for hundreds of years.\nAnswer (True or False):", "ct_subject": "Communion is the body and blood of Christ given out during mass. Communion is only given to believers baptized in the Christian Church. Wednesday comes from Old English Wodnesdaeg referring to Woden, also called Odin. Odin was the pagan god of Norse mythology. Vikings, believers in Norse mythology, clashed with Christians in Wessex and Northumbria for hundreds of years.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Will", "0.9999324"], [" the", "1.0"], [" name", "0.99994934"], [" of", "0.99968994"], [" the", "0.98321646"], [" new", "0.9838816"], [" church", "0.99799913"], [" be", "1.0"], [" changed", "1.0"], [" to", "0.9999443"], [" \"", "0.9998548"], ["The", "1.0"], [" Church", "1.0"], [" of", "1.0"], [" the", "1.0"], [" First", "0.99977416"], [" Born", "0.9990402"], ["\"?", "0.9963133"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" new", "0.97778225"]]}, "1000": {"qid": "198d416b99a4f41cb56d", "term": "Moulin Rouge", "description": "cabaret in Paris, France", "question": "Could Moulin Rouge have been hypothetically used as Spain's Spanish American War triage center?", "answer": true, "facts": ["The Moulin Rouge cabaret in France had a capacity of 850 people.", "Spain had 700-800 injured during Spanish American War."], "decomposition": ["How many people can be seated in Moulin Rouge?", "How many Spaniards were injured during the Spanish American War?", "Ia #1 greater than #2?"], "evidence": [[[["Moulin Rouge-2"], "no_evidence"], [["Spanish\u2013American War-55"]], ["operation"]], [[["Moulin Rouge-2"], "no_evidence"], [["Spanish\u2013American War-57"], "no_evidence"], ["no_evidence", "operation"]], [[["Moulin Rouge! (musical)-3"], "no_evidence"], [["Spanish\u2013American War-55"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the U.S. Army use the term \"Militia\" in the Civil War?\nFacts: The term \"Militia\" was used by the U.S. Army in the Civil War.\nAnswer (True or False): True\nQuestion: Did the U.S. Army use the term \"Militia\"", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could Moulin Rouge have been hypothetically used as Spain's Spanish American War triage center?\nFacts: The Moulin Rouge cabaret in France had a capacity of 850 people. Spain had 700-800 injured during Spanish American War.\nAnswer (True or False):", "ct_subject": "The Moulin Rouge cabaret in France had a capacity of 850 people. Spain had 700-800 injured during Spanish American War.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.32436037"], [" the", "1.0"], [" U", "0.80106807"], [".", "1.0"], ["S", "1.0"], [".", "1.0"], [" Army", "0.96093446"], [" use", "0.9999156"], [" the", "1.0"], [" term", "0.9999696"], [" \"", "1.0"], ["M", "0.9989385"], ["ilit", "0.55390483"], ["ia", "1.0"], ["\"", "1.0"], [" in", "0.15145479"], [" the", "1.0"], [" Civil", "0.32889205"], [" War", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" term", "0.9999896"], [" \"", "1.0"], ["M", "0.9999999"], ["ilit", "1.0"], ["ia", "1.0"], ["\"", "1.0"], [" was", "1.0"], [" used", "0.99996495"], [" by", "0.99328595"], [" the", "1.0"], [" U", "0.9955739"], [".", "1.0"], ["S", "1.0"], [".", "1.0"], [" Army", "1.0"], [" in", "0.99999523"], [" the", "1.0"], [" Civil", "1.0"], [" War", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99998856"], [" the", "1.0"], [" U", "1.0"], [".", "1.0"], ["S", "1.0"], [".", "1.0"], [" Army", "1.0"], [" use", "1.0"], [" the", "1.0"], [" term", "1.0"], [" \"", "1.0"], ["M", "1.0"], ["ilit", "1.0"], ["ia", "1.0"], ["\"", "1.0"]]}, "1001": {"qid": "5b223afd5634a367d859", "term": "Metallica", "description": "American heavy metal band", "question": "Did Metallica band members cutting their hair hurt their sales?", "answer": true, "facts": ["Metallica famously cut their hair in 1996 which caused a huge divide in their fanbase.", "Metallica's best selling album, The Black Album, was released in 1991 and has sold over 20 million copies.", "Since 1996, Metallica have released 5 studio albums.", "Metalica's 5 studio albums since 1996 have sold around a combined 14 million copies"], "decomposition": ["When did Metallica band members cut their hair?", "How many copies of their best selling album has been sold?", "How many copies of their last five albums have been sold altogether?", "Is #1 after the release date of #2 and before those of #3, and #2 greater than #3?"], "evidence": [[[["Metallica-25"]], [["Metallica (album)-22"]], [["Death Magnetic-44", "Death Magnetic-45", "Hardwired... to Self-Destruct-15", "Load (album)-2", "Reload (Metallica album)-6", "St. Anger-3"], "no_evidence"], ["operation"]], [[["Metallica-25"]], [["Metallica discography-1"], "no_evidence"], ["no_evidence"], ["operation"]], [[["Load (album)-1", "Load (album)-13"], "no_evidence"], [["Metallica (album)-3"]], [["Death Magnetic-45", "Hardwired... to Self-Destruct-2", "Load (album)-2", "Reload (Metallica album)-6", "St. Anger-3"], "no_evidence"], ["no_evidence", "operation"]], [[["Metallica-25"]], [["Metallica (album)-22"]], [["Death Magnetic-47", "Hardwired... to Self-Destruct-15", "Lulu (Lou Reed and Metallica album)-16", "Reload (Metallica album)-1", "St. Anger-3"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: The first person to be executed in the United States was John Brown, who was hanged", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Metallica band members cutting their hair hurt their sales?\nFacts: Metallica famously cut their hair in 1996 which caused a huge divide in their fanbase. Metallica's best selling album, The Black Album, was released in 1991 and has sold over 20 million copies. Since 1996, Metallica have released 5 studio albums. Metalica's 5 studio albums since 1996 have sold around a combined 14 million copies\nAnswer (True or False):", "ct_subject": "Metallica famously cut their hair in 1996 which caused a huge divide in their fanbase. Metallica's best selling album, The Black Album, was released in 1991 and has sold over 20 million copies. Since 1996, Metallica have released 5 studio albums. Metalica's 5 studio albums since 1996 have sold around a combined 14 million copies", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9700716"], [" is", "1.0"], [" the", "1.0"], [" name", "0.59861356"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9978387"], [" person", "0.99589044"], [" to", "1.0"], [" be", "0.9937696"], [" executed", "0.81158394"], [" in", "0.9999987"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9997371"], [" first", "1.0"], [" person", "1.0"], [" to", "0.9939791"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" John", "0.99280614"], [" Brown", "1.0"], [",", "0.9999814"], [" who", "1.0"], [" was", "1.0"], [" hanged", "1.0"]]}, "1002": {"qid": "35f6213b1bddcf156472", "term": "B", "description": "letter in the Latin alphabet", "question": "Does the letter B's place in alphabet exceed number of 2008 total lunar eclipses?", "answer": true, "facts": ["The letter B is the second letter in the Latin Alphabet.", "There was one total lunar eclipse in 2008."], "decomposition": ["What number represents the position of letter B in the English alphabet?", "How many total lunar eclipses occurred in 2008?", "Is #1 greater than #2?"], "evidence": [[[["B-1"]], [["August 2008 lunar eclipse-1"]], ["operation"]], [[["B-1"]], [["February 2008 lunar eclipse-9"], "no_evidence"], ["operation"]], [[["B-1"]], [["August 2008 lunar eclipse-1"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance between two points on the earth?\nFacts: The distance between the earth and the moon is about 238,000 miles.\nAnswer (True or False): True\nQuestion: What is the longest distance between two points on the earth?\nFacts: The distance between the earth and the moon is about 238,000 miles.\nAnswer (True or", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does the letter B's place in alphabet exceed number of 2008 total lunar eclipses?\nFacts: The letter B is the second letter in the Latin Alphabet. There was one total lunar eclipse in 2008.\nAnswer (True or False):", "ct_subject": "The letter B is the second letter in the Latin Alphabet. There was one total lunar eclipse in 2008.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9776049"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.98278934"], [" distance", "0.99999774"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"], [" on", "0.9998772"], [" the", "0.9997327"], [" earth", "0.9991653"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" distance", "0.5805068"], [" between", "1.0"], [" the", "0.9627449"], [" earth", "0.9988294"], [" and", "1.0"], [" the", "1.0"], [" moon", "0.34323853"], [" is", "1.0"], [" about", "1.0"], [" 238", "0.99506986"], [",", "1.0"], ["000", "1.0"], [" miles", "1.0"], [".", "1.0"], ["\n", "0.99999964"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"], [" on", "1.0"], [" the", "1.0"], [" earth", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" distance", "1.0"], [" between", "1.0"], [" the", "1.0"], [" earth", "1.0"], [" and", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 238", "1.0"], [",", "1.0"], ["000", "1.0"], [" miles", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"]]}, "1003": {"qid": "63360dd6c8c5bdc31bbe", "term": "Gray whale", "description": "species of mammal", "question": "Would a baby gray whale fit in a tractor-trailer?", "answer": true, "facts": ["Gray whales measure 4.9 m (16 ft) in length for newborns.", "A semi-trailer is 48 feet long."], "decomposition": ["How large is a baby gray whale?", "How large is a tractor-trailer?", "Is #2 greater than #1?"], "evidence": [[[["Gray whale-8"]], [["Trailer (vehicle)-4"]], [["Trailer (vehicle)-4"], "operation"]], [[["Gray whale-21"]], [["Semi-trailer truck-25"]], ["operation"]], [[["Gray whale-21"]], [["Semi-trailer truck-18"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance a person can walk in one day?\nFacts: The longest distance a person can walk in one day is about 1,000 miles.\nAnswer (True or False): True\nQuestion: What is the longest distance a person can walk in one year?\nFacts: The longest distance a person can walk in one year is about 1,", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a baby gray whale fit in a tractor-trailer?\nFacts: Gray whales measure 4.9 m (16 ft) in length for newborns. A semi-trailer is 48 feet long.\nAnswer (True or False):", "ct_subject": "Gray whales measure 4.9 m (16 ft) in length for newborns. A semi-trailer is 48 feet long.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999813"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.97843117"], [" distance", "0.9999999"], [" a", "0.9797238"], [" person", "0.96446025"], [" can", "0.87200373"], [" walk", "0.9999981"], [" in", "0.9999604"], [" one", "0.99905115"], [" day", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9998772"], [" longest", "0.9329357"], [" distance", "1.0"], [" a", "1.0"], [" person", "1.0"], [" can", "1.0"], [" walk", "1.0"], [" in", "1.0"], [" one", "1.0"], [" day", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 1", "0.5969919"], [",", "1.0"], ["000", "0.999998"], [" miles", "0.99995196"], [".", "0.7632716"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" person", "1.0"], [" can", "1.0"], [" walk", "1.0"], [" in", "1.0"], [" one", "1.0"], [" year", "0.99115986"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" person", "1.0"], [" can", "1.0"], [" walk", "1.0"], [" in", "1.0"], [" one", "1.0"], [" year", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 1", "0.46225712"], [",", "1.0"]]}, "1004": {"qid": "3799fbe219d3a3eb0482", "term": "Twenty-third Amendment to the United States Constitution", "description": "Grants residents of Washington, D.C. the right to vote in U.S. presidential elections", "question": "Was Harry Truman's presidency unaffected by the twenty-third Amendment to the US Constitution?", "answer": true, "facts": ["The 23rd Amendment to the US Constitution was passed in 1961.", "Harry Truman was the President from 1945-1953."], "decomposition": ["When was Harry Truman the president of the United States?", "When was the 23rd Amendment passed?", "Is #2 after #1?"], "evidence": [[[["Harry S. Truman-1"]], [["Twenty-third Amendment to the United States Constitution-1"]], ["operation"]], [[["Harry S. Truman-99"], "no_evidence"], [["Twenty-third Amendment to the United States Constitution-11"], "no_evidence"], ["operation"]], [[["Harry S. Truman-1"]], [["Twenty-third Amendment to the United States Constitution-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: The first person to be executed in the United States was John Brown, who was hanged in 1859.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be executed in the United States?\nFacts: The first person", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Harry Truman's presidency unaffected by the twenty-third Amendment to the US Constitution?\nFacts: The 23rd Amendment to the US Constitution was passed in 1961. Harry Truman was the President from 1945-1953.\nAnswer (True or False):", "ct_subject": "The 23rd Amendment to the US Constitution was passed in 1961. Harry Truman was the President from 1945-1953.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99764544"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99998283"], [" of", "1.0"], [" the", "1.0"], [" first", "0.99043536"], [" person", "0.8987762"], [" to", "1.0"], [" be", "0.99999607"], [" executed", "0.97385466"], [" in", "0.9987355"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999615"], [" first", "1.0"], [" person", "1.0"], [" to", "0.9952474"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" John", "0.98344237"], [" Brown", "1.0"], [",", "0.8845155"], [" who", "0.9999987"], [" was", "1.0"], [" hanged", "1.0"], [" in", "0.99998856"], [" 18", "1.0"], ["59", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" executed", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"]]}, "1005": {"qid": "ac55fc84d16659a1f0df", "term": "Joker (character)", "description": "Fictional character in the DC Universe", "question": "Could Bart Simpson have owned comics with The Joker?", "answer": true, "facts": ["The first appearance of the Joker was in 1940.", "Bart Simpson first appeared as a child in \"The Simpsons\" in 1987."], "decomposition": ["When was the first appearance of the Joker?", "When did Bart Simpson first appear?", "Did #2 come after #1?"], "evidence": [[[["Joker (character)-1"]], [["Bart Simpson-1"]], ["operation"]], [[["Joker (character)-1"]], [["Bart Simpson-1"]], ["operation"]], [[["Joker (character)-59"]], [["Bart Simpson-13"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be killed by a shark?\nFacts: The first shark to be killed by a human was a great white shark in 1851.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be killed by a shark?\nFacts: The first shark to be killed by a human was a great white shark", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could Bart Simpson have owned comics with The Joker?\nFacts: The first appearance of the Joker was in 1940. Bart Simpson first appeared as a child in \"The Simpsons\" in 1987.\nAnswer (True or False):", "ct_subject": "The first appearance of the Joker was in 1940. Bart Simpson first appeared as a child in \"The Simpsons\" in 1987.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9712631"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999896"], [" of", "1.0"], [" the", "1.0"], [" first", "0.5778177"], [" person", "0.98552805"], [" to", "1.0"], [" be", "0.9998584"], [" killed", "0.9971975"], [" by", "1.0"], [" a", "1.0"], [" shark", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" shark", "0.99989116"], [" to", "0.99999964"], [" be", "0.9999987"], [" killed", "1.0"], [" by", "0.9999999"], [" a", "1.0"], [" human", "1.0"], [" was", "1.0"], [" a", "0.999992"], [" great", "1.0"], [" white", "1.0"], [" shark", "1.0"], [" in", "0.99999976"], [" 18", "0.96510625"], ["51", "0.99791235"], [".", "1.0"], ["\n", "0.9999943"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" killed", "1.0"], [" by", "1.0"], [" a", "1.0"], [" shark", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" shark", "1.0"], [" to", "1.0"], [" be", "1.0"], [" killed", "1.0"], [" by", "1.0"], [" a", "1.0"], [" human", "1.0"], [" was", "1.0"], [" a", "1.0"], [" great", "1.0"], [" white", "1.0"], [" shark", "1.0"]]}, "1006": {"qid": "fc60a290ab2e9a467f1c", "term": "Spinach", "description": "species of plant", "question": "For bone growth, is kale more beneficial than spinach?", "answer": true, "facts": ["Calcium is an important nutrient for bone health.", "Kale has more calcium per serving than spinach."], "decomposition": ["What nutrient is critical for bone growth?", "How much #1 does kale contain?", "How much #1 does spinach contain?", "Is #2 greater than #3?"], "evidence": [[[["Bone growth factor-3"]], [["Kale-11"]], [["Spinach-7"]], ["operation"]], [[["Calcium-36"]], [["Kale-11"]], [["Spinach-7"]], ["operation"]], [[["Calcium-3"]], [["Kale-11"], "no_evidence"], [["Spinach-7"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the best way to prevent heart disease?\nFacts: The best way to prevent heart disease is to eat a healthy diet. The best way to prevent heart disease is to exercise regularly.\nAnswer (True or False): True\nQuestion: What is the best way to prevent cancer?\nFacts: The best way to prevent cancer is to eat a healthy diet. The best way to prevent cancer is to", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: For bone growth, is kale more beneficial than spinach?\nFacts: Calcium is an important nutrient for bone health. Kale has more calcium per serving than spinach.\nAnswer (True or False):", "ct_subject": "Calcium is an important nutrient for bone health. Kale has more calcium per serving than spinach.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.80700195"], [" is", "1.0"], [" the", "1.0"], [" best", "0.9956124"], [" way", "1.0"], [" to", "1.0"], [" prevent", "0.59834945"], [" heart", "0.23701057"], [" disease", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" best", "1.0"], [" way", "1.0"], [" to", "1.0"], [" prevent", "1.0"], [" heart", "1.0"], [" disease", "1.0"], [" is", "1.0"], [" to", "1.0"], [" eat", "0.99999607"], [" a", "1.0"], [" healthy", "0.99991524"], [" diet", "1.0"], [".", "0.99950516"], [" The", "0.6929797"], [" best", "1.0"], [" way", "1.0"], [" to", "1.0"], [" prevent", "1.0"], [" heart", "1.0"], [" disease", "1.0"], [" is", "1.0"], [" to", "1.0"], [" exercise", "0.99999726"], [" regularly", "1.0"], [".", "1.0"], ["\n", "0.99655604"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" best", "1.0"], [" way", "1.0"], [" to", "1.0"], [" prevent", "1.0"], [" cancer", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" best", "1.0"], [" way", "1.0"], [" to", "1.0"], [" prevent", "1.0"], [" cancer", "1.0"], [" is", "1.0"], [" to", "1.0"], [" eat", "1.0"], [" a", "1.0"], [" healthy", "1.0"], [" diet", "1.0"], [".", "1.0"], [" The", "1.0"], [" best", "1.0"], [" way", "1.0"], [" to", "1.0"], [" prevent", "1.0"], [" cancer", "1.0"], [" is", "1.0"], [" to", "1.0"]]}, "1007": {"qid": "c92281b901ba7765c2e2", "term": "French toast", "description": "bread soaked in beaten eggs and then fried", "question": "Can a goat be used for one of the ingredients in French toast?", "answer": true, "facts": ["French toast is made from bread, eggs, milk, and cinnamon.", "Goats are able to produce milk, similar to cows.", "Goats milk is used in a variety of cheeses and milks sold in super markets."], "decomposition": ["What common dairy product can be obtained from goats?", "What are the typical ingredients of French toast?", "Is #1 included in #2?"], "evidence": [[[["Goat-46"]], [["French toast-1"]], ["operation"]], [[["Goat-1"]], [["French toast-2"]], ["operation"]], [[["Goat-46"]], [["French toast-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a cow and a goat?\nFacts: A cow is a female goat. A goat is a male goat.\nAnswer (True or False): True\nQuestion: What is the difference between a cow and a goat?\nFacts: A cow", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can a goat be used for one of the ingredients in French toast?\nFacts: French toast is made from bread, eggs, milk, and cinnamon. Goats are able to produce milk, similar to cows. Goats milk is used in a variety of cheeses and milks sold in super markets.\nAnswer (True or False):", "ct_subject": "French toast is made from bread, eggs, milk, and cinnamon. Goats are able to produce milk, similar to cows. Goats milk is used in a variety of cheeses and milks sold in super markets.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999317"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9866895"], [" between", "1.0"], [" a", "1.0"], [" cow", "0.8078046"], [" and", "1.0"], [" a", "1.0"], [" goat", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" cow", "1.0"], [" is", "1.0"], [" a", "1.0"], [" female", "1.0"], [" goat", "0.9999999"], [".", "1.0"], [" A", "0.9999796"], [" goat", "1.0"], [" is", "1.0"], [" a", "1.0"], [" male", "1.0"], [" goat", "1.0"], [".", "1.0"], ["\n", "0.99999976"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" cow", "1.0"], [" and", "1.0"], [" a", "1.0"], [" goat", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" cow", "1.0"]]}, "1008": {"qid": "59f7a8f6031615bfd3f7", "term": "Winter", "description": "one of the Earth's four temperate seasons, occurring between autumn and spring", "question": "Is Christmas celebrated during winter?", "answer": true, "facts": ["Winter begins around December 20.", "Christmas is celebrated on December 25, a few days later."], "decomposition": ["When is Christmas celebrated?", "When does winter begin in the US?", "When does winter end in the US?", "Is #1 between #2 and #3?"], "evidence": [[[["Christmas-1"]], [["Winter-8"]], [["Winter-8"]], ["operation"]], [[["Christmas-1"]], [["Winter-9"]], [["Winter-9"]], ["operation"]], [[["Christmas-28"]], [["Winter-9"], "no_evidence"], [["Winter-9"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is there a difference between the English language and the French language?\nFacts: The English language is the language of the English people. The French language is the language of the French people.\nAnswer (True or False): True\nQuestion: Is there a difference between the English language and the French language?\nFacts: The English language is the language of the English people. The French language is the language of the French people.\nAnswer (", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Christmas celebrated during winter?\nFacts: Winter begins around December 20. Christmas is celebrated on December 25, a few days later.\nAnswer (True or False):", "ct_subject": "Winter begins around December 20. Christmas is celebrated on December 25, a few days later.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99715185"], [" there", "0.5705843"], [" a", "1.0"], [" difference", "0.99999833"], [" between", "1.0"], [" the", "0.98421454"], [" English", "0.86239016"], [" language", "0.13447548"], [" and", "1.0"], [" the", "1.0"], [" French", "1.0"], [" language", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.29967785"], [" English", "1.0"], [" language", "1.0"], [" is", "1.0"], [" the", "0.9986952"], [" language", "0.99971265"], [" of", "0.9999999"], [" the", "1.0"], [" English", "1.0"], [" people", "0.6383992"], [".", "1.0"], [" The", "1.0"], [" French", "1.0"], [" language", "1.0"], [" is", "1.0"], [" the", "1.0"], [" language", "1.0"], [" of", "1.0"], [" the", "1.0"], [" French", "1.0"], [" people", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999964"], [" there", "1.0"], [" a", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" the", "1.0"], [" English", "1.0"], [" language", "1.0"], [" and", "1.0"], [" the", "1.0"], [" French", "1.0"], [" language", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" English", "1.0"], [" language", "1.0"], [" is", "1.0"], [" the", "1.0"], [" language", "1.0"], [" of", "1.0"], [" the", "1.0"], [" English", "1.0"], [" people", "1.0"], [".", "1.0"], [" The", "1.0"], [" French", "1.0"], [" language", "1.0"], [" is", "1.0"], [" the", "1.0"], [" language", "1.0"], [" of", "1.0"], [" the", "1.0"], [" French", "1.0"], [" people", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"]]}, "1009": {"qid": "d2e19ba0ffb3418fc545", "term": "Sun bear", "description": "bear found in tropical forest habitats of Southeast Asia", "question": "Do sun bears stay active during winter?", "answer": true, "facts": ["The sun bear is a species in the family Ursidae occurring in the tropical forests of Southeast Asia.", " Sun bears do not seem to hibernate.", "Hibernation is a seasonal heterothermy characterized by low body-temperature, slow breathing and heart-rate, and low metabolic rate. It most commonly occurs during winter months."], "decomposition": ["What characterizes the state of hibernation that some animals go into during winter?", "Are sun bears known to not exhibit the behavior described by #1?"], "evidence": [[[["Hibernation-3"]], [["Sun bear-2"], "operation"]], [[["Hibernation-1"]], [["Sun bear-2"], "operation"]], [[["Hibernation-1"]], [["Sun bear-11"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance a person has ever walked?\nFacts: The longest distance a person has ever walked is about 2,000 miles.\nAnswer (True or False): True\nQuestion", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do sun bears stay active during winter?\nFacts: The sun bear is a species in the family Ursidae occurring in the tropical forests of Southeast Asia.  Sun bears do not seem to hibernate. Hibernation is a seasonal heterothermy characterized by low body-temperature, slow breathing and heart-rate, and low metabolic rate. It most commonly occurs during winter months.\nAnswer (True or False):", "ct_subject": "The sun bear is a species in the family Ursidae occurring in the tropical forests of Southeast Asia.  Sun bears do not seem to hibernate. Hibernation is a seasonal heterothermy characterized by low body-temperature, slow breathing and heart-rate, and low metabolic rate. It most commonly occurs during winter months.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99994206"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.9998493"], [" distance", "0.9999311"], [" a", "1.0"], [" person", "0.61074674"], [" has", "0.9951959"], [" ever", "0.99885726"], [" walked", "0.9868629"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" person", "1.0"], [" has", "1.0"], [" ever", "1.0"], [" walked", "1.0"], [" is", "1.0"], [" about", "0.99999857"], [" 2", "0.9613428"], [",", "1.0"], ["000", "0.9999709"], [" miles", "1.0"], [".", "0.99597365"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"]]}, "1010": {"qid": "2325fcbd3c9d829d366e", "term": "Cape Town", "description": "Legislative capital of South Africa", "question": "Is Cape Town south of the Equator?", "answer": true, "facts": ["Cape Town is an important city in South Africa.", "South Africa is located entirely south of the Equator."], "decomposition": ["What country is Cape Town located in?", "Is #1 located south of the equator?"], "evidence": [[[["Cape Town-74"], "operation"], ["no_evidence"]], [[["Cape Town-3"]], [["Equator-4"]]], [[["Cape Town-1"]], [["South Africa-2"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the United States of America a country?\nFacts: The United States of America is a country.\nAnswer (True or False): True\nQuestion: Is the United States of America a state?\nFacts: The United States of America is a state.\nAnswer (True or False): True\nQuestion: Is the United States of America a territory?\nFacts: The United States of America is a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Cape Town south of the Equator?\nFacts: Cape Town is an important city in South Africa. South Africa is located entirely south of the Equator.\nAnswer (True or False):", "ct_subject": "Cape Town is an important city in South Africa. South Africa is located entirely south of the Equator.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99990726"], [" the", "0.9999999"], [" United", "0.9497766"], [" States", "1.0"], [" of", "0.9256418"], [" America", "1.0"], [" a", "1.0"], [" country", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "1.0"], [" States", "1.0"], [" of", "1.0"], [" America", "1.0"], [" is", "1.0"], [" a", "1.0"], [" country", "1.0"], [".", "1.0"], ["\n", "0.9992254"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" of", "1.0"], [" America", "1.0"], [" a", "1.0"], [" state", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "1.0"], [" States", "1.0"], [" of", "1.0"], [" America", "1.0"], [" is", "1.0"], [" a", "1.0"], [" state", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" of", "1.0"], [" America", "1.0"], [" a", "1.0"], [" territory", "0.9929448"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "1.0"], [" States", "1.0"], [" of", "1.0"], [" America", "1.0"], [" is", "1.0"], [" a", "1.0"]]}, "1011": {"qid": "5d4ae0455b8641ff0c03", "term": "Silicon", "description": "Chemical element with atomic number 14", "question": "Will silicon wedding rings outsell bromine wedding rings?", "answer": true, "facts": ["Wedding rings are typically made of precious shiny stones such as diamonds.", "Silicon is a solid rock like element at room temperature that has a natural lustre.", "Bromine is a liquid at room temperature that is toxic to the touch."], "decomposition": ["What state of matter is necessary for something to be worn as a ring at room temperature?", "What state of matter is bromine at room temperature?", "Is #2 the same as #1?", "If #3 is no, then a bromide wedding ring does not exist.", "What state of matter is silicon at room temperature? (see comment for #4)"], "evidence": [[[["Ring (jewellery)-1", "Solid-1"]], [["Bromine-1"]], ["operation"], ["operation"], [["Silicon-1"]]], [["no_evidence"], [["Bromine-1"]], ["operation"], ["no_evidence"], [["Silicon-1"]]], [[["Solid-1"]], [["Bromine-1"]], ["operation"], ["operation"], [["Silicon-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"flake\"?\nFacts: A snowflake is a single crystal that is formed when a cloud of water freezes. A \"flake\" is a single crystal that is formed when a cloud of water freezes.\nAnswer (True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Will silicon wedding rings outsell bromine wedding rings?\nFacts: Wedding rings are typically made of precious shiny stones such as diamonds. Silicon is a solid rock like element at room temperature that has a natural lustre. Bromine is a liquid at room temperature that is toxic to the touch.\nAnswer (True or False):", "ct_subject": "Wedding rings are typically made of precious shiny stones such as diamonds. Silicon is a solid rock like element at room temperature that has a natural lustre. Bromine is a liquid at room temperature that is toxic to the touch.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.999998"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9999298"], [" between", "1.0"], [" a", "0.9999999"], [" \"", "0.9999989"], ["s", "0.85184246"], ["now", "0.9889332"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["flake", "0.82863885"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" snow", "0.9998739"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "1.0"], [" crystal", "1.0"], [" that", "0.9989291"], [" is", "0.99987304"], [" formed", "0.9999844"], [" when", "0.9972892"], [" a", "0.9951015"], [" cloud", "0.94458103"], [" of", "1.0"], [" water", "0.99999917"], [" freezes", "0.8342717"], [".", "1.0"], [" A", "1.0"], [" \"", "0.99999976"], ["flake", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "0.9150263"], [" crystal", "1.0"], [" that", "1.0"], [" is", "1.0"], [" formed", "1.0"], [" when", "1.0"], [" a", "1.0"], [" cloud", "1.0"], [" of", "1.0"], [" water", "0.9997249"], [" freezes", "1.0"], [".", "0.9999999"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"]]}, "1012": {"qid": "a5775fcc073499d35da1", "term": "Amazonas (Brazilian state)", "description": "State of Brazil", "question": "Does walking across Amazonas put a person's life at risk?", "answer": true, "facts": ["Amazonas is mostly tropical jungle.", "Tropical jungles contain dangerous creatures.", "Dangerous creatures put people's lives at risk."], "decomposition": ["What is the Amazons made up of?", "Does #1 contain anything dangerous?", "Does #2 put people's life at risk?"], "evidence": [[[["Amazon River-1"]], [["Amazon River-75"], "no_evidence"], [["Piranha-16"], "operation"]], [[["Amazon River-19"]], [["Amazon River-76"]], [["Shark attack-1"]]], [[["Amazon basin-2"]], [["Amazon basin-10"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the river that runs through the Amazon?\nFacts: The Amazon River is the largest river in the world. It is the largest river in the world.\nAnswer (True or False): True\nQuestion: What is the name of the river that runs through the Amazon?\nFacts: The Amazon River is the largest river in the world. It is the largest", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does walking across Amazonas put a person's life at risk?\nFacts: Amazonas is mostly tropical jungle. Tropical jungles contain dangerous creatures. Dangerous creatures put people's lives at risk.\nAnswer (True or False):", "ct_subject": "Amazonas is mostly tropical jungle. Tropical jungles contain dangerous creatures. Dangerous creatures put people's lives at risk.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9382602"], [" is", "1.0"], [" the", "1.0"], [" name", "0.93905306"], [" of", "1.0"], [" the", "1.0"], [" river", "0.14944421"], [" that", "0.9999484"], [" runs", "0.9999999"], [" through", "1.0"], [" the", "1.0"], [" Amazon", "0.982385"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Amazon", "1.0"], [" River", "0.99999976"], [" is", "1.0"], [" the", "0.9840007"], [" largest", "1.0"], [" river", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [".", "1.0"], [" It", "0.9999999"], [" is", "1.0"], [" the", "0.99999976"], [" largest", "1.0"], [" river", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [".", "1.0"], ["\n", "0.9893497"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" river", "1.0"], [" that", "1.0"], [" runs", "1.0"], [" through", "1.0"], [" the", "1.0"], [" Amazon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Amazon", "1.0"], [" River", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" river", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [".", "1.0"], [" It", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"]]}, "1013": {"qid": "d439d1a70c443691145d", "term": "Daytona 500", "description": "Auto race held in Daytona, Florida, United States", "question": "Will electric car struggle to finish Daytona 500?", "answer": true, "facts": ["The Daytona 500 is a 2.5 mile long race.", "The Daytona 500 requires 200 laps to complete.", "The best electric car engines last around 390 miles."], "decomposition": ["How long (in miles) is the Daytona 500 race?", "What is the maximum electric range (in miles) of the world's best selling electric car?", "Is #2 less than #1?"], "evidence": [[[["Daytona 500-1"]], [["Tesla Model 3-1"]], ["operation"]], [[["Daytona 500-7"]], [["Electric car-20"]], ["operation"]], [[["Daytona 500-7"]], [["Electric car-3"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance a person has ever walked?\nFacts: The longest distance a person has ever walked is about 2,000 miles.\nAnswer (True or False): True\nQuestion: What is the longest distance a person has ever run?\nFacts: The longest distance a person has ever run is about 2,000 miles.\nAnswer (True or", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Will electric car struggle to finish Daytona 500?\nFacts: The Daytona 500 is a 2.5 mile long race. The Daytona 500 requires 200 laps to complete. The best electric car engines last around 390 miles.\nAnswer (True or False):", "ct_subject": "The Daytona 500 is a 2.5 mile long race. The Daytona 500 requires 200 laps to complete. The best electric car engines last around 390 miles.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99992335"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.98599386"], [" distance", "1.0"], [" a", "0.77367353"], [" person", "0.9296471"], [" has", "0.98703194"], [" ever", "0.9999347"], [" walked", "0.9999988"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" person", "1.0"], [" has", "1.0"], [" ever", "1.0"], [" walked", "1.0"], [" is", "1.0"], [" about", "0.8940431"], [" 2", "0.99912745"], [",", "1.0"], ["000", "1.0"], [" miles", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" person", "1.0"], [" has", "1.0"], [" ever", "1.0"], [" run", "0.9630057"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" person", "1.0"], [" has", "1.0"], [" ever", "1.0"], [" run", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 2", "0.99999535"], [",", "1.0"], ["000", "1.0"], [" miles", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"]]}, "1014": {"qid": "42b892d5dd158c0fd26a", "term": "Swastika", "description": "a geometrical figure and an ancient religious icon in the cultures of Eurasia and 20th-century symbol of Nazism", "question": "Are swastikas used in the most common religion in India?", "answer": true, "facts": ["The swastika is a religious symbol that is used in Hinduism, Buddhism, and Jainism.", "Almost 80% of people in India practice Hinduism."], "decomposition": ["Which religions use the swastika as a symbol?", "What is the most common religion in India?", "Is #2 included in #1?"], "evidence": [[[["Swastika-1"]], [["Hinduism in India-1"]], ["operation"]], [[["Swastika-1"]], [["Hinduism-1"]], ["operation"]], [[["Swastika-1"]], [["Religion in India-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there more people in the world today who are born with a birth defect than there were in the past?\nFacts: There are more people born with birth defects today than there were in the past.\nAnswer (True or False): True\nQuestion: Are there more people in the world today who are born with a birth defect than there were", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are swastikas used in the most common religion in India?\nFacts: The swastika is a religious symbol that is used in Hinduism, Buddhism, and Jainism. Almost 80% of people in India practice Hinduism.\nAnswer (True or False):", "ct_subject": "The swastika is a religious symbol that is used in Hinduism, Buddhism, and Jainism. Almost 80% of people in India practice Hinduism.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.99998295"], [" there", "0.99976844"], [" more", "0.5768251"], [" people", "1.0"], [" in", "0.9999832"], [" the", "1.0"], [" world", "1.0"], [" today", "1.0"], [" who", "0.3220249"], [" are", "0.99638367"], [" born", "0.30915734"], [" with", "0.9976803"], [" a", "0.9999925"], [" birth", "0.9998765"], [" defect", "0.9997849"], [" than", "1.0"], [" there", "0.9978806"], [" were", "1.0"], [" in", "1.0"], [" the", "1.0"], [" past", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" There", "0.9997969"], [" are", "1.0"], [" more", "1.0"], [" people", "1.0"], [" born", "0.99998295"], [" with", "1.0"], [" birth", "0.9999726"], [" defects", "1.0"], [" today", "1.0"], [" than", "1.0"], [" there", "1.0"], [" were", "1.0"], [" in", "1.0"], [" the", "1.0"], [" past", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "1.0"], [" more", "1.0"], [" people", "1.0"], [" in", "0.99996066"], [" the", "1.0"], [" world", "1.0"], [" today", "1.0"], [" who", "1.0"], [" are", "0.9999958"], [" born", "1.0"], [" with", "1.0"], [" a", "1.0"], [" birth", "1.0"], [" defect", "1.0"], [" than", "1.0"], [" there", "1.0"], [" were", "1.0"]]}, "1015": {"qid": "02f5a6e6eedc57dfa22d", "term": "Quantum mechanics", "description": "Branch of physics that acts as an abstract framework formulating all the laws of nature", "question": "Did Terry Pratchett write about quantum mechanics?", "answer": true, "facts": ["\u201cWhat're quantum mechanics?\"  \"I don't know. People who repair quantums, I suppose.\u201d", "\"Granny Weatherwax wouldn\u2019t know what a pattern of quantum inevitability was if she found it eating her dinner. If you mentioned the words \u2018paradigm of space-time\u2019 to her she\u2019d just say \u2018What?\u2019 But that didn\u2019t mean she was ignorant. It just meant that she didn\u2019t have truck with words, especially gibberish.\""], "decomposition": ["What was Terry Pratchett's occupation?", "Which subject deals with the study of quantum mechanics?", "Is #2 a part of #1's job?"], "evidence": [[[["Terry Pratchett-1"]], [["Quantum mechanics-1"]], ["no_evidence"]], [[["Terry Pratchett-1"]], [["Quantum mechanics-1"]], ["operation"]], [[["The Science of Discworld-1"]], [["Quantum mechanics-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Terry Pratchett write about quantum mechanics?\nFacts: \u201cWhat're quantum mechanics?\" \"I don't know. People who repair quantums, I suppose.\u201d \"Granny Weatherwax wouldn\u2019t know what a pattern of quantum inevitability was if she found it eating her dinner. If you mentioned the words \u2018paradigm of space-time\u2019 to her she\u2019d just say \u2018What?\u2019 But that didn\u2019t mean she was ignorant. It just meant that she didn\u2019t have truck with words, especially gibberish.\"\nAnswer (True or False):", "ct_subject": "\u201cWhat're quantum mechanics?\" \"I don't know. People who repair quantums, I suppose.\u201d \"Granny Weatherwax wouldn\u2019t know what a pattern of quantum inevitability was if she found it eating her dinner. If you mentioned the words \u2018paradigm of space-time\u2019 to her she\u2019d just say \u2018What?\u2019 But that didn\u2019t mean she was ignorant. It just meant that she didn\u2019t have truck with words, especially gibberish.\"", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"]]}, "1016": {"qid": "ee3000c7feb6bfb4ef2f", "term": "Final Fantasy VI", "description": "1994 video game", "question": "Does Final Fantasy VI require electricity to play?", "answer": true, "facts": ["Final Fantasy VI is a video game.", "Video games are played using a video game console and television.", "Video game consoles and televisions require electricity in order to function."], "decomposition": ["Which device(s) would be needed to play the video game Final Fantasy VI?", "Do any of #1 run on electricity?"], "evidence": [[[["Final Fantasy VI-2"]], [["Super Nintendo Entertainment System-22"]]], [[["Final Fantasy VI-1", "Super Nintendo Entertainment System-1"]], ["operation"]], [[["Final Fantasy VI-1"]], [["Video game console-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the character in the movie \"The Princess Bride\"?\nFacts: The Princess Bride is a book by William Goldman. The book is about a man named Buttercup who marries a woman named Buttercup. The book is about a man named Buttercup who marries a woman named Buttercup.\nAnswer (True or False): True\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Final Fantasy VI require electricity to play?\nFacts: Final Fantasy VI is a video game. Video games are played using a video game console and television. Video game consoles and televisions require electricity in order to function.\nAnswer (True or False):", "ct_subject": "Final Fantasy VI is a video game. Video games are played using a video game console and television. Video game consoles and televisions require electricity in order to function.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9990656"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99999976"], [" of", "1.0"], [" the", "1.0"], [" character", "0.99999976"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "0.9999999"], ["The", "1.0"], [" Princess", "0.9627441"], [" Bride", "1.0"], ["\"?", "0.5311106"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Princess", "0.9912487"], [" Bride", "1.0"], [" is", "1.0"], [" a", "1.0"], [" book", "0.48417827"], [" by", "0.9999956"], [" William", "1.0"], [" Goldman", "1.0"], [".", "1.0"], [" The", "0.9999738"], [" book", "0.9999975"], [" is", "1.0"], [" about", "0.9977951"], [" a", "1.0"], [" man", "0.9295144"], [" named", "0.914213"], [" Butter", "1.0"], ["cup", "1.0"], [" who", "0.9575466"], [" mar", "0.88261133"], ["ries", "1.0"], [" a", "1.0"], [" woman", "0.9995796"], [" named", "1.0"], [" Butter", "0.99982077"], ["cup", "1.0"], [".", "1.0"], [" The", "1.0"], [" book", "0.999995"], [" is", "1.0"], [" about", "0.99999833"], [" a", "1.0"], [" man", "1.0"], [" named", "1.0"], [" Butter", "1.0"], ["cup", "1.0"], [" who", "1.0"], [" mar", "1.0"], ["ries", "1.0"], [" a", "1.0"], [" woman", "1.0"], [" named", "1.0"], [" Butter", "1.0"], ["cup", "1.0"], [".", "1.0"], ["\n", "0.99999404"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"]]}, "1017": {"qid": "1647d6f49307d7678d18", "term": "Anorexia nervosa", "description": "Eating disorder characterized by refusal to maintain a healthy body weight, and fear of gaining weight due to a distorted self image", "question": "Would a person with Anorexia nervosa be more likely to break a bone than a regular person?", "answer": true, "facts": ["People with Anorexia Nervosa restrict food and as a result lack essential nutrients.", "Many people with Anorexia Nervosa, are at high risk for osteoporosis(and to a lesser extent bulimia nervosa) will have low bone density and consequently reduced bone strength.", "People with Anorexia Nervosa, are at high risk for osteoporosis. "], "decomposition": ["What kind of eating behavior do people with anorexia nervosa exhibit?", "Does #1 lead to reduced bone strength?"], "evidence": [[[["Anorexia nervosa-1"]], [["Osteoporosis-1"], "operation"]], [[["Anorexia nervosa-1"]], [["Anorexia nervosa-54"]]], [[["Anorexia nervosa-4"]], [["Malnutrition-3"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human being?\nFacts: The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would a person with Anorexia nervosa be more likely to break a bone than a regular person?\nFacts: People with Anorexia Nervosa restrict food and as a result lack essential nutrients. Many people with Anorexia Nervosa, are at high risk for osteoporosis(and to a lesser extent bulimia nervosa) will have low bone density and consequently reduced bone strength. People with Anorexia Nervosa, are at high risk for osteoporosis. \nAnswer (True or False):", "ct_subject": "People with Anorexia Nervosa restrict food and as a result lack essential nutrients. Many people with Anorexia Nervosa, are at high risk for osteoporosis(and to a lesser extent bulimia nervosa) will have low bone density and consequently reduced bone strength. People with Anorexia Nervosa, are at high risk for osteoporosis. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99145645"], [" is", "1.0"], [" the", "1.0"], [" average", "0.99999404"], [" weight", "0.8418836"], [" of", "1.0"], [" a", "1.0"], [" human", "0.9958021"], [" being", "0.9999995"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"]]}, "1018": {"qid": "44875993b96eb08c70b4", "term": "Jennifer Lawrence", "description": "American actress", "question": "Is Jennifer Lawrence's middle name similar to the name of a Scorsese collaborator?", "answer": true, "facts": ["Jennifer Lawrence's middle name is Shrader.", "Paul Schrader is a screenwriter and director.", "Paul Schrader wrote the screenplay for Taxi Driver, Raging Bull, The Last Temptation of Christ, and Bringing Out the Dead.", "Martin Scorsese directed Taxi Driver, Raging Bull, The Last Temptation of Christ, and Bringing Out the Dead."], "decomposition": ["What is Jennifer Lawrence's middle name?", "Who has collaborated with Scorsese?", "Does #2 include someone with #1 in their name?"], "evidence": [[[["Jennifer Lawrence-1"]], [["Paul Schrader-1"]], ["operation"]], [[["Jennifer Lawrence-1"]], [["Paul Schrader-1"], "no_evidence"], ["operation"]], [[["Jennifer Lawrence-1"]], [["Paul Schrader-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the character in the movie The Wolf of Wall Street the same as the name of a famous actor?\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Jennifer Lawrence's middle name similar to the name of a Scorsese collaborator?\nFacts: Jennifer Lawrence's middle name is Shrader. Paul Schrader is a screenwriter and director. Paul Schrader wrote the screenplay for Taxi Driver, Raging Bull, The Last Temptation of Christ, and Bringing Out the Dead. Martin Scorsese directed Taxi Driver, Raging Bull, The Last Temptation of Christ, and Bringing Out the Dead.\nAnswer (True or False):", "ct_subject": "Jennifer Lawrence's middle name is Shrader. Paul Schrader is a screenwriter and director. Paul Schrader wrote the screenplay for Taxi Driver, Raging Bull, The Last Temptation of Christ, and Bringing Out the Dead. Martin Scorsese directed Taxi Driver, Raging Bull, The Last Temptation of Christ, and Bringing Out the Dead.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999999"], [" the", "0.9999995"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" character", "0.99928325"], [" in", "1.0"], [" the", "0.9999987"], [" movie", "1.0"], [" The", "0.32864052"], [" Wolf", "0.63698167"], [" of", "1.0"], [" Wall", "1.0"], [" Street", "1.0"], [" the", "1.0"], [" same", "1.0"], [" as", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" a", "0.9999999"], [" famous", "1.0"], [" actor", "0.9967919"], ["?", "1.0"], ["\n", "1.0"]]}, "1019": {"qid": "7e3bd1602e5d32ab1e59", "term": "Ten-pin bowling", "description": "sport", "question": "Can a ten-pin bowling pin be a deadly weapon?", "answer": true, "facts": ["A regulation ten-pin bowling pin weighs around four pounds.", "The average rolling pin used in cooking weighs slighty over four pounds.", "A 2015 case covered in the BBC involved a rolling pin as the murder weapon."], "decomposition": ["How much does a ten-pin bowling pin weigh?", "What kind of pin has been used as a murder weapon?", "How much does #2 weigh?", "Is #3 roughly the same as #1?"], "evidence": [[[["Bowling pin-2"]], [["Sammy White's Brighton Bowl-3"]], [["Candlepin bowling-11"]], ["operation"]], [[["Bowling pin-2"]], [["Firing pin-1"]], [["Lock time-4"], "no_evidence"], [["Bowling pin-2"]]], [[["Bowling pin-2"]], ["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]], [[["Bowling pin-2"]], [["Rolling pin-3"], "no_evidence"], [["Rolling pin-2"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human being?\nFacts: The average human being weighs around 100 pounds.\nAnswer (True or False): True\nQuestion: What is the average weight of a human being?\nFacts: The average human being weighs around 100 pounds.\nAnswer (True or", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can a ten-pin bowling pin be a deadly weapon?\nFacts: A regulation ten-pin bowling pin weighs around four pounds. The average rolling pin used in cooking weighs slighty over four pounds. A 2015 case covered in the BBC involved a rolling pin as the murder weapon.\nAnswer (True or False):", "ct_subject": "A regulation ten-pin bowling pin weighs around four pounds. The average rolling pin used in cooking weighs slighty over four pounds. A 2015 case covered in the BBC involved a rolling pin as the murder weapon.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99988234"], [" is", "1.0"], [" the", "1.0"], [" average", "0.9125409"], [" weight", "0.9985777"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "0.31679726"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.9999924"], [" being", "0.99999726"], [" weighs", "0.9989159"], [" around", "0.9983943"], [" 100", "0.47919083"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "1.0"], [" of", "1.0"], [" a", "1.0"], [" human", "0.9999206"], [" being", "1.0"], ["?", "0.9998585"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "1.0"], [" being", "1.0"], [" weighs", "1.0"], [" around", "1.0"], [" 100", "1.0"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"]]}, "1020": {"qid": "f8cc260e3863b0be4b81", "term": "Hypothermia", "description": "A human body core temperature below 35.0\u00b0C", "question": "Would you be more likely to die of hypothermia in New York than Florida?", "answer": true, "facts": ["Central New York Winters are between 12-30 degrees Fahrenheit.", "Florida winters are between 65 and 77 degrees Fahrenheit."], "decomposition": ["What is the typical temperature range of the coldest time of the year in New York?", "What is the typical temperature range of the coldest time of the year in Florida?", "Is #1 lower than #2?"], "evidence": [[[["New York City-62"], "no_evidence"], [["Climate of Florida-7", "Climate of Florida-8"], "no_evidence"], ["operation"]], [[["New York (state)-43"]], [["Florida-45"]], ["operation"]], [[["New York (state)-43"]], [["Geography of Florida-5"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human being?\nFacts: The average human being weighs about 100 pounds.\nAnswer (True or False): True\nQuestion: What is the average height of a human being?\nFacts: The average human being is 5'9\" tall.\nAnswer (True or False): True\nQuestion: What is the average weight of", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would you be more likely to die of hypothermia in New York than Florida?\nFacts: Central New York Winters are between 12-30 degrees Fahrenheit. Florida winters are between 65 and 77 degrees Fahrenheit.\nAnswer (True or False):", "ct_subject": "Central New York Winters are between 12-30 degrees Fahrenheit. Florida winters are between 65 and 77 degrees Fahrenheit.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9998841"], [" is", "1.0"], [" the", "1.0"], [" average", "0.99994254"], [" weight", "0.9987086"], [" of", "1.0"], [" a", "0.9999999"], [" human", "0.99964607"], [" being", "0.99998605"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999788"], [" average", "1.0"], [" human", "0.9939604"], [" being", "0.99984705"], [" weighs", "0.99999917"], [" about", "0.9864093"], [" 100", "0.9945147"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" height", "0.9999174"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], [" being", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.99998367"], [" being", "1.0"], [" is", "1.0"], [" 5", "0.66471905"], ["'", "0.5759607"], ["9", "0.42436874"], ["\"", "0.99400985"], [" tall", "0.99889827"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "1.0"], [" of", "1.0"]]}, "1021": {"qid": "5f1b1891c28dd8ef8bcf", "term": "Cuisine of Hawaii", "description": "Cuisine of Hawaii", "question": "Does the cuisine of Hawaii embrace foods considered gross in the continental US?", "answer": true, "facts": ["SPAM has a reputation for being an unpleasant dish in the continental US.", "SPAM is so popular in Hawaii that it has made it onto McDonalds menus. "], "decomposition": ["Does SPAM have a reputation as an unpleasant dish in the continental US?", "Does SPAM have a reputation as a pleasant dish in Hawaii?", "Are #1 and #2 positive?"], "evidence": [[[["Spam (food)-34"]], [["Spam (food)-43"]], [["Spam (food)-34", "Spam (food)-43"]]], [[["Spam (food)-34", "Spam (food)-8"]], [["Spam (food)-10"]], ["operation"]], [[["Spam (food)-8"]], [["Spam (food)-10"]], [["Spam (food)-10"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the city of New York City derived from the name of the city of New York?\nFacts: The name of the city of New York is derived from the name of the city of New York. The name of the city of New York is derived from the name of the city of New York.\nAnswer (True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does the cuisine of Hawaii embrace foods considered gross in the continental US?\nFacts: SPAM has a reputation for being an unpleasant dish in the continental US. SPAM is so popular in Hawaii that it has made it onto McDonalds menus. \nAnswer (True or False):", "ct_subject": "SPAM has a reputation for being an unpleasant dish in the continental US. SPAM is so popular in Hawaii that it has made it onto McDonalds menus. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9900697"], [" the", "0.999928"], [" name", "0.9999987"], [" of", "1.0"], [" the", "1.0"], [" city", "0.99999976"], [" of", "0.999995"], [" New", "0.99919695"], [" York", "1.0"], [" City", "0.99986315"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" name", "0.99997723"], [" of", "1.0"], [" the", "0.99999976"], [" city", "1.0"], [" of", "1.0"], [" New", "0.9999994"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99999976"], [" name", "1.0"], [" of", "1.0"], [" the", "0.99355185"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" is", "0.9997874"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [".", "1.0"], [" The", "0.9596429"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [" is", "0.99999726"], [" derived", "1.0"], [" from", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [".", "1.0"], ["\n", "0.9482055"], ["Answer", "0.9954898"], [" (", "1.0"], ["True", "1.0"]]}, "1022": {"qid": "dc06eea2fe093bed09df", "term": "Fibonacci number", "description": "integer in the infinite Fibonacci sequence", "question": "Is the Fibonacci number sequence longer than every number discovered in Pi?", "answer": true, "facts": ["The Fibonaacci number is a sequence of numbers that adds a number to the one before it and goes on forever.", "Pi is a sequence of numbers and 2.7 trillion digits were discovered in 2010."], "decomposition": ["How many numbers are in Pi?", "How many numbers are in the Fibonacci number sequence?", "Is #2 larger than #1?"], "evidence": [[[["Pi-2"], "no_evidence"], [["Fibonacci prime-3"], "no_evidence"], ["no_evidence", "operation"]], [[["Piphilology-65"]], [["Fibonacci-12"], "no_evidence"], ["operation"]], [[["Pi-2"]], [["Random Fibonacci sequence-4"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the Fibonacci number sequence longer than every number discovered in Pi?\nFacts: The Fibonaacci number is a sequence of numbers that adds a number to the one before it and goes on forever. Pi is a sequence of numbers and 2.7 trillion digits were discovered in 2010.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is the Fibonacci number sequence longer than every number discovered in Pi?\nFacts: The Fibonaacci number is a sequence of numbers that adds a number to the one before it and goes on forever. Pi is a sequence of numbers and 2.7 trillion digits were discovered in 2010.\nAnswer (True or False):", "ct_subject": "The Fibonaacci number is a sequence of numbers that adds a number to the one before it and goes on forever. Pi is a sequence of numbers and 2.7 trillion digits were discovered in 2010.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9999753"], [" the", "0.99997973"], [" Fib", "0.9450328"], ["on", "1.0"], ["acci", "1.0"], [" number", "1.0"], [" sequence", "1.0"], [" longer", "1.0"], [" than", "1.0"], [" every", "1.0"], [" number", "1.0"], [" discovered", "1.0"], [" in", "1.0"], [" Pi", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Fib", "1.0"], ["ona", "1.0"], ["acci", "1.0"], [" number", "1.0"], [" is", "1.0"], [" a", "1.0"], [" sequence", "1.0"], [" of", "1.0"], [" numbers", "1.0"], [" that", "1.0"], [" adds", "1.0"], [" a", "1.0"], [" number", "1.0"], [" to", "1.0"], [" the", "1.0"], [" one", "1.0"], [" before", "1.0"], [" it", "1.0"], [" and", "1.0"], [" goes", "1.0"], [" on", "1.0"], [" forever", "1.0"], [".", "1.0"], [" Pi", "1.0"], [" is", "1.0"], [" a", "1.0"], [" sequence", "1.0"], [" of", "1.0"], [" numbers", "1.0"], [" and", "1.0"], [" 2", "1.0"], [".", "1.0"], ["7", "1.0"], [" trillion", "1.0"], [" digits", "1.0"], [" were", "1.0"], [" discovered", "1.0"], [" in", "1.0"], [" 2010", "1.0"], [".", "1.0"]]}, "1023": {"qid": "90e003dc429517ba3ebc", "term": "Lionel Richie", "description": "American singer-songwriter, musician, record producer and actor", "question": "Does  Lionel Richie believe in holistic medicine?", "answer": true, "facts": ["Lionel Richie suffered prolonged throat problems and had surgery four times in four years before being told by conventional doctors that he could lose his singing career. ", "Lionel Richie finally turned to a holistic doctor who said that the problem was simply acid reflux caused by foods he was eating before going to bed."], "decomposition": ["Which doctor diagnosed Lionel Richie satisfactorily after he had surgeries for a prolonged throat problem?", "Is #1 a holistic doctor?"], "evidence": [[["no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], ["operation"]], [["no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the moon made of cheese?\nFacts: The moon is made of cheese.\nAnswer (True or False): True\nQuestion: Is the moon made of cheese?\nFacts: The moon is made of cheese.\nAnswer", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does  Lionel Richie believe in holistic medicine?\nFacts: Lionel Richie suffered prolonged throat problems and had surgery four times in four years before being told by conventional doctors that he could lose his singing career.  Lionel Richie finally turned to a holistic doctor who said that the problem was simply acid reflux caused by foods he was eating before going to bed.\nAnswer (True or False):", "ct_subject": "Lionel Richie suffered prolonged throat problems and had surgery four times in four years before being told by conventional doctors that he could lose his singing career.  Lionel Richie finally turned to a holistic doctor who said that the problem was simply acid reflux caused by foods he was eating before going to bed.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9962274"], [" the", "0.97409815"], [" moon", "0.54077816"], [" made", "0.99999976"], [" of", "1.0"], [" cheese", "0.9999999"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" moon", "1.0"], [" is", "1.0"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" moon", "0.9999044"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" moon", "1.0"], [" is", "1.0"], [" made", "1.0"], [" of", "1.0"], [" cheese", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"]]}, "1024": {"qid": "3502d54b8241e79903a9", "term": "Missionary", "description": "member of a religious group sent into an area to do evangelism", "question": "Were the first missionaries required to attend mass on Sundays?", "answer": true, "facts": ["The word \"mission\" originates from 1598 when the Jesuits sent members abroad.", "Jesuits are a Roman Catholic order of religious men", "The Roman Catholic religion requires members to attend mass on Sundays"], "decomposition": ["What religion were the first missionaries?", "Does #1 require mass attendance?"], "evidence": [[[["Missionary-4"]], [["Mass (liturgy)-1", "Mass (liturgy)-13"], "operation"]], [[["Missionary-4"]], [["Eucharist in the Catholic Church-76"], "no_evidence"]], [[["Guadalupe Missionaries-1"]], [["Mass (liturgy)-13"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"sister\" and a \"sister-in-law\"?\nFacts: A \"sister\" is a woman who is married to another woman. A \"sister-in-law\" is a woman who is married to another man.\nAnswer (True or False): True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Were the first missionaries required to attend mass on Sundays?\nFacts: The word \"mission\" originates from 1598 when the Jesuits sent members abroad. Jesuits are a Roman Catholic order of religious men The Roman Catholic religion requires members to attend mass on Sundays\nAnswer (True or False):", "ct_subject": "The word \"mission\" originates from 1598 when the Jesuits sent members abroad. Jesuits are a Roman Catholic order of religious men The Roman Catholic religion requires members to attend mass on Sundays", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.96990526"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.8451148"], [" between", "1.0"], [" a", "0.9998802"], [" \"", "1.0"], ["s", "0.738867"], ["ister", "0.9995578"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.9743251"], ["ister", "1.0"], ["-", "1.0"], ["in", "1.0"], ["-", "1.0"], ["law", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999256"], [" \"", "0.9999932"], ["s", "1.0"], ["ister", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" woman", "0.99999523"], [" who", "1.0"], [" is", "1.0"], [" married", "1.0"], [" to", "1.0"], [" another", "0.6035303"], [" woman", "1.0"], [".", "1.0"], [" A", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["ister", "1.0"], ["-", "1.0"], ["in", "1.0"], ["-", "1.0"], ["law", "1.0"], ["\"", "1.0"], [" is", "1.0"], [" a", "1.0"], [" woman", "1.0"], [" who", "1.0"], [" is", "1.0"], [" married", "1.0"], [" to", "1.0"], [" another", "1.0"], [" man", "0.54880947"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"]]}, "1025": {"qid": "0ea87e725225e7ce5c51", "term": "Christina Aguilera", "description": "American singer, songwriter, actress, and television personality", "question": "Was Christina Aguilera born in the forgotten borough?", "answer": true, "facts": ["Christina Maria Aguilera was born on December 18, 1980, in Staten Island, New York.", "Staten Island has sometimes been called \"the forgotten borough\" by inhabitants who feel neglected by the city government."], "decomposition": ["Where was Christina Aguilera born?", "What place is known as the forgotten borough?", "Is #1 the same as #2?"], "evidence": [[[["Christina Aguilera-4"]], [["Staten Island-2"]], ["operation"]], [[["Christina Aguilera-4"]], [["Staten Island-2"]], ["operation"]], [[["Christina Aguilera-2"]], [["Staten Island-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city of New York?\nFacts: New York City is the largest city in the United States. It is located in the state of New York.\nAnswer (True or False): True\nQuestion: What is the name of the city of New York?\nFacts: New", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Christina Aguilera born in the forgotten borough?\nFacts: Christina Maria Aguilera was born on December 18, 1980, in Staten Island, New York. Staten Island has sometimes been called \"the forgotten borough\" by inhabitants who feel neglected by the city government.\nAnswer (True or False):", "ct_subject": "Christina Maria Aguilera was born on December 18, 1980, in Staten Island, New York. Staten Island has sometimes been called \"the forgotten borough\" by inhabitants who feel neglected by the city government.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9696511"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999964"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "0.9996197"], [" New", "0.99996734"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" New", "1.0"], [" York", "1.0"], [" City", "0.9999844"], [" is", "1.0"], [" the", "1.0"], [" largest", "0.9999505"], [" city", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [".", "0.99999774"], [" It", "0.9997054"], [" is", "1.0"], [" located", "0.99999976"], [" in", "1.0"], [" the", "0.87947065"], [" state", "1.0"], [" of", "1.0"], [" New", "1.0"], [" York", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "0.88661075"], [" of", "1.0"], [" New", "0.9999999"], [" York", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" New", "1.0"]]}, "1026": {"qid": "29cbf28f323d10a53c4d", "term": "Heart failure", "description": "condition in which the heart is unable to provide sufficient pump action", "question": "Would ramen be bad for someone with heart failure?", "answer": true, "facts": ["People with heart failure have to limit their sodium intake.", "Ramen is notorious for having incredibly high sodium levels. "], "decomposition": ["What is the recommended maximum daily sodium allowance for someone with heart failure?", "How much sodium is in a bowl of ramen?", "Is #2 greater than #1 divided by three?"], "evidence": [[[["Reference Daily Intake-15"], "no_evidence"], [["Ramen-11"], "no_evidence"], ["operation"]], [[["Reference Daily Intake-14"], "no_evidence"], [["Instant noodle-23"], "no_evidence"], ["no_evidence", "operation"]], [[["Sodium-35"], "no_evidence"], [["Instant noodle-12"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the best way to get rid of a bad smell?\nFacts: The best way to get rid of a bad smell is to use a deodorant.\nAnswer (True or False): True\nQuestion: What is the best way to get rid of a bad smell?\nFacts: The best way to get rid of a bad smell is to use a deodorant.\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would ramen be bad for someone with heart failure?\nFacts: People with heart failure have to limit their sodium intake. Ramen is notorious for having incredibly high sodium levels. \nAnswer (True or False):", "ct_subject": "People with heart failure have to limit their sodium intake. Ramen is notorious for having incredibly high sodium levels. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999472"], [" is", "1.0"], [" the", "1.0"], [" best", "0.15334953"], [" way", "1.0"], [" to", "1.0"], [" get", "0.9999895"], [" rid", "0.9999176"], [" of", "1.0"], [" a", "0.9999999"], [" bad", "1.0"], [" smell", "0.9764306"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" best", "1.0"], [" way", "1.0"], [" to", "1.0"], [" get", "1.0"], [" rid", "1.0"], [" of", "1.0"], [" a", "1.0"], [" bad", "1.0"], [" smell", "1.0"], [" is", "1.0"], [" to", "1.0"], [" use", "0.99955446"], [" a", "1.0"], [" de", "0.9817702"], ["odor", "1.0"], ["ant", "1.0"], [".", "1.0"], ["\n", "0.97308546"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" best", "1.0"], [" way", "1.0"], [" to", "1.0"], [" get", "1.0"], [" rid", "1.0"], [" of", "1.0"], [" a", "1.0"], [" bad", "1.0"], [" smell", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" best", "1.0"], [" way", "1.0"], [" to", "1.0"], [" get", "1.0"], [" rid", "1.0"], [" of", "1.0"], [" a", "1.0"], [" bad", "1.0"], [" smell", "1.0"], [" is", "1.0"], [" to", "1.0"], [" use", "1.0"], [" a", "1.0"], [" de", "1.0"], ["odor", "1.0"], ["ant", "1.0"], [".", "1.0"], ["\n", "1.0"]]}, "1027": {"qid": "30a4396aa0743165d8d6", "term": "Ohio University", "description": "public university in Athens, Ohio, United States", "question": "Would the current president of Ohio University hypothetically wear a jockstrap?", "answer": true, "facts": ["The current president of Ohio University is Duane Nellis.", "Duane Nellis is a man.", "A jockstrap is an undergarment for protecting the testes and penis during cycling, contact sports or other vigorous physical activity.", "The testes and penis are the sexual organs of men."], "decomposition": ["Which gender wears jockstrap?", "Who is the current President of Ohio University?", "Does #2 identify with the gender #1?"], "evidence": [[[["Jockstrap-1"]], [["Duane Nellis-1"]], ["operation"]], [[["Jockstrap-1"]], [["Kristina M. Johnson-1"], "no_evidence"], ["operation"]], [[["Thong-27"]], [["Duane Nellis-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the year 2000?\nFacts: The first person to be born in the year 2000 was a man named John Smith. John Smith was", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would the current president of Ohio University hypothetically wear a jockstrap?\nFacts: The current president of Ohio University is Duane Nellis. Duane Nellis is a man. A jockstrap is an undergarment for protecting the testes and penis during cycling, contact sports or other vigorous physical activity. The testes and penis are the sexual organs of men.\nAnswer (True or False):", "ct_subject": "The current president of Ohio University is Duane Nellis. Duane Nellis is a man. A jockstrap is an undergarment for protecting the testes and penis during cycling, contact sports or other vigorous physical activity. The testes and penis are the sexual organs of men.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9984647"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9975368"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9997993"], [" person", "0.9243847"], [" to", "1.0"], [" be", "0.9879239"], [" born", "0.18718712"], [" in", "1.0"], [" the", "1.0"], [" year", "0.9770747"], [" 2000", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], [" was", "1.0"], [" a", "0.7663509"], [" man", "0.97527486"], [" named", "1.0"], [" John", "0.99977297"], [" Smith", "0.89467156"], [".", "1.0"], [" John", "0.99999917"], [" Smith", "1.0"], [" was", "0.99999654"]]}, "1028": {"qid": "d9b4f68792ef86249d18", "term": "Wednesday", "description": "Day of the week", "question": "Did Wednesday have something to do with Thor?", "answer": true, "facts": ["Wednesday is the middle of the modern work week and comes from the name Wodan.", "The Germanic god Woden is also known as Wodanaz or Odin.", "Odin, in Norse mythology, was the father of Thor."], "decomposition": ["Which Germanic god is the name 'Wednesday' etymologically related to?", "Is #1 related to Thor?"], "evidence": [[[["Wednesday-5"]], [["Odin-4"]]], [[["Wednesday-1"]], [["Thor-3"], "operation"]], [[["Wednesday-5"]], [["Odin-4"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first man to walk on the moon?\nFacts: The first man to walk on the moon was Neil Armstrong.\nAnswer (True or False): True\nQuestion: What is the name of the first man to walk on the moon?\nFacts: The first", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Wednesday have something to do with Thor?\nFacts: Wednesday is the middle of the modern work week and comes from the name Wodan. The Germanic god Woden is also known as Wodanaz or Odin. Odin, in Norse mythology, was the father of Thor.\nAnswer (True or False):", "ct_subject": "Wednesday is the middle of the modern work week and comes from the name Wodan. The Germanic god Woden is also known as Wodanaz or Odin. Odin, in Norse mythology, was the father of Thor.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9952821"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9729806"], [" man", "0.9535007"], [" to", "0.86296517"], [" walk", "0.9864231"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9979887"], [" first", "0.9999918"], [" man", "1.0"], [" to", "1.0"], [" walk", "1.0"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], [" was", "1.0"], [" Neil", "0.9999912"], [" Armstrong", "1.0"], [".", "1.0"], ["\n", "0.99869883"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" walk", "0.9993383"], [" on", "1.0"], [" the", "1.0"], [" moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" first", "1.0"]]}, "1029": {"qid": "a1dabe439511af470303", "term": "Lil Wayne", "description": "American rapper, record executive and actor from Louisiana", "question": "Will AC/DC album sales buy more B-52 bombers than Lil Wayne's?", "answer": true, "facts": ["The B-52 bomber plane cost 60 million dollars in 2018.", "AC/DC has sold over 200 million albums.", "Lil Wayne has sold 120 million records worldwide."], "decomposition": ["How much does one B-52 bomber cost?", "How much is AC/DC worth due to the sales of their albums?", "Lil Wayne has made how much from his album sales?", "Is #2 more than both #1 and #3?"], "evidence": [[[["Boeing B-52 Stratofortress-6"], "no_evidence"], [["AC/DC-5"], "no_evidence"], [["Lil Wayne-4"], "no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], [["AC/DC-5"], "no_evidence"], ["no_evidence"], ["operation"]], [[["Bomber B-5"], "no_evidence"], [["AC/DC-2"], "no_evidence"], [["Lil Wayne-3"], "operation"], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the most expensive item in the world?\nFacts: The most expensive item in the world is the $1,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Will AC/DC album sales buy more B-52 bombers than Lil Wayne's?\nFacts: The B-52 bomber plane cost 60 million dollars in 2018. AC/DC has sold over 200 million albums. Lil Wayne has sold 120 million records worldwide.\nAnswer (True or False):", "ct_subject": "The B-52 bomber plane cost 60 million dollars in 2018. AC/DC has sold over 200 million albums. Lil Wayne has sold 120 million records worldwide.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99982053"], [" is", "1.0"], [" the", "1.0"], [" most", "0.88833785"], [" expensive", "0.99999833"], [" item", "0.9992843"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" most", "1.0"], [" expensive", "1.0"], [" item", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [" is", "1.0"], [" the", "1.0"], [" $", "0.9916325"], ["1", "0.98945075"], [",", "0.6156259"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"], [",", "1.0"], ["000", "1.0"]]}, "1030": {"qid": "e0bfafb25cde72664a3a", "term": "Larry King", "description": "American television and radio host", "question": "Could Larry King's marriages be counted on two feet?", "answer": true, "facts": ["The typical person has 10 toes spread across their two feet.", "Larry King has been married 8 times.", "You can count each marriage on each toe."], "decomposition": ["How many times has Larry King been married?", "How many toes do most people have?", "Is #2 at least as much as #1?"], "evidence": [[[["Larry King-37"]], [["Toe-2"]], ["operation"]], [[["Larry King-43"]], [["Toe-2"]], ["operation"]], [[["Larry King-37"]], [["Toe-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average weight of a human?\nFacts: The average human weighs about 100 pounds.\nAnswer (True or False): True\nQuestion: What is the average height of a human?\nFacts: The average human height is 5 feet, 9 inches.\nAnswer (True or False): True\nQuestion: What is the average weight of a human?", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could Larry King's marriages be counted on two feet?\nFacts: The typical person has 10 toes spread across their two feet. Larry King has been married 8 times. You can count each marriage on each toe.\nAnswer (True or False):", "ct_subject": "The typical person has 10 toes spread across their two feet. Larry King has been married 8 times. You can count each marriage on each toe.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9995988"], [" is", "1.0"], [" the", "1.0"], [" average", "0.99998105"], [" weight", "0.99318403"], [" of", "1.0"], [" a", "1.0"], [" human", "0.9997737"], ["?", "0.9838733"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "0.99995923"], [" weighs", "0.40646666"], [" about", "0.99999964"], [" 100", "0.9972421"], [" pounds", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" height", "0.9999989"], [" of", "1.0"], [" a", "1.0"], [" human", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" human", "1.0"], [" height", "1.0"], [" is", "1.0"], [" 5", "0.92704254"], [" feet", "0.98608124"], [",", "0.98486567"], [" 9", "0.3345662"], [" inches", "1.0"], [".", "0.99916995"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" weight", "1.0"], [" of", "1.0"], [" a", "1.0"], [" human", "0.99999094"], ["?", "0.99999964"]]}, "1031": {"qid": "41e98899dde8ec95e2d1", "term": "Gladiator", "description": "combatant who entertained audiences in the Roman Republic and Roman Empire", "question": "Were gladiators associated with the Coloseum?", "answer": true, "facts": ["The Colosseum was a stadium in the ancient city of Rome, large by even today's standards.", "During the Roman era, it was primarily used to host large spectacles including gladiatorial combat, mock battles, and executions."], "decomposition": ["What kind of building was the Colosseum and which city did it exist in?", "In the era of #1, which kind of events were held in such buildings?", "Did any of #2 involve gladiators?"], "evidence": [[[["Colosseum-1"]], [["Colosseum-2"]], [["Colosseum-2"], "operation"]], [[["Colosseum-1"]], [["Colosseum-1"]], [["Colosseum-2"], "operation"]], [[["Colosseum-1"]], [["Colosseum-2"]], [["Inaugural games of the Flavian Amphitheatre-2"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Romans have a word for \"gladiator\"?\nFacts: The Romans had a word for \"gladiator\" but it was not \"gladiator\". The word \"gladiator\" was used to describe a person who fought in a gladiatorial contest.\nAnswer (", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Were gladiators associated with the Coloseum?\nFacts: The Colosseum was a stadium in the ancient city of Rome, large by even today's standards. During the Roman era, it was primarily used to host large spectacles including gladiatorial combat, mock battles, and executions.\nAnswer (True or False):", "ct_subject": "The Colosseum was a stadium in the ancient city of Rome, large by even today's standards. During the Roman era, it was primarily used to host large spectacles including gladiatorial combat, mock battles, and executions.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9975667"], [" the", "1.0"], [" Romans", "0.99871576"], [" have", "0.9999995"], [" a", "1.0"], [" word", "0.99979216"], [" for", "1.0"], [" \"", "0.99999845"], ["gl", "0.99999595"], ["adiator", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Romans", "0.99999905"], [" had", "0.99653083"], [" a", "1.0"], [" word", "1.0"], [" for", "1.0"], [" \"", "1.0"], ["gl", "1.0"], ["adiator", "1.0"], ["\"", "1.0"], [" but", "0.9963989"], [" it", "1.0"], [" was", "1.0"], [" not", "1.0"], [" \"", "0.99988997"], ["gl", "1.0"], ["adiator", "1.0"], ["\".", "0.9999999"], [" The", "0.9994216"], [" word", "1.0"], [" \"", "0.96406597"], ["gl", "1.0"], ["adiator", "1.0"], ["\"", "1.0"], [" was", "0.9999999"], [" used", "0.9952167"], [" to", "0.9929489"], [" describe", "0.99960333"], [" a", "0.99985385"], [" person", "1.0"], [" who", "1.0"], [" fought", "1.0"], [" in", "1.0"], [" a", "1.0"], [" gl", "0.991034"], ["adi", "1.0"], ["atorial", "1.0"], [" contest", "0.99875546"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"]]}, "1032": {"qid": "daf439d2f206dfc1c662", "term": "Pink (singer)", "description": "American singer, songwriter, and actress", "question": "Are there Pink music videos that are triggering for eating disorder patients?", "answer": true, "facts": ["The video for 'Stupid Girls' features a scene where Pink and a woman share a toothbrush to induce vomiting in the bathroom.", "Images or discussion of purging activity can be triggering for people with Eating Disorders."], "decomposition": ["What are the depictions in Pink's music video 'Stupid Girls'?", "What are some situations that can be triggering for people with eating disorders?", "Are any of #2 included in #1?"], "evidence": [[[["Stupid Girls-12"]], [["Eating disorder-7"], "no_evidence"], ["operation"]], [[["Stupid Girls-12", "Stupid Girls-13"]], [["Eating disorder-1"], "no_evidence"], ["no_evidence", "operation"]], [[["Stupid Girls-13"]], [["Eating disorder-9"]], [["Stupid Girls-13"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any books that are triggering for people with anorexia?\nFacts: The book 'The Secret' by Dr. Robert Whitaker is a book that is triggering for people with anorexia.\nAnswer (True or False): True\nQuestion: Are there any books that are", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are there Pink music videos that are triggering for eating disorder patients?\nFacts: The video for 'Stupid Girls' features a scene where Pink and a woman share a toothbrush to induce vomiting in the bathroom. Images or discussion of purging activity can be triggering for people with Eating Disorders.\nAnswer (True or False):", "ct_subject": "The video for 'Stupid Girls' features a scene where Pink and a woman share a toothbrush to induce vomiting in the bathroom. Images or discussion of purging activity can be triggering for people with Eating Disorders.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.99560934"], [" there", "1.0"], [" any", "0.9996679"], [" books", "0.7509268"], [" that", "1.0"], [" are", "0.99061674"], [" triggering", "1.0"], [" for", "1.0"], [" people", "1.0"], [" with", "1.0"], [" an", "0.9972893"], ["ore", "1.0"], ["xia", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9877751"], [" book", "1.0"], [" '", "1.0"], ["The", "1.0"], [" Secret", "0.98542434"], ["'", "1.0"], [" by", "0.99995387"], [" Dr", "0.99435836"], [".", "1.0"], [" Robert", "0.9999999"], [" Whit", "1.0"], ["aker", "1.0"], [" is", "1.0"], [" a", "1.0"], [" book", "0.9984522"], [" that", "1.0"], [" is", "0.9999273"], [" triggering", "0.99999523"], [" for", "1.0"], [" people", "1.0"], [" with", "1.0"], [" an", "1.0"], ["ore", "1.0"], ["xia", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "1.0"], [" any", "1.0"], [" books", "1.0"], [" that", "1.0"], [" are", "1.0"]]}, "1033": {"qid": "4648058ffbc40905a042", "term": "Rupert Murdoch", "description": "Australian-born American media mogul", "question": "Does Rupert Murdoch's alma mater have more history than the USA?", "answer": true, "facts": ["Rupert Murdoch's alma mater is Worcester College.", "Worcester College was founded in 1714.", "The first documented use of the term the United States of America was in a January 2, 1776 letter."], "decomposition": ["What is Rupert Murdoch's alma mater?", "When was #1 founded?", "When was the United States founded?", "Is #2 prior to #3?"], "evidence": [[[["Rupert Murdoch-8"]], [["Worcester College, Oxford-1"]], [["United States Declaration of Independence-1"]], ["operation"]], [[["Rupert Murdoch-8"]], [["Worcester College, Oxford-1"]], [["United States Declaration of Independence-1"]], ["operation"]], [[["Rupert Murdoch-8"]], [["Worcester College, Oxford-1"]], [["United States-27"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the country that has the most people in prison?\nFacts: The United States of America has the most people in prison of any country in the world.\nAnswer (True or False): True\nQuestion: What is the name of the country that has the most people in prison", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Rupert Murdoch's alma mater have more history than the USA?\nFacts: Rupert Murdoch's alma mater is Worcester College. Worcester College was founded in 1714. The first documented use of the term the United States of America was in a January 2, 1776 letter.\nAnswer (True or False):", "ct_subject": "Rupert Murdoch's alma mater is Worcester College. Worcester College was founded in 1714. The first documented use of the term the United States of America was in a January 2, 1776 letter.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9958568"], [" is", "1.0"], [" the", "1.0"], [" name", "0.7504898"], [" of", "1.0"], [" the", "1.0"], [" country", "0.8933965"], [" that", "1.0"], [" has", "0.9874047"], [" the", "1.0"], [" most", "0.99983597"], [" people", "1.0"], [" in", "0.6088485"], [" prison", "0.8799438"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" United", "0.99999666"], [" States", "1.0"], [" of", "1.0"], [" America", "1.0"], [" has", "1.0"], [" the", "1.0"], [" most", "1.0"], [" people", "1.0"], [" in", "1.0"], [" prison", "1.0"], [" of", "0.9993443"], [" any", "1.0"], [" country", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], [".", "1.0"], ["\n", "0.9996301"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" country", "1.0"], [" that", "1.0"], [" has", "1.0"], [" the", "1.0"], [" most", "1.0"], [" people", "1.0"], [" in", "1.0"], [" prison", "1.0"]]}, "1034": {"qid": "10353348b0d7077924be", "term": "Paparazzi", "description": "profession", "question": "Were paparazzi involved in the death of a member of the royal family?", "answer": true, "facts": ["Diana Spencer was being pursued by paparazzi when her vehicle was involved in a fatal accident.", "Diana Spencer was known as 'Princess Diana' and was the Princess of Wales."], "decomposition": ["What were the circumstances surrounding the death of Diana Spencer?", "Is Diana Spencer a member of the royal family?", "Was paparazzi involved in #1?", "Are #2 and #3 positive?"], "evidence": [[[["Diana, Princess of Wales-4"]], [["Diana, Princess of Wales-3"]], [["Death of Diana, Princess of Wales-2"]], ["operation"]], [[["Death of Diana, Princess of Wales-1"]], [["Diana, Princess of Wales-1", "Diana, Princess of Wales-26"]], [["Death of Diana, Princess of Wales-2"]], ["operation"]], [[["Diana, Princess of Wales-53"]], [["Diana, Princess of Wales-1"]], [["Diana, Princess of Wales-53"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Titanic sink because of a fire?\nFacts: The Titanic was not designed to sink. The Titanic was designed to be a floating hotel. The Titanic was not designed to sink. The Titanic was designed to be a floating hotel. The Titanic was not designed to sink. The Titanic was designed to", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Were paparazzi involved in the death of a member of the royal family?\nFacts: Diana Spencer was being pursued by paparazzi when her vehicle was involved in a fatal accident. Diana Spencer was known as 'Princess Diana' and was the Princess of Wales.\nAnswer (True or False):", "ct_subject": "Diana Spencer was being pursued by paparazzi when her vehicle was involved in a fatal accident. Diana Spencer was known as 'Princess Diana' and was the Princess of Wales.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99910045"], [" the", "1.0"], [" Titanic", "0.99901927"], [" sink", "1.0"], [" because", "1.0"], [" of", "1.0"], [" a", "0.8672133"], [" fire", "0.87708604"], ["?", "0.999265"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Titanic", "1.0"], [" was", "0.9989648"], [" not", "0.9999937"], [" designed", "0.99782926"], [" to", "1.0"], [" sink", "1.0"], [".", "1.0"], [" The", "0.9999832"], [" Titanic", "1.0"], [" was", "1.0"], [" designed", "0.9999771"], [" to", "1.0"], [" be", "0.9895998"], [" a", "0.9999994"], [" floating", "0.99996686"], [" hotel", "0.9999964"], [".", "1.0"], [" The", "0.026853625"], [" Titanic", "1.0"], [" was", "1.0"], [" not", "0.97472274"], [" designed", "1.0"], [" to", "1.0"], [" sink", "1.0"], [".", "1.0"], [" The", "1.0"], [" Titanic", "1.0"], [" was", "1.0"], [" designed", "1.0"], [" to", "1.0"], [" be", "1.0"], [" a", "1.0"], [" floating", "1.0"], [" hotel", "1.0"], [".", "1.0"], [" The", "0.9999913"], [" Titanic", "1.0"], [" was", "1.0"], [" not", "1.0"], [" designed", "1.0"], [" to", "1.0"], [" sink", "1.0"], [".", "1.0"], [" The", "1.0"], [" Titanic", "1.0"], [" was", "1.0"], [" designed", "1.0"], [" to", "1.0"]]}, "1035": {"qid": "c426b03932f052ae91af", "term": "Moon Jae-in", "description": "President of South Korea", "question": "Was Moon Jae-in born outside of Khanbaliq?", "answer": true, "facts": ["Khanbaliq was the winter capital of the Mongol Empire. ", "Khanbaliq was located at the center of what is now modern day Beijing, China.", "Moon Jae-In was born in Geoje, South Korea."], "decomposition": ["Where was Moon Jae-in born?", "What is the modern day location of Khanbaliq?", "Is #1 different from #2?"], "evidence": [[[["Moon Jae-in-5"]], [["Khanbaliq-1"]], ["operation"]], [[["Moon Jae-in-2"]], [["Khanbaliq-1"]], ["operation"]], [[["Moon Jae-in-5"]], [["Khanbaliq-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Mongols have a written language?\nFacts: The Mongols did not have a written language. They used a combination of pictographs and ideograms.\nAnswer (True or False): True\nQuestion: Did the Mongols have a written language?\nFacts: The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Was Moon Jae-in born outside of Khanbaliq?\nFacts: Khanbaliq was the winter capital of the Mongol Empire.  Khanbaliq was located at the center of what is now modern day Beijing, China. Moon Jae-In was born in Geoje, South Korea.\nAnswer (True or False):", "ct_subject": "Khanbaliq was the winter capital of the Mongol Empire.  Khanbaliq was located at the center of what is now modern day Beijing, China. Moon Jae-In was born in Geoje, South Korea.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.95646113"], [" the", "0.99997795"], [" Mong", "0.95401967"], ["ols", "1.0"], [" have", "0.99043965"], [" a", "1.0"], [" written", "0.21473356"], [" language", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999999"], [" Mong", "0.9999999"], ["ols", "1.0"], [" did", "0.2559505"], [" not", "0.9999987"], [" have", "1.0"], [" a", "1.0"], [" written", "1.0"], [" language", "1.0"], [".", "1.0"], [" They", "0.9433512"], [" used", "0.6450566"], [" a", "1.0"], [" combination", "0.9999999"], [" of", "1.0"], [" pict", "0.82644767"], ["ographs", "0.9999999"], [" and", "0.15269044"], [" ide", "0.97942364"], ["ograms", "0.9999893"], [".", "0.9843277"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.9999993"], [" the", "1.0"], [" Mong", "1.0"], ["ols", "1.0"], [" have", "1.0"], [" a", "1.0"], [" written", "1.0"], [" language", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"]]}, "1036": {"qid": "73520a07f183b7111236", "term": "Karaoke", "description": "form of entertainment involving singing to recorded music", "question": "Were karaoke and the turtle power tiller patented in the same country?", "answer": true, "facts": ["Roberto L. del Rosario holds the patent for the karaoke system", "del Rosario is Filipino", "Magdalena Smith Villaruz patented the turtle power tiller", "Villaruz is Filipino "], "decomposition": ["Who is the patent holder of the karaoke system?", "Which country is #1 from?", "Who patented the turtle power tiller?", "Which country is #3 from?", "Is #2 the same as #4?"], "evidence": [[[["Karaoke-8"]], [["Roberto del Rosario-1"]], [["Magdalena Villaruz-2"]], [["Magdalena Villaruz-1"]], ["operation"]], [[["Karaoke-5"], "no_evidence"], [["Daisuke Inoue-3"]], [["Magdalena Villaruz-1"]], [["Magdalena Villaruz-1"]], ["operation"]], [[["Roberto del Rosario-2"]], [["Philippines-1", "Roberto del Rosario-1"]], [["Magdalena Villaruz-1"]], [["Magdalena Villaruz-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the country that has the highest number of people who have been killed by sharks?\nFacts: The country with the highest number of people who have been killed by sharks is the Philippines.\nAnswer (True or False): True\nQuestion: What is the name of the country", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Were karaoke and the turtle power tiller patented in the same country?\nFacts: Roberto L. del Rosario holds the patent for the karaoke system del Rosario is Filipino Magdalena Smith Villaruz patented the turtle power tiller Villaruz is Filipino \nAnswer (True or False):", "ct_subject": "Roberto L. del Rosario holds the patent for the karaoke system del Rosario is Filipino Magdalena Smith Villaruz patented the turtle power tiller Villaruz is Filipino ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.87573195"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9999999"], [" of", "1.0"], [" the", "1.0"], [" country", "0.99996936"], [" that", "0.9986022"], [" has", "0.9578885"], [" the", "1.0"], [" highest", "0.8156087"], [" number", "1.0"], [" of", "1.0"], [" people", "0.9998834"], [" who", "1.0"], [" have", "1.0"], [" been", "0.9995561"], [" killed", "1.0"], [" by", "0.9998752"], [" sharks", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999993"], [" country", "0.9916773"], [" with", "1.0"], [" the", "1.0"], [" highest", "1.0"], [" number", "1.0"], [" of", "1.0"], [" people", "0.9999989"], [" who", "0.9972031"], [" have", "1.0"], [" been", "1.0"], [" killed", "1.0"], [" by", "1.0"], [" sharks", "1.0"], [" is", "1.0"], [" the", "0.901628"], [" Philippines", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" country", "1.0"]]}, "1037": {"qid": "8f936e225ee1fe225f66", "term": "Shaggy (musician)", "description": "Reggae singer and former U.S. Marine", "question": "Would Shaggy and Redenbacher popcorn founder both raise hand during first name roll call?", "answer": true, "facts": ["Roll call is when teachers call the names of students and they raise their hand to show they are present.", "The founder of Redenbacher popcorn was Orville Redenbacher.", "Reggae musician Shaggy was born Orville Richard Burrell."], "decomposition": ["What is the first name of the person who founded Redenbacher popcorn?", "What is the first name of Reggae musician Shaggy?", "Is #1 the same as #2?"], "evidence": [[[["Orville Redenbacher-1"]], [["Shaggy (musician)-1"]], ["operation"]], [[["Orville Redenbacher-1"]], [["Shaggy (musician)-1"]], ["operation"]], [[["Orville Redenbacher-1"]], [["Shaggy (musician)-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: The first person to be born in the United States was a woman named Sarah Hensley.\nAnswer (True or", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would Shaggy and Redenbacher popcorn founder both raise hand during first name roll call?\nFacts: Roll call is when teachers call the names of students and they raise their hand to show they are present. The founder of Redenbacher popcorn was Orville Redenbacher. Reggae musician Shaggy was born Orville Richard Burrell.\nAnswer (True or False):", "ct_subject": "Roll call is when teachers call the names of students and they raise their hand to show they are present. The founder of Redenbacher popcorn was Orville Redenbacher. Reggae musician Shaggy was born Orville Richard Burrell.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99999726"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99996984"], [" of", "1.0"], [" the", "1.0"], [" first", "0.99996877"], [" person", "0.99422556"], [" to", "1.0"], [" be", "0.99998367"], [" born", "0.9995005"], [" in", "1.0"], [" the", "1.0"], [" United", "0.99999976"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.98134977"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [" was", "1.0"], [" a", "0.9962495"], [" woman", "0.9999976"], [" named", "1.0"], [" Sarah", "0.9151287"], [" H", "0.9843796"], ["ens", "1.0"], ["ley", "0.9466846"], [".", "1.0"], ["\n", "0.9599379"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"]]}, "1038": {"qid": "e4a0eeb26daef5087fe9", "term": "Moustache", "description": "Facial hair grown on the upper lip", "question": "Has a baby ever had a moustache?", "answer": true, "facts": ["When babies are in the womb, many have hair known as lanugo.", "Lanugo is unpigmented, downy hair that is sometimes found on the body of fetuses and babies.", "Lanugo can grow anywhere on a baby's body but is usually shed before the baby is born and is reabsorbed by the mother."], "decomposition": ["What kind of hair does a baby have when it is in the womb?", "Can #1 grow anywhere on the body?"], "evidence": [[[["Lanugo-1"]], [["Lanugo-1"]]], [[["Lanugo-1"]], ["no_evidence", "operation"]], [[["Lanugo-1"]], [["Lanugo-1"], "no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a man and a woman?\nFacts: A man is a male human being. A woman is a female human being.\nAnswer (True or False", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Has a baby ever had a moustache?\nFacts: When babies are in the womb, many have hair known as lanugo. Lanugo is unpigmented, downy hair that is sometimes found on the body of fetuses and babies. Lanugo can grow anywhere on a baby's body but is usually shed before the baby is born and is reabsorbed by the mother.\nAnswer (True or False):", "ct_subject": "When babies are in the womb, many have hair known as lanugo. Lanugo is unpigmented, downy hair that is sometimes found on the body of fetuses and babies. Lanugo can grow anywhere on a baby's body but is usually shed before the baby is born and is reabsorbed by the mother.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9990386"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.05337223"], [" between", "1.0"], [" a", "1.0"], [" man", "0.96871"], [" and", "0.99998987"], [" a", "1.0"], [" woman", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999999"], [" man", "1.0"], [" is", "1.0"], [" a", "1.0"], [" male", "0.87590647"], [" human", "0.9611531"], [" being", "1.0"], [".", "1.0"], [" A", "1.0"], [" woman", "1.0"], [" is", "1.0"], [" a", "1.0"], [" female", "1.0"], [" human", "1.0"], [" being", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"]]}, "1039": {"qid": "ea9b3204830068115fcb", "term": "John Muir", "description": "Scottish-born American naturalist and author", "question": "Would John Muir not likely have a vitamin D deficiency?", "answer": true, "facts": ["John Muir frequently spent time exploring various places in nature.", "Spending time in nature increases your exposure to sunlight.", "Skin exposure to sunlight increases vitamin D levels in the body."], "decomposition": ["What is the most common cause of vitamin D deficiency?", "What was the nature of John Muir's life's work?", "Does #2 ensure that he does not experience #1?"], "evidence": [[[["Vitamin D-17"]], [["John Muir-2"]], [["John Muir-2"], "operation"]], [[["Vitamin D-13"]], [["John Muir-52"]], ["operation"]], [[["Vitamin D deficiency-1"]], [["John Muir-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the average age of a person who has a heart attack?\nFacts: The average age of a person who has a heart attack is between 65 and 75 years old.\nAnswer (True or False): True\nQuestion: What is the average age of a person who has a stroke?\nFacts: The average age of a", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Would John Muir not likely have a vitamin D deficiency?\nFacts: John Muir frequently spent time exploring various places in nature. Spending time in nature increases your exposure to sunlight. Skin exposure to sunlight increases vitamin D levels in the body.\nAnswer (True or False):", "ct_subject": "John Muir frequently spent time exploring various places in nature. Spending time in nature increases your exposure to sunlight. Skin exposure to sunlight increases vitamin D levels in the body.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.057302743"], [" is", "1.0"], [" the", "1.0"], [" average", "0.8070521"], [" age", "0.88159573"], [" of", "1.0"], [" a", "0.9998946"], [" person", "0.99999654"], [" who", "0.9997888"], [" has", "0.99999726"], [" a", "0.7089214"], [" heart", "0.99995315"], [" attack", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" age", "1.0"], [" of", "1.0"], [" a", "1.0"], [" person", "1.0"], [" who", "1.0"], [" has", "1.0"], [" a", "1.0"], [" heart", "1.0"], [" attack", "1.0"], [" is", "1.0"], [" between", "0.9999813"], [" 65", "0.92009366"], [" and", "1.0"], [" 75", "0.9933496"], [" years", "0.72396857"], [" old", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" average", "1.0"], [" age", "1.0"], [" of", "1.0"], [" a", "1.0"], [" person", "1.0"], [" who", "1.0"], [" has", "0.99999607"], [" a", "1.0"], [" stroke", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" average", "1.0"], [" age", "1.0"], [" of", "1.0"], [" a", "1.0"]]}, "1040": {"qid": "50b58835d8ab6da72c32", "term": "Snow leopard", "description": "species of mammal", "question": "Can a snow leopard swim?", "answer": true, "facts": ["except for giraffes and apes, all four legged mammals can swim", "a snow leopard is a mammal", "snow leopards have four legs"], "decomposition": ["Is a snow leopard a four legged mammal?"], "evidence": [[[["Cat-1", "Snow leopard-1"], "no_evidence"]], [["no_evidence", "operation"]], [[["Felidae-1", "Quadrupedalism-1", "Snow leopard-1"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance a person can walk in one day?\nFacts: The longest distance a person can walk in one day is about 2,000 miles.\nAnswer (True or False): True\nQuestion: What is the longest distance a person can walk in one year?\nFacts: The longest distance a person can walk in one year is about 2,000 miles.\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can a snow leopard swim?\nFacts: except for giraffes and apes, all four legged mammals can swim a snow leopard is a mammal snow leopards have four legs\nAnswer (True or False):", "ct_subject": "except for giraffes and apes, all four legged mammals can swim a snow leopard is a mammal snow leopards have four legs", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.998147"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.9999441"], [" distance", "0.9999198"], [" a", "0.99420387"], [" person", "0.8478762"], [" can", "0.9999999"], [" walk", "0.9999751"], [" in", "1.0"], [" one", "0.9999999"], [" day", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99999094"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" person", "1.0"], [" can", "1.0"], [" walk", "1.0"], [" in", "1.0"], [" one", "1.0"], [" day", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 2", "0.77359647"], [",", "1.0"], ["000", "0.9999974"], [" miles", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" person", "1.0"], [" can", "1.0"], [" walk", "0.9966911"], [" in", "1.0"], [" one", "1.0"], [" year", "0.99967325"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" longest", "1.0"], [" distance", "1.0"], [" a", "1.0"], [" person", "1.0"], [" can", "1.0"], [" walk", "1.0"], [" in", "1.0"], [" one", "1.0"], [" year", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 2", "0.9063974"], [",", "1.0"], ["000", "1.0"], [" miles", "1.0"], [".", "1.0"], ["\n", "1.0"]]}, "1041": {"qid": "7bee5f4847890e925f3e", "term": "Dance", "description": "A performing art consisting of movement of the body", "question": "Is waltz less injurious than slam dance?", "answer": true, "facts": ["The waltz is a rhythmic dance performed in triple time by a couple.", "A slam dance is a type of dance in which leaping dancers collide against each other."], "decomposition": ["What kinds of body movements are involved in waltz?", "What kinds of body movements are involved in slam dance?", "Is #1 less likely to cause injuries than #2?"], "evidence": [[[["Waltz-2"], "no_evidence"], [["Moshing-1"]], [["Moshing-4"], "operation"]], [[["Waltz-5"]], [["Moshing-1"]], [["Moshing-4"], "operation"]], [[["Ballroom dance-28", "Waltz-1"]], [["Moshing-1"]], [["Moshing-4"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the word \"snow\" a contraction of \"snowball\"?\nFacts: The word \"snow\" is a contraction of the word \"snowball\".\nAnswer (True or False): True\nQuestion: Is the word \"snow\" a contraction of \"snowball\"?\nFacts: The word \"snow", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is waltz less injurious than slam dance?\nFacts: The waltz is a rhythmic dance performed in triple time by a couple. A slam dance is a type of dance in which leaping dancers collide against each other.\nAnswer (True or False):", "ct_subject": "The waltz is a rhythmic dance performed in triple time by a couple. A slam dance is a type of dance in which leaping dancers collide against each other.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9982247"], [" the", "0.99279684"], [" word", "0.99999976"], [" \"", "1.0"], ["s", "0.9471195"], ["now", "0.9992974"], ["\"", "0.9999937"], [" a", "0.9848369"], [" contraction", "0.9998795"], [" of", "1.0"], [" \"", "0.8943448"], ["s", "1.0"], ["now", "1.0"], ["ball", "0.99999607"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" word", "1.0"], [" \"", "0.99999464"], ["s", "1.0"], ["now", "1.0"], ["\"", "0.9993667"], [" is", "1.0"], [" a", "1.0"], [" contraction", "1.0"], [" of", "1.0"], [" the", "0.9999076"], [" word", "0.9999964"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["ball", "1.0"], ["\".", "0.96599334"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["\"", "1.0"], [" a", "1.0"], [" contraction", "1.0"], [" of", "1.0"], [" \"", "0.99978787"], ["s", "1.0"], ["now", "1.0"], ["ball", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" word", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"]]}, "1042": {"qid": "8c36b27e75cee7f0ec0c", "term": "French toast", "description": "bread soaked in beaten eggs and then fried", "question": "Can French Toast hypothetically kill a Lannister?", "answer": true, "facts": ["The Lannister's are a wealthy family in the Song of Ice and Fire book series.", "French Toast is made from bread dipped in an egg batter.", "Salmonella is a deadly bacteria that can be carried by spoiled eggs."], "decomposition": ["What are the ingredients in French toast?", "Which things in #1 can spoil?", "What diseases can be carried by spoiled #2?", "What species are susceptible to #3?", "Are the Lannisters members of a species listed in #4?"], "evidence": [[[["French toast-1"]], [["Egg as food-35"]], [["Salmonella enterica-3"]], [["Salmonella-18"]], [["World of A Song of Ice and Fire-44"], "operation"]], [[["French toast-1"]], [["French toast-1"]], [["Dairy-58"], "no_evidence"], [["Raw milk-14"], "no_evidence"], [["World of A Song of Ice and Fire-44"], "no_evidence", "operation"]], [[["French toast-7"]], [["Milk-118"]], [["Foodborne illness-6"]], [["Foodborne illness-31"]], [["Game of Thrones-10"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the character in the movie \"The Avengers\" who is the leader of the Avengers?\nFacts: The character is named Tony Stark.\nAnswer (True or False): True\nQuestion: What is the name of the character in the movie \"The Avengers\" who is the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can French Toast hypothetically kill a Lannister?\nFacts: The Lannister's are a wealthy family in the Song of Ice and Fire book series. French Toast is made from bread dipped in an egg batter. Salmonella is a deadly bacteria that can be carried by spoiled eggs.\nAnswer (True or False):", "ct_subject": "The Lannister's are a wealthy family in the Song of Ice and Fire book series. French Toast is made from bread dipped in an egg batter. Salmonella is a deadly bacteria that can be carried by spoiled eggs.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999901"], [" is", "1.0"], [" the", "1.0"], [" name", "0.9865959"], [" of", "1.0"], [" the", "1.0"], [" character", "1.0"], [" in", "0.99998844"], [" the", "1.0"], [" movie", "0.99765056"], [" \"", "1.0"], ["The", "1.0"], [" Avengers", "0.9999187"], ["\"", "0.9997999"], [" who", "0.99999964"], [" is", "0.99999905"], [" the", "1.0"], [" leader", "0.31032878"], [" of", "1.0"], [" the", "1.0"], [" Avengers", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999815"], [" character", "0.99999857"], [" is", "0.99534297"], [" named", "0.99384093"], [" Tony", "0.99988925"], [" Stark", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" character", "1.0"], [" in", "1.0"], [" the", "1.0"], [" movie", "1.0"], [" \"", "1.0"], ["The", "1.0"], [" Avengers", "1.0"], ["\"", "1.0"], [" who", "1.0"], [" is", "1.0"], [" the", "1.0"]]}, "1043": {"qid": "910b0e2092a5074851b7", "term": "Brewing", "description": "production of beer", "question": "Can brewing occur in a prison environment?", "answer": true, "facts": ["Pruno is a product made almost exclusively in prisons.", "Pruno is a fermented beverage that is made of fruit to produce alcohol."], "decomposition": ["What is Pruno?", "Where is #1 made?", "Is #2 same as prison?"], "evidence": [[[["Pruno-1"]], [["Pruno-1"]], ["operation"]], [[["Pruno-1"]], [["Pruno-1"]], [["Pruno-1"]]], [[["Pruno-1"]], [["Pruno-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Can a person be born with a heart condition?\nFacts: A heart condition is a condition that affects the heart. A heart condition is not a disease.\nAnswer (True or False): True\nQuestion: Can a person be born with a brain condition?\nFacts: A brain condition is a condition that affects the brain. A brain condition is not a disease.\nAnswer (True or", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can brewing occur in a prison environment?\nFacts: Pruno is a product made almost exclusively in prisons. Pruno is a fermented beverage that is made of fruit to produce alcohol.\nAnswer (True or False):", "ct_subject": "Pruno is a product made almost exclusively in prisons. Pruno is a fermented beverage that is made of fruit to produce alcohol.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Can", "0.81379503"], [" a", "0.9999963"], [" person", "1.0"], [" be", "1.0"], [" born", "0.99325025"], [" with", "0.9999492"], [" a", "1.0"], [" heart", "0.999337"], [" condition", "0.9933543"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999342"], [" heart", "0.97005916"], [" condition", "1.0"], [" is", "1.0"], [" a", "1.0"], [" condition", "1.0"], [" that", "0.9962664"], [" affects", "0.35457942"], [" the", "1.0"], [" heart", "1.0"], [".", "0.99999726"], [" A", "0.9998871"], [" heart", "0.999995"], [" condition", "1.0"], [" is", "0.9612245"], [" not", "0.9999999"], [" a", "1.0"], [" disease", "0.9999747"], [".", "1.0"], ["\n", "0.9996939"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Can", "0.99999976"], [" a", "1.0"], [" person", "1.0"], [" be", "1.0"], [" born", "1.0"], [" with", "1.0"], [" a", "1.0"], [" brain", "0.957663"], [" condition", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" brain", "1.0"], [" condition", "1.0"], [" is", "1.0"], [" a", "1.0"], [" condition", "1.0"], [" that", "1.0"], [" affects", "1.0"], [" the", "1.0"], [" brain", "1.0"], [".", "1.0"], [" A", "1.0"], [" brain", "1.0"], [" condition", "1.0"], [" is", "1.0"], [" not", "1.0"], [" a", "1.0"], [" disease", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"]]}, "1044": {"qid": "1d4a642a03f37de1b5d0", "term": "Wheelchair", "description": "chair with wheels, used by people for whom walking is difficult or impossible due to illness, injury, or disability", "question": "Do American wheelchair users know what the ADA is?", "answer": true, "facts": ["The ADA is the Americans with Disabilities Act.", "Non-ADA compliant businesses include those without wheelchair access points."], "decomposition": ["Which areas of interest are affected by the ADA?", "Is any of #1 of particular interest to wheelchair users in America?"], "evidence": [[[["Americans with Disabilities Act of 1990-1"]], [["Disability-4"], "operation"]], [[["American Association of People with Disabilities-1"]], [["American Association of People with Disabilities-1"]]], [[["Americans with Disabilities Act of 1990-6"]], [["Americans with Disabilities Act of 1990-6"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"carpet\" and a \"tarp\"?\nFacts: A carpet is a piece of fabric that is used to cover a floor. A tarp is a piece of fabric that is used to cover a floor.\nAnswer (True or False): True\nQuestion: What is the difference between a \"carpet\" and a \"tarp\"?\nFacts:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do American wheelchair users know what the ADA is?\nFacts: The ADA is the Americans with Disabilities Act. Non-ADA compliant businesses include those without wheelchair access points.\nAnswer (True or False):", "ct_subject": "The ADA is the Americans with Disabilities Act. Non-ADA compliant businesses include those without wheelchair access points.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9999949"], [" between", "1.0"], [" a", "0.99999344"], [" \"", "1.0"], ["c", "0.506085"], ["arp", "0.99999356"], ["et", "1.0"], ["\"", "0.9802336"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["t", "0.7416626"], ["arp", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.99999976"], [" carpet", "1.0"], [" is", "1.0"], [" a", "1.0"], [" piece", "0.7783014"], [" of", "1.0"], [" fabric", "0.98040843"], [" that", "1.0"], [" is", "1.0"], [" used", "1.0"], [" to", "1.0"], [" cover", "1.0"], [" a", "0.9999999"], [" floor", "1.0"], [".", "1.0"], [" A", "1.0"], [" t", "1.0"], ["arp", "1.0"], [" is", "1.0"], [" a", "1.0"], [" piece", "1.0"], [" of", "1.0"], [" fabric", "0.9999994"], [" that", "1.0"], [" is", "1.0"], [" used", "1.0"], [" to", "1.0"], [" cover", "1.0"], [" a", "1.0"], [" floor", "0.99998593"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["c", "1.0"], ["arp", "1.0"], ["et", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["t", "1.0"], ["arp", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"]]}, "1045": {"qid": "b18b7cbde476888d0059", "term": "Family of Barack Obama", "description": "List of members of the family of Barack Obama", "question": "Does Lupita Nyongo have citizenship in paternal Family of Barack Obama's origin country?", "answer": true, "facts": ["Actress Lupita Nyongo has dual citizenship in Kenya and Mexico.", "Barack Obama's father was born in Nyang\u2019oma Kogelo, Rachuonyo District, Kenya Colony.", "Barack Obama's father was a Kenyan of the Luo tribe."], "decomposition": ["In what nations does Lupita Nyongo have citizenship?", "Who is Barack Obama's father?", "In what nations does #2 have citizenship?", "Is at least one country in #1 also found in #3?"], "evidence": [[[["Lupita Nyong'o-7"]], [["Barack Obama-6"]], [["Barack Obama-6"]], ["operation"]], [[["Lupita Nyong'o-7"]], [["Barack Obama Sr.-1"]], [["Barack Obama-6"]], ["operation"]], [[["Lupita Nyong'o-7"]], [["Barack Obama Sr.-1"]], [["Barack Obama Sr.-3"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the name of the President's mother, Stanley Ann Dunham, a reference to the famous American actress, actress, and singer, Stanley Ann Dunham?\nFacts: Stanley Ann Dunham was born in Chicago, Illinois. Barack Obama's mother was", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Lupita Nyongo have citizenship in paternal Family of Barack Obama's origin country?\nFacts: Actress Lupita Nyongo has dual citizenship in Kenya and Mexico. Barack Obama's father was born in Nyang\u2019oma Kogelo, Rachuonyo District, Kenya Colony. Barack Obama's father was a Kenyan of the Luo tribe.\nAnswer (True or False):", "ct_subject": "Actress Lupita Nyongo has dual citizenship in Kenya and Mexico. Barack Obama's father was born in Nyang\u2019oma Kogelo, Rachuonyo District, Kenya Colony. Barack Obama's father was a Kenyan of the Luo tribe.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99802303"], [" the", "0.999998"], [" name", "1.0"], [" of", "0.9999999"], [" the", "1.0"], [" President", "0.993052"], ["'s", "0.9949393"], [" mother", "0.9992316"], [",", "1.0"], [" Stanley", "1.0"], [" Ann", "1.0"], [" Dunham", "1.0"], [",", "1.0"], [" a", "1.0"], [" reference", "0.999461"], [" to", "1.0"], [" the", "1.0"], [" famous", "0.9981522"], [" American", "0.32288262"], [" actress", "1.0"], [",", "0.90347934"], [" actress", "0.9999994"], [",", "1.0"], [" and", "1.0"], [" singer", "0.99007225"], [",", "1.0"], [" Stanley", "1.0"], [" Ann", "1.0"], [" Dunham", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Stanley", "0.99969447"], [" Ann", "1.0"], [" Dunham", "1.0"], [" was", "0.99937004"], [" born", "1.0"], [" in", "1.0"], [" Chicago", "0.99959666"], [",", "1.0"], [" Illinois", "1.0"], [".", "0.37516853"], [" Barack", "0.9999032"], [" Obama", "1.0"], ["'s", "1.0"], [" mother", "1.0"], [" was", "0.82981336"]]}, "1046": {"qid": "2c0ad6834e39382c2e4f", "term": "Equator", "description": "Intersection of a sphere's surface with the plane perpendicular to the sphere's axis of rotation and midway between the poles", "question": "Is most coffee produced South of the Equator?", "answer": true, "facts": ["The countries with the highest coffee production are in South America.", "Almost all of South America is in the Southern Hemisphere."], "decomposition": ["Which countries produce the most coffee?", "Which hemisphere are most of #1 located?", "Is #2 south of the equator?"], "evidence": [[[["Coffee-45"], "no_evidence"], ["no_evidence"], [["Brazil-43"], "no_evidence", "operation"]], [[["Coffee-5"], "no_evidence"], [["Southern Hemisphere-9"]], [["Southern Hemisphere-1"], "operation"]], [[["Coffee-5"]], [["Southern Hemisphere-9"]], [["Southern Hemisphere-9"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the Earth round?\nFacts: The Earth is not round. It is a sphere.\nAnswer (True or False): True\nQuestion: Is the Earth flat?\nFacts: The Earth is flat.\nAnswer (True or False): True\nQuestion: Is the Earth round?\nFacts: The Earth is round.\nAnswer (True or False): True\nQuestion: Is the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is most coffee produced South of the Equator?\nFacts: The countries with the highest coffee production are in South America. Almost all of South America is in the Southern Hemisphere.\nAnswer (True or False):", "ct_subject": "The countries with the highest coffee production are in South America. Almost all of South America is in the Southern Hemisphere.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99988925"], [" the", "0.998686"], [" Earth", "0.8183839"], [" round", "0.9999877"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" not", "0.99999714"], [" round", "1.0"], [".", "1.0"], [" It", "0.97168714"], [" is", "1.0"], [" a", "1.0"], [" sphere", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "0.99998903"], [" Earth", "1.0"], [" flat", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" flat", "0.92326"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" Earth", "1.0"], [" round", "0.9998086"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Earth", "1.0"], [" is", "1.0"], [" round", "0.9999999"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"]]}, "1047": {"qid": "c681171e816f3117df0b", "term": "Tongue", "description": "mouth organ that tastes and facilitates speech", "question": "Is the tongue part of a creature's head?", "answer": true, "facts": ["A creature's tongue is inside its mouth.", "A creature's mouth is part of its head."], "decomposition": ["In what body part is the tongue located?", "Is #1 located in the head?"], "evidence": [[[["Tongue-1"]], ["no_evidence", "operation"]], [[["Tongue-5"]], [["Tongue-5"]]], [[["Tongue-1"]], [["Head-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the human body a living organism?\nFacts: The human body is a living organism.\nAnswer (True or False): True\nQuestion: Is the human body a living organism?\nFacts: The human body is a living organism.\nAnswer (True or False): True\nQuestion: Is the human body a living organism?\nFacts: The human body is a living organism.\nAnswer (True or", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is the tongue part of a creature's head?\nFacts: A creature's tongue is inside its mouth. A creature's mouth is part of its head.\nAnswer (True or False):", "ct_subject": "A creature's tongue is inside its mouth. A creature's mouth is part of its head.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9970499"], [" the", "0.9999958"], [" human", "0.99503964"], [" body", "0.99113125"], [" a", "0.99199307"], [" living", "0.99999404"], [" organism", "0.9996973"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" human", "1.0"], [" body", "1.0"], [" is", "1.0"], [" a", "0.9999999"], [" living", "1.0"], [" organism", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" human", "1.0"], [" body", "1.0"], [" a", "1.0"], [" living", "1.0"], [" organism", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" human", "1.0"], [" body", "1.0"], [" is", "1.0"], [" a", "1.0"], [" living", "1.0"], [" organism", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" human", "1.0"], [" body", "1.0"], [" a", "1.0"], [" living", "1.0"], [" organism", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" human", "1.0"], [" body", "1.0"], [" is", "1.0"], [" a", "1.0"], [" living", "1.0"], [" organism", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"]]}, "1048": {"qid": "307c3ae5cff437eee937", "term": "Rush Limbaugh", "description": "American radio talk show host, commentator, author, and television personality", "question": "Does Coast to Coast AM have more longevity than the Rush Limbaugh show?", "answer": true, "facts": ["As of 2020, The Rush Limbaugh Show has been on the airwaves since 1988.", "As of 2020, Coast to Coast AM has been on the airwaves since 1984."], "decomposition": ["When did the Rush Limbaugh show first air?", "When did Coast to Coast AM first air?", "Is #2 before #1?"], "evidence": [[[["The Rush Limbaugh Show-1"]], [["Coast to Coast AM-2"]], ["operation"]], [[["The Rush Limbaugh Show-1"]], [["Coast to Coast AM-2"]], ["operation"]], [[["Rush Limbaugh-1"]], [["Coast to Coast AM-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: John Smith.\nAnswer (True or False): True\nQuestion: What is the name of the first person to be born in the United States?\nFacts: John Smith.\nAnswer (True or False): True\nQuestion:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Coast to Coast AM have more longevity than the Rush Limbaugh show?\nFacts: As of 2020, The Rush Limbaugh Show has been on the airwaves since 1988. As of 2020, Coast to Coast AM has been on the airwaves since 1984.\nAnswer (True or False):", "ct_subject": "As of 2020, The Rush Limbaugh Show has been on the airwaves since 1988. As of 2020, Coast to Coast AM has been on the airwaves since 1984.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.15462084"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99587715"], [" of", "1.0"], [" the", "1.0"], [" first", "0.9944067"], [" person", "0.9996356"], [" to", "1.0"], [" be", "0.9864879"], [" born", "0.98864204"], [" in", "1.0"], [" the", "1.0"], [" United", "0.29720852"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "0.8694596"], [" Smith", "1.0"], [".", "0.99999857"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" person", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" John", "1.0"], [" Smith", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"]]}, "1049": {"qid": "f39895e719acd479e7ba", "term": "E.T. the Extra-Terrestrial", "description": "1982 American science fiction film directed by Steven Spielberg", "question": "Is the E.T. the Extra-Terrestrial Atari Landfill story an urban legend?", "answer": true, "facts": ["An urban legend is a humorous or horrifying story based on hearsay that is circulated as true.", "E.T. the Extra Terrestrial was panned as one of the worst video games ever made.", "A widespread story stated that thousands of copies of E.T. the Extra Terrestrial video game were buried in a landfill", "A former Atari manager stated that 728,000 Atari games were in fact buried in a landfill.", "The Atari landfill was dug up and nearly 900 games were recovered, but there was only one copy of E.T. included."], "decomposition": ["Was what the widespread landfill rumor concerning copies of E.T. the Extra Terrestial video game?", "When the landfill was dug up, were the claims in #1 found to be false?", "Considering #2, does the rumor fit the description of an urban legend?"], "evidence": [[[["Atari video game burial-2"]], [["Atari video game burial-17"]], [["Legend-15"]]], [[["Atari video game burial-1"]], [["Atari video game burial-18"]], ["operation"]], [[["E.T. the Extra-Terrestrial (video game)-3"]], ["operation"], [["Urban legend-1"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is the E.T. the Extra-Terrestrial Atari Landfill story an urban legend?\nFacts: An urban legend is a humorous or horrifying story based on hearsay that is circulated as true. E.T. the Extra Terrestrial was panned as one of the worst video games ever made. A widespread story stated that thousands of copies of E.T. the Extra Terrestrial video game were buried in a landfill A former Atari manager stated that 728,000 Atari games were in fact buried in a landfill. The Atari landfill was dug up and nearly 900 games were recovered, but there was only one copy of E.T. included.\nAnswer (True or False):", "ct_subject": "An urban legend is a humorous or horrifying story based on hearsay that is circulated as true. E.T. the Extra Terrestrial was panned as one of the worst video games ever made. A widespread story stated that thousands of copies of E.T. the Extra Terrestrial video game were buried in a landfill A former Atari manager stated that 728,000 Atari games were in fact buried in a landfill. The Atari landfill was dug up and nearly 900 games were recovered, but there was only one copy of E.T. included.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"]]}, "1050": {"qid": "1e0f0676ee91a330262a", "term": "Lapidary", "description": "gemstone cutter", "question": "Does a lapidary work with items that are studied by geologists?", "answer": true, "facts": ["Some of the things geologists study include gemstones, minerals, and stone", "Lapidarists work with stone, minerals and gemstones"], "decomposition": ["What are the materials a lapidary works with?", "What do geologists study?", "Is any of #1 derived from #2?"], "evidence": [[[["Lapidary-1"]], [["Geologist-9"]], ["operation"]], [[["Lapidary-1"]], [["Geology-1"]], ["operation"]], [[["Lapidary-1"]], [["Geology-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a lapidary and a jeweler?\nFacts: A lapidary is a person who works with stones and gemstones. A jeweler is a person who works with metals.\nAnswer (True or False): True\nQuestion: What is the difference between a lapidary and a jeweler?\nFacts: A", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does a lapidary work with items that are studied by geologists?\nFacts: Some of the things geologists study include gemstones, minerals, and stone Lapidarists work with stone, minerals and gemstones\nAnswer (True or False):", "ct_subject": "Some of the things geologists study include gemstones, minerals, and stone Lapidarists work with stone, minerals and gemstones", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99992335"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.9999988"], [" between", "1.0"], [" a", "1.0"], [" lap", "0.9999999"], ["id", "1.0"], ["ary", "1.0"], [" and", "1.0"], [" a", "1.0"], [" jewel", "1.0"], ["er", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999752"], [" lap", "1.0"], ["id", "1.0"], ["ary", "1.0"], [" is", "0.9358009"], [" a", "1.0"], [" person", "1.0"], [" who", "1.0"], [" works", "0.99999857"], [" with", "1.0"], [" stones", "0.9997925"], [" and", "0.98821133"], [" gem", "0.9875438"], ["stones", "1.0"], [".", "1.0"], [" A", "1.0"], [" jewel", "1.0"], ["er", "1.0"], [" is", "1.0"], [" a", "1.0"], [" person", "1.0"], [" who", "1.0"], [" works", "1.0"], [" with", "1.0"], [" metals", "1.0"], [".", "0.9745554"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" lap", "1.0"], ["id", "1.0"], ["ary", "1.0"], [" and", "1.0"], [" a", "1.0"], [" jewel", "1.0"], ["er", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"]]}, "1051": {"qid": "3fe8e6070086a6cb0a8c", "term": "Tibia", "description": "larger of the two bones of the leg below the knee for vertebrates", "question": "Is the tibia required for floor exercises?", "answer": true, "facts": ["The tibia is a bone in the lower leg", "Floor exercises are a program in gymnastics competitions", "Gymnastics requires use of arms and legs, as well as other parts of the body"], "decomposition": ["What sport are floor exercises part of?  ", "What body parts does #1 require?", "What part of the body part is the tibia?", "Is #3 in #2?"], "evidence": [[[["Floor (gymnastics)-17"]], [["Leg-6"]], [["Tibia-1"]], ["operation"]], [[["Floor (gymnastics)-1"]], [["Gymnastics-1"]], [["Tibia-1"]], ["operation"]], [[["Floor (gymnastics)-1"]], [["Floor (gymnastics)-2"]], [["Tibia-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the tibia required for floor exercises?\nFacts: The tibia is a bone in the lower leg Floor exercises are a program in gymnastics competitions Gymnastics requires use of arms and legs, as well as other parts of the body\nAnswer (True or False): True\nQuestion: Is the tibia required for floor exercises?", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is the tibia required for floor exercises?\nFacts: The tibia is a bone in the lower leg Floor exercises are a program in gymnastics competitions Gymnastics requires use of arms and legs, as well as other parts of the body\nAnswer (True or False):", "ct_subject": "The tibia is a bone in the lower leg Floor exercises are a program in gymnastics competitions Gymnastics requires use of arms and legs, as well as other parts of the body", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99829227"], [" the", "0.9984933"], [" t", "0.9999995"], ["ibia", "1.0"], [" required", "1.0"], [" for", "1.0"], [" floor", "1.0"], [" exercises", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" t", "1.0"], ["ibia", "1.0"], [" is", "1.0"], [" a", "1.0"], [" bone", "1.0"], [" in", "1.0"], [" the", "1.0"], [" lower", "1.0"], [" leg", "1.0"], [" Floor", "1.0"], [" exercises", "1.0"], [" are", "1.0"], [" a", "1.0"], [" program", "1.0"], [" in", "1.0"], [" gymn", "1.0"], ["astics", "1.0"], [" competitions", "1.0"], [" Gym", "1.0"], ["n", "1.0"], ["astics", "1.0"], [" requires", "1.0"], [" use", "1.0"], [" of", "1.0"], [" arms", "1.0"], [" and", "1.0"], [" legs", "1.0"], [",", "1.0"], [" as", "1.0"], [" well", "1.0"], [" as", "1.0"], [" other", "1.0"], [" parts", "1.0"], [" of", "1.0"], [" the", "1.0"], [" body", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" t", "1.0"], ["ibia", "1.0"], [" required", "1.0"], [" for", "1.0"], [" floor", "1.0"], [" exercises", "1.0"], ["?", "1.0"]]}, "1052": {"qid": "463faa7aaa7d9e1e366b", "term": "Europa (moon)", "description": "The smallest of the four Galilean moons of Jupiter", "question": "Is Europa linked to Viennese waltzes?", "answer": true, "facts": ["Europa is a moon of Jupiter", "Europa played an important role in Stanley Kubrick's film 2001: A Space Odyssey", "The soundtrack to 2001: A Space Odyssey prominently featured The Blue Danube", "The Blue Danube is a famous Viennese waltz composed by Johan Strauss II"], "decomposition": ["Which moon of Jupiter played an important role in the film '2001: A Space Odyssey'?", "Is #1 Europa?", "Which soundtrack was prominently featured in the movie?", "Is #3 a Viennese waltz?", "Are #2 and #4 positive?"], "evidence": [[["no_evidence"], ["operation"], [["2001: A Space Odyssey (film)-2", "The Blue Danube-14"]], [["The Blue Danube-1"]], ["operation"]], [[["2001: A Space Odyssey (film)-7"], "no_evidence"], [["Europa (moon)-1"], "operation"], [["2001: A Space Odyssey (film)-2"]], [["The Blue Danube-1"]], ["operation"]], [[["2001: A Space Odyssey (film)-7"], "no_evidence"], ["no_evidence", "operation"], [["2001: A Space Odyssey (film)-2"]], [["Johann Strauss II-1", "The Blue Danube-1"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is the moon of Jupiter called Europa?\nFacts: Europa is a moon of Jupiter Europa is named after the Roman goddess of the sea and the moon Europa is the only moon in the solar system that has a surface that is not covered by ice.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is Europa linked to Viennese waltzes?\nFacts: Europa is a moon of Jupiter Europa played an important role in Stanley Kubrick's film 2001: A Space Odyssey The soundtrack to 2001: A Space Odyssey prominently featured The Blue Danube The Blue Danube is a famous Viennese waltz composed by Johan Strauss II\nAnswer (True or False):", "ct_subject": "Europa is a moon of Jupiter Europa played an important role in Stanley Kubrick's film 2001: A Space Odyssey The soundtrack to 2001: A Space Odyssey prominently featured The Blue Danube The Blue Danube is a famous Viennese waltz composed by Johan Strauss II", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" the", "1.0"], [" moon", "0.9997938"], [" of", "0.5473757"], [" Jupiter", "1.0"], [" called", "0.9995943"], [" Europa", "0.9334164"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Europa", "1.0"], [" is", "1.0"], [" a", "1.0"], [" moon", "1.0"], [" of", "1.0"], [" Jupiter", "1.0"], [" Europa", "1.0"], [" is", "1.0"], [" named", "0.32330194"], [" after", "1.0"], [" the", "1.0"], [" Roman", "0.999984"], [" goddess", "1.0"], [" of", "1.0"], [" the", "1.0"], [" sea", "1.0"], [" and", "0.58868885"], [" the", "1.0"], [" moon", "0.84924006"], [" Europa", "0.99945575"], [" is", "1.0"], [" the", "0.9907184"], [" only", "0.9999999"], [" moon", "1.0"], [" in", "0.9990157"], [" the", "1.0"], [" solar", "0.9990113"], [" system", "1.0"], [" that", "0.9996207"], [" has", "0.9999993"], [" a", "1.0"], [" surface", "0.10654948"], [" that", "1.0"], [" is", "1.0"], [" not", "0.76006955"], [" covered", "1.0"], [" by", "0.99821246"], [" ice", "0.9999981"], [".", "0.9791821"]]}, "1053": {"qid": "d667c4a43fea660f1a5f", "term": "Cholera", "description": "Bacterial infection of the small intestine", "question": "Is a platypus immune from cholera?", "answer": true, "facts": ["Cholera is a bacteria that damages the small intestines in humans.", "The intestines are part of the stomach of humans.", "A platypus does not have a stomach."], "decomposition": ["What parts of the body does Cholera damage?", "Does a platypus not have #1?"], "evidence": [[[["2016\u20132020 Yemen cholera outbreak-6"]], [["Platypus-4"], "no_evidence"]], [[["Vibrio cholerae-6"]], ["no_evidence"]], [[["Diseases and epidemics of the 19th century-10"]], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is a platypus a mammal?\nFacts: Platypuses are not mammals.\nAnswer (True or False): True\nQuestion: Is a platypus a reptile?\nFacts: Platypuses are not reptiles.\nAnswer (True or False): True\nQuestion: Is a platypus a fish?", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is a platypus immune from cholera?\nFacts: Cholera is a bacteria that damages the small intestines in humans. The intestines are part of the stomach of humans. A platypus does not have a stomach.\nAnswer (True or False):", "ct_subject": "Cholera is a bacteria that damages the small intestines in humans. The intestines are part of the stomach of humans. A platypus does not have a stomach.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.99994326"], [" a", "0.99999523"], [" plat", "1.0"], ["yp", "1.0"], ["us", "1.0"], [" a", "0.9999957"], [" mammal", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Plat", "0.98701525"], ["yp", "1.0"], ["uses", "1.0"], [" are", "1.0"], [" not", "0.9117625"], [" mammals", "1.0"], [".", "1.0"], ["\n", "0.9335925"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" a", "1.0"], [" plat", "1.0"], ["yp", "1.0"], ["us", "1.0"], [" a", "1.0"], [" rept", "0.83495927"], ["ile", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Plat", "1.0"], ["yp", "1.0"], ["uses", "1.0"], [" are", "1.0"], [" not", "1.0"], [" reptiles", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" a", "1.0"], [" plat", "1.0"], ["yp", "1.0"], ["us", "1.0"], [" a", "1.0"], [" fish", "0.8509223"], ["?", "1.0"]]}, "1054": {"qid": "e40cbe4b89942e256377", "term": "Jerry Seinfeld", "description": "American comedian and actor", "question": "Did Jerry Seinfeld have reason to cheer in 1986?", "answer": true, "facts": ["Jerry Seinfeld is a fan of the New York Mets baseball team", "The New York Mets won a World Series title in 1986"], "decomposition": ["Do fans cheer if their team wins?", "Is Jerry Seinfeld a NY Mets fan?", "Did the NY Mets win the World Series in 1986?", "Is #1, #2 and #3 \"yes\"?"], "evidence": [[[["Cheering-20"]], [["Jerry Seinfeld-28"]], [["1986 World Series-4"]], ["operation"]], [[["Cheering-18"], "no_evidence"], [["Jerry Seinfeld-28"]], [["1986 World Series-1"]], ["operation"]], [[["Cheering-1"]], [["The Boyfriend (Seinfeld)-2"]], [["1986 World Series-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Beatles have a song called \"I Want to Hold Your Hand\" in 1964?\nFacts: The Beatles did not have a song called \"I Want to Hold Your Hand\" in 1964.\nAnswer (True or False): True\nQuestion: Did the Beatles have a song called \"I Want to Hold Your Hand\" in 1964?\nFacts: The Beatles did not have a song", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Jerry Seinfeld have reason to cheer in 1986?\nFacts: Jerry Seinfeld is a fan of the New York Mets baseball team The New York Mets won a World Series title in 1986\nAnswer (True or False):", "ct_subject": "Jerry Seinfeld is a fan of the New York Mets baseball team The New York Mets won a World Series title in 1986", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.20906818"], [" the", "1.0"], [" Beatles", "0.9964162"], [" have", "1.0"], [" a", "0.99999964"], [" song", "1.0"], [" called", "1.0"], [" \"", "1.0"], ["I", "0.9898046"], [" Want", "0.9999999"], [" to", "1.0"], [" Hold", "1.0"], [" Your", "1.0"], [" Hand", "1.0"], ["\"", "0.87773573"], [" in", "0.9999999"], [" 1964", "0.97620934"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Beatles", "1.0"], [" did", "0.9725319"], [" not", "1.0"], [" have", "1.0"], [" a", "1.0"], [" song", "1.0"], [" called", "1.0"], [" \"", "1.0"], ["I", "1.0"], [" Want", "1.0"], [" to", "1.0"], [" Hold", "1.0"], [" Your", "1.0"], [" Hand", "1.0"], ["\"", "1.0"], [" in", "1.0"], [" 1964", "1.0"], [".", "0.99999666"], ["\n", "0.96876717"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "1.0"], [" the", "1.0"], [" Beatles", "1.0"], [" have", "1.0"], [" a", "1.0"], [" song", "1.0"], [" called", "1.0"], [" \"", "1.0"], ["I", "1.0"], [" Want", "1.0"], [" to", "1.0"], [" Hold", "1.0"], [" Your", "1.0"], [" Hand", "1.0"], ["\"", "1.0"], [" in", "1.0"], [" 1964", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Beatles", "1.0"], [" did", "1.0"], [" not", "1.0"], [" have", "1.0"], [" a", "1.0"], [" song", "1.0"]]}, "1055": {"qid": "4148e19a53dfbb020484", "term": "Anchovy", "description": "Family of fishes", "question": "Are there bones in an anchovy pizza?", "answer": true, "facts": ["Anchovies used on pizza are typically packed whole in oil or water. ", "Anchovies on pizza are not usually cut or filleted in any way."], "decomposition": ["Which fishes are used in anchovy pizza?", "Are #1 usually packed whole into the pizza?"], "evidence": [[[["Anchovies as food-3"]], [["Anchovies as food-2"]]], [[["Anchovies as food-2"]], [["Anchovies as food-2"], "no_evidence", "operation"]], [[["Anchovy-3"]], [["Anchovy-3"]]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any bones in a chicken?\nFacts: Chicken bones are usually not used in pizza.\nAnswer (True or False): True\nQuestion: Are there any bones in a chicken wing?\nFacts: Chicken wings are usually not used in pizza.\nAnswer (True or False): True\nQuestion: Are there any bones in a chicken wing?\nFacts", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are there bones in an anchovy pizza?\nFacts: Anchovies used on pizza are typically packed whole in oil or water.  Anchovies on pizza are not usually cut or filleted in any way.\nAnswer (True or False):", "ct_subject": "Anchovies used on pizza are typically packed whole in oil or water.  Anchovies on pizza are not usually cut or filleted in any way.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "1.0"], [" any", "0.9684019"], [" bones", "0.9991055"], [" in", "1.0"], [" a", "1.0"], [" chicken", "1.0"], ["?", "0.99975556"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Chicken", "0.99998295"], [" bones", "1.0"], [" are", "1.0"], [" usually", "0.9999002"], [" not", "0.80688304"], [" used", "0.9336105"], [" in", "1.0"], [" pizza", "1.0"], [".", "1.0"], ["\n", "0.7051446"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "1.0"], [" any", "1.0"], [" bones", "1.0"], [" in", "1.0"], [" a", "1.0"], [" chicken", "0.99541926"], [" wing", "0.9996228"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Chicken", "1.0"], [" wings", "1.0"], [" are", "1.0"], [" usually", "0.9999498"], [" not", "1.0"], [" used", "1.0"], [" in", "1.0"], [" pizza", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "1.0"], [" there", "1.0"], [" any", "1.0"], [" bones", "1.0"], [" in", "1.0"], [" a", "1.0"], [" chicken", "0.9999944"], [" wing", "0.87575877"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"]]}, "1056": {"qid": "9aed4083afb75e9f247c", "term": "Douglas fir", "description": "species of tree", "question": "Have Douglas fir been used to fight wars?", "answer": true, "facts": ["Douglas fir are a type of tree.", "Douglas fir are used to make ships.", "The Minesweeper is a small warship made from Douglas fir.", "The Minesweeper was made of wood to reduce it's risk magnetic signature and likely hood of detonating mines."], "decomposition": ["What are the uses of Douglas Fir?", "Does #1 include vehicles of war?"], "evidence": [[[["Douglas fir-16", "Douglas fir-23"], "no_evidence"], ["operation"]], [["no_evidence"], [["War-1"], "no_evidence"]], [["no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the city of Rome?\nFacts: Rome is the name of the city in which the Roman Empire was founded.\nAnswer (True or False): True\nQuestion: What is the name of the city of Rome?\nFacts:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Have Douglas fir been used to fight wars?\nFacts: Douglas fir are a type of tree. Douglas fir are used to make ships. The Minesweeper is a small warship made from Douglas fir. The Minesweeper was made of wood to reduce it's risk magnetic signature and likely hood of detonating mines.\nAnswer (True or False):", "ct_subject": "Douglas fir are a type of tree. Douglas fir are used to make ships. The Minesweeper is a small warship made from Douglas fir. The Minesweeper was made of wood to reduce it's risk magnetic signature and likely hood of detonating mines.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.99996984"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99990606"], [" of", "1.0"], [" the", "1.0"], [" city", "0.70333517"], [" of", "0.9999999"], [" Rome", "0.9908881"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Rome", "0.9998779"], [" is", "0.99772936"], [" the", "0.9989322"], [" name", "0.9957235"], [" of", "1.0"], [" the", "1.0"], [" city", "0.993929"], [" in", "0.985581"], [" which", "0.9999999"], [" the", "1.0"], [" Roman", "1.0"], [" Empire", "0.99999714"], [" was", "1.0"], [" founded", "1.0"], [".", "1.0"], ["\n", "0.9999993"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" city", "1.0"], [" of", "1.0"], [" Rome", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"]]}, "1057": {"qid": "e8cc7615cfaf45069eb5", "term": "Ludacris", "description": "American rapper and actor", "question": "Can you watch the Borgia's World of Wonders before Ludacris's Release Therapy finishes?", "answer": true, "facts": ["World of Wonders is an episode of the Showtime TV series The Borgias, with a run time of 49 minutes.", "Ludacris's 2006 album Release Therapy has a run time of 62 minutes."], "decomposition": ["What is the run time of  the Borgia's World of Wonders?", "What is the run time of  Ludacris's Release Therapy?", "Is #1 shorter than #2?"], "evidence": [[[["The Borgias (2011 TV series)-14"], "no_evidence"], [["Release Therapy-1"], "no_evidence"], ["operation"]], [[["The Borgias (2011 TV series)-1", "The Borgias (2011 TV series)-4"], "no_evidence"], [["Release Therapy-1"], "no_evidence"], ["no_evidence", "operation"]], [["no_evidence"], ["no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the song that Ludacris sings in the movie \"The Big Lebowski\"?\nFacts: The song is \"The Big Lebowski\" by The Dude.\nAnswer (True or False): True\nQuestion: What is the name of the song", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can you watch the Borgia's World of Wonders before Ludacris's Release Therapy finishes?\nFacts: World of Wonders is an episode of the Showtime TV series The Borgias, with a run time of 49 minutes. Ludacris's 2006 album Release Therapy has a run time of 62 minutes.\nAnswer (True or False):", "ct_subject": "World of Wonders is an episode of the Showtime TV series The Borgias, with a run time of 49 minutes. Ludacris's 2006 album Release Therapy has a run time of 62 minutes.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999883"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" song", "0.69287163"], [" that", "0.9982724"], [" Lud", "0.9990343"], ["ac", "1.0"], ["ris", "1.0"], [" sings", "0.99999964"], [" in", "0.9788316"], [" the", "1.0"], [" movie", "0.9628893"], [" \"", "0.98705924"], ["The", "1.0"], [" Big", "0.99972993"], [" Le", "1.0"], ["b", "1.0"], ["owski", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.99999964"], [" song", "1.0"], [" is", "1.0"], [" \"", "0.9968568"], ["The", "1.0"], [" Big", "1.0"], [" Le", "1.0"], ["b", "1.0"], ["owski", "1.0"], ["\"", "0.9978173"], [" by", "1.0"], [" The", "0.9998908"], [" Dude", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" song", "0.99999785"]]}, "1058": {"qid": "da76a093d3b1cc47c6f2", "term": "Traffic collision", "description": "occurs when a vehicle collides with another vehicle, pedestrian, animal, road debris, or other stationary obstruction, such as a tree, pole or building or drives off the road", "question": "Can a traffic collision make someone a millionaire?", "answer": true, "facts": ["Traffic collisions sometimes result in extremely expensive physical damage.", "Physical damage is compensated by insurance companies in the form of monetary payment.", "Million dollar verdicts are sometimes awarded for traffic collisions that result in major damage. "], "decomposition": ["What can kind of damage can traffic collisions cause?", "If #1 occurs, how would insurance companies react?", "Can #2 sometimes occur in a million dollar verdict?"], "evidence": [[[["Personal injury-1"], "no_evidence"], [["Personal injury-26"], "no_evidence"], [["Pain and suffering-5"], "no_evidence", "operation"]], [[["Traffic collision-1"]], [["Vehicle insurance in the United States-1"]], [["Traffic collision-84"]]], [[["Traffic collision-1"]], [["Vehicle insurance-1"]], ["no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake with a hole in it\"?\nFacts: A snowflake is a single snowflake. A snowflake with a hole in it is a single snowflake with a hole in it.\nAnswer (True or False): True\nQuestion:", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Can a traffic collision make someone a millionaire?\nFacts: Traffic collisions sometimes result in extremely expensive physical damage. Physical damage is compensated by insurance companies in the form of monetary payment. Million dollar verdicts are sometimes awarded for traffic collisions that result in major damage. \nAnswer (True or False):", "ct_subject": "Traffic collisions sometimes result in extremely expensive physical damage. Physical damage is compensated by insurance companies in the form of monetary payment. Million dollar verdicts are sometimes awarded for traffic collisions that result in major damage. ", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999968"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.99772257"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.9999393"], ["s", "0.98447996"], ["now", "0.9999975"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], [" with", "0.30286115"], [" a", "0.9999995"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], ["\"?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "0.9999999"], [" snow", "0.9999201"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "0.99999785"], [" snow", "0.9690066"], ["flake", "1.0"], [".", "0.9023491"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" with", "1.0"], [" a", "1.0"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], [" is", "1.0"], [" a", "1.0"], [" single", "0.9999962"], [" snow", "1.0"], ["flake", "1.0"], [" with", "1.0"], [" a", "1.0"], [" hole", "1.0"], [" in", "1.0"], [" it", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"]]}, "1059": {"qid": "5d2e46bc031e1ccf88b4", "term": "Spaghetti", "description": "Type of pasta", "question": "Is it unusual to eat spaghetti without a fork?", "answer": true, "facts": ["Spaghetti noodles are long and thin, they are difficult to scoop and must be twirled.", "Spaghetti is never served in a restaurant without a fork."], "decomposition": ["Is Spaghetti usually eaten using a fork?"], "evidence": [[[["Spaghetti-1"], "no_evidence"]], [[["Italian cuisine-17", "Spaghetti-1"], "no_evidence", "operation"]], [[["Spaghetti-15"], "no_evidence"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the difference between a \"snowflake\" and a \"snowflake\"?\nFacts: A snowflake is a small, round, hard, and shiny object. A snowflake is not a \"snowflake\".\nAnswer (True or False): True\nQuestion: What is the difference between a \"snowflake\" and a \"snow", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is it unusual to eat spaghetti without a fork?\nFacts: Spaghetti noodles are long and thin, they are difficult to scoop and must be twirled. Spaghetti is never served in a restaurant without a fork.\nAnswer (True or False):", "ct_subject": "Spaghetti noodles are long and thin, they are difficult to scoop and must be twirled. Spaghetti is never served in a restaurant without a fork.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.07124914"], [" is", "1.0"], [" the", "1.0"], [" difference", "0.786931"], [" between", "1.0"], [" a", "1.0"], [" \"", "0.9995701"], ["s", "0.9652888"], ["now", "0.9651061"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "0.9999925"], ["now", "1.0"], ["flake", "1.0"], ["\"?", "0.99997866"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" A", "1.0"], [" snow", "0.9999988"], ["flake", "1.0"], [" is", "1.0"], [" a", "1.0"], [" small", "0.9951336"], [",", "1.0"], [" round", "0.9999534"], [",", "1.0"], [" hard", "0.9997359"], [",", "1.0"], [" and", "0.9999198"], [" shiny", "1.0"], [" object", "0.99989605"], [".", "0.99999297"], [" A", "1.0"], [" snow", "1.0"], ["flake", "1.0"], [" is", "1.0"], [" not", "0.93338495"], [" a", "1.0"], [" \"", "0.98986673"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\".", "0.99998844"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" difference", "1.0"], [" between", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"], ["flake", "1.0"], ["\"", "1.0"], [" and", "1.0"], [" a", "1.0"], [" \"", "1.0"], ["s", "1.0"], ["now", "1.0"]]}, "1060": {"qid": "818a301e9962bdf987a9", "term": "Chinese Americans", "description": "Ethnic group", "question": "Do Chinese Americans face discrimination at a Federal level in the US?", "answer": true, "facts": ["The President of the United States frequently referred to the COVID-19 pandemic as a 'Chinese Virus' and 'Kung Flu', encouraging the use of derogatory language towards Chinese Americans.", "The President of the United States has not called for the violence and hate towards Chinese Americans in response to COVID-19 to end."], "decomposition": ["Who is the head of the US Federal Government?", "Does #1 behave in a discriminatory way toward Chinese Americans?"], "evidence": [[[["Federal government of the United States-17"]], [["Anti-Chinese sentiment in the United States-28"], "no_evidence"]], [[["Donald Trump-1"]], [["Donald Trump-128"], "no_evidence", "operation"]], [[["Donald Trump-1"]], [["Donald Trump-128", "Donald Trump-154"], "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Do the Chinese people have a unique culture?\nFacts: The Chinese people have a unique culture. The Chinese people have a unique culture. The Chinese people have a unique culture. The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Do Chinese Americans face discrimination at a Federal level in the US?\nFacts: The President of the United States frequently referred to the COVID-19 pandemic as a 'Chinese Virus' and 'Kung Flu', encouraging the use of derogatory language towards Chinese Americans. The President of the United States has not called for the violence and hate towards Chinese Americans in response to COVID-19 to end.\nAnswer (True or False):", "ct_subject": "The President of the United States frequently referred to the COVID-19 pandemic as a 'Chinese Virus' and 'Kung Flu', encouraging the use of derogatory language towards Chinese Americans. The President of the United States has not called for the violence and hate towards Chinese Americans in response to COVID-19 to end.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Do", "0.82154375"], [" the", "0.99995935"], [" Chinese", "1.0"], [" people", "0.99988353"], [" have", "1.0"], [" a", "1.0"], [" unique", "0.72078216"], [" culture", "0.99999964"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "0.9999999"], [" Chinese", "1.0"], [" people", "1.0"], [" have", "0.9980934"], [" a", "1.0"], [" unique", "1.0"], [" culture", "1.0"], [".", "0.9942835"], [" The", "0.9999989"], [" Chinese", "1.0"], [" people", "1.0"], [" have", "0.9994273"], [" a", "1.0"], [" unique", "1.0"], [" culture", "1.0"], [".", "1.0"], [" The", "0.99999976"], [" Chinese", "1.0"], [" people", "1.0"], [" have", "1.0"], [" a", "1.0"], [" unique", "1.0"], [" culture", "1.0"], [".", "1.0"], [" The", "1.0"]]}, "1061": {"qid": "45c1de8a7cfc9ad51786", "term": "Asteroid", "description": "Minor planet that is not a comet", "question": "Could largest asteroid crush a whole city?", "answer": true, "facts": ["The largest asteroids are the size of miniature planets.", "Mercury is the smallest planet and has a radius of 1,516 miles.", "New York City is 13.4 miles long and 2.3 miles wide. ", "Mercury weighs 3.285 \u00d7 10^23 kg."], "decomposition": ["What is the size of the largest asteroid?", "What is the size of New York City?", "Is #1 bigger than #2?"], "evidence": [[[["Ceres (dwarf planet)-1"]], [["New York City-1"]], ["operation"]], [[["Asteroid-38"]], [["New York City-1"]], [["New York City-1", "Tunguska event-1"], "operation"]], [[["Ceres (dwarf planet)-1"]], [["New York metropolitan area-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the largest number of people that can fit in a room?\nFacts: The largest number of people that can fit in a room is 2,000.\nAnswer (True or False): True\nQuestion: What is the largest number of people that can fit", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Could largest asteroid crush a whole city?\nFacts: The largest asteroids are the size of miniature planets. Mercury is the smallest planet and has a radius of 1,516 miles. New York City is 13.4 miles long and 2.3 miles wide.  Mercury weighs 3.285 \u00d7 10^23 kg.\nAnswer (True or False):", "ct_subject": "The largest asteroids are the size of miniature planets. Mercury is the smallest planet and has a radius of 1,516 miles. New York City is 13.4 miles long and 2.3 miles wide.  Mercury weighs 3.285 \u00d7 10^23 kg.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9993892"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" number", "0.91063565"], [" of", "1.0"], [" people", "1.0"], [" that", "0.999979"], [" can", "0.96688277"], [" fit", "0.99970466"], [" in", "0.91090953"], [" a", "1.0"], [" room", "0.99370956"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" largest", "1.0"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" that", "1.0"], [" can", "1.0"], [" fit", "1.0"], [" in", "1.0"], [" a", "1.0"], [" room", "1.0"], [" is", "1.0"], [" 2", "0.99999905"], [",", "0.99855393"], ["000", "0.99998367"], [".", "1.0"], ["\n", "0.90096533"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" largest", "1.0"], [" number", "1.0"], [" of", "1.0"], [" people", "1.0"], [" that", "1.0"], [" can", "1.0"], [" fit", "1.0"]]}, "1062": {"qid": "606181aacf5722b85a0a", "term": "Mona Lisa", "description": "Painting by Leonardo da Vinci", "question": "After viewing the Mona Lisa, could you get lunch nearby on foot?", "answer": true, "facts": ["The Mona Lisa is housed in The Louvre.", "There are many restaurants within walking distance of The Louvre."], "decomposition": ["Where is the Mona Lisa located?", "Is #1 a place likely to have at least a restaurant/hotel nearby?"], "evidence": [[[["Mona Lisa-54"]], [["Louvre-60"]]], [[["Mona Lisa-29"]], [["Louvre-60"]]], [[["Louvre-1", "Mona Lisa-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the longest distance between two points on Earth?\nFacts: The distance between the Earth and the Sun is about 93 million miles.\nAnswer (True or False): True\nQuestion: What is the longest distance between two points on the Moon?\nFacts: The distance between the Earth and the Moon is about 238,000 miles.\nAnswer (True or False):", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: After viewing the Mona Lisa, could you get lunch nearby on foot?\nFacts: The Mona Lisa is housed in The Louvre. There are many restaurants within walking distance of The Louvre.\nAnswer (True or False):", "ct_subject": "The Mona Lisa is housed in The Louvre. There are many restaurants within walking distance of The Louvre.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.9999783"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.9975262"], [" distance", "0.9999889"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"], [" on", "0.9967231"], [" Earth", "0.99958295"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" distance", "0.99998415"], [" between", "1.0"], [" the", "0.99999094"], [" Earth", "0.99999464"], [" and", "1.0"], [" the", "1.0"], [" Sun", "0.83640844"], [" is", "1.0"], [" about", "1.0"], [" 93", "1.0"], [" million", "1.0"], [" miles", "1.0"], [".", "1.0"], ["\n", "0.9718355"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" longest", "0.99999964"], [" distance", "1.0"], [" between", "1.0"], [" two", "1.0"], [" points", "1.0"], [" on", "1.0"], [" the", "1.0"], [" Moon", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" distance", "0.99999964"], [" between", "1.0"], [" the", "1.0"], [" Earth", "0.99870026"], [" and", "1.0"], [" the", "1.0"], [" Moon", "1.0"], [" is", "1.0"], [" about", "1.0"], [" 238", "0.999795"], [",", "1.0"], ["000", "1.0"], [" miles", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"]]}, "1063": {"qid": "777828272c5e0915f4d1", "term": "Ivan the Terrible", "description": "Grand Prince of Moscow and 1st Tsar of Russia", "question": "Did Ivan the Terrible's father and grandfather have nicer nicknames?", "answer": true, "facts": ["Ivan the Terrible was nicknamed terrible because of his harsh rule.", "Ivan the Terrible's father, Vasili III Ivanovich, was nicknamed Vasili the Adequate.", "Ivan the Terrible's grandfather, Ivan III Vasilyevich, was nicknamed Ivan the Great."], "decomposition": ["Who was Ivan the Terrible's father?", "Who was the father of #1?", "Do #1 and #2 have nicer nicknames than \"the Terrible\"?"], "evidence": [[[["Vasili III of Russia-1"]], [["Ivan III of Russia-1", "Vasili III of Russia-1"]], ["operation"]], [[["Vasili III of Russia-1"]], [["Vasili III of Russia-1"]], [["Vasili III of Russia-1"], "operation"]], [[["Vasili III of Russia-1"]], [["Ivan III of Russia-1"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Did the Roman Empire have a king?\nFacts: The Roman Empire had a king. The Roman Empire had a king. The Roman Empire had a king. The Roman Empire had a king. The Roman Empire had a king.", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Did Ivan the Terrible's father and grandfather have nicer nicknames?\nFacts: Ivan the Terrible was nicknamed terrible because of his harsh rule. Ivan the Terrible's father, Vasili III Ivanovich, was nicknamed Vasili the Adequate. Ivan the Terrible's grandfather, Ivan III Vasilyevich, was nicknamed Ivan the Great.\nAnswer (True or False):", "ct_subject": "Ivan the Terrible was nicknamed terrible because of his harsh rule. Ivan the Terrible's father, Vasili III Ivanovich, was nicknamed Vasili the Adequate. Ivan the Terrible's grandfather, Ivan III Vasilyevich, was nicknamed Ivan the Great.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Did", "0.99998987"], [" the", "1.0"], [" Roman", "0.99999976"], [" Empire", "0.9999999"], [" have", "1.0"], [" a", "1.0"], [" king", "0.99988055"], ["?", "0.9821308"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [" had", "1.0"], [" a", "1.0"], [" king", "1.0"], [".", "0.9985475"], [" The", "1.0"], [" Roman", "1.0"], [" Empire", "1.0"], [" had", "0.99611366"], [" a", "1.0"], [" king", "1.0"], [".", "1.0"], [" The", "0.9831943"], [" Roman", "1.0"], [" Empire", "1.0"], [" had", "1.0"], [" a", "1.0"], [" king", "1.0"], [".", "1.0"], [" The", "0.99999917"], [" Roman", "1.0"], [" Empire", "1.0"], [" had", "1.0"], [" a", "1.0"], [" king", "1.0"], [".", "1.0"], [" The", "0.99999666"], [" Roman", "1.0"], [" Empire", "1.0"], [" had", "1.0"], [" a", "1.0"], [" king", "1.0"], [".", "1.0"]]}, "1064": {"qid": "7a096b2fc559fd5c7919", "term": "Green", "description": "Additive primary color visible between blue and yellow", "question": "Is a paleo dieter unlikely to color beverages green for St. Patrick's Day?", "answer": true, "facts": ["There is no natural source for green food coloring approved by the FDA", "A paleo diet avoids artificial colors and flavors"], "decomposition": ["What are some common FDA approved sources of green color applied to beverages?", "What kind of foods would a paleo dieter avoid?", "Is #1 included in #2?"], "evidence": [[[["Food coloring-11"]], [["Paleolithic diet-3"]], ["operation"]], [[["Food coloring-15"]], [["Paleolithic diet-12"]], [["Paleolithic diet-12"], "operation"]], [[["Fast Green FCF-1"]], [["Paleolithic diet-3"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is it possible to get a tattoo that is not a tattoo?\nFacts: No. Tattoos are permanent and cannot be removed.\nAnswer (True or False): True\nQuestion: Is it possible to get a tattoo that is not a tattoo?\nFacts: No. Tattoos are permanent and cannot be removed.\nAnswer (True or False): True\n", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is a paleo dieter unlikely to color beverages green for St. Patrick's Day?\nFacts: There is no natural source for green food coloring approved by the FDA A paleo diet avoids artificial colors and flavors\nAnswer (True or False):", "ct_subject": "There is no natural source for green food coloring approved by the FDA A paleo diet avoids artificial colors and flavors", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "0.90449375"], [" possible", "0.9831383"], [" to", "1.0"], [" get", "0.92262715"], [" a", "1.0"], [" tattoo", "0.9802885"], [" that", "0.99999845"], [" is", "0.9980336"], [" not", "1.0"], [" a", "0.9999715"], [" tattoo", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" No", "0.9996327"], [".", "0.96649754"], [" Tatt", "0.9999738"], ["oos", "1.0"], [" are", "1.0"], [" permanent", "1.0"], [" and", "0.6118888"], [" cannot", "0.9999999"], [" be", "1.0"], [" removed", "1.0"], [".", "0.9999999"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" it", "1.0"], [" possible", "1.0"], [" to", "1.0"], [" get", "1.0"], [" a", "1.0"], [" tattoo", "1.0"], [" that", "1.0"], [" is", "1.0"], [" not", "1.0"], [" a", "1.0"], [" tattoo", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" No", "1.0"], [".", "1.0"], [" Tatt", "1.0"], ["oos", "1.0"], [" are", "1.0"], [" permanent", "1.0"], [" and", "1.0"], [" cannot", "1.0"], [" be", "1.0"], [" removed", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"]]}, "1065": {"qid": "6cc5056659f843124e0b", "term": "Waiting staff", "description": "staff serving in restaurant or private homes", "question": "Are there some countries where waiting staff need no tip?", "answer": true, "facts": ["In Japan, leaving a tip for a server is considered rude.", "In Denmark, servers and wait staff are well paid and tipping is very uncommon."], "decomposition": ["In how many countries is it socially acceptable to not tip the waiting staff?", "Is #1 greater than one?"], "evidence": [[[["Gratuity-15", "Gratuity-18", "Gratuity-36", "Gratuity-48"]], ["operation"]], [[["Gratuity-2"], "no_evidence"], ["no_evidence", "operation"]], [[["Gratuity-19"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any countries where the government has a monopoly on money?\nFacts: In the United States, the government has a monopoly on money. In the United Kingdom, the government has a monopoly on money. In Canada, the government has a monopoly on money. In Australia, the government has a monopoly on money. In New Zealand, the government has a monopoly on money", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are there some countries where waiting staff need no tip?\nFacts: In Japan, leaving a tip for a server is considered rude. In Denmark, servers and wait staff are well paid and tipping is very uncommon.\nAnswer (True or False):", "ct_subject": "In Japan, leaving a tip for a server is considered rude. In Denmark, servers and wait staff are well paid and tipping is very uncommon.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.758025"], [" there", "1.0"], [" any", "1.0"], [" countries", "1.0"], [" where", "1.0"], [" the", "0.99981576"], [" government", "0.9999671"], [" has", "0.99906117"], [" a", "0.9999999"], [" monopoly", "1.0"], [" on", "1.0"], [" money", "0.97283685"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" In", "0.9999237"], [" the", "1.0"], [" United", "1.0"], [" States", "1.0"], [",", "1.0"], [" the", "1.0"], [" government", "0.9827572"], [" has", "1.0"], [" a", "1.0"], [" monopoly", "1.0"], [" on", "1.0"], [" money", "1.0"], [".", "1.0"], [" In", "1.0"], [" the", "1.0"], [" United", "1.0"], [" Kingdom", "1.0"], [",", "1.0"], [" the", "1.0"], [" government", "1.0"], [" has", "1.0"], [" a", "1.0"], [" monopoly", "1.0"], [" on", "1.0"], [" money", "1.0"], [".", "1.0"], [" In", "1.0"], [" Canada", "0.69078344"], [",", "1.0"], [" the", "1.0"], [" government", "1.0"], [" has", "1.0"], [" a", "1.0"], [" monopoly", "1.0"], [" on", "1.0"], [" money", "1.0"], [".", "1.0"], [" In", "0.99999344"], [" Australia", "1.0"], [",", "1.0"], [" the", "1.0"], [" government", "1.0"], [" has", "1.0"], [" a", "1.0"], [" monopoly", "1.0"], [" on", "1.0"], [" money", "1.0"], [".", "1.0"], [" In", "0.99986875"], [" New", "1.0"], [" Zealand", "1.0"], [",", "1.0"], [" the", "1.0"], [" government", "1.0"], [" has", "1.0"], [" a", "1.0"], [" monopoly", "1.0"], [" on", "1.0"], [" money", "1.0"]]}, "1066": {"qid": "e1ebc4b698eb5bd4fbbe", "term": "Citrus", "description": "genus of fruit-bearing plants (source of fruit such as lemons and oranges)", "question": "Is there a Marvel villain with the same name as a kind of citrus fruit?", "answer": true, "facts": ["Mandarins are a type of orange popular in Asian cuisine.", "The Mandarin is also the name of a villain associated with Iron Man in the Marvel universe."], "decomposition": ["Which popular villains has Marvel's Ironman faced off against?", "Do any of #1's name also refer to a citrus fruit?"], "evidence": [[[["Iron Man-24"]], [["Mandarin orange-1"]]], [[["Iron Man-24"]], [["Mandarin orange-1"], "operation"]], [[["Captain Citrus-1"], "no_evidence"], ["no_evidence", "operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Is there a Marvel villain with the same name as a kind of citrus fruit?\nFacts: Mandarins are a type of orange popular in Asian cuisine. The Mandarin is also the name of a villain associated with Iron Man in the Marvel universe.\nAnswer (True or False): True\nQuestion: Is there a Marvel villain with the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Is there a Marvel villain with the same name as a kind of citrus fruit?\nFacts: Mandarins are a type of orange popular in Asian cuisine. The Mandarin is also the name of a villain associated with Iron Man in the Marvel universe.\nAnswer (True or False):", "ct_subject": "Mandarins are a type of orange popular in Asian cuisine. The Mandarin is also the name of a villain associated with Iron Man in the Marvel universe.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "0.9877631"], [" there", "1.0"], [" a", "1.0"], [" Marvel", "0.99999"], [" villain", "1.0"], [" with", "1.0"], [" the", "1.0"], [" same", "1.0"], [" name", "1.0"], [" as", "1.0"], [" a", "1.0"], [" kind", "0.9999988"], [" of", "1.0"], [" citrus", "1.0"], [" fruit", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Mand", "1.0"], ["ar", "1.0"], ["ins", "1.0"], [" are", "1.0"], [" a", "1.0"], [" type", "1.0"], [" of", "1.0"], [" orange", "1.0"], [" popular", "1.0"], [" in", "1.0"], [" Asian", "1.0"], [" cuisine", "1.0"], [".", "1.0"], [" The", "1.0"], [" Mandarin", "1.0"], [" is", "1.0"], [" also", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" a", "1.0"], [" villain", "1.0"], [" associated", "1.0"], [" with", "1.0"], [" Iron", "1.0"], [" Man", "1.0"], [" in", "1.0"], [" the", "1.0"], [" Marvel", "1.0"], [" universe", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Is", "1.0"], [" there", "1.0"], [" a", "1.0"], [" Marvel", "1.0"], [" villain", "1.0"], [" with", "1.0"], [" the", "1.0"]]}, "1067": {"qid": "423d83b2533271615c06", "term": "Breast cancer", "description": "cancer that originates in the mammary gland", "question": "Are amoebas safe from breast cancer?", "answer": true, "facts": ["Breast cancer is a disease that occurs in the mammary tissues of mammals.", "Amoebas are single cell organisms that lack mammary tissue."], "decomposition": ["What tissue does breast cancer affect?", "Is having #1 a necessary condition for breast cancer?", "By #2, is it the case that if an organism lacks #1 they cannot get breast cancer?"], "evidence": [[[["Breast cancer-1"]], [["Breast cancer-2"]], [["Breast cancer-2"]]], [[["Breast cancer-1"]], ["operation"], ["operation"]], [[["Breast cancer-2"]], [["Breast cancer-8"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: Are there any other diseases that can be transmitted by mosquitoes?\nFacts: Mosquitoes are not known to transmit any other diseases.\nAnswer (True or False): True\nQuestion: Are there any other diseases that can be transmitted by mosquitoes?\nFacts: Mosquitoes are not known to transmit any other diseases.\nAnswer (True or False): True\nQuestion", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are amoebas safe from breast cancer?\nFacts: Breast cancer is a disease that occurs in the mammary tissues of mammals. Amoebas are single cell organisms that lack mammary tissue.\nAnswer (True or False):", "ct_subject": "Breast cancer is a disease that occurs in the mammary tissues of mammals. Amoebas are single cell organisms that lack mammary tissue.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.9848043"], [" there", "0.9999981"], [" any", "0.9999999"], [" other", "0.75549453"], [" diseases", "0.9390598"], [" that", "1.0"], [" can", "0.27729866"], [" be", "1.0"], [" transmitted", "0.9997451"], [" by", "0.99999976"], [" mosquitoes", "0.9962457"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Mos", "0.45239982"], ["quit", "1.0"], ["oes", "1.0"], [" are", "1.0"], [" not", "0.49427128"], [" known", "1.0"], [" to", "1.0"], [" transmit", "1.0"], [" any", "1.0"], [" other", "0.6028738"], [" diseases", "0.9999999"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" Are", "0.9998814"], [" there", "1.0"], [" any", "1.0"], [" other", "1.0"], [" diseases", "1.0"], [" that", "1.0"], [" can", "1.0"], [" be", "1.0"], [" transmitted", "1.0"], [" by", "1.0"], [" mosquitoes", "0.9999995"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Mos", "1.0"], ["quit", "1.0"], ["oes", "1.0"], [" are", "1.0"], [" not", "1.0"], [" known", "1.0"], [" to", "1.0"], [" transmit", "1.0"], [" any", "1.0"], [" other", "1.0"], [" diseases", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"]]}, "1068": {"qid": "79595ffe6b93c5e84056", "term": "Selfie", "description": "Photographic self-portrait", "question": "Are selfies more dangerous than plague in modern times?", "answer": true, "facts": ["There are an average of 7 human plague cases reported each year according to the CDC.", "Selfies have caused people to fall off of cliffs while trying to get the perfect picture.", "From October 2011 and November 2017, there were 259 selfie deaths in 137 incidents."], "decomposition": ["How many cases of the plague are there yearly?", "How many people die yearly while taking selfies?", "Is #2 greater than #1?"], "evidence": [[[["Epidemiology of plague-23"]], ["no_evidence"], ["operation"]], [[["Epidemiology of plague-23"], "operation"], ["no_evidence"], ["no_evidence"]], [[["Epidemiology of plague-1"]], [["Selfie-53"], "no_evidence"], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the most popular color in the world?\nFacts: Blue\nAnswer (True or False): True\nQuestion: What is the most popular color in the world?\nFacts: Blue\nAnswer (True or False): True\nQuestion: What is the", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Are selfies more dangerous than plague in modern times?\nFacts: There are an average of 7 human plague cases reported each year according to the CDC. Selfies have caused people to fall off of cliffs while trying to get the perfect picture. From October 2011 and November 2017, there were 259 selfie deaths in 137 incidents.\nAnswer (True or False):", "ct_subject": "There are an average of 7 human plague cases reported each year according to the CDC. Selfies have caused people to fall off of cliffs while trying to get the perfect picture. From October 2011 and November 2017, there were 259 selfie deaths in 137 incidents.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.91463214"], [" is", "1.0"], [" the", "1.0"], [" most", "0.99972767"], [" popular", "0.99895537"], [" color", "0.7180509"], [" in", "0.9999944"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" Blue", "0.8963544"], ["\n", "0.9946438"], ["Answer", "1.0"], [" (", "0.99639016"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" most", "1.0"], [" popular", "1.0"], [" color", "1.0"], [" in", "1.0"], [" the", "1.0"], [" world", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "0.99974555"], ["acts", "1.0"], [":", "1.0"], [" Blue", "0.99999917"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"]]}, "1069": {"qid": "2079d6c0fea33860fbc3", "term": "Achilles", "description": "Greek mythological hero", "question": "Does Thiago Moises May 13 2020 submission move hypothetically hurt Achilles?", "answer": true, "facts": ["Thiago Moises is a mixed martial arts fighter in the UFC.", "Thiago Moises beat Michael Johnson by a heel hook submission.", "Greek hero Achilles had one weakness, his heel."], "decomposition": [" What was Thiago Moises' winning move the match he played on May 13 2020?", "Which part of the opponent's body did #1 affect?", "Which part of Achilles' body is his weaknes?", "Is #2 the same as #3?"], "evidence": [[["no_evidence"], ["no_evidence"], [["Achilles-2"]], ["operation"]], [["no_evidence"], [["Heel-1"]], [["Achilles-2"]], ["operation"]], [["no_evidence"], [["Paul Sass-6"], "no_evidence"], [["Achilles-2"]], ["operation"]]], "first_generated_token": "True", "gpt2_main_completion": " True\nQuestion: What is the name of the first man to be born in the year 2000?\nFacts: The year 2000 is the year of the first human birth.\nAnswer (True or False): True\nQuestion: What is the name of the first man to be born in the year 2000?\nFacts: The", "ct_prompt": "Question: Are more people today related to Genghis Khan than Julius Caesar?\nFacts: Julius Caesar had three children. Genghis Khan had sixteen children. Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan.\nAnswer (True or False): True\nQuestion: Does Thiago Moises May 13 2020 submission move hypothetically hurt Achilles?\nFacts: Thiago Moises is a mixed martial arts fighter in the UFC. Thiago Moises beat Michael Johnson by a heel hook submission. Greek hero Achilles had one weakness, his heel.\nAnswer (True or False):", "ct_subject": "Thiago Moises is a mixed martial arts fighter in the UFC. Thiago Moises beat Michael Johnson by a heel hook submission. Greek hero Achilles had one weakness, his heel.", "good_gpt2_prompt": true, "token_probs": [[" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "0.90931875"], [" is", "1.0"], [" the", "1.0"], [" name", "0.99999475"], [" of", "1.0"], [" the", "1.0"], [" first", "0.999995"], [" man", "0.871309"], [" to", "1.0"], [" be", "0.22008273"], [" born", "0.9969747"], [" in", "0.9999999"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"], [" year", "0.9998702"], [" 2000", "1.0"], [" is", "0.9045149"], [" the", "0.99999833"], [" year", "1.0"], [" of", "0.7578294"], [" the", "1.0"], [" first", "0.57738256"], [" human", "1.0"], [" birth", "1.0"], [".", "1.0"], ["\n", "1.0"], ["Answer", "1.0"], [" (", "1.0"], ["True", "1.0"], [" or", "1.0"], [" False", "1.0"], ["):", "1.0"], [" True", "1.0"], ["\n", "1.0"], ["Question", "1.0"], [":", "1.0"], [" What", "1.0"], [" is", "1.0"], [" the", "1.0"], [" name", "1.0"], [" of", "1.0"], [" the", "1.0"], [" first", "1.0"], [" man", "1.0"], [" to", "1.0"], [" be", "1.0"], [" born", "1.0"], [" in", "1.0"], [" the", "1.0"], [" year", "1.0"], [" 2000", "1.0"], ["?", "1.0"], ["\n", "1.0"], ["F", "1.0"], ["acts", "1.0"], [":", "1.0"], [" The", "1.0"]]}}